<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[IGV-report工具]]></title>
    <url>%2F0003.%E5%BA%94%E7%94%A8-03.software%2FSoftware-%E5%8F%AF%E8%A7%86%E5%8C%96-igv%2F</url>
    <content type="text"><![CDATA[简介IGV 是基因组可视化方向一个被广为熟知的软件，相比JBrowse和GBrowse等需要进行服务器部署的高门槛服务，IGV本身提供了Windows桌面版，也满足了很多非生信背景的数据查看诉求。最近在进行流程可视化时，偶然发现IGV有开发了很多的扩展，比如igv-reports 和 igv-notebook 来适配更多的应用场景。 安装整个igv report模块是基于python(&gt;3.6)开发的，可以通过pip安装，在这里为了避免出现一些和现有环境的版本冲突，我们直接使用conda创建一个新的环境并进行安装，示例如下：1conda create -n igv_report igv-reports 生成报告报告的生成也比较简单，执行如下命令，然后打开 example.html 即可。1234567create_report Demo.vcf \--genome hg19 \--tracks Demo.vcf Demo.bam \--info-columns DUPLEX BAYES \ # vcf info 字段的key--sample-columns AF DP \ # vcf FORMAT 字段的Key--fasta $path/hg19.fa \--output example.html 由于结果表格中可以对vcf的INFO字段和FORMAT字段的信息进行提取，用于在最终表格结果中进行展示，因此我们可以通过对vcf进行个性化定制，实现最终表格信息的定制化展示。 查看结果实际效果，我们可以查看官方给的一些demo示例。Demo]]></content>
      <categories>
        <category>software</category>
        <category>Linux</category>
        <category>可视化</category>
      </categories>
      <tags>
        <tag>可视化</tag>
        <tag>IGV</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[天池项目：生命科学赛道——生物学年龄评价与年龄相关疾病风险预测]]></title>
    <url>%2F0003.%E5%BA%94%E7%94%A8-99.other%2F%E5%A4%A9%E6%B1%A0%E9%A1%B9%E7%9B%AE%E8%AE%B0%E5%BD%95%2F</url>
    <content type="text"><![CDATA[一些比赛项目的时间记录，本文档记录 阿里云天池 项目，生命科学赛道——生物学年龄评价与年龄相关疾病风险预测 的相关记录。 环境安装项目使用机器学习的方法，基于甲基化数据进行年龄预测。所以需要先进行相关环境的安装。避免历史环境的干扰，本项目使用conda直接创建了一个全新的基础环境。12# 基于python=3.11 创建全新的conda环境，同时安装 h5py和scikit-learn包conda create -n tianchi2023 python=3.11 h5py scikit-learn 后期为了更好的对数据进行可视化操作，和交互式的进行模型的训练，安装了jupyter1pip3 install jupyter 阿里云的jupyter存在端口权限，需要进行端口映射。1ssh liubo4@120.24.188.250 -L127.0.0.1:8889:127.0.0.1:8889 使用官方baseline| 迭代版本 | 使用方法 | 成绩 || 第 1 次 |天池提供的baseline （ElasticNet()） | (8.579980758705524, 9.329089, 7.830872352677162) || 第 2 次 | ElasticNet(0.1) | (5.659034220859258, 6.588447, 4.729621347754892) || 第 3 次 | ElasticNet(0.05) | (5.513496300186773, 6.4005265, 4.626466076783459) || 第 4 次 | ElasticNet(0.01) | (6.3587627338640615, 7.2758136, 5.441711888168797) || 第 5 次 | Lasso(alpha=0.05) | (5.638377567734381, 6.5338798, 4.742875378541272) | 训练集和仅使用健康人群| 迭代版本 | 训练参数 | 成绩 || 第 2 次 | ElasticNet(0.1) | (6.068306326553264, 6.2521014, 5.884511231750328) || 第 3 次 | ElasticNet(0.05) | (6.002052213263324, 6.1584253, 5.845679095410925) || 第 5 次 | Lasso(alpha=0.05) | (6.085405076269716, 6.349815, 5.820995260724246) | 训练集和仅使用健康人群，位点数扩展至20000| 迭代版本 | 训练参数 | 成绩 | 用时 || 第 1 次 | ElasticNet(0.1) | (4.474091597156978, 4.4790444, 4.469138756905508) | 33s || 第 2 次 | ElasticNet(0.05) | (4.429926482228302, 4.287037, 4.57281606870465) | || 第 3 次 | Lasso(alpha=0.05) | (4.400938993393588, 4.433994, 4.367884170411444) | 34s | 训练集和仅使用健康人群，位点数扩展至100000（ 20W已经无法再16GB下运行）| 迭代版本 | 训练参数 | 成绩 | 用时 || 第 1 次 | ElasticNet(0.1) | (3.8419492549466012, 3.6467113, 4.037187160405898) | 165s || 第 2 次 | ElasticNet(0.05) | (3.726990224723529, 3.4416575, 4.012322906264685) | 162s || 第 3 次 | Lasso(alpha=0.05) | (3.886337189746082, 3.7988966, 3.9737778283599625) | 165s | 训练集混用健康和非健康人群，位点数扩展至100000（ 20W已经无法再16GB下运行）| 迭代版本 | 训练参数 | 成绩 | 用时 || 第 1 次 | ElasticNet(0.1) | (3.4642932672596456, 3.707379, 3.221207431812382) | 215s || 第 2 次 | ElasticNet(0.05) | (3.262297007905778, 3.4586637, 3.0659303138004477) | 213s || 第 3 次 | Lasso(alpha=0.05) | (3.4673917658963997, 3.76499, 3.1697934404689465) | 209s | 训练集混用健康和非健康人群，服务器运行位点扩展至全部数据(训练集 70%)| 迭代版本 | 训练参数 | 成绩 | 用时 || 第 1 次 | ElasticNet(0.05) | (3.1139186428174943, 3.1427836, 3.0850536438198026) | 839s || 第 1 次 | ElasticNet(0.1) | (3.2288436084070304, 3.2900968, 3.1675904570179183) | 817s || 第 1 次 | Lasso (0.1) | (3.153165288490824, 3.384533, 2.9217976485148514) | 816s | 训练集用健康人群，服务器运行位点扩展至全部数据(训练集 70%)| 迭代版本 | 训练参数 | 成绩 | 用时 || 第 1 次 | ElasticNet(0.05) | (3.9090402715439425, 3.5266752, 4.291405318783686) | 639s | 训练集用全部，服务器运行位点扩展至全部数据,Nan使用位点的甲基化均值填充 (训练集 70%)| 迭代版本 | 训练参数 | 成绩 | 用时 || 第 1 次 | ElasticNet(0.05) | (2.976909323353664, 3.2731206, 2.6806980049989533) | 815s | 训练集用全部，服务器运行位点扩展至全部数据,Nan使用位点的甲基化均值填充 (训练集 80%)| 迭代版本 | 训练参数 | 成绩 | 用时 || 第 1 次 | ElasticNet(0.01) | (2.9917520688953543, 3.058976, 2.9245282028084088) | 964s | 20230828]]></content>
      <categories>
        <category>机器学习</category>
        <category>天池项目</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[人类参考基因组序列知多少]]></title>
    <url>%2F0001.%E7%9F%A5%E8%AF%86-04.Database%2FDatabase-reference%2F</url>
    <content type="text"><![CDATA[做人源相关的研究，不管是科研探索还是医学方向的临床应用。都离不开人类参考基因组的使用。人类参考基因组有很多版本，在2022年也新发布了T2T的全基因组完整图谱。之前我们可能了解到的只有Hg19和GRCh38比较多。知道Hg19是旧的，GRCh38是更新的版本。所以借此机会，捋一下人类基因组参考序列发布的时间节点，同时也梳理下各个不同版本参考基因组版本的特点。 第一阶段 - 国际人类基因组测序联盟提到人类基因组，离不开人类基因组计划，这项计划和曼哈顿原子弹计划 、阿波罗登月计划并称为20世纪人类三大科学计划。人类基因组计划 (HGP)于 1990年 启动，该计划的标志性目标是对人类基因组的 30 亿个碱基 进行测序。其他目标包括生成人类基因组的物理和遗传图谱，以及生物医学研究中使用的关键模型生物的绘图和测序。为了执行人类基因组测序的这部分工作，国际人类基因组测序联盟 (IHGSC) 成立，这是一个开放式合作组织，涉及六个国家的 20 个中心，包括美国、英国、法国、德国、日本和我国科学家。1998年，HGP正式实施了百慕大原则：自动发布&gt;1kb的序列组装体，最好在24小时内；立即出版完成的注释序列；并将整个序列在公共领域免费提供以供研究和开发。来尽可能规避专利和商业化问题限制基因组序列的应用。2000年3月14日，美国总统克林顿和英国首相托尼·布莱尔 联合宣布人类基因组序列“应该世界各地的科学家都可以免费获得”2001年，国际人类基因组测序联盟（the International Human Genome Sequencing Consortium）报告了人类基因组常染色质部分的草图序列。2001年2月，同时发表了两篇文章（由Venter 等人在 Science 和国际人类基因组测序联盟在 Nature 上发表），描述了人类基因组测序草案基因组序列。该序列包括 26,588 个有强有力确凿证据的蛋白质编码转录本，以及另外约 12,000 个具有小鼠同源物或其他薄弱证据的计算衍生基因。2004年，HGP的工作最终发表了高度准确（每 100,000 个碱基约 1 个错误）的人类基因组序列（Build 35/hg17）包含 28.5 亿个核苷酸，仅被 341 个缺口打断，覆盖了约 99% 的常染色质基因组。项目内容发表在Nature:Finishing the euchromatic sequence of the human genome，此前发布的版本还有NCBI33、NCBI34、NCBI35。2006年，国际人类基因组测序联盟提交了他们的最终版本的参考基因组序列（NCBI36/hg18）。 第二阶 - 段参考基因组联盟后续人类参考基因组由 GRC（Genome Reference Consortium）进行维护。GRC 也是有多个国家机构组成，成员包括Sanger研究所、 McDonnell基因研究所、EMBL-EBI:欧洲生物信息研究所、NCBI:国际生物信息中心等。 2009年,GRC 发布了GRCh37，也就是搭乘基因测序发展的快车，成为行业内使用最多的 Hg19。该参考基因组于2009年发布，后续共计进行了13个补丁版本的发布。其中最后一个修正版本 GRCh37.p13 发布于2013年6月。是人类基因组的第19版参考序列，完成了2.9GB长度的测序（总预计3.1GB）。包含了基因、非编码区域和其他功能元件的位置信息。 2013年底，GRC 发布了[GRCh38]，截止2022年2月，GRCh38累计发布了14个修正版本。 在人类基因组草图发布后的20多年里，随着技术的进步不断进行和升级，我们可以看到参考基因组序列也一直在进行着非常高频升级和迭代。早期的基因组版本，NCBI33、NCBI34、NCBI35、NCBI36已经退出了历史的舞台，甚至于很多人可能不曾听到。而Hg19、GRCh38为疾病研究、医学研究等研究的开展奠定了基础，极大的推进了人类科学研究进展。但放眼整个人类参考基因组仍然仅仅覆盖了基因组的常染色质部分，而重要的异染色质区域由于技术上的问题，一直存在缺失，整个基因组中仍有8%的区域未覆盖。|参考基因组|GRCh37|GRCh38.p14||-|-|-||Ref.Version|GRCh37|GRCh38||Genome size|3.1 Gb|3.1 Gb||Total ungapped length|2.9 Gb|2.9 Gb||Gaps between scaffolds|271|349||Number of chromosomes|24|24||Number of scaffolds|249|473||Scaffold N50|46.4 Mb|67.8 Mb||Scaffold L50|21|16||Number of contigs|350|999||Contig N50|38.5 Mb|57.9 Mb||Contig L50|24|18||GC percent|40.5|40.5||Assembly level|Chromosome|Chromosome| 第三阶段 - T2T联盟2022年， Telomere-to-Telomere (T2T)国际研究联盟基于细胞系（葡萄胎）构建了第一个完整的从头到尾无间隙人类参考基因组T2T-CHM13，填补了最后缺失的约2亿碱基对的测序，成果发布在Science上发表（“The complete sequence of a human genome”）。该研究针对CHM13进行了多种测序（包括30× PacBio HiFi 、120× ONT 、100× ILMN、70× Hi-C 、BioNano optical maps 、Strand-seq ）并结合新开发的组装算法，组装增加了五个完整的染色体臂，实现了除 Y 染色体之外的所有染色体的无间隙组装。T2T-CHM13 组装代表了比 GRCh38 更完整、更具代表性和更准确的参考。 2023年7月14日，浙江大学张国捷教授团队与深圳农业基因组研究所阮珏团队，以及华大生命科学研究院合作，通过开发算法，以个体的父本和母本数据作为参考系，完美地将不同染色体上的数据区分，将人的46条染色体的数据分别组装出来。然后对因为数据过于复杂而仍然存在的69个缺口进行了手工补洞。最终，获得了健康个体（汉族男性）的完整二倍体基因组。该完整基因组作为东亚人群遗传学研究的参考序列，可以提高东亚人群的序列比对并降低错误率，对单碱基多态性的检测准确率也会更高。该完整图谱的绘制，为我国开展精准医疗研究提供了更准确的参考基因组。 在人类全基因组图发布不久，上海交通大学毛亚飞课题组在Genome Biology发表题为Characterization of large-scale genomic differences in the first complete human genome的研究论文，比较分析了T2T-CHM13完整基因组与当前人类参考基因组模版（GRCh38）之间的大规模基因组差异。，系统地表征了两个人类基因组组装之间的大型结构变异（≥10 kbp），通过新开发的结构变异分析工具网站（SynPlotter）验证238个基因组差异区域并发现了67个新鉴定的结构差异区域。 参考资料相关文献 Yang, C., Zhou, Y., Song, Y. et al. The complete and fully-phased diploid genome of a male Han Chinese. Cell Res (2023). https://doi.org/10.1038/s41422-023-00849-5 Yang, X., Wang, X., Zou, Y. et al. Characterization of large-scale genomic differences in the first complete human genome. Genome Biol 24, 157 (2023). https://doi.org/10.1186/s13059-023-02995-w Sergey Nurk et al. ,The complete sequence of a human genome.Science376,44-53(2022).DOI:10.1126/science.abj6987 Bonfield, James K. and James E. Galagan. “Finishing the euchromatic sequence of the human genome.” Nature 431 (2004): 931-945. J. Craig Venter et al. ,The Sequence of the Human Genome.Science291,1304-1351(2001).DOI:10.1126/science.1058040 International Human Genome Sequencing Consortium. Initial sequencing and analysis of the human genome. Nature 409, 860–921 (2001). https://doi.org/10.1038/35057062 相关基因组获取 Genome assembly GRCh37 Genome assembly GRCh37.p13 Genome assembly GRCh38 Genome assembly GRCh38.p14 Genome assembly T2T-CHM13v2.0 T2T human genome GitHub仓库 Milestones in Genomic Sequencing]]></content>
      <categories>
        <category>知识沉淀</category>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>数据库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[人工智能、机器学习、深度学习和神经网络的关系解析]]></title>
    <url>%2F0002.%E5%BC%80%E5%8F%91-05.MachineLearn%2F%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD.%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0.%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%92%8C%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%85%B3%E7%B3%BB%E8%A7%A3%E6%9E%90%2F</url>
    <content type="text"></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[几个大模型网站]]></title>
    <url>%2F0003.%E5%BA%94%E7%94%A8-99.other%2F%E6%90%AD%E5%BB%BA%E8%87%AA%E5%B7%B1%E7%9A%84openAI%2F</url>
    <content type="text"><![CDATA[问答模型chatgptBGI文心一言Bard星火大模型通义千问 图像生成dreamstudiomidjourney PPT 生成gamma]]></content>
      <categories>
        <category>AI</category>
      </categories>
      <tags>
        <tag>chatgpt</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Recommendations for Next-Generation Sequencing Germline Variant Confirmation]]></title>
    <url>%2F0001.%E7%9F%A5%E8%AF%86-03.standard%2FRecommendations_for_Next-Generation_Sequencing_Germline_Variant_Confirmation%2F</url>
    <content type="text"><![CDATA[背景新一代测序 (NGS) 的引入是临床实验室遗传学史上最重要的技术进步之一。自 2000 年代初问世以来，NGS 已经从一种创新的研究工具转变为一种通用且有效的临床分子诊断方法。 , 与传统的 DNA 基因分型和测序方法相比，NGS 已被证明更加灵活和可扩展，并且既省时又经济。然而，NGS 有局限性。在其他挑战中，NGS 变异检出的准确性可能因方法、变异类型和大小、基因组区域以及样本类型和质量而有很大差异。对提供假阳性 (FP) NGS 结果的适当担忧导致早期依赖验证性检测来减少这种可能性，并且验证性测试仍然被许多当前的生育相关的临床实践指南所推荐，但是细节描述确很少被明确的提及。在本指南中，为临床时间提供了更具体的建议，这些建议与现有指南相一致，提供了额外的细节，因此可能有助于促进实验室之间的标准化、透明度和质量改进。 迄今为止，一些专业组织已经通过立场文件或推荐的最佳实践解决了种系检测中的交叉确认问题。在一项早期工作中，美国疾病预防控制中心召集了 Nex-StoCT 工作组，该工作组在 2012 年发布了一套用于基于 NGS 的临床测试质量保证的原则和指南。他们建议“对 NGS 检测到的所有临床可操作变异进行确认测试。”美国医学遗传学会 (ACMG) 发布的 2013 年 NGS 标准中包含类似的建议，即“所有以疾病为重点的和/或诊断测试都包括使用配套技术确认最终结果”。该指南进一步指出，实验室“必须具有丰富的 NGS 技术经验……才能决定使用正交技术进行结果确认。”该指南没有就什么构成广泛经验或需要哪些数据来验证不包括确认所有报告变异的实验室工作流程提供进一步指导。 2015 年，美国病理学家学会发布了 NGS 标准，指出每个执行 NGS 的实验室“必须制定一项政策，明确记录确认测试的适应症和/或记录他们的化验验证如何确定不需要此类测试。”美国病理学家学会在其 2022 年版分子病理学清单中指出，“实验室按照其政策中的规定对 NGS 结果进行验证性测试，并记录 NGS 验证性结果的相关性”，并指出实验室必须在验证过程中确定是否和当指示进行确认测试时。EuroGentest 和欧洲人类遗传学学会在 2016 年提出了 NGS 指南，包括在补充材料中，“目前，建议确认所有报告的变异，以确保没有发生样本交换和/或验证信息学管道”这些出版物中的每一个都承认，他们关于 NGS 变异交叉确认的立场受到最近采用的 NGS 技术和实验室缺乏其性能的可靠经验的影响。这些出版物预计，随着 NGS 技术和生物信息学的发展，确认的必要性将会降低。ACMG 在 2021 年更新了其 NGS 标准，建议“实验室应根据从大型和多样化数据集中提取的实验室特定质量指标识别变异的工作流程，为每个变异类别建立并提供确认测试策略以及读取比对的目视检查。在没有经过验证的方法的情况下，实验室应继续进行交叉确认。” 同样，2020 年 ACMG CFTR 测试技术标准指出，“实验室应确定……报告某些变异是否需要交叉确认，以及将使用什么标准来做出该决定。建议基于 NGS 方法的 CFTR 报告明确说明报告的变异是否已使用替代方法得到确认；如果不采用交叉确认，建议在报告的方法部分简要说明用于做出该决定的标准。 技术背景NGS与 Sanger 测序相比，NGS 产生的 reads 相对较短，并且这些 reads 中的每一个都可能具有相对较高的原始数据错误率。 NGS 通过生成覆盖每个目标位置的大量读数来弥补这些挑战。由于产生的数据量大且数据复杂，NGS 依赖于精心设计的生物信息学软件、数据库和算法来识别每个 DNA 标本中的遗传变异。正如其他 AMP/美国病理学家学院指南中所述，这些生物信息学系统还计算各种质量指标，这些指标可以帮助表明每个变异调用的技术可靠性。使用这些指标，很可能是 FP 的调用通常会被过滤掉。但是，过滤后的剩余调用仍可能包含 TP 和 FP 结果的混合。 NGS 错误率的临床考虑FP 错误会对医疗保健和患者结果产生重大影响。在某些临床情况下，阳性（无论是 TP 还是 FP）基因检测报告直接影响有关预防性手术、终止妊娠、医疗干预或治疗选择的决定。 FP 还可能导致误诊和过早结束对患者疾病真正原因的搜索。而是随着NGS检测的快速发展，应用范围越来越光放，即时错误率已大大降低，但是具有临床意义的FP的可能性也会增加。例如，考虑每测试 1,000,000 个碱基对 (1 Mb) 仅产生一个 FP 的 NGS 测试。这种低错误率与最近的一些（但不是全部）NGS 性能研究一致。在蛋白质编码区，人类大约每 2 kb 就有一个种系 DNA 变异（这一比率在非编码区更高）。因此，每 Mb 产生 1 FP 的测试将具有 99.8% 的分析（或技术）阳性预测值 (PPV) 和基于每个 bp 的分析 FP 率为 0.0001%。这种性能水平可能看起来很高，但可能还不够。例如，BRCA1 和 BRCA2 总计大约 10 kb（10,000 bp）的蛋白质编码序列，因此以这个 FP 率测试这两个基因的每 100 名患者预计会有一个分析 FP。如果将测试扩展到包括跨越 200 kb 的基因组，那么预计每五名测试患者就有一个 FP。在跨越 30 Mb 的外显子组序列中，每位患者的结果中预计有 30 个 FP。 要了解 FP 的影响，应该考虑临床 PPV，而不仅仅是分析 PPV。临床 PPV 考虑了变异解释和接受测试的患者群体。继续上面使用的 BRCA1/2 示例，假设每 100 名患者发生一个分析性 FP 错误，并且这些 FP 中有一半可能看起来是致病性的，导致每 200 名测试的患者有一个潜在的重要 FP。一般而言，当将 BRCA1/2 检测应用于具有强烈提示遗传性乳腺癌或卵巢癌个人史或家族史的患者时，大多数（并非所有）种族群体中约有 10% 的患者被发现为真正阳性。 , 因此，此示例测试的临床 PPV 在这些患者中大约为 95%——21 个阳性结果中有 1 个是假的。然而，在筛查环境中，如果 0.5% 的患者真正呈阳性，则临床 PPV 将仅为 50%。换句话说，这个示例筛选测试的阳性结果可能是假的，也可能是真的。在筛选环境中使用的基因组将具有更低的临床 PPV（TP/(TP+FP))。 总之，即使分析性 FP 的比率很低，NGS 测试的基因和患者数量越来越多，也可能导致大量 FP 临床报告。 NGS 错误的技术考虑所有实验室方法都可能出错。已建立的技术，如 Sanger 测序，通常被称为金标准，只有在认识到这些方法的局限性时才适用。由于 NGS 更新且发展迅速，其局限性可能不太为人所知，尽管这种情况也在发生变化。对于某些类别的变异，某些 NGS 方法现在可能与 Sanger 测序一样准确， , , 这提出了一个有效的问题：使用 Sanger 测序确认 NGS 结果会降低整体准确性吗？如本文所述，如果使用得当，答案是否定的，因为 Sanger 和 NGS 是正交技术，这意味着它们产生的错误在大多数情况下是不相关的（参见建议 4）。然而，由于这两种方法都不是完美的，因此当两种方法不一致时盲目地假设一种方法（在本例中为 Sanger）是正确的可能会增加错误率，正如已经证明的那样。相反，如果差异得到适当调查和解决，那么组合过程（NGS + Sanger 确认）的特异性将优于单独使用任何一种方法。实验室或许能够建立严格的标准，将高置信度的 TP 变异调用与那些不太自信且可能是也可能不是 TP 的变异调用区分开来。重要的是要认识到这些严格的标准与质量过滤器是分开的，质量过滤器用于删除很可能是 FP 的变异调用。各种研究表明，简单的技术标准（例如 NGS 读取深度）并不能单独充分区分高置信度 TP 和候选 TP 类别。理想情况下，单个稳健的变异调用质量分数可以做到这一点，尽管最常用的变异调用者产生的分数（截至本出版物发布时）同样被证明是不够的。相反，标准的组合似乎更有效。 对于包括 NGS 在内的任何实验室技术，FP 错误都可以分为两种类型：系统错误或随机错误。随机错误可以在任何时间或任何地点发生，而系统错误优先发生在特定情况下（例如，在重复序列中）。在现代 NGS 中，系统错误占所有 NGS FP 的很大一部分，这既带来了机遇，也带来了复杂性。机会：由于系统误差背后的因素可以通过适当的研究进行量化，因此可以识别最有可能成为系统性 FP 的患者的变异调用，并确保这些变异得到确认。复杂的是，根据定义，系统错误会反复出现，这意味着使用相同的技术在多个样本中观察相同或相似的变异几乎无法确信变异调用实际上是真实的。要知道这样的变异调用是否是 TP，需要一种正交方法。 本指南的局限性建议旨在应用于使用 NGS 检测种系（宪法）DNA 变异的临床测试。我们简要描述了可能适用于通过种系检测检测到的某些其他变异类型的特殊注意事项（例如，种系镶嵌和线粒体变异，需要单倍型分析才能正确解释的变异）。其他临床 DNA 测试不在范围内（包括肿瘤测序和液体活检测试，两者都旨在检测体细胞突变，以及无创产前筛查，旨在检测母体血液中的胎儿变异）。建议旨在适用于撰写本文时临床实验室常用的短读长 NGS 平台和化学品 [例如，Illumina 和 Ion Torrent ]。我们没有详细考虑临床实验室中不太常见的 NGS 平台、不再销售的平台或任何较新的单分子长读长平台。 建议 Index Recommendation 1 Clinical laboratories offering germline testing using NGS should establish a written policy regarding orthogonal confirmation of NGS results./使用 NGS 提供种系检测的临床实验室应制定有关 NGS 结果交叉确认的书面政策。 2 Laboratories’ orthogonal confirmation policy should be overseen and approved by a qualified and appropriately certified medical professional with training and experience in NGS./实验室的交叉确认政策应由具有 NGS 培训和经验的合格且经过适当认证的医疗专业人员监督和批准。 3 Laboratories’ confirmatory methods, platforms, and associated bioinformatics should be validated and maintained under appropriate regulatory oversight, as for other aspects of the test./实验室的确认方法、平台和相关的生物信息学应在适当的监管监督下得到验证和维护，就像测试的其他方面一样。 4 Laboratories’ confirmatory methods should be orthogonal. Discrepant results between NGS and a confirmatory assay should be investigated and resolved, rather than accepting any one method to be always correct./实验室的确认方法应该是正交的。应调查和解决 NGS 与验证性测定之间的差异结果，而不是接受任何一种方法始终是正确的。 5 Laboratories should perform confirmatory testing for reported germline variants with significant clinical implications, except for variant calls meeting technical criteria rigorously demonstrated to ensure high positive predictive value from NGS alone./实验室应对已报告的具有重大临床意义的种系变异进行验证性测试，但符合严格证明的技术标准的变异检出除外，以确保单独从 NGS 获得高阳性预测值。 6 Laboratories should clearly articulate their specific policies, criteria, and methods regarding orthogonal confirmation in written materials readily available on request./实验室应在可应要求提供的书面材料中清楚地阐明其关于交叉确认的具体政策、标准和方法。 7 Laboratories’ clinical test reports should summarize orthogonal confirmation policy in every report, and when exceptions to the policy are made, these should be clearly indicated./实验室的临床试验报告应在每份报告中总结交叉确认政策，当政策有例外时，应明确指出。 8 Special considerations apply to certain NGS-based test types and findings./特殊注意事项适用于某些基于 NGS 的测试类型和结果。 因全文较长，在此仅针对生信和方法学相关部分进行展开，其他部分（更多的是关于整体执行流程的规范和要求等，请参考原文） Recommendation 1: Clinical Laboratories Offering Germline Testing Using NGS Should Establish a Written Policy Regarding Orthogonal Confirmation of NGS Results实验室应该形成规范，明确什么情况下需要对结果进行确认。制定策略时，考虑的变量包括测试内容（例如，单基因与外显子组测序）、NGS 平台、变异类型、变异分类、质量指标、临床影响和其他因素。 Recommendation 2: Laboratories’ Orthogonal Confirmation Policy Should be Overseen and Approved by a Qualified and Appropriately Certified Medical Professional with Training and Experience in Next-Generation Sequencing制定验证策略的人，应该是结合遗传学相关背景，由经验丰富的专业人员制定和实施。 Recommendation 3: Laboratories’ Confirmatory Methods, Platforms, and Associated Bioinformatics Should be Validated and Maintained Under Appropriate Regulatory Oversight as for Other Aspects of the Test由于实际限制，特别是需要确认新的变异，分析验证可以接受特定于平台或方法的。例如，可以接受基于方法的基于 Sanger 扩增子测序的验证，而不是验证单个引物对——这是一项不切实际且繁重的任务。这种基于方法的方法应包括对整个确认工作流程的端到端验证，包括 PCR 引物设计、湿实验室 PC​​R 扩增测定以及用于读取 Sanger 测序数据的手册或软件程序。如果使用多种 PCR 扩增方案（例如，富含 GC 的 PCR 和长程 PCR），则必须分别验证每种 PCR 方法。由于此方法未验证单个引物对，因此在 NGS 结果与确认结果不同的情况下，对任何差异进行额外调查至关重要。 Recommendation 4: Laboratories’ Confirmatory Methods Should be Orthogonal: Discrepant Results between NGS and a Confirmatory Assay Should be Investigated and Resolved, Rather Than Accepting any One Method to be Always Correct确认测试的目的是发现 FP 并防止它们被报告。重要的是要记住，许多 NGS FP 错误是系统性的 , （即，可能会重复）并且没有任何检测是 100% 准确的（请参阅技术背景）。因此，应选择验证性检测，以使验证性检测的错误概况与主要 NGS 检测的错误概况尽可能少相关。以不同方式操作且具有最小重叠误差分布的两种测定的概念通常称为正交性。选择验证性检测需要实验室主任仔细判断。尽管可能无法实现完全正交，但可以避免明显的问题。例如，简单地重复主要的 NGS 测定不会提供任何程度的正交性，并且可能会导致报告系统性 FP。我们建议确认分析既不使用相同的核心测序技术，也不使用相同的文库制备方法。强烈反对使用不同的生物信息学管道重新分析相同的原始数据作为确认方法。扩增子的 Sanger 测序在当前实践中经常用于对由 NGS 识别的序列变异进行交叉确认，NGS 通常使用基于杂交的靶向。同样，微阵列、定量 PCR 或多重连接依赖性探针扩增通常用于确认 NGS 检测到的 CNV。鉴于方法上的差异，人们期望这些策略具有很大程度的正交性。因为没有一种检测是 100% 准确的，所以当主要检测和验证检测之间的结果发生冲突时，不应假定这两种检测都是正确的。相反，在从报告中包含或排除变异之前，应尽可能合理地调查和解决差异的原因（图 3）。调查应包括检查主要和确认化验的基础数据，还可能包括重复测试或第三种（已验证的）测试方法。实验室应将验证性检测的敏感性限制视为导致差异的潜在原因，并根据需要对特定患者或变异体采用另一种方法。例如，在感兴趣区域缺乏足够探针覆盖的拷贝数微阵列可能会产生 NGS 检测到的 TP CNV 的假阴性确认结果。同样，由于等位基因丢失，Sanger 测序可能会提供假阴性确认结果。在初始 NGS 测试或确认过程（即样本交换）中测试不正确的样本是造成差异的另一个潜在原因，应在调查期间予以排除。当无法在主要和确认检测之间实现明确的解决方案时，报告或省略相关变异的决定应由经验丰富的临床分子专业人员做出。如果报告了具有不一致的交叉方法结果的变异，则应在报告中描述验证性分析的结果以及对变异是 TP 的可能性的解释。 Recommendation 5: Laboratories Should Perform Confirmatory Testing for Reported Germline Variants with Significant Clinical Implications, Except for Variant Calls Meeting Technical Criteria Rigorously Demonstrated to Ensure High Positive Predictive Value from NGS Alone实验室关于哪些变异需要交叉确认的最终决定将取决于两种类型的标准，这两种标准都应在实验室的确认政策中明确描述，并分别应用于可能包含在测试报告中的每个变异调用。 技术标准：根据可用数据，此变异调用成为分析 TP 与 FP 的可能性有多大？ 医学标准：这种变异对患者护理产生重大临床影响的可能性有多大？ 技术标准可以制定严格的标准来识别极不可能是 FP 的变异调用（图 1）。尽管读取深度、变异类型、变异等位基因分数、基因组背景和许多当前的质量评分计算已被证明不足以单独用于此目的，但这些标准的组合已被证明是有效的（见前述技术考虑）。这些组合可能包括以下元素。 质量指标： 变异检出工具通常会为每个变异检出产生许多质量指标。例如，常用的 GATK 变异调用程序会生成指南推荐的多个分数，并且每个分数都有助于识别最高置信度的变异调用。汇总分数可能无法像单独考虑多个基础质量指标那样有效。 变异类型： SNV 在人类 DNA 中比插入缺失更为普遍。然而，Indel 变异调用往往比 SNV 调用具有更高的错误率，并且更有可能被解释为许多基因的致病性，而功能丧失是一种致病机制。对于某些变异类型（例如，SNV），通常更容易积累具有正交数据的大型数据集，以确定哪些调用可能需要或不需要确认。对于其他变异类型（插入缺失和 CNV），实验室可能没有足够大的数据集来以统计合理的方式确定标准。由于这些原因，变异类型可能是确定哪些变异需要确认的一个重要标准。 SNV 和插入缺失的最佳质量阈值可能不同。 复杂区域： 基因组背景可能是识别容易出现 FP 且可能未被通用质量指标充分标记的区域的重要标准。这些区域通常包括低复杂性重复序列，例如均聚物和短串联重复序列、移动元件以及具有高度相似的旁系同源基因或假基因的基因。在验证过程中，实验室可能会识别基因组中由 NGS 识别的变异不可靠的区域，并且应始终进行交叉确认。 变异频率： 变异频率是具有复杂含义的重要质量指标。等位基因分数的变异调用（种系 DNA 中远离 50:50 变异等位基因分数的杂合子）通常是 FP，尽管它们可能是受技术人工制品（例如，参考偏差或错误映射）影响的 TP。 如果没有额外的充足验证，不建议使用以下方法 在不同样本中重复确认相同突变。一些指南建议，在验证研究或常规实践中，实验室可能会在特定变异被确认为 TP 一定次数后停止确认。不幸的是，系统错误在 NGS 中很普遍，不同样本中相同变异的调用可能具有截然不同的质量水平。研究表明，对于相同变异的后续观察结果的准确性，重复确认的预测能力有限。 人工审查 NGS 数据作为确认的替代方法。人工审查 NGS 数据可以识别变异检出软件出现 FP 错误的某些情况。实验室政策可能允许在不使用二次检测的情况下去除此类变异。然而，反之亦然：在人工审查中没有发现问题并不一定意味着变异是没问题的。可能导致 FP 的生化和绘图问题在人工审查中并不总是很明显。 不同实验室使用的 NGS 过程可能存在许多实质性的和细微的差异，并且不同测试检查的目标之间通常存在重要差异。每个实验室都应使用自己的数据集，并使用这些数据制定自己的标准，而不是试图使用其他实验室的数据或标准。优化算法（可能是启发式的、统计的或基于机器学习的）已被证明可用于确定哪些质量指标和阈值的组合最能提供信息。这些方法可能需要大量的输入数据集。具有良好特征的样本，例如瓶中基因组联盟目前发布的七个样本（https://www.nist.gov/programs-projects/genome-bottle。但是这些数据也会存在一个问题，1.没有覆盖全部的基因组区域。2.该项目数据主要是蛋白编码区的SNV变异。不建议将确定的标准应用于不同类型的数据集合。一旦建立了技术标准应该进行详细的验证，验证方案可以参考AMP提供的详细建议（https://www.amp.org/resources/validation-resources） 医学标准当技术评估表明可报告变异存在成为分析性 FP 的风险时，如果此类 FP 可能对当前或未来医疗状况的诊断、治疗或管理质量产生不利影响，则建议进行交叉确认病人。对于与患者检测指征相关的高或中度外显率基因的致病性和可能致病性发现，通常就是这种情况。相比之下，良性变异通常没有临床影响，通常不会被报道。因此通常不需要确认良性变异。 不确定的变异解释：尽管意义不明的变异 (VUS) 通常不会影响临床决策，但一些 VUS 可能在以后被重新分类为致病性或可能致病性。因此，如果变异由于其初始 VUS 分类而未得到确认，则分析 FP 可能会出现问题。如果实验室政策是部分或所有 VUS 因其分类而未得到确认，则应向临床医生明确说明该政策及其含义。如果 VUS 后来被重新分类为致病性或可能致病性，则实验室应将其当前的标准政策应用于变异，以确定在发布修订报告之前是否需要进行确认测试。 主要发现与次要发现：虽然主要发现（即与患者的检测指征直接相关的那些）通常具有最大的临床影响，但次要（有时称为偶然）发现也可能具有重大影响。例如，考虑在接受心血管疾病诊断的患者的外显子组序列中继发发现致病性 BRCA1。 ACMG , 推荐的基因变异或药物遗传学相关基因变异是常见的二次发现类型，可能会在临床种系检测中报告，并且值得交叉确认。 携带者发现：在隐性基因中发现的单等位基因致病变异可能对生殖医学产生重大临床影响。此外，在一些发现这种携带者的患者中，稍后可能会使用不同的测试方法发现同一基因中的第二个致病变异，从而使最初的发现与他们的医疗保健直接相关 外显率：尽管低外显率变异可能带来相对较低的疾病风险，但高外显率和低外显率的致病变异都可以表明在当前的医疗实践指南下临床护理发生了重大变化。 在决定哪些变异需要基于医学理由进行确认时，实验室应仔细权衡风险。实验室政策应该让临床医生清楚了解，以便他们可以自信地知道哪些变异在任何临床报告中需要确认，哪些不需要 Recommendation 6: Laboratories Should Clearly Articulate Their Specific Policies, Criteria, and Methods Regarding Orthogonal Confirmation in Written Materials Readily Available on Request实验室应提供有关交叉确认政策的书面材料和总结确认政策验证结果的书面文件，包括实验室是否已将某些变异排除在技术确认之外。书面文件应该是最新的，包括更新和修正，并包括实验室的标准标准，报告的变异是否接受或不接受确认测试。请注意，一般性陈述（例如，符合高质量标准的变异）并不能单独满足此建议。书面文件还应包括针对初始 NGS 数据和确认测试数据不一致的变异检出所采取的措施。此外，文件应包括为标准政策的例外情况采取的行动，包括由于技术原因无法进行确认的情况，例如样本可用性不足或确认化验未能产生可靠数据。 Recommendation 7: Laboratories’ Clinical Test Reports Should Summarize Orthogonal Confirmation Policy in Every Report, and when Exceptions to the Policy Are Made, These Should be Clearly Indicated实验室应在报告中包含其标准交叉确认政策的摘要。这可以是一个简明的总结，也可以是对实验室标准的参考，以确定哪些报告的变异需要进行确认测试。如果实验室没有确认政策或不定期进行交叉确认，则应说明这一点。可能存在由于技术原因无法进行确认的情况，例如样本可用性不足或确认化验未能产生可靠数据。临床报告中应明确记录实验室标准变异确认标准的所有例外情况。 示例报告语言：此变异未通过既定的 NGS 置信度阈值，因此需要根据实验室的标准确认政策通过正交方法进行确认。反复的 Sanger 测序确认尝试未能产生可解释的数据。我们建议在具有经过验证的基因特异性方法的临床实验室中确认此变异。 Recommendation 8: Special Considerations Apply to Certain NGS-Based Test Types and FindingsSample Identity Confirmation 样本身份确认正交检测可用于双重目的，即确认变体是分析性 TP 并确保它存在于正确患者的 DNA 样本中。因此，确认化验可以检测 NGS 期间的样本或数据混淆，特别是当确认化验是对患者初始标本的单独等分试样进行时。如果实验室不对某些或所有具有临床意义的变异进行确认，则强烈建议使用替代的阳性样本追踪方法。示例包括单核苷酸多态性检测，可以与 NGS 结果进行比较以确认样本身份，或使用掺入寡核苷酸。这些机制还有一个额外的优势，即可以跟踪具有阴性或阳性发现的样本。 Location Resolution 位置分辨率短读长 NGS 平台不能总是准确地确定在具有同源假基因或基因家族成员的基因中观察到的变异的具体位置。例如，映射到 PMS2 基因的变体实际上可能源自 PMS2CL 假基因，该假基因在该基因的特定区域与 PMS2 的序列有 99% 相同。使用例如长程 PCR 或长读长测序的确认分析既可以确定潜在致病变异位于哪个位点，也可以确定它是否实际上是一个分析的TP。 Mosaic Variants在 NGS 结果中以低变异等位基因分数观察到的变异提出了具体的考虑。由于多种原因，无论是生物学原因还是技术原因，变体都可能出现在低等位基因部分，包括以下原因: 嵌合体 不确定的克隆性造血，可在血液、口腔和唾液标本的淋巴细胞衍生 DNA 中观察到。 来自同源区域（例如，假基因）的读数错配。 参考偏差：NGS 读数包含与参考基因组中存在的不同等位基因的事实可能不容易映射到正确的基因组位置。 系统错误如果实验室在临床上报告了低等位基因分数的变异，那么尝试解决等位基因不平衡原因的适当程序就很重要。报告语言也很重要，特别是当对低等位基因分数的根本原因存在歧义时。确认技术，尤其是 Sanger 测序，可能降低了对实际镶嵌变异的敏感性，因此可以看到相对常见的情况，即通过 NGS 观察到的 TP 在确认分析中是假阴性。 Mitochondrial Variants 线粒体变异由于异质性，线粒体基因组中的变异可能存在于低等位基因部分。除了上述与镶嵌变异相关的挑战外，线粒体变异可以是多等位基因，在 DNA 混合物中存在超过两个等位基因，这种情况类似于肿瘤测序中因异质性而遇到的情况。实验室确认政策应详细说明如何处理和报告此类情况。 Haplotyping/Phasing 单体型/定相某些变异的临床解释可能需要单倍型分析（即确定同一基因或区域中多个变异的顺式或反式关系）。该步骤在隐性基因（确定何时观察到双等位基因、复合杂合变异）和需要多个顺式变异来识别特定等位基因（例如，HLA 或细胞色素基因中的星号等位基因）的基因中可能很重要。短读长 NGS 平台通常无法确定大于几百个碱基对的基因组区域的相位。可以通过以下确定更大距离的单倍型和相位。 Mendelian analysis (when family members are available for testing). Imputation or other statistical inference methods. Certain confirmatory assays, such as long-range PCR or long-read sequencing. Familial Sample Testing 家族样本检测通常在基因检测中，家族样本与先证者的样本一起进行检测，以帮助进行变异分类（即确定共分离和/或阶段）。如果家庭成员使用 NGS 进行测试并收到个性化报告，则应按照与先证者相同的标准对向这些人报告的变异进行交叉确认。因为许多 NGS FPs 是系统的，而不是随机的，所以强烈建议不要简单地在多个个体中观察相同的变异，无论是否相关，作为一种确认技术。 Tumor Testing仅肿瘤测序可以检测种系变异，尽管目前的肿瘤测试对是否以及如何报告此类变异存在差异。关于这个主题的指南不断发展。 , 在某些情况下，患者被转介到不同的实验室进行 NGS 种系后续检测，在这种情况下，种系实验室可能会将先前的肿瘤检测视为对未发现的任何种系变异的确认。这是否合适取决于种系实验室主任对这两个测试是否正交的判断（见建议 4）。例如，如果两个测试使用相同的测序平台和捕获方法，则可能会重复出现系统错误。肿瘤测试和种系测试使用不同标本这一事实并不能单独使测试正交。]]></content>
      <categories>
        <category>NGS</category>
        <category>肿瘤检测</category>
        <category>标准化</category>
      </categories>
      <tags>
        <tag>AMP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[notepad++ 配置]]></title>
    <url>%2F0003.%E5%BA%94%E7%94%A8-04.windows%2FNotePad%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[添加Markdownnotepad++是一个十分强大的编辑器，除了可以用来制作一般的纯文字说明文件，也十分适合编写计算机程序代码。Notepad++ 不仅有语法高亮度显示，也有语法折叠功能，并且支持宏以及扩充基本功能的外挂模组。当时对markdown支持不够。这里通过插件与自定义语法让notepad++变成一个markdown书写工具。 下载所需文件 由于GitHub不时抽风，我这里将需要用到的工具打包上传到网盘。下载链接提取码：4hvm 导入语法规则打开Notepad++，点击“语言” 选择“自定义语言格式” 点击“导入”，选择下载并解压后文件夹中的“userDefineLang_markdown.xml”文件。导入完成后重启notepad++，点击“语言”，选择“markdown”即可。 安装实时预览插件打开notepad++，点击“设置”，选择“导入-导入插件”，将之前下载的文件中的“NppMarkdown.dll”导入即可。 打开插件打开notepad++，点击“插件”，选择“NppMarkdown” 在右侧出现的“preview markdown”窗口底部，勾选“live preview” 同时点击“preview”即可。]]></content>
      <categories>
        <category>环境配置</category>
      </categories>
      <tags>
        <tag>Windows</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Git-3.git仓库清理的方式]]></title>
    <url>%2F0003.%E5%BA%94%E7%94%A8-99.other%2FGit-3.git%E4%BB%93%E5%BA%93%E6%B8%85%E7%90%86%E7%9A%84%E6%96%B9%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[相关材料官方文档官方文档-中文 背景在具体的业务时间过程中，会不断的产生需求，而在开发测试过程中，也会出现一次开发过程中，提交了多次commit的情况。因此有时候为了保证整体git log的记录完整性和仓库]]></content>
      <categories>
        <category>编程拾慧</category>
        <category>Git</category>
      </categories>
      <tags>
        <tag>培训</tag>
        <tag>Git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[浏览器-Firefox配置]]></title>
    <url>%2F0003.%E5%BA%94%E7%94%A8-04.windows%2F%E6%B5%8F%E8%A7%88%E5%99%A8-Firefox%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[Firefox Multi-Account Containers针对Firefox浏览器的多账号管理插件，主要针对一些网站，可能会存在多个账号（因公注册/因私注册）等，或为了进行权限分割，有时候需要注册多个账号（一个运维管理/个人使用）等。通过该插件可以更好地实现多账号的分离和管理。]]></content>
      <categories>
        <category>环境配置</category>
      </categories>
      <tags>
        <tag>Windows</tag>
        <tag>Firefox</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Recommendations for the Use of in Silico Approaches for Next-Generation Sequencing Bioinformatic Pipeline Validation]]></title>
    <url>%2F0001.%E7%9F%A5%E8%AF%86-03.standard%2FRecommendations-Use_of_in_Silico_Approaches_for_Next-Generation_Sequencing_Bioinformatic_Pipeline_Validation%2F</url>
    <content type="text"><![CDATA[背景随着NGS在分子诊断中变异（体系和胚系）检测的应用普及。临床NGS检测产品的检测范围越来越大，带来的一个优势是可以识别更多基因中的变异；但是同时也使性能验证面临巨大的挑战。临床方法学验证过程中很难获得真实样本，能同时具有大量关注且能够被检测到频率也合适的变异。所以大多数实验室都是通过对具有已知变异的标准品或细胞系进行测序分析，并依靠测序指标（即具有足够覆盖率的目标部分）来推断其他区域的性能。 在这个大背景下，很多实验室会开始考虑使用模拟数据进行性能评估，使用数据模拟的方法在临床实验室中具有非常高的实用性，可作为临床检测验证的辅助工具。数据模拟可以采用多种形式，包括纯模拟数据或将变异插入现有测序数据中。通过模拟数据可以便捷的构建出真是样本中难以获取的变异数据。这些数据使实验室能够更准确地测试生物信息学流程的性能，而无需对其他病例进行测序。例如，可以模拟低频变异，以测试流程的分析灵敏度，或模拟一系列不同长度的插入/缺失以确定软件的性能边界。目前也有一些已经被广泛应用的方案，例如样本层面通过合成基因组混入目标变异，或生成目标变异的计算机模拟数据混入测序数据（Fastq/Bam）文件中。从而进行更全面的方法学验证和生信流程验证 在本文中，主要介绍不同类型的数据模拟类型及其优势和局限性、模拟实现方法，以及如何在临床分子诊断实验室中更好的使用模拟数据。 数据模拟类型模拟数据可以广义地定义为人工操作或生成的任何数据。大体上可以主要划分为下述几个大类。 A. 纯模拟数据（Purely Simulated Data ）通过模拟参考序列的测序reads（纯模拟数据）从头生成Fastq数据。这类数据通常是用于生信流程的原始数据，可以较便捷的大量生成几乎任何类型的变异。这些数据对于样本稀缺且方法不成熟的复杂变异类型特别有用。但模拟reads生成过程通常不能完整的模拟测序技术的噪音或偏差，尤其是在变异难以检测的重复区域（例如，在均聚物处）。也很难对临床测试中最常用的靶向测序方法的偏差和覆盖分布进行模拟，同样也很难对福尔马林固定、石蜡包埋过程引入的噪音进行模拟。 B. 混合样本（Mixing Sample Data）来自物理样本的两个数据文件以不同的比例混合，模拟具有不同频率的变异，和真实数据相比，可以更好的帮助评估生信流程在不同变异频率的性能表现。该策略可以混合来自两个充分表征的正常样本的 BAM 或 FASTQ 文件，通过不同的组成可以混合出不同频率范围的变异，能够测试大量不同变异的检测限。这个方法不能模拟肿瘤中出现的许多体细胞变异的复杂性（例如，拷贝数变异和大的结构变异）。正常基因组通常不存在临床感兴趣的变异，因此实验室也可以将包含感兴趣变异的肿瘤基因组与相应的正常样本混合，以测试它们在不同等位基因部分检测这些变异的能力。正常细胞大部分是已知的胚系变异，而且两个样本可能会有一些变异位点的重叠也会对下游变异检出带来一些困扰。正常细胞系通常会在一部分细胞中出现变异，如果这些变异在参考样本中没有得到很好的表征，则这些变异可能看起来是假阳性体细胞变异。 C. Fastq抽样（Downsampling FASTQ Files ）对来自单个物理样本的数据进行采样，以模拟较低覆盖深度对变体检出的影响（下采样 FASTQ 文件）。为了测试流程在不同覆盖率水平下检测变异的能力，可以对高覆盖率的 FASTQ 文件进行下采样（即，可以从更高覆盖率的数据集中随机选择一小部分读取）。一般来说，重要的是从整个 FASTQ 文件中随机抽取所需的reads数据，而不仅仅是读取的前 X 百分比。对于双端测序，进行PE同步采样 [例如，使用像 seqtk 这样的工具（https://github.com/lh3/seqtk)]。该策略可以识别流程在较低覆盖水平检测变异的能力局限性，尽管它需要使用上述策略之一确保数据中存在感兴趣的变异信息。 D. 修改实验数据（Manipulated Assay Data ）将单核苷酸变异 (SNV)、插入/缺失 (indel) 甚至结构变异等变异插入到实验室数据文件（BAM 或 FASTQ）中，以评估生信流程正确识别和注释变异的能力。这种方法的优点是可以很好地处理目标或全基因组测序数据，并保留真实数据的许多错误概况和偏差。它还可以在不同等位基因分数的不同基因组背景下验证比实际样本允许的数量多得多的不同类型的变体。将这些 BAM 文件转换回 FASTQ 通常很重要，以便从初始对齐步骤开始测试流程。这种方法有一些重要的局限性：i) 因为它依赖于在修改reads之前正确比对的reads，它不会模拟在难以比对区域中可能发生的所有比对错误，例如带有假基因的基因； ii) 它不能模拟变异出现在异常区域的偏差（例如，导致更高测序错误率的均聚物或串联重复序列的扩展，或引入可能导致系统测序错误的 GGT 序列基序）； iii) 对较大变异进行模拟具有挑战性（例如，对由大删除或大插入中的排序错误引起的覆盖率下降和断点进行建模）； iv）目前的工具不能便捷的操作一些特殊测序技术产生的原始数据格式，例如 Ion Torrent（Thermo Fisher Scientific，Waltham，MA）流数据，来自 PacBio HiFi（Pacific Biosciences，Menlo Park，CA）的原始读数，以及来自纳米孔测序的 fast5 文件（ Oxford Nanopore Technologies, Oxford, UK)， E. 重分析（Data Reanalysis）对仅对生物信息学流程进行更改而不对上游的湿实验流程进行更改时，可以通过使用新流程对来自各种样本（包括参考材料和临床样本）的现有未修改数据进行分析来帮助验证优化的效果。这种方法具有使用现有真实数据的优势，这些数据具有所用方法的所有偏差和错误。实验室可以测试重现以前版本的流程检测到所有变异的能力，但不会测试以前未检测到的变异的检测性能。 Modifying Reference Genome针对单倍体基因组，还可以通过编辑参考基因组，当参考基因组发生变化时，被测序的个体应该在该位置有一个变异（假设个体与原始参考相匹配）。这种方法最适用于单倍体样本或单倍体染色体，如假常染色体区域外男性的 X 染色体和 Y 染色体。一个例外是，如果二倍体个体在某个位置具有杂合变异，并且更改参考以匹配该变异，则该变异将被逆转（例如，C&gt;T SNV 将变为 T&gt;C SNV , 或 2-bp 删除将更改为 2-bp 插入)。 可以用于进行数据模拟的软件清单 模拟数据的应用流程的不同开发阶段/不同程度的变更测试，对模拟数据的需求本身也会存在一些差异。这里主要需要明确，如果涉及下机数据获取前（建库、测序等）的变更，则一定需要使用真实数据进行补充验证。每种用途原文由相对详实的介绍，但因为整体不复杂也比较符合大家的主观认识，所以这里只列出每种用途的建议模拟方法。|用途|模拟数据方法||-|-||基准生物信息学工具|纯模拟数据；修改实验数据（如适用）||(a)新变异(b)检测限(c)最少的测序读数|(a) 修改实验数据 (b) 混合多个样本；(c)Fastq抽样||(a)实验室协议变更(b)变更不影响管道中工具的限制(c)变更影响管道中工具的限制|(a) 生物样本。计算机数据可以根据变化进行补充。(b) 现有的分析数据。计算机数据可以根据变化进行补充。||Proficiency testing 能力验证|修改实验数据||Variant annotations 变体注释|修改实验数据， VCF 文件操作| 模拟数据的未来发展Copy Number Variants拷贝数变异是临床上重要的一类遗传改变，在癌症和体质性疾病的管理中具有诊断、治疗和预后意义。拷贝数变异通常比小变异更难检测，特别是当拷贝数改变 (CNA) 存在于亚克隆（肿瘤）或嵌合体（种系）时等级。因此，对旨在识别 CNA 的临床 NGS 检测进行全面验证非常重要。但是对应的CNA样本却相对少见，很难采购样本进行全面验证。所以也表现出对CNA数据模拟的需求。 Bamgineer 是最近发布的一种算法，可以将任何所需级别的用户定义的等位基因特异性 CNA 引入到 BAM 文件中。当从 BAM 文件中采样读取时，该算法会考虑配对末端测序数据中的reads对。这种方法试图保留 BAM 文件中的原始偏差，并更好地模拟真实样本中的 CNA。该算法可应用于许多用例，例如在无细胞 DNA 样本和亚克隆 CNA 检测中以低等位基因负荷模拟 CNA。 VarBen 是一种新的综合性计算机变异模拟算法，可在 BAM 文件中引入各种遗传改变，包括 SNV、插入、删除、大型结构变异，包括拷贝数改变、重复以及平衡和不平衡易位。 Translocation (Gene Fusion) Assessment在临床 NGS 分析中，易位通常通过靶向 DNA/RNA 测序、RNA 测序或全基因组测序进行检测。在 DNA 水平上，大多数易位发生在内含子中，内含子可能包含难以分析的重复或低复杂性区域。类似地，易位必须导致足够数量的基因融合转录本才能被 RNA 测序检测到。这些问题使得易位检测性能的广泛验证对于 NGS 分析至关重要。然而，对于许多易位，例如 ROS1、RET 或 NTRK 中的易位，可能很难找到足够数量的易位病例来全面测试检测验证过程中的易位检测。因此通过数据模拟进行全面的验证是很有必要的。 RNA SequencingRNA 测序在临床实验室中变得越来越普遍，用于检测易位/融合事件、测量基因水平表达、解决不确定意义的变异以及测量等位基因特异性表达。 RNA 本身非常不稳定，因此难以为质量控制目的对物理样本进行重复测试。存在多种 RNA 测序模拟工具： Polyester，rlsim ,RNASeqReadSimulator和 simCT TMB and MSI TestingTMB 和 MSI 均已被证明是癌症治疗反应的重要标志物，这两个指标的报告现已包含在肿瘤芯片检测中。 TMB 和 MSI 分别表示根据观察到的每兆 DNA 碱基的体细胞变异数和基因组特定区域中二核苷酸重复序列的扩展计算的测量值。两种类型的潜在事件（体细胞变异或微卫星扩展）都可以通过当前的计算机基因组建模工具进行模拟，但工作组不知道专门设计用于模拟 TMB 或 MSI 的软件。 Minimal Residual Disease Testing 最小残留病害检测UMI/UID等技术的开发已经可以帮助克服NGS测序错误率瓶颈，从而可以检测频率远低于1%的变异。但是迄今为止，还没有开发出通过将独特的分子指标与掺入变体相结合来模拟计算机中最小残留疾病测试的软件。随着这种测试形式的激增，此类技术将对临床社区产生巨大的价值。 Long-Read Sequencing Methods直到最近，Pacific Biosciences 和 Oxford Nanopore Technologies 的长读长测序方法对于大多数临床应用来说都是昂贵的和/或具有不可接受的高错误率。然而，长读可以准确识别短读具有挑战性的变异，例如具有同源基因或假基因和结构变异的基因。随着长读对临床应用的成本效益和准确性越来越高，还需要开发在长读文件中模拟和编辑变体的技术。 临床实验室中使用模拟数据的建议 Recommendation 1: The Laboratory May Use in Silico Data Files to Supplement NGS Analytical Validation, Particularly to Assess Analytical Sensitivity or False-Negative Rates for Specific Variants; However, in Silico Data Files Cannot Supplant the Use of Physical Samples (eg, Patient Samples)计算机数据可以用用来作为补充，但是不能替代物理样本，同时模拟数据只能用来评估灵敏性，无法对特异性进行评估 Recommendation 2: The Laboratory Should Understand the Functional Limitations of the Type(s) of in Silico Data Being Utilized在使用模拟数据时，用该充分了解所使用模拟数据的局限性。前文讨论了每种类型的模拟数据局限性。了解在 NGS 生物信息学管道验证中使用的计算机数据类型的功能限制以及对建立和/或监测分析性能特征的潜在下游影响以避免严重缺陷至关重要。例如，与从头模拟的计算机数据相比，通过修改现有数据文件生成的模拟数据可能更好地反映系统测序错误、脱靶reads、配对末端距离和临床测序面板靶向基因的覆盖变异性.一般来说，应该使用自己实验室按照标准工作流程生成的多个数据集。 Recommendation 3: The Laboratory Should Understand the Limitations of Most in Silico Data for Assessing Performance in Particular Genome Contexts and Variant Types Susceptible to Systematic Sequencing Errors (eg, Homopolymers and Tandem Repeats) and Mapping Errors (eg, Genes with Pseudogenes)重要的是要了解大多数计算机数据的局限性，以评估特定基因组背景和变异类型的性能。特别是，即使修改真实数据文件也不会模仿一些系统错误，例如均聚物和串联重复。因为修改真实数据取决于读取的正确映射，它通常也无法评估难以映射区域或片段重复的错误，例如具有假基因或高度同源基因的基因，如 PRSS1、PMS2 和 SMN1/SMN2，或有错误的基因在 GRCh38 中，如 CBS、U2AF1 和 KCNE1。 Recommendation 4: The Laboratory May Use in Silico Samples for Testing Required for Minor Updates to Clinical Bioinformatics Software Pipelines用于测试软件/工具/数据库更新或版本更改的模拟数据应使用实验室的现有数据。模拟数据可以用于进行生信软件和流程的升级。 Recommendation 5: Commercial Vendors and Internal Pipeline Developers Should Include Options in Their Analysis Pipelines to Facilitate Easier in Silico Data File Import and Analysis by Clinical Laboratories和其他生物信息学软件一样，数据模拟软件包在所需输入和预期输出、许可条款、操作系统兼容性和软件依赖性、错误修复和维护的规律性以及安装和使用的简易性方面各不相同。除了功能和可用性之外，在临床测序工作流程中采用之前，还应考虑特定软件的质量和社区接受度，尽管不可否认，这些可能难以严格评估。]]></content>
      <categories>
        <category>NGS</category>
        <category>肿瘤检测</category>
        <category>标准化</category>
      </categories>
      <tags>
        <tag>AMP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ACP考试知识点]]></title>
    <url>%2F0003.%E5%BA%94%E7%94%A8-01.cloud_computing%2F%E4%BA%91%E8%AE%A1%E7%AE%97-ACP%E8%80%83%E8%AF%95%E7%9F%A5%E8%AF%86%E7%82%B9%2F</url>
    <content type="text"><![CDATA[地域、可用区问题负载均衡： 不支持跨地域，可跨可用区。OSS: 不支持跨地域，可跨可用区。SLB（Server Load Balance）服务通过设置虚拟服务地址（IP），将位于同一地域（Region）的多台云服务器（Elastic Compute Service，简称ECS）资源虚拟成一个高性能、高可用的应用服务池 存储OSS OSS 提供多种鉴权和授权机制及IP黑白名单、防盗链（用户只有通过URL签名或匿名访问Object时，才会进行防盗链验证）、主子账号功能。 OSS 存储Object上传文件的大小，通过简单上传、表单上传、追加上传的方式上传单个文件，文件的大小不能超过5 GB ；通过分片上传的方式上传单个文件，文件的大小不能超过48.8 TB。 OSS 提供多种文件上传的方法，以方便用户在不同场景中使用，OSS支持 从OSS管理控制台直接上传、通过OpenAPI上传、通过SDK上传、通过云市场例的FTP工具上传 。 OSS图片处理API 一个用户最多可以创建10个channel、每个channel中存放的数量没有限制，但是单个object的最大大小为20M，每个channel存放的总和没有限制。 图片名称具有全局唯一性且不能修改。 OSS开启版本控制后，则不支持设置合规保留策略、镜像回源或静态网站托管； 如果bucket已经开启版本控制，则无法返回到非版本化状态。 OSS收费包含三个部分，存储空间费用、流量费用和OSS API的费用， OSS API请求付费，目前云服务器和OSS间的请求次数不分内外网都会计费。 存储费用支持包年包月 和 按量付费。 仅公网下行收费，公网上传/内网上下行都不收费。 OSS免费支持 DDos攻击、自动流量清洗和黑洞功能。 修改已上传Object的元数据方法：将Object下载到本地，删除原来的Object,重新上传更改元数据后的Object； 通过CopyObject或者UploadPartCopy接口，对Object进行拷贝修改目标文件的元数据，再将原文件删除。 23年6月，新增加了set-meta 可以直接对数据的meta数据进行更新或删除 OSS支持使用对象标签对存储的Bucket 进行分类，用户可以针对桶标签的Object设置生命周期规则、访问权限 OSS 在断电、断网等导致某个机房不可用时，仍然能够提供强一致性的服务能力，切换过程无感知满足关键业务 RTO=0 和RPO=0 的强需求。 RPO（Recovery Point Objective）即数据恢复点目标，主要指的是业务系统所能容忍的数据丢失量。 RTO（Recovery Time Objective）即恢复时间目标，主要指的是所能容忍的业务停止服务的最长时间，也就是从灾难发生到业务系统恢复服务功能所需要的最短时间周期。 OSS提供保存访问日志的功能，对BucketA开启访问日志记录后，自动将请求日志，以小时为单位，按着固定命名规则，生成一个Object写入用户指定的BucketB， BucketA 和Bucket B可以是不同的Bucket，但是必须属于同一个用户。 OSS域名绑定描述正确的是：OSS cname绑定的域名必须经过工信部（不是阿里云）的备案 ； OSS域名绑定，仅支持OSS以三级域名访问方式进行绑定，访问方式为（Bucket name.${region}.aliyuncs.com 。 常见状态码 阿里云OSS和自建存储相比，有以下优点： 海量存储 、 低成本、 安全、 多线BGP接入。 针对不同的应用场景， OSS有 1.使用KMS托管秘钥进行加解密，2.使用OSS完全托管加密 两种服务器端加密方式。 OSS 原生图片处理限制有：1.调用resize默认不允许放大； 2. 支持bmp、webp图片格式。|状态码|含义||-|-||403|用户账号禁用、AK不匹配等 |本地盘 云盘和本地盘异同点：云盘和本地盘均支持SSD和快照，性能均和磁盘类型有关和容量无关 ； 本地盘不支持多副本的机制 ； 磁盘快照不支持压缩。 ECS ECS数据恢复在使用云服务器ECS的过程中，有时难免会出现数据被误删除的情况，在阿里云上恢复被误删除的数据有多种方式，比如： 通过ECS管理控制台回滚已创建的快照 通过ECS管理控制台恢复该磁盘对应的数据块副本 使用开源工具Extundelete恢复误删的数据 ECS自动快照是保存在独立于用户自己的OSS Bucket里面，不能保存在用户的Bucket里面 阿里云服务器ECS出于安全考虑默认自带安全组（仅开放了22号和3389号端口，22：远程连接Linux服务器；3389：远程连接Windows云服务器） 云服务器ECS实例镜像选择Aliyun Linux镜像后，可以兼容Red Hat系列的操作系统（包含 Red Hat Enterprise Linux, Fedora, CentOS, Scientific Linux, Oracle Linux) 如果ECS实例发生了非预期宕机或运维，阿里云默认自动重启恢复实例，如果挂在了本地盘，可以选择的恢复方式如下：1.自动重启恢复、2.禁止重启恢复，3.自动重新部署。 阿里云的ECS支持用户名和密码登陆、SSH密钥对的鉴权方式。 阿里云可以使用SSH密钥对登陆云服务器ECS，SSH秘钥在应用过程中的优势有:1.密钥对安全强度远高于常规用户口令，可以杜绝暴力破解威胁； 2. SSH密钥对便于远程登录大量Linux实例，方便管理。 云服务器ECS主要包含以下功能组件： 实例：等同于一台虚拟服务器，内含CPU、内存、操作系统、网络配置、磁盘等基础的计算组件。 ECS 系统盘挂载盘：/dev/xvda ； ECS数据盘：/dev/xcdb,/dev/xcdc……/dev/xcdz 只有在稳定（运行中、已停止、已过期）状态才能挂载磁盘，其他状态都不行。 ECS使用过程中，可以使用扩容功能（创建一块新云盘，作为数据盘挂载到实例上），挂载扩容后只是扩大存储容量（不会扩容文件系统），需要通过LVM手动扩容分区和刷新文件系统。 一块全新的Windows数据盘挂载到ECS实例后，还不能直接存储数据，通常您需要完成磁盘联机、新建分区、格式化等初始化操作后，才能供系统读写数据。 更换ECS实例/操作系统后，重新分配新的系统盘（系统盘ID会更新），旧系统盘会被释放。系统盘的云盘类型、实例IP地址以及弹性网卡和MAC地址保持不变；旧系统盘的自动快照策略自动失效，需要重新设置自动快照策略；就系统盘如果开启了自动快照随云盘释放，则自动快照会被自动删除，未开启则不会自动删除，等到到期后释放。 ECS相比于传统服务器，拥有 每份数据多分副本、 单份数据损坏可在短时间内快速恢复、无需额外的开发，支持自动的磁盘数据备份。 Windows示例系统盘由40G扩容到100G，分区C显示大小99.9GB，但是可用空间（百分比）反而比之前更小，这个错误出现的原因是： 虚拟内存被开启且设置的是系统自动管理。 ECS的相关操作注意事项： 禁止使用ECS实例做流量穿透服务 不要随意修改网卡MAC地址 对于4GiB以上内存的云服务器，需选择64位操作系统 不要编译Liuⅸ系统的内核，或对内核进行任何其他操作 在Liux实例里，您重启系统后，可能会出现数据盘分区丢失或者数据丢失的问题。这可能是因为您未在etc/fstabi文件里设置自动挂载。此时，您可以先手动挂载数据盘分区。如果手动挂载时报分区表丢失，您可以尝试如下三种办法进行处理：1.通过fdisk恢复分区；2.通过testdisk恢复分区；3.通过testdisk直接恢复数据。 阿里云api的访问地址是： http://ecs.aliyuncs.com/ 如果仅在特定的时段有较高的CPU性能需求，为了尽可能节约成本，应该选用突发性能型实例。不同类型实例的适用场景： 突发性能型实例（Burstable Performance Instances）： 示例：AWS T3、Google Cloud N1、Azure B系列 适用场景：适合工作负载的 CPU 使用率具有突发性需求的场景，例如开发环境、小型应用服务器、低流量网站、测试和开发环境。 密集计算型实例（Compute-Optimized Instances）： 示例：AWS C5、Google Cloud C2、Azure F系列 适用场景：适合需要高性能计算的工作负载，例如科学计算、模拟、高性能集群、批处理处理、视频编码和游戏服务器等。 通用计算型实例（General Purpose Instances）： 示例：AWS M5、Google Cloud N2、Azure Dv3系列 适用场景：适用于广泛的工作负载，包括 Web 服务器、应用服务器、小型数据库、微服务、中间件和开发/测试环境等。 共享型实例（Shared Burstable Instances）： 示例：AWS T2、Google Cloud E2、Azure B1S 适用场景：1.中小型网站和Web应用程序；2.开发环境、构建服务器、代码存储库、微服务、测试和暂存环境等：3.轻量级数据库、缓存；4.轻量级企业应用、综合应用服务；适合个人项目、学习、小型网站、轻负载应用或低流量网站等对计算资源需求较低的场景。 ECS的生命周期管理状态包括：中间状态（待启动、启动中、停止中）和稳定状态（已删除、已停止、即将过期、已过期、欠费回收中、过期回收中、已锁定、等待释放） 磁盘操作时间更换系统盘需要ECS实例处于已停止状态； 更换系统盘，原系统的所在实例的ID不会发生变化； 更换系统盘，原系统的自动快照会被释放，手动创建的快照不会被释放，这些快照仍然可以创建自定义镜像。 更换迁移 ECS从经典网络迁移至VPC网络，实例本身系统硬件配置无变化（实例ID不变）；网络切换VPC网络，实例的私网IP和公网IP会变化。 监控指标 ECS监控指标分基础监控和操作系统级别指标监控，操作系统级别监控指标包含内存使用率、平均负载、磁盘IO读/写、磁盘使用率、TCP连接数、进程总数。 云盘 ESSD AutoPL云盘：支持根据业务需求自定义云盘的预配置性能以及性能突发。该类云盘在保持ESSD云盘原有功能与性能的同时，可以实现云盘容量与云盘性能解耦。 建议在以下业务场景中使用： 应用于ESSD云盘所适用的场景（大型OLTP数据库、NoSQL数据库和ELK分布式日志等场景）。 业务所需的云盘容量固定，但需要更高的云盘性能支撑业务的运行。 业务波动较大，波峰高频出现。需要云盘具备应对突发业务的能力。 ESSD PL-X云盘：具备超高IOPS（Input/Output Operations Per Second）、超高吞吐量和超低时延等多维度的超高性能。您可以在配置ESSD PL-X云盘容量的同时，根据业务需求灵活自定义云盘的IOPS。更多信息，请参见ESSD PL-X云盘。建议在对云盘性能有更高要求的OLTP数据库和KV数据库场景中使用。 ESSD云盘：基于新一代分布式块存储架构的超高性能云盘产品，结合25GE网络和RDMA技术，单盘可提供高达100万的随机读写能力和更低的单路时延能力。建议在大型OLTP数据库、NoSQL数据库和ELK分布式日志等场景中使用。 SSD云盘：具备稳定的高随机读写性能、高可靠性的高性能云盘产品。建议在I/O密集型应用、中小型关系数据库和NoSQL数据库等场景中使用。 高效云盘：具备高性价比、中等随机读写性能、高可靠性的云盘产品。建议在开发与测试业务和系统盘等场景中使用。 弹性伸缩 弹性伸缩只支持ECS的伸缩，不支持其他服务的伸缩。只能使用阿里云的ECS实例。 用户手动添加到伸缩组中的ECS实例不会停止和释放，弹性伸缩只会停止和释放自动创建的ECS. 弹性伸缩的伸缩模式有多种，如定时模式、动态模式、固定数量模式、自定义模式以及健康模式。 开启实例释放保护后，用户不能手动释放，但是不能阻止合理的自动释放行为：1.账号欠费15天； 2.设置了自动释放时间到期； 3. 实例存在安全合规风险；4.弹性伸缩自动创建，缩容时被移除释放 。 报警任务支持：CPU、内存、系统平均负载、内外网出和入流量，TCP总连接数和已建立连接数。 在Forcedelete为False时，删除伸缩组需要满足：1. 伸缩组没有任何伸缩活动正在执行。2.伸缩组当前的ECS实例数量（Total Capacity）为0 阿里云的同一个ECS只能加入一个伸缩组。经典网络的ECS不能加入专有网络VPC。 伸缩活动特点：1.伸缩活动不可以中断， 2. 伸缩活动保持ECS实例级失误的完整性，而非伸缩活动级事物的完整性。 伸缩组支特关联负载均衡实例和RDS实例，但是暂时不能关联Redis实例。如果您有业务数据存诸在Redis实例上，手动配置ECS实例加入或移出Rdis实例白名单，操作效率较低。您可以通过生命周期挂钩和OOS模板将ECS实例自动加入和移出Redis实例白名单。 弹性伸缩使用伸缩配置创建ECS实例，这种方式不够灵活，需要进行更多的自定义设置时，可以使用生命周期挂钩 或 实例自定义数据。 伸缩配置 弹性伸缩的伸缩配置支持多种特性，例如： 标签、SSH密钥对、实例RAM角色、实例自定义数据。 弹性伸缩创建出来的机器配置和规格相同，但是不会出现IP相同的情况。 伸缩组 创建伸缩组后，负载均衡实例、RDS数据库实例 不可以修改。 ECS同一时刻只能加入一个伸缩组。 伸缩组自动扩展必备的活动是: 1. ECS实例分配IP； 2. 启动ECS实例。 冷却时间 如果在伸缩活动中，没有ECS实例成功加入或者移出伸缩组，则弹性伸缩服务不会开始计算冷却时间 如果您停用再启用伸缩组，伸缩组启用后的首次伸缩活动可以立即执行，不会受冷却时间影响。当伸缩组启用后首次成功执行伸缩活动，弹性伸缩服务才开始计算冷却时间。 冷却时间不能为空，如果不配置使用默认冷却时间。 由于伸缩组正在发生伸缩活动或者伸缩组停用等原因，导致定时任务触发执行伸缩规则失败后，在LaunchExpirationTime内，定时任务会自动重试触发，否则放弃本次定时触发。 自动触发的伸缩活动有冷却时间，冷却时间指伸缩组成功执行伸缩活动后的一段锁定时间。在冷却时间内，伸缩组会拒绝由报警任务触发的伸缩活动请求。但非报警任务（手动执行任务、定时任务、健康检查等）触发的伸缩活动可以立即执行，绕过冷却时间。ECS实例状态 保护状态：处于保护状态的ECS实例负载均衡权重不受影响。弹性伸缩不会检查处于保护状态的ECS实例健康状态，也不会释放ECS实例，不希望被移出伸缩组的ECS实例转为保护状态。 备用状态：设置备用状态会把ECS的负载均衡权重置零，弹性伸缩不会检查处于备用状态的ECS实例健康状态，也不会释放实例。 移除策略 移除策略选择最早伸缩配置对应的实例：筛选添加时间最早的伸缩配置和启动模板对应的实例。手动添加的实例没有关联伸缩配置或启动模板，因此不会首先选出手动添加的实例。如果已移出全部关联的实例，仍需要继续移出实例，则随机移出手动添加的实例。 负载均衡 性能保障型实例的三个关键指标： 最大连接数-Max Connection最大连接数定义了一个负载均衡实例能够承载的最大连接数量。当实例上的连接超过规格定义的最大连接数时，新建连接请求将被丢弃。 每秒新建连接数-Connection Per Second（CPS）每秒新建连接数定义了新建连接的速率。当新建连接的速率超过规格定义的每秒新建连接数时，新建连接请求将被丢弃。 每秒查询数-Query Per Second（QPS）每秒请求数是七层监听特有的概念，指的是每秒可以完成的HTTP或HTTPS的查询（请求）的数量。当请求速率超过规格所定义的每秒查询数时，新建连接请求将被丢弃。 SLB判断流量转发的顺序：当用户流量经过负载均衡端口时，首先判断其是否能够匹配上某条“转发规则”， 如果匹配，则将流量转发到该规则的后端服务器组上；若不匹配并且在该监听上设置了虚拟服务器，那么将流量转发到该虚拟服务器组上； 若用户没有在该监听上设置虚拟服务器组，即将流量转发到实例级别添加的各后端服务器中。 公网类型负载均衡实例，系统会为其分配一个公网服务地址。 负载均衡的必要配置： 负载均衡SLB实例的属性配置 / LoadBalancer：负载均衡实例。 负载均衡SLB实例的监听配置 / Listener：用户定制的监听器，定义了负载均衡策略和转发规则。 负载均衡SLB实例的后端ECS实例配置 / BackendServer：后端的一组ECS（云服务器）。 四层和七层负载均衡差异点 四层负载均衡，采用开源软件LVS（linux virtual server），并根据云计算需求对其进行了定制化。适合无状态的 ; 七层负载均衡，采用开源软件Tengine 四层网络是基于 源IP实现， 7层网络是基于Cookie实现的 健康检查过程中，4层服务健康检查基于端口，七层服务检查是基于返回的健康码。 四层无法关闭健康检查 ，七层可以关闭健康检查。 共享实例带宽, 如果给某个监听设置带宽峰值，则会再总带宽中剥离出对应的带宽作为该监控的独享带宽（其他监听任务无法使用） 主备服务器组仅可适用于TCP和UDP监听。 负载均衡SLB不提供CNAME地址只提供IP地址；后端服务器的权重之和没有限制； 后端服务池中的ECS可能由于异常机制导致不能正常运行，会出现非运行中的情况。 一般情况下默认路由在有外网情况下会先走外网网卡，如无外网则走内网网卡。 用户在使用SLB负载均衡时，发现在HTTP请求的头部增加了Transfer-Encoding:chunked字段，但是从本地主机直接访问后端服务器时是没有这个字段的是因为：由于七层负载均基于Tengine反向代理实现。 健康检查： 4层负载均衡健康检查通过检查端口，7层负载均衡健康检查通过服务器端返回码。 CLB可以包年包月（计费项：实例费用、规格费用、公网网络费）和按量付费（实例费用、规格费用/LCU费、公网网络费） 监听四层负载均衡（传输层） 四层负载均衡，采用开源软件LVS（linux virtual server），并根据云计算需求对其进行了定制化。 会话保持基于IP 在4层SLB服务中，不支持添加进服务器池的云服务器既作为RS,又作为客户端向所在的SLB发送请求。返回的数据包只在云服务器内部转发，不经过SLB，所以通过配置在SLB内的云服务器去telnet SLB的VIP是不通的。 TCP（Transmission Control Protocol，传输控制协议) 建议的应用场景 注重可靠性，对数据准确性要求高，速度可以相对较慢的场景； 示例：文件传输、发送或接收邮件、远程登录、无特殊要求的 Web 应用 健康检查： 健康检查通过定制的TCP探测来获取状态信息。 特性 面向连接的协议； 在正式收发数据前，必须和对方建立可靠的连接； 基于源地址会话保持，在网络层可直接看到来源地址； 监听支持 TCP 和 HTTP 两种方式进行健康检查； 转发路径短，所以性能比 7 层更好，数据传输速度更快 UDP （User Datagram Protocol，用户数据包协议） 建议的应用场景 关注实时性而相对不注重可靠性的场景； 示例：视频聊天、金融实时行情推送 健康检查： 健康检查通过UDP报文探测来获取状态信。 特性 面向非连接的协议； 在数据发送前不与对方进行三次握手，直接进行数据包发送，不提供差错恢复和数据重传； 可靠性相对低；数据传输快 通过UDP保温探测来获取状态信息。 七层负载均衡（应用层） 七层负载均衡，采用开源软件Tengine。 会话保持基于cookie 针对七层（HTTP或HTTPS协议）监听，健康检查通过HTTP HEAD探测来获取状态信息， HTTP/HTTPS监听可使用植入cookie和重写cookie来进行会话保持。HTTP 建议的应用场景 需要对数据内容进行识别的应用，如 Web 应用、小的手机游戏等 特性 应用层协议，主要解决如何包装数据； 基于 Cookie 会话保持；使用 X-Forward-For 获取源地址； 监听只支持 HTTP 方式健康检查 HTTPs 建议的应用场景 有更高的安全性需求，需要加密传输的应用 特性 加密传输数据，可以阻止未经授权的访问； 统一的证书管理服务，用户可以将证书上传到负载均衡，解密操作直接在负载均衡上完成 网络经典网络经典网络支持私网互通。 专有网络VPC 专有网络VPC下面的ECS，无论是否绑定EIP，在设置安全组的时候，只可以设置内网规则，外网规则是不可选的。 专有网络VPC的高级功能包括：网络ACL、流日志和自定义路由表。 创建专有网络后，系统会在路由表中自动添加一条以 100.64.00.0/10 为目标网段的路有条目，用于VPC内的云产品通信。 每个交换机的第一个和最后三个IP地址为系统保留地址。以192.168.1.0/24为例，192.168.1.0、 192.168.1.253、192.168.1.254和192.168.1.255这些地址是系统保留地址 一个交换机只能绑定一张路由表，交换机的路由策略由其关联的路由表管理。 VPC网络下，可以通过弹性公网IP、NAT网关、公网负载均衡和ECS固定公网IP等方式链接公网。 实现VPC中ECS服务器切换/迁移到同VPC下的其他交换机，包括以下几步， 打开云服务器管理控制台； 找到对应的需要切换/迁移的云服务器； 修改云服务器的私网地址; 选择您所需的交换机，同时指定新交换机下的IP; 使用隧道封装技术对云服务器的IP报文进行封装，所以云服务器的数据链路层（二层MAC地址）信息不会进入物理网络，实现了不同云服务器间二层网络隔离，因此也实现了不同VPC间二层网络隔离。；VPC内的云服务器使用安全组防火墙进行三层网络访问控制。 对VPC下的云服务器实例的私网IP进行修改，可以通过 需要修改的地址超出交换机地址范围时，变更ECS实例所在的交换机并变更ECS的私网IP ； 需要修改的地址没有超出ECS所在交换机地址范围时，直接变更ECS实例的私网IP 方式变更。 安全组和交换机绑定； 子网和交换机绑定； 路由表和交换机绑定。 vpc目前只有两种状态： pending（配置中）， available(可用) 。 创建VPC后，可以通过创建交换器为专有网络划分一个或多个子网，设置交换机的IPv4网段时，用户需要了解交换机的网段限制： 交换机的网段必须是VPC网段的子集； 交换机的第一个和最后三个IP为系统保留地址； 如果交换机有和其他专有网络的交换机或本地数据中心通信的需求，确保交换机的网段和要通信的网段不冲突； 交换机的网段不能与所属VPC路由表中路由的目标网段范围相同或大于该范围； 交换机网段不能与所属VPC下其他交换机的网段重叠。 创建VPC后，需要完成创建交换机的操作之后才能够在专有网络内创建其他的云产品实例。 默认交换机和非默认交换机的异同点： 默认交换机只能创建在默认专有网络中 默认交换机与非默认交换机支持相同操作方式，且规格限制一致 默认交换机由阿里云为您创建，您自行创建的均为非默认交换机 EIP 经典网络公网IP转换为EIP后，1. 采用按使用流量计费方式，2.公网带宽值和原ECS实例保持一致，3.不能挂在到经典网络类型ECS实例上， 4.不同于专有网络VPC类型ECS实例，经典网络ECS实例具有公网网卡，如果经典网络公网IP转换为EIP，则无法保留公网网卡和实例的MAC地址。 通过对绑定EIP的ECS实例分配一块弹性网卡，并将EIP绑定到弹性网卡，可以实现公网出口IP保持一致。 云数据库不能绑定EIP 1个弹性公网IP只能绑定到1个ECS上 ； 弹性公网IP只能绑定到相同地域（可以是不同可用区）的VPC类型的云服务器ECS实例上；不能绑定到经典网络上。 网络ACL 特性 网络ACL规则仅过滤绑定的交换机中ECS实例的流量（包括SLB实例转发给ECS实例的流量）。 网络ACL的规则是无状态的，即设置入方向规则的允许请求后，需要同时设置相应的出方向规则，否则可能会导致请求无法响应。无状态指的是每个网络ACL规则都是独立的，不考虑其他规则或之前的通信历史记录。换句话说，ACL规则仅根据每个流量数据包的源地址、目的地址、协议类型和端口号等信息来决定是否允许该数据包通过，而不关心之前是否有类似的数据包通过了。因此，即使两个数据包从同一个源地址发送，但如果它们的目的地址、协议类型或端口号不同，则它们可能会被ACL处理为完全不同的情况。-- by chatGPT 网络ACL无任何规则时，会拒绝所有出入方向的访问。 网络ACL与交换机绑定，不过滤同一交换机内的ECS实例间的流量 CDN CDN加速，在添加域名页面，源站信息可以选择：IP、源站域名、OSS域名、函数计算域名。 CND节点在全国多地均有分布，所以与其他云产品间流量通过公网传输，不管是否同一地域都会产生回源流量费。 开启CND加速时，源站域名不能与加速域名相同，否则会造成循环解析，无法回源。 安全阿里云的云盾补丁管理服务里发布的补丁来自阿里云自己研发。 安全管家 大型互联网用户，现有网上资产比较多；但是不清楚自己那些资产联网了，无法及时对这些资产进行清理和关停，存在较大的安全隐患，可以使用阿里云的 安全管家。 操作审计 操作审计（ActionTrail）帮助您监控并记录阿里云账号的活动，包括通过阿里云控制台、OpenAPI、开发者工具对云上产品和服务的访问和使用行为。您可以将这些行为事件下载或保存到日志服务SLS或对象存储OSS，然后进行行为分析、安全分析、资源变更行为追踪和行为合规性审计等操作。 云盾 数据风控 目前只支持在Web中插入指定的JS代码的方式来采集信息。 数据风控 产品具体可以防范 垃圾注册、营销作弊、盗卡支付 云盾面可以免费开通的功能包括：DDoS基础防护、阿里绿网、安骑士 云盾加密服务采用 符合 安骑士 安骑士是云盾提供的保护服务器的产品，功能包括：1.木马文件检查，异地登陆报警，操作系统暴力破解，高危漏洞修复，网页防篡改 事态感知（可以预测即将发生的安全事件） 态势感知具备异常登录检测、网站后门查杀、进程异常行为、敏感文件算改、异常网络连接、Linux软件漏洞、Windows系统漏洞、Web-CMS漏洞、应急漏洞、Web漏洞扫描、主机基线、云产品基线、资产指纹、AK和账号密码泄露、数据大屏、日志检索、全量日志分析等功能。 事态感知支持通过手机短信和电子邮件的方式向用户发送报警信息。 事态感知是云上资产的诊断服务，不是针对ECS的托管服务。 安全组 安全组授权主要限制IP,端口，和安全组。无法指定MAC地址。 安全组是一种虚拟防火墙，具备状态监测和包过滤功能。而不是ECS实例之间的隔离。 实例加入安全组的规则如下： 实例至少加入一个安全组，可以同时加入多个安全组。 实例上挂载的弹性网卡中，辅助网卡可以加入和实例不同的安全组。 实例不支持同时加入普通安全组和企业安全组。 安全组只能设置内网访问规则（无论是否绑定EIP） 创建安全组的顺序是：创建安全组-添加规则-ECS加入-管理安全组-管理安全组规则 云安全 云安全中心提供 添加白名单 的功能，开启后，云安全中心不在对对应风险项进行告警。 防火墙（收费） Web应用防火墙的功能包括： 防Web应用系统的密码破解、敏感信息泄露。 云防火墙是用户上云后的首个安全组件，支持全网流量识别、统一策略管控、入侵检测、日志等核心功能。 应对CC攻击 实人认证 实人认证服务是指依托活体检测、人脸比对等生物识别技术、证件OCR识别技术等，进行的自然人真实身份的校验服务，目前仅支持拥有中华人民共和国第二代居民身份证的人士进行认证。 云监控 自定义监控是指：用户可以对自己关心的业务进行监控，将采集到的监控数据上报至云监控，由云监控进行数据的处理，并根据结果进行报警； 自定义的监控项的数量是没有数量限制的，且阿里云提供接口可以给开发者上报数据。 云监控可以通过手机短信、钉钉机器人、电子邮件、电话进行报警通知。 云监控提供 站点监控、 云服务监控、自定义监控、报警服务 功能， 负载均衡开启云监控后，可以使用的报警方式有短信、邮件、旺旺。不支持电话报警。 攻击 跨站脚本攻击(XSS: Cross-site scripting) 可被用于进行窃取隐私，钓鱼欺骗，偷取密码，传播恶意代码等攻击行为，主要发生在用户浏览器。 CC(Challenge Collapsar)是指攻击者借助代理服务器生成指向受害主机的合法请求，实现DDOS和伪装。CC主要是用来攻击页面的。预防CC注册是Web应用防火墙的功能。 日志服务告警管理 告警策略是告警管理系统的配置实体，当告警管理系统接收到告警事务（含恢复通知）时，自动按照对应的告警策略，进行告警降噪等操作。 告警合并告警合并是将具有相同特征的告警进行分组，便于进行统一通知或后续处理，在一定程度上避免告警风暴。 告警静默告警静默用于禁止一段时间内的告警通知。例如在特定时间内维护测试环境，会产生大量的相关告警，此时可通过静默功能避免接收到大量的告警通知。 告警策略继承告警策略之间可以有继承关系，最终的作用效果相当于父策略和子策略合并后的作用效果。更多信息，请参见告警策略继承机制。 告警抑制告警抑制用于组织有某告警引发的其他告警通知。 数据隔离不同告警策略之间的数据是完全隔离的。 其他 针对常见的HTTP字段（如IP、URL、Referer、UA、参数等）进行条件组合，配置支持业务场景定制化的防护策略，可用于盗链防护、网站管理后台保护等 IaaS、PaaS、SaaS IaaS（Infrastructure as a Service，基础架构即服务）： 基于云的服务，按需付费，用于存储，网络和虚拟化等服务。 PaaS（Platform as a Service，平台即服务）： Internet上可用的硬件和软件工具。 SaaS（Software as a Service，软件即服务）： 可通过互联网通过第三方获得的软件。 DaaS（Data as a Service，数据即服务）：最新产生的，稳定的数据提供 虚拟化技术包括：全虚拟化、半虚拟化、硬件辅助虚拟化。 SDN（Software Defined Network) :软件定义网络 阿里云作为云计算服务公提供商，提供的安全保障服务 APP客户端经常因本地DNS篡改导致连不上服务器，可以使用HTTPDNS， HTTPDNS是一款面向移动开发者推出的一款域名解析产品，具有域名防劫持、精准调度等特性。 SMC 服务器迁移中心SMC（Server Migration Center）是阿里云自主研发的迁移平台。使用SMC，可将您的单台或多台迁移源迁移至阿里云。迁移源（或源服务器）概指您的待迁移IDC服务器、虚拟机、其他云平台的云主机或其他类型的服务器。迁移过程无需停机，不影响原服务器业务。 一个完整的云计算环境由云端、计算机网络和终端三部门组成（也就是常说的云、管、端）。云端就是指计算设备，负责完成软件的计算；终端是指我们用来完成输入/输入的设备；计算机网络负责将云端和终端连接起来，完成信息传输（将终端的输入指令传输到云端，将云端的执行结果反馈给终端）。 CAP原则又称为CAP理论，主要思想是在任何一个分布式系统中都无法同时满足CAP。 C（Consistency）：表示一致性，所有的节点同一时间看到的是相同的数据。 A（Avaliablity）：表示可用性，不管是否成功，确保一个请求都能接收到响应。 P（Partion Tolerance）：分区容错性，系统任意分区后，在网络故障时，仍能操作。 SQL注入可能造成的危害包括： 网页被篡改，数据被篡改，核心数据被窃取，数据库所在服务器被攻击编程傀儡主机 可以公网接入的云服务是 EIP、 NAT网关、 固定公网IP 堡垒机服务功能特性：1.满足《萨班斯法案》、金融监管、《等级保护》的审计合规要求；2. 支持SSH、Windows远程桌面、SFTP等常见运维协议。 某用户在创建ECS实例(分配了公网P地址)后的六小时内，想要更换该ECS实例的公网IP地址，但在控制台操作界面找不到更换公网P选项，这种问题的原因可能是: ECS实例为VPC网络，停止实例时选择停机不收费。 停机不收费的ECS则为按量付费的ECS, 按量付费的ECS在购买创建后的6小时内，并不能更换公网IP，这是由于更换公网IP需要停止实例，而按量付费的ECS实例在停止后会自动释放公网IP地址，所以按量付费ECS实例停止后，并没有“更换公网IP”的选项。 PPPoE是拨号上网相关协议，已经过时了。不可能在API协议中使用。 在创建ECS实例后，如果需要更换ECS实例的操作系统，用户可以通过更换系统盘来更换操作系统。用户进行更换系统盘操作后，下列选项中不一定发生的是：原系统盘自动快照被自动删除。（如果未开启自动快照随云盘释放，则到期后自动释放。） ECS服务器使用注意事项说法正确的是：1.云服务器的内核和操作系统版本不要随意升级；2. Liux的云服务器数据盘未做分区和格式化，使用前请挂载数据盘；3. 网卡的MAC地址不要修改。 Google在2003年到2006年公布了关于GFS、MapReduce和BigTable三篇大数据处理系统技术论文。 云数据库RDS不能绑定EIP。 网络通信五元组： 源IP地址，源端口，目的IP地址，目的端口，传输层协议。 BGP机房特点： BGP机房的服务器并不是都具备双机或多机冗错功能。参考BGP机房的优点：(1)服务器只需要设置一个P地址，最佳访问路由是由网络上的骨干路由器根据路由跳数与其它技术指标来确定的，不会占用服务器的任何系统资源。服务器的上行路由与下行路由都能选择最优的路径，所以能真正实现高速的单P高速访问。(2)由于BGP协议本身具有冗余备份、消除环路的特点，所以当DC服务商有多条BGP互联线路时可以实现略由的相互备份，在一条线路出现故障时路由会自动切换到其它线路。(3)使用BGP协议还可以使网络具有很强的扩展性可以将DC网络与其他运营商互联，轻松实现单P多线路，故到所有互联运营商的用户访问都很快。这个是双IP双线无法比拟的。 PolarDB 是阿里巴巴自研的新一代云原生数据库，在计算存储分离架构下，利用了软硬件结合的优势，为用户提供具备极致弹性、高性能、海量存储、安全可靠的数据库服务。100%兼容MySQL和PostgreSQL生态，高度兼容Oracle语法。 主机边界防火墙可以对ECS实例间的入流量和出流量进行访问控制，限制ECS实例间的未授权访问。 TTL：TTL是Time-To-Live的缩写，指生存时间。而域名解析中提到的TTL值是指全国各地的localdns服务器中缓存解析结果的时间周期。 ACK集群需要自行创建Master节点和Worker节点，仅支持专有网络VPC。]]></content>
      <categories>
        <category>云计算</category>
      </categories>
      <tags>
        <tag>阿里云</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Highly aneuploid non-small cell lung cancer shows enhanced responsiveness to concurrent radiation and immune checkpoint blockade]]></title>
    <url>%2F0001.%E7%9F%A5%E8%AF%86-05.Paper%2FPMID36443406%2F</url>
    <content type="text"></content>
      <categories>
        <category>NGS</category>
        <category>文献</category>
      </categories>
      <tags>
        <tag>变异检测</tag>
        <tag>氧化损伤</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[单细胞检测概述]]></title>
    <url>%2F0001.%E7%9F%A5%E8%AF%86-01.NGS%2F2022-11-22.%E5%8D%95%E7%BB%86%E8%83%9E%E6%A3%80%E6%B5%8B%2F</url>
    <content type="text"><![CDATA[参考资料单细胞测序的技术概述单细胞转录组测序技术全流程解析]]></content>
      <categories>
        <category>NGS</category>
        <category>单细胞</category>
      </categories>
      <tags>
        <tag>NGS</tag>
        <tag>单细胞</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[免疫组库]]></title>
    <url>%2F0001.%E7%9F%A5%E8%AF%86-01.NGS%2F2022-11-03.%E5%85%8D%E7%96%AB%E7%BB%84%E5%BA%93%2F</url>
    <content type="text"><![CDATA[概念什么是免疫组库从出生开始，人体每天都会受到大量外来物质的侵扰，但并不会每天得病。这是因为人类在演化的过程中，拥有了一套强大的免疫系统来对抗外来物质的入侵，这些物质称为抗原(Antigen)。免疫系统包括物理屏障、非特异性免疫、特异性免疫系统。T/B 细胞是特异性免疫系统的两大细胞群。免疫组库(immune repertoire, IR)是指在某一时间, 某个个体的循环系统内所有功能多样性T细胞和B细胞的总和。 B细胞/T细胞简介而B细胞和T细胞则会记忆人类出生后所接触过的抗原，并同时产生记忆B细胞和记忆T细胞，记忆B细胞和记忆T细胞使人体在受到相同抗原的第二次刺激后可以更迅速的发出反应，这便形成了免疫记忆。 T细胞在外观上与B细胞非常相似，在一个普通的显微镜下可能无法将它们区分开来。但B细胞与T细胞也有区别： B细胞在骨髓中成熟，而T细胞在胸腺中成熟； B细胞制造的抗体可以识别所有有机分子，但T细胞专门识别蛋白抗原； B细胞可以以抗体的形式分泌受体，但是T细胞受体保持在细胞表面上； 最重要的区别是，B细胞能够自己识别抗原，而T细胞不能直接识别抗原表位，只有在被抗原呈递细胞（APC）“正确呈递”的情况下才会识别出抗原，即只能特异性识别APC或靶细胞表面提成的抗原肽-MHC分子复合物（pMHC）。 T细胞与B细胞表面上都存在受体分子，分别称为T细胞受体（TCRs:T-cell receptors）和B细胞受体（BCRs:B-cell receptors）。机体的每一种抗体都与一种特殊的抗原结合在一起，因此，为了使抗体能够与许多不同的抗原结合，需要分泌许多不同的抗体分子。 根据免疫学家粗略估计，大概需要1亿种抗体才可以满足人体需要。拿BCRs来说，由于抗体的每个抗原都是由一个重链和一个轻链组成的，所以我们可以将大约10000个不同的重链和10000个不同的轻链组合在一起，从而得到我们需要的1亿个不同的抗体。 BCR/TCR简介▲图2 BCR的结构以其在B细胞上的位置 ▲图3 TCR的结构以其在T细胞上的位置 BCR是表达于B细胞表面的免疫球蛋白（Immunoglobulin, Ig），B细胞通过BCR识别抗原，接受抗原刺激，启动体液免疫应答。BCR的重链（Heavy Chain, IgH）由编码可变区（Virable Domain, V区）的V基因片段（variable gene segment）、D基因片段（diversity gene segment）和J基因片段（joining gene segment）以及编码恒定区（Constant Domain, C区）的C基因片段组成。BCR的轻链（Light Chain，IgL）V区只有V基因片段和J基因片段。 TCR每条肽链的细胞膜外区各含1个V区和1个C区，V区中含有3个互补决定区（CDR1、CDR2、CDR3），是TCR识别pMHC的功能区。TCR跨膜区带有正电，可与CD3形成盐桥，形成TCR-CD3复合物，TCR识别抗原所产生的活化信号由CD3转导至T细胞内。CD3具有5种肽链，即γ、δ、ε、ζ 和 η 链，均为跨膜蛋白。 免疫组库的克隆型通过在免疫系统的免疫球蛋白和T细胞受体产生的早期阶段可变的（V）、多样性（D）和连接（J）基因片段经由体细胞重组的重排来确定。受体的重排V（D）J部分（即V区）非常重要，因为它负责表位识别，当V（D）J被翻译成氨基酸序列时， V-区域可以被细分成由前导系列、框架（FR）1、互补性决定区1（CDR1）、FR2、CDR2、FR3、CDR3、FR4和C-结构域组成的几个部分。其中CDR3特别重要，因为既往研究表明该区域与抗原特异性有关，故研究CDR3序列的特异性与多样性对于识别TCR/BCR具有重要意义。 因此有些“免疫组库”意指实际在一个或多个个体的淋巴细胞中检测到的一组不同的CDR3序列。 抗体多样性1977年的诺贝尔奖得主利根川解开了机体抗体多样性这个谜团：机体通过模型化设计（modular design）达到抗体的多样性。 （重链胚系基因经过重排先形成D-J连接，然后发生V-DJ连接，编码功能性V区基因） 即在每一个B细胞中，含有重链基因序列的染色体上有四种不同的DNA基因片段分别称为V、D、J、C。在人类中，大约有40个不同的V段，大约25个不同的D段，6个不同的J段等，为了合成一个成熟的重链基因，每一个B细胞都选择（随机的）一种基因片段并将它们像这样组合在一起，含有抗体轻链遗传信息的染色体也通过选择基因片段并将它们粘在一起来组装，多种不同的基因片段进行混合和匹配增加了产生抗体的多样性。 但这还不够，为了使抗体的多样化继续增强，当基因片段连接在一起时，额外的DNA碱基会被增添或删除，这种重组多样性和连接多样性使得只需要少量的遗传信息就能创造出超出想象的抗体多样性。 与BCRs一样，TCRs也是由一种混合与搭配模块化设计策略来制造的。因此，TCRs与BCRs一样具有多样性。可简单理解为：T细胞和B细胞基因座上大量的V、D、J基因片段在T细胞受体（TCR）和B细胞受体（BCR）的形成过程中会产生各种多样性重组，V-D-J基因的重组赋予了每一种T、B细胞独特的TCR、BCR，从而使得每一个TCR和BCR序列能有效的成为一个T、B细胞克隆的唯一生物标志物。 免疫组库的应用预后评估Keane（2017）等通过高通量免疫组库测序技术（HST-IR）对92例弥漫性大B细胞淋巴瘤患者肿瘤组织中TCR库进行测序分析，结果显示 ：与正常淋巴结组织相比，病变淋巴结组织中TCR多样性明显受限，TCR的多样性程度与肿瘤组织中免疫检查点相关蛋白（如PD-L1和PD-L2等）的表达呈正相关，这可能为选择免疫治疗方案提供证据支持 ；且在患者肿瘤组织微环境中，TCR库表现为高克隆低多样性特征，这些高频克隆型的扩增与患者不良预后相关。 血液MRD免疫组库的测定一般TCRs/BCRs的免疫组库测序有2种策略： （1）基于细胞DNA，利用TCR/BCR基因的重排规律，在TCR（α/β/γ/δ 链）和BCR（重链/轻链）的可变区基因设置上游引物，在连接区J基因和恒定区C基因设置下游引物，通过PCR扩增出目的链的PCR产物，但该方法存在扩增不均一的不足，影响结果的准确性； （2）基于细胞RNA，采用5’RACE技术，基于引入的接头序列进行第二次无偏好PCR扩增，克服不均一性的问题，但RNA样品处理较DNA样品复杂，重复性也不如DNA样品好。 得益于液体活检技术和高通量技术的快速发展和广泛应用，以血浆cfDNA/ctDNA为起始样本，针对特异性免疫亚克隆扩增的免疫组库测序技术使得“液体活检”更好的应用于疾病的风险评估与预测。 其主要流程一般是先提取血浆的cfDNA，对BCR H链（重链）进行全长多重PCR扩增和TCR β链的CDR3区域进行多重PCR扩增，然后将2种扩增产物混合后再进行一次PCR扩增，获得文库后进行上机测序，再对测序数据进行免疫组库精准信息分析。 参考资料海普洛斯 (1) Tang J. et al. Trends in the global immuno-oncology landscape [J] .Nat Rev Drug Discov. 2018 Oct 26. (2) Tang J. et al. Comprehensive analysis of the clinical immuno-oncology landscape [J] .Ann Oncol. 2018 Jan 1;29(1):84-91. (3) Ye B. et al. High-throughput sequencing of the immune repertoire in oncology: Applications for clinical diagnosis, monitoring, and immunotherapies [J] . Cancer Lett. 2018 Mar 1;416:42-56. (4) Keane C. et al. The T-cell Receptor Repertoire Influences the Tumor Microenvironment and Is Associated with Survival in Aggressive B-cell Lymphoma [J] . Clin Cancer Res, 2017, 23(7):1820-1828. (5) 曹雪涛.医学免疫学. [M] .第9版.北京：人民卫生出版社.2018-07 (6) Lauren Sompayrac. How the Immune System Works [M] .FIFth EdItIon.2016. (7) the production mechanism and show utility in noninvasive prenatal testing. Proc Natl Acad Sci USA. May 29;115(22):E5106-E5114.（2018） (8) Horlbeck MA, et al. Nucleosomes impede Cas9 access to DNA in vivo and in vitro. eLife 5:e12677. (2016)]]></content>
      <categories>
        <category>知识沉淀</category>
        <category>NGS</category>
      </categories>
      <tags>
        <tag>免疫组库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[cromwell SGE配置]]></title>
    <url>%2F0002.%E5%BC%80%E5%8F%91-04.pipeline%2Fpipeline-frameworks-cromwell-SGE%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[支持平台cromwell安装后可以直接进行单机，单节点的任务投递。1java -jar cromwell.jar run pipeline.wdl -i pipeline.json 但是 cromwell 不仅支持本地计算机任务调度，同时支持集群/云计算作业管理系统。cromwell针对包含 SGE、AWS、Docker 在内的等多种平台和容器均提供了配置文件模板，可以通过简单的配置实现基于各平台的调度。 Cloud Providers AWS: Amazon Web Services (documentation) TES: is a backend that submits jobs to a server with protocol defined by GA4GH (documentation) PAPIv2: Google Pipelines API backend (version 2!) (documentation) Containers Docker: an example backend that only runs workflows with docker in every command Singularity: run Singularity containers locally (documentation) Singularity+Slurm: An example using Singularity with SLURM (documentation) TESK is the same, but intended for Kubernetes. See the TES docs at the bottom. udocker: to interact with udocker locally documentation udocker+Slurm: to interact with udocker on SLURM (documentation) Workflow Managers HtCondor: a workload manager at UW-Madison (documentation) LSF: the Platform Load Sharing Facility backend (documentation) SGE: a backend for Sungrid Engine (documentation) slurm: SLURM workload manager (documentation) 集群配置SGE官方针对不同的集群/云作业管理系统提供了相关的配置文件（https://github.com/broadinstitute/cromwell/tree/develop/cromwell.example.backends），但是本质都是讲调度命令嵌入其中。**作业调度系统的配置文件并非完整的配置文件，必须添加到 [cromwell.examples.conf](https://github.com/broadinstitute/cromwell/blob/develop/cromwell.example.backends/cromwell.examples.confhttps://github.com/broadinstitute/cromwell/blob/develop/cromwell.example.backends/cromwell.examples.conf) 的 backend 部分才可以正常工作** 1234567891011121314151617181920212223242526272829303132333435363738394041424344# cromwell.sge.config# 完整配置文件include required(classpath(&quot;application&quot;))backend &#123;default = SGE providers &#123; SGE &#123; actor-factory = &quot;cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory&quot; config &#123; concurrent-job-limit = 5 runtime-attributes = &quot;&quot;&quot; Int cpu = 1 Float? memory_gb String? sge_queue String? sge_project &quot;&quot;&quot; submit = &quot;&quot;&quot; qsub \ -terse \ -V \ -b y \ -N $&#123;job_name&#125; \ -wd $&#123;cwd&#125; \ -o $&#123;out&#125;.qsub \ -e $&#123;err&#125;.qsub \ -pe smp $&#123;cpu&#125; \ $&#123;&quot;-l mem_free=&quot; + memory_gb + &quot;g&quot;&#125; \ $&#123;&quot;-q &quot; + sge_queue&#125; \ /usr/bin/env bash $&#123;script&#125; &quot;&quot;&quot; kill = &quot;qdel $&#123;job_id&#125;&quot; check-alive = &quot;qstat -j $&#123;job_id&#125;&quot; job-id-regex = &quot;(\\d+)&quot; &#125; &#125; &#125;&#125; 使用sge进行任务投递1java -Dconfig.file=cromwell.examples.conf -jar /home/liubo4/software/cromwell-84.jar run helloword.wdl -i helloword.wdl.input.json SGE&amp;docker1234567891011121314151617181920212223242526272829303132333435363738394041424344# cromwell.sge.docker.config# 完整配置文件include required(classpath(&quot;application&quot;))backend &#123;default = SGE providers &#123; SGE &#123; actor-factory = &quot;cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory&quot; config &#123; concurrent-job-limit = 5 runtime-attributes = &quot;&quot;&quot; Int cpu = 1 Float? memory_gb String? sge_queue String? sge_project &quot;&quot;&quot; submit = &quot;&quot;&quot; qsub \ -terse \ -V \ -b y \ -N $&#123;job_name&#125; \ -wd $&#123;cwd&#125; \ -o $&#123;out&#125;.qsub \ -e $&#123;err&#125;.qsub \ -pe smp $&#123;cpu&#125; \ $&#123;&quot;-l mem_free=&quot; + memory_gb + &quot;g&quot;&#125; \ $&#123;&quot;-q &quot; + sge_queue&#125; \ /usr/bin/env bash $&#123;script&#125; &quot;&quot;&quot; kill = &quot;qdel $&#123;job_id&#125;&quot; check-alive = &quot;qstat -j $&#123;job_id&#125;&quot; job-id-regex = &quot;(\\d+)&quot; &#125; &#125; &#125;&#125; referenceCromwell ReadtheDoc SGECromwell Example Backends[https://blog.csdn.net/tanzuozhev/article/details/120629170]]]></content>
      <categories>
        <category>编程拾慧</category>
        <category>任务调度</category>
        <category>pipeline</category>
        <category>python</category>
        <category>framework</category>
      </categories>
      <tags>
        <tag>流程开发</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[docker - 常用命令 - 查看日志]]></title>
    <url>%2F0002.%E5%BC%80%E5%8F%91-04.pipeline%2Fpipeline-container-docker-%E6%9F%A5%E7%9C%8B%E6%97%A5%E5%BF%97%2F</url>
    <content type="text"><![CDATA[官方资料docker logs 使用1$ docker logs [OPTIONS] CONTAINER Options Name, shorthand Default Description –details Show extra details provided to logs –follow , -f Follow log output –since Show logs since timestamp (e.g. 2013-01-02T13:23:37Z) or relative (e.g. 42m for 42 minutes) –tail , -n all Number of lines to show from the end of the logs –timestamps , -t Show timestamps –until Show logs before a timestamp (e.g. 2013-01-02T13:23:37Z) or relative (e.g. 42m for 42 minutes) 示例 docker logs 命令，可以跟踪容器的日志并且输出日志的时间 1docker logs -f -t busybox 写入指定容器在某时间段的日志 1docker logs --since=&quot;2022-09-19T01:00:00&quot; --until &quot;2022-09-21T09:40:00&quot; busybox &amp;&gt;file.txt 将最近1分钟的日志写到file.txt文件 1docker logs --since 50m busybox &amp;&gt;file.txt 查看实时日志，仅仅显示最新的100条日志数据 1docker logs -f -t --tail 100 busybox &amp;&gt;file.txt]]></content>
      <categories>
        <category>镜像</category>
        <category>云计算</category>
        <category>pipeline</category>
        <category>container</category>
      </categories>
      <tags>
        <tag>流程开发</tag>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[箱线图的详细解析]]></title>
    <url>%2F0001.%E7%9F%A5%E8%AF%86-07.%E5%B8%B8%E8%A7%81%E7%BB%9F%E8%AE%A1%E5%90%91%E5%9B%BE%E8%A1%A8%E8%AF%A6%E8%A7%A3%2Fboxplot%2F</url>
    <content type="text"><![CDATA[箱线图介绍箱形图是一张图表，能很好地指示数据中的值如何分布，尽管与直方图或密度图相比，箱线图似乎是原始的，但它们具有占用较少空间的优势，这在比较许多组或数据集之间的分布时非常有用。上图箱线图，箱线图是一个能够通过5个数字来描述数据的分布的标准方式，这5个数字包括：最小值，第一分位，中位数，第三分位数，最大值，箱线图能够明确的展示离群点的信息，同时能够让我们了解数据是否对称，数据如何分组、数据的峰度； 箱线图是一种基于五位数摘要（“最小”，第一四分位数（Q1），中位数，第三四分位数（Q3）和“最大”）显示数据分布的标准化方法。 中位数（Q2 / 50th百分位数）：数据集的中间值； 第一个四分位数（Q1 / 25百分位数）：最小数（不是“最小值”）和数据集的中位数之间的中间数； 第三四分位数（Q3 / 75th Percentile）：数据集的中位数和最大值之间的中间值（不是“最大值”）； 四分位间距（IQR）：第25至第75个百分点的距离； 晶须（蓝色显示） 离群值（显示为绿色圆圈） 最大值：Q3 + 1.5 * IQR 范围内的最大值（剔除异常值） 最小值：Q1 -1.5 * IQR 范围内的最小值（剔除异常值） 特殊注意点如果会发现，箱线图中IQR相同，但是我们做箱线图时，图中的两条虚线(上下边缘到上下四分位的线)长度经常不同。上下边缘的定义应该是上下四分位数加1.5IQR范围内数据的最大最小值，也就是说IQR并不是个定值，而是个取值范围。 1234567891011121314举例[1，20，20，20，30，30，35]，n=7，可以容易看出其各项参数：中位数：20下四分位数：位置【（n+1）/4=2】，值=20上四分位数：位置【（n+1）*3/4=6】，值30IQR：30-20=10其中上下四分位数的求值，如果数组为奇数，可以通过举例的方式直接求得，但是如果数组为偶数，假设下四分位求得的位置为2.25，并且第二位为2，第三位为3，那么下四分位的值为：2*（2.25-2）+3*（3-2.25）=2*（0.25）+3*（0.75）=2.75再来看上下边缘，根据公式（Q1-1.5IQR或Q3+1.5IQR），我们可以得到相应值。下边缘=10上边缘=40但是我们观察数据可以发现，最大值为35，所以上边缘只能取到35。最小值为1，下边缘可以取到10。所以作图上虚线长度为35，尔下虚线长度为10。另外我们可以确定1是异常值。 箱线图和概率密度图关系对于一个近似正态数据，整天分布图和箱线图的概率分布关系如下图 ， 参考信息wikipedia统计学知识门户 - 箱线图如何深刻理解箱线图（boxplot）]]></content>
      <categories>
        <category>知识沉淀</category>
        <category>可视化</category>
      </categories>
      <tags>
        <tag>箱线图</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[微信公众号接入chatgpt]]></title>
    <url>%2F0001.%E7%9F%A5%E8%AF%86-08.IT%E5%90%91%E7%9F%A5%E8%AF%86%E6%9D%82%E8%AE%B0%2F%E5%BE%AE%E4%BF%A1%E5%85%AC%E4%BC%97%E5%8F%B7%E6%8E%A5%E5%85%A5chatgpt%2F</url>
    <content type="text"></content>
      <categories>
        <category>知识沉淀</category>
        <category>IT</category>
      </categories>
      <tags>
        <tag>线程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[概念解析 线程进程核心cpu]]></title>
    <url>%2F0001.%E7%9F%A5%E8%AF%86-08.IT%E5%90%91%E7%9F%A5%E8%AF%86%E6%9D%82%E8%AE%B0%2F%E6%A6%82%E5%BF%B5%E8%A7%A3%E6%9E%90%20-%20cpu%E7%BA%BF%E7%A8%8B%E8%BF%9B%E7%A8%8B%E8%A7%A3%E6%9E%90%2F</url>
    <content type="text"><![CDATA[概念解析物理 cpu 数（physical cpu）指主板上实际插入的 cpu 硬件个数（socket）。（但是这一概念经常被泛泛的说成是 cpu 数，这很容易导致与 core 数，processor 数等概念混淆，所以此处强调是物理 cpu 数）。 由于在主板上引入多个 cpu 插槽需要更复杂的硬件支持（连接不同插槽的 cpu 到内存和其他资源），通常只会在服务器上才这样做。在家用电脑中，一般主板上只会有一个 cpu 插槽。 核心（core）一开始，每个物理 cpu 上只有一个核心（a single core），对操作系统而言，也就是同一时刻只能运行一个进程/线程。 为了提高性能，cpu 厂商开始在单个物理 cpu 上增加核心（实实在在的硬件存在），也就出现了双核心 cpu（dual-core cpu）以及多核心 cpu（multiple cores），这样一个双核心 cpu 就是同一时刻能够运行两个进程/线程的。 多线程技术（simultaneous multithreading）和 超线程技术（hyper–threading/HT）本质一样，是为了提高单个 core 同一时刻能够执行的多线程数的技术（充分利用单个 core 的计算能力，尽量让其“一刻也不得闲”）。simultaneous multithreading 缩写是 SMT，AMD 和其他 cpu 厂商的称呼。 hyper–threading 是 Intel 的称呼，可以认为 hyper–threading 是 SMT 的一种具体技术实现。 在类似技术下，产生了如下等价术语： 虚拟 core： virtual core 逻辑 processer： logical processor 线程：thread 所以可以这样说：某款采用 SMT 技术的 4 核心 AMD cpu 提供了 8 线程同时执行的能力；某款采用 HT 技术的 2 核心 Intel cpu 提供了 4 线程同时执行的能力。 系统信息查询Linux1234567891011121314151617181920212223# 查看物理 cpu 数：cat /proc/cpuinfo| grep "physical id"| sort| uniq| wc -l# 查看每个物理 cpu 中 核心数(core 数)：cat /proc/cpuinfo | grep "cpu cores" | uniq#查看总的逻辑 cpu 数（processor 数）：cat /proc/cpuinfo| grep "processor"| wc -l#查看 cpu 型号：cat /proc/cpuinfo | grep name | cut -f2 -d: | uniq -c# 判断 cpu 是否 64 位：# 检查 cpuinfo 中的 flags 区段，看是否有 lm （long mode） 标识# lscpu 命令可以同时看到上述信息。比如：lscpu---$ CPU(s): 24$ On-line CPU(s) list: 0-23$ Thread(s) per core: 2$ Core(s) per socket: 6$ Socket(s): 2]]></content>
      <categories>
        <category>知识沉淀</category>
        <category>IT</category>
      </categories>
      <tags>
        <tag>线程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[查看系统信息命令]]></title>
    <url>%2F0003.%E5%BA%94%E7%94%A8-02.Linux%2FLinux-%E5%B8%B8%E7%94%A8shell%E5%91%BD%E4%BB%A4-%E6%9F%A5%E7%9C%8B%E7%B3%BB%E7%BB%9F%E4%BF%A1%E6%81%AF%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[命令简介 命令 简介 uname -a 查看内核/操作系统/CPU信息 head -n 1 /etc/issue 查看操作系统版本 cat /proc/cpuinfo 查看CPU信息 hostname 查看计算机名 lspci -tv 列出所有PCI设备 lsusb -tv 列出所有USB设备 lsmod 列出加载的内核模块 env 查看环境变量资源 free -m 查看内存使用量和交换区使用量 df -h 查看各分区使用情况 du -sh &lt;目录名&gt; 查看指定目录的大小 grep MemTotal /proc/meminfo 查看内存总量 grep MemFree /proc/meminfo 查看空闲内存量 uptime 查看系统运行时间、用户数、负载 cat /proc/loadavg 查看系统负载磁盘和分区 mount column -t 查看挂接的分区状态 fdisk -l 查看所有分区 swapon -s 查看所有交换分区 hdparm -i /dev/hda 查看磁盘参数(仅适用于IDE设备) dmesg grep IDE 查看启动时IDE设备检测状况网络 ifconfig 查看所有网络接口的属性 iptables -L 查看防火墙设置 route -n 查看路由表 netstat -lntp 查看所有监听端口 netstat -antp 查看所有已经建立的连接 netstat -s 查看网络统计信息进程 ps -ef 查看所有进程 top 实时显示进程状态用户 w 查看活动用户 id &lt;用户名&gt; 查看指定用户信息 last 查看用户登录日志 cut -d: -f1 /etc/passwd 查看系统所有用户 cut -d: -f1 /etc/group 查看系统所有组 crontab -l 查看当前用户的计划任务服务 chkconfig –list 列出所有系统服务 chkconfig –list grep on 列出所有启动的系统服务程序 rpm -qa 查看所有安装的软件包 du -sh 查看指定目录的大小 grep MemTotal /proc/meminfo 查看内存总量 grep MemFree /proc/meminfo 查看空闲内存量 uptime 查看系统运行时间、用户数、负载 cat /proc/loadavg 查看系统负载磁盘和分区 mount column -t 查看挂接的分区状态 fdisk -l 查看所有分区 swapon -s 查看所有交换分区 hdparm -i /dev/hda 查看磁盘参数(仅适用于IDE设备) dmesg \ grep IDE 查看启动时IDE设备检测状况网络 ifconfig 查看所有网络接口的属性 iptables -L 查看防火墙设置 route -n 查看路由表 netstat -lntp 查看所有监听端口 netstat -antp 查看所有已经建立的连接 netstat -s 查看网络统计信息进程 ps -ef 查看所有进程 top 实时显示进程状态用户 w 查看活动用户 id 查看指定用户信息 last 查看用户登录日志 cut -d\ -f1 /etc/passwd 查看系统所有用户 cut -d\ -f1 /etc/group 查看系统所有组 crontab -l 查看当前用户的计划任务服务 chkconfig –list 列出所有系统服务 chkconfig –list \ grep on 列出所有启动的系统服务程序 rpm -qa 查看所有安装的软件包 cat /proc/cpuinfo 查看CPU相关参数的linux系统命令 cat /proc/partitions 查看linux硬盘和分区信息的系统信息命令 cat /proc/meminfo 查看linux系统内存信息的linux系统命令 cat /proc/version 查看版本，类似uname -r cat /proc/ioports 查看设备io端口 cat /proc/interrupts 查看中断 cat /proc/pci 查看pci设备的信息 cat /proc/swaps 查看所有swap分区的信息 查看linux系统版本信息（Oracle Linux、Centos Linux、Redhat Linux、Debian、Ubuntu） 查看Linux系统版本的命令（3种方法） cat /etc/issue，此命令也适用于所有的Linux发行版。 123 [root@S-CentOS home]# cat /etc/issue CentOS release 6.5 (Final) Kernel \r on an \m lsb_release -a，即可列出所有版本信息： 123 [root@S-CentOS ~]# lsb_release -a LSB Version: :base-4.0-amd64:base-4.0-noarch:core-4.0-amd64:core-4.0-noarch:graphics-4.0-amd64:graphics-4.0-noarch:printing-4.0-amd64:printing-4.0-noarch Distributor ID: CentOS cat /etc/redhat-release，这种方法只适合Redhat系的Linux： 12 [root@S-CentOS home]# cat /etc/redhat-release CentOS release 6.5 (Final) |系统|发行版本|–|内核版本、位数||-|-|-|-||RedHat| cat /etc/issue| cat /etc/redhat-release| lsb_release -a||CentOS| cat /etc/issue| cat /etc/centos-release| cat /proc/version||Debian| cat /etc/issue| cat /etc/debian_version| cat /proc/version||Ubuntu| cat /etc/issue| cat /etc/lsb_release| cat /proc/version||Oracle| cat /etc/issue| cat /etc/oracle-release| lsb_release -a| 查看Linux内核版本命令（两种方法） cat /proc/version uname -a 查看CPU信息 12cat /proc/cpuinfo # 查看CPU信息cat /proc/cpuinfo |grep "model name" &amp;&amp; cat /proc/cpuinfo |grep "physical id" # CPU大小 ps 说明：Linux下可以在/proc/cpuinfo中看到每个cpu的详细信息。但是对于双核的cpu，在cpuinfo中会看到两个cpu。常常会让人误以为是两个单核的cpu。其实应该通过Physical Processor ID来区分单核和双核。而Physical Processor ID可以从cpuinfo或者dmesg中找到. flags 如果有 ht 说明支持超线程技术 判断物理CPU的个数可以查看physical id 的值，相同则为 内存大小 1cat /proc/meminfo |grep MemTotal 查看内存使用量和交换区使用量 1free -m 硬盘大小 1fdisk -l |grep Disk 查看各分区使用情况 1df -h 列出所有PCI设备 1lspci -tv 列出所有USB设备 1lsusb -tv 查看环境变量资源 1env]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Shell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CNV检测-CNVKit]]></title>
    <url>%2F0003.%E5%BA%94%E7%94%A8-03.software%2FSoftware-CNV%E6%A3%80%E6%B5%8B-CNVKit%2F</url>
    <content type="text"><![CDATA[Publish原文链接: CNVkit Genome-Wide Copy Number Detection and Visualization from Targeted DNA Sequencing-annotatedcnvkit.readthedocs原理介绍PPTCNVKit若干算法问题详解 检测原理在上述文献、PPT中已经有相对详细的介绍，此处不再进行展开。针对目前一些可视化的需求，补充记录一些关于CNVKit结果展示的相关功能和使用方法。 可视化CNVKit提供了比较详细的可视化模块三种可视化的主体绘图命令如下:123cnvkit.py scatter -hcnvkit.py diagram -hcnvkit.py heatmap -h scatter说明1cnvkit.py scatter Sample.cnr -s Sample.cns 扩展参数1234567python cnvkit.py scatter Sample.cnr -s Sample.cns-c chr7 # 指定参考基因组，可以进一步细化指定具体范围chr5:100-50000000-g BRAF,MET # 指定展示的基因，多个基因使用逗号（","）分隔-v Sample.vcf #``` ### 示例 /jdfstj1/B2C_COM_P1/PipeAdmin/02.software/Conda/bin/python /jdfstj1/B2C_COM_P1/PipeAdmin/04.Pipeline/01.AIO.v2.01/bin/cnvkit.py scatter ../Analyze/cnv/pancancer689__DX1790_huangrenhua_20S8320498R_20B8320498__Cancer.markdup.cnr -s ../Analyze/cnv/pancancer689__DX1790_huangrenhua_20S8320498R_20B8320498__Cancer.markdup.cns -c chr3:150000000-220000000 -g EIF4A2,TIPARP,TP631234567![image](Software-CNV检测-CNVKit/demo_test1.jpg)## diagram ```shellpython cnvkit.py diagram Sample.cnr -s Sample.cns-t 2 # 对判定为CNV的阈值进行调整（仅达到阈值的基因会进行单独展示） 异常处理记录Qt载入失败 具体报错示例如下： 12345678Reinstalling the application may fix this problem.Showing 918 probes and 1 selected genes in region chr15This application failed to start because it could not find or load the Qt platform plugin &quot;xcb&quot;in &quot;&quot;.Available platform plugins are: eglfs, minimal, minimalegl, offscreen, xcb.Reinstalling the application may fix this problem. 处理解决方案]]></content>
      <categories>
        <category>NGS</category>
        <category>编程拾慧</category>
        <category>software</category>
        <category>Linux</category>
        <category>Python</category>
      </categories>
      <tags>
        <tag>CNV</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Cancer statistics, 2022]]></title>
    <url>%2F0001.%E7%9F%A5%E8%AF%86-05.Paper%2FCancer_statistics-2022%2F</url>
    <content type="text"><![CDATA[参考文献Cancer incidence and mortality in China, 2016Latest global cancer data: Cancer burden rises to 19.3 million new cases and 10.0 million cancer deaths in 2020 Cancer statistics, 2022]]></content>
      <categories>
        <category>NGS</category>
        <category>文献</category>
      </categories>
      <tags>
        <tag>cancer population</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[NGS数据去重/压缩]]></title>
    <url>%2F0003.%E5%BA%94%E7%94%A8-03.software%2Fsoftware-%E8%BD%AF%E4%BB%B6%E7%BB%BC%E8%BF%B0-%E5%8E%8B%E7%BC%A9%E5%8E%BB%E9%87%8D%2F</url>
    <content type="text"><![CDATA[参考文献[1]. UMIErrorCorrect and UMIAnalyzer: Software for Consensus Read Generation, Error Correction,and Visualization Using Unique Molecular Identifiers # 全文下载 [2]. UMIc: A Preprocessing Method for UMI Deduplication and Reads Correction # 全文下载 [3]. UMI-linked consensus sequencing enables phylogenetic analysis of directed evolution # 全文下载 [4]. Gencore: an efficient tool to generate consensus reads for error suppressing and duplicate removing of NGS data # 全文下载 [5]. UMI-Gen: A UMI-based read simulator for variant calling evaluation in paired-end sequencing NGS libraries # 全文下载 [6]. UMI-tools: modeling sequencing errors in Unique Molecular Identifiers to improve quantification accuracy # 全文下载 [7]. Alignment-free clustering of UMI tagged DNA molecules # 全文下载 [8]. UMI-VarCal: a new UMI-based variant caller that efficiently improves low-frequency variant detection in paired-end sequencing NGS libraries # 全文下载 [9]. High efficiency error suppression for accurate detection of low-frequency variants # 全文下载]]></content>
      <categories>
        <category>NGS</category>
        <category>software</category>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>去重</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[建库试剂盒供应商]]></title>
    <url>%2F0004.%E8%A1%8C%E4%B8%9A-%E4%B8%8A%E4%B8%8B%E6%B8%B8%E8%A1%8C%E4%B8%9A%E5%85%AC%E5%8F%B8%2F0004-01.%E5%BB%BA%E5%BA%93%E8%AF%95%E5%89%82%E7%9B%92%E4%BE%9B%E5%BA%94%E5%95%86%2F</url>
    <content type="text"><![CDATA[诺唯赞VAHTS Universal Pro DNA Library Prep Kit for MGI产品优势 高效修复DNA损伤: 有效修复碱基损伤、切刻、缺口和3’端封闭等问题 建库效率高: 文库转化率和扩增产出双提升 模板兼容性广: 兼容 gDNA，FFPE DNA，cfDNA，Amplicons 等样本 兼容不同质量样本: DNA修复酶对不同DIN值的FFPE DNA均有优异的表现 建库耗时短: 单个文库构建仅需90min IDTIntegrated DNA Technologies (IDT; Coralville, IA, USA),KAPA Biosystems (Wilmington, MA, USA)New England Biolabs (NEB; Ipswich, MA, USA).]]></content>
      <categories>
        <category>行业公司</category>
        <category>供应商</category>
      </categories>
      <tags>
        <tag>酶切建库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[捕获探针]]></title>
    <url>%2F0004.%E8%A1%8C%E4%B8%9A-%E4%B8%8A%E4%B8%8B%E6%B8%B8%E8%A1%8C%E4%B8%9A%E5%85%AC%E5%8F%B8%2F0004-01.%E6%8D%95%E8%8E%B7%E6%8E%A2%E9%92%88%E4%BE%9B%E5%BA%94%E5%95%86%2F</url>
    <content type="text"><![CDATA[IDT官网：简介：相关业务备注： Agilent官网：简介：相关业务备注： TWIST官网：简介：相关业务备注： ROCHE官网：简介：相关业务备注： 艾吉泰康官网：简介：相关业务备注： 迈基诺官网：简介：相关业务备注： 影子基因官网：www.yingzigene.com简介：相关业务备注： 何因官网：简介：相关业务备注： 纳昂达官网：简介：相关业务备注： 博瑞迪官网：简介：相关业务备注： 伯科生物官网：简介：相关业务备注：]]></content>
      <categories>
        <category>行业公司</category>
        <category>供应商</category>
      </categories>
      <tags>
        <tag>探针合成</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[NGS-重测序常见的可视化方案]]></title>
    <url>%2F0001.%E7%9F%A5%E8%AF%86-01.NGS%2FNGS-%E9%87%8D%E6%B5%8B%E5%BA%8F%E5%B8%B8%E8%A7%81%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%96%B9%E6%A1%88%2F</url>
    <content type="text"><![CDATA[SVSV可视化的原理如下 基于重构染色体，将设计融合的reads进行比对后，从bam中看到的结果如下 CNV使用CNVkit的话，本身提供了比较多的可视化组件1cnvkit.py scatter -s TR_95_T.cn&#123;s,r&#125; -c chr12:50000000-80000000 -g CDK4,MDM2 也可以展示整体染色体层面的CNV变异结果1cnvkit.py diagram -s Sample.cns Sample.cnr]]></content>
      <categories>
        <category>NGS</category>
        <category>变异检测</category>
      </categories>
      <tags>
        <tag>变异检测</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度和检测限测算]]></title>
    <url>%2F0001.%E7%9F%A5%E8%AF%86-01.NGS%2FNGS-Dev-%E6%B7%B1%E5%BA%A6%E5%92%8C%E6%A3%80%E6%B5%8B%E9%99%90%E6%B5%8B%E7%AE%97%2F</url>
    <content type="text"><![CDATA[测序深度或覆盖深度被定义为覆盖给定核苷酸位置的reads的数量，生物信息学工具极其依赖于足够的覆盖深度，以便灵敏和特异地检测变异。覆盖深度与稳定检测样本的变异之间的关系很简单，因为更高数量的高质量测序数据为特定位置的碱基检测供了信心，无论来自测序样本的碱基调用是否是与参考碱基相同（未识别出变异）或者是非参考碱基（识别出变异）。然而，许多因素会影响所需的深度，包括测序平台，目标区域的序列复杂性（与基因组的多个区域具有同源性的区域、重复序列元件或假基因的存在以及GC富集区域）。此外，用于目标富集的文库制备和需要评估的变异类型也是重要的考虑因素。因此，必须在检测开发和验证过程中系统地评估每个 NGS 测试的覆盖模型。 深度下限(仅考虑检测灵敏性达标)NGS检测过程中，为了报证临床检测的灵敏性，需要位点深度达到一定水平，才能报证一些低频的变异位点可以被有效的检出。ps：由于该深度仅关注灵敏性，因此针对部分错误富集区域特异性可能并不理想 3条/正负支持 0.01 0.015 0.02 0.025 0.03 0.035 0.04 0.045 0.05 300 49.22% 74.59% 88.25% 94.72% 97.63% 98.93% 99.52% 99.78% 99.90% 400 67.56% 88.20% 95.93% 98.59% 99.51% 99.83% 99.94% 99.98% 99.99% 500 80.16% 94.66% 98.58% 99.62% 99.89% 99.97% 99.99% 100.00% 100.00% 600 88.15% 97.59% 99.50% 99.89% 99.98% 99.99% 100.00% 100.00% 100.00% 700 93.00% 98.90% 99.82% 99.97% 99.99% 100.00% 100.00% 100.00% 100.00% 800 95.88% 99.50% 99.93% 99.99% 100.00% 100.00% 100.00% 100.00% 100.00% 900 97.57% 99.77% 99.98% 100.00% 100.00% 100.00% 100.00% 100.00% 100.00% 1000 98.56% 99.89% 99.99% 100.00% 100.00% 100.00% 100.00% 100.00% 100.00% 4条/正负支持 0.01 0.015 0.02 0.025 0.03 0.035 0.04 0.045 0.05 300 32.34% 61.92% 81.62% 91.88% 96.57% 98.57% 99.40% 99.74% 99.89% 400 52.87% 81.56% 93.84% 98.06% 99.39% 99.80% 99.93% 99.98% 99.99% 500 69.64% 91.79% 98.04% 99.53% 99.88% 99.97% 99.99% 100.00% 100.00% 600 81.49% 96.49% 99.37% 99.88% 99.98% 99.99% 100.00% 100.00% 100.00% 700 89.13% 98.52% 99.79% 99.97% 99.99% 100.00% 100.00% 100.00% 100.00% 800 93.76% 99.37% 99.93% 99.99% 100.00% 100.00% 100.00% 100.00% 100.00% 900 96.47% 99.73% 99.98% 100.00% 100.00% 100.00% 100.00% 100.00% 100.00% 1000 98.01% 99.88% 99.99% 100.00% 100.00% 100.00% 100.00% 100.00% 100.00% 5条/正负支持 0.01 0.015 0.02 0.025 0.03 0.035 0.04 0.045 0.05 300 17.56% 45.21% 69.91% 85.59% 93.72% 97.43% 98.98% 99.60% 99.84% 400 35.69% 69.85% 88.91% 96.47% 98.96% 99.70% 99.91% 99.97% 99.99% 500 54.24% 85.47% 96.44% 99.22% 99.83% 99.96% 99.99% 100.00% 100.00% 600 69.78% 93.59% 98.93% 99.83% 99.97% 99.99% 100.00% 100.00% 100.00% 700 81.18% 97.33% 99.69% 99.96% 99.99% 100.00% 100.00% 100.00% 100.00% 800 88.79% 98.92% 99.91% 99.99% 100.00% 100.00% 100.00% 100.00% 100.00% 900 93.55% 99.57% 99.97% 100.00% 100.00% 100.00% 100.00% 100.00% 100.00% 1000 96.38% 99.83% 99.99% 100.00% 100.00% 100.00% 100.00% 100.00% 100.00% 测算数据下载表1234567891011121314151617supportRead=4 # 支持reads条数下限depthList=c(300,400,500,600,700,800,900,1000)freqList=c(0.01,0.015,0.02,0.025,0.03,0.035,0.04,0.045,0.05)result=""for (depth in depthList)&#123; for (freq in freqList)&#123; p=0 for (k in supportRead:depth) &#123; if(!is.nan(choose(depth,k)*((1-freq)**(depth-k))*(freq**k)*(1-0.5**(k-1))))&#123; p=p+(choose(depth,k)*((1-freq)**(depth-k))*(freq**k)*(1-0.5**(k-1))) &#125; &#125; tmp=paste(depth,freq,p) print(tmp) result=paste(result,tmp,sep="\n") &#125;&#125; Establishing Criteria for Depth of Sequencing一个理想的测序深度，应该是可以有效的对阳性位点和阴性位点进行区分的。而不是单纯的保证灵敏性，忽略假阳性带来的风险。因此在上述基础上，可以延伸出来一个更为立项的深度预测模型，预先对检测体系进行评估,获得整个检测体系的错误率。然后根据错误率、预期的检测下限和假阳性或假阴性结果的容忍度来估计所需的覆盖深度。（这些性能参数可以并且应该在开发阶段进行评估，以帮助定义验证的接受标准。例如，对于给定比例的突变等位基因，可以使用二项分布方程来确定检测到最小数量等位基因的概率：整体方法，和前面的一样，只是在这个部分中，我们需要把错误率（引发假阳性）纳入考量，我们把阳性位点和阴性位点的检出频率分布放到一起，可以得到如下图关系,我们要做的，就是寻找一个深度，可以让对应的理想性能的灵敏性和特异性都能满足我们预期的性能需求。123456789101112131415161718192021222324252627282930313233343536373839404142library("ggplot2")Totaldepth=5000 # 预期深度，控制评估的深度上限ErrorRate=0.004 # 基于产品首批高深度产品，评估获得LOBLoDRate = 0.01 # 产品预期的检测性能，后期LOD需要单独进行评估补充。CI = 0.9 # 对性能结果要求的置信区间。depthlist = c(1:Totaldepth)ErrorReadNumlist = depthlistSupportReadNumlist = depthlistfor (depth in 1:Totaldepth) &#123; ErrorReadNum = round(depth * ErrorRate) # 考虑错误率统计过程中本身已经是错误率的最高值，所以不再进行二项分布扩展。 #ErrorReadNum = qbinom(CI,depth,ErrorRate) # 错误率考虑上95置信区间。 SupportReadNum = qbinom(1-CI,depth,LoDRate) ErrorReadNumlist[depth] = ErrorReadNum SupportReadNumlist[depth] = SupportReadNum&#125;Difference= SupportReadNumlist - ErrorReadNumlisttype=c(rep(paste('Error:',ErrorRate),times=Totaldepth),rep(paste('Detect:',LoDRate),times=Totaldepth),rep('Difference',times=Totaldepth))depth=c(depthlist,depthlist,depthlist)read_num=c(ErrorReadNumlist,SupportReadNumlist, Difference)data=data.frame(type, depth,read_num)# ggplot(data,aes(x=depth,y=read_num,cluster=type,color=type))+geom_line()+geom_hline(yintercept = 2)## 确定目标深度Target_depth="Not Find "for(i in Totaldepth:1)&#123; if(Difference[i]&lt;2)&#123; Target_depth = i+1 break &#125;&#125;## 绘图ggplot(data,aes(x=depth,y=read_num,cluster=type,color=type)) + geom_line() + geom_hline(yintercept = 2) + annotate(geom = 'text', x= 2000, y = 30, label = paste("Totaldepth=5000\nLOBRate=0.0013\nLODRate=0.01\nCI=0.99\nDepth=",Target_depth )) 示例结果如下图 202303补充《Standardization of Sequencing Coverage Depth in NGS: Recommendation for Detection of Clonal and Subclonal Mutations in Cancer Diagnostics》 发布计算方法和上述介绍原理一致，同时开源了计算代码和在线工具。 参考文献[1]. Guidelines for Validation of Next-Generation Sequencing-Based Oncology Panels [2]. Standardization of Sequencing Coverage Depth in NGS: Recommendation for Detection of Clonal and Subclonal Mutations in Cancer Diagnostics]]></content>
      <categories>
        <category>NGS</category>
        <category>肿瘤检测</category>
        <category>标准化</category>
      </categories>
      <tags>
        <tag>基础概念</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LoD LoB LoQ]]></title>
    <url>%2F0001.%E7%9F%A5%E8%AF%86-01.NGS%2FNGS-Dev-LoDLoBLoQ%2F</url>
    <content type="text"><![CDATA[目前LOB、LoD概念整体上已经基本达成共识，但是在具体概念细节上，其实还是存在一些会引发歧义的内容。因此在展示LOB指标的时候，还是尽量在结果展示阶段，说明具体的计算方法或参考资料。以避免引起一些不必要的问题。 Limit of Blank (LoB)A blank sample was defined as a sample containing a single genotype (i.e pre-transplantation samples). The average measured background of a second genotype in blank samples detected in the informative markers for all theoretical combinations of pairs was defined as LOB. [1] LoB is the highest apparent analyte concentration expected to be found when replicates of a blank sample containing no analyte are tested. LoB = mean-blank + 1.645(SD-blank) . The raw analytical signal is preferable for establishing LoB as analysers may report all signal values below a certain fixed limit as “zero concentration”).[2] Limit of Detection (LoD)LOD was defined as the lowest chimerism value that could be reliably distinguished from the LOB.[1] LoD is the lowest analyte concentration likely to be reliably distinguished from the LoB and at which detection is feasible. LoD is determined by utilising both the measured LoB and test replicates of a sample known to contain a low concentration of analyte. LoD = LoB + 1.645(SD low concentration sample) [2] Limit of Quantitation (LoQ)The LOQ represents the lowest measured MC according to a predefined accuracy goal. We defined the LOQ as the lowest level of MC measured at or above the LOD with CV&lt;20%. [1] LoQ is the lowest concentration at which the analyte can not only be reliably detected but at which some predefined goals for bias and imprecision are met. The LoQ may be equivalent to the LoD or it could be at a much higher concentration.[2] 参考文献 Development and performance of a next generation sequencing (NGS) assay for monitoring of mixed chimerism Limit of Blank, Limit of Detection and Limit of Quantitation [高通量测序技术，主编：李金明] Method Validation Essentials, Limit of Blank, Limit of Detection and Limit of Quantitation FDA-SUMMARY OF SAFETY AND EFFECTIVENESS DATA Protocols of determining limits of detection and limits of quantitation for quantitative analytical methods FDA:Test Developers of Serology Tests that Detect or Correlate to Neutralizing Antibodies]]></content>
      <categories>
        <category>NGS</category>
        <category>肿瘤检测</category>
        <category>标准化</category>
      </categories>
      <tags>
        <tag>基础概念</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[概念-方法学相关指标]]></title>
    <url>%2F0001.%E7%9F%A5%E8%AF%86-01.NGS%2FNGS-Dev-%E6%96%B9%E6%B3%95%E5%AD%A6%E7%9B%B8%E5%85%B3%E6%8C%87%E6%A0%87%2F</url>
    <content type="text"><![CDATA[准确性相关指标测序得到的变异与参考变异结果的一致性程度 ，主要是测试方法得到的检测结果与已知样本变异信息的一致性匹配程度。 对准确性的评价可通过两部分进行：①通过检测已知序列的人基因组DNA（如标准细胞株）来评价测序本身的准确性；②通过检测临床样本进行验证，包括含有疾病相关突变的样本和含有与待检突变相同突变类型的样本。可将高通量测序与另一方法同时检测临床样本来评价，比较高通量测序与另一方法之间结果的差异，不一致的结果再用第三种方法确认，通过PPA和NPA来评价定性测定的准确度。不同变异类型的比较方法如下，且将这些方法作为比较方法之前，均应先经过性能验证或性能确认： 变异类型 不同变异类型验证对比方法 SNV、InDel Sanger测序、等位基因特异性PCR、SNP arrays 等 CNV 实时荧光定量PCR、荧光原位杂交FISH、微阵列比较基因组杂交CGH SV 实时荧光定量PCR、荧光原位杂交FISH 通过检测阳性符合率（PPA，positive percent agreement），阴性符合率（NPA，negative percent agreement），技术阳性预测值（TPPV，technical positive predictive value） 来评价检测准确性，并预先对这3个指标进行检测阈值设定，以评估测试是否可以达到预期目标。需要对这3个指标阈值设定可接受的具体值以及95%置信区间的下限值。并且需要针对受检的每种变异类型均需要设定。 准确性计算可以先对数据进行统计整理，构建如下表格：然后基于该表格，按下述方式进行计算。 阳性符合率（PPA）PPA定义为检测到的已知变异数量（TP，true posities）除以总的已知变异数量（TP加FN，false negatives），需要对每种变异类型计算PPA。PPA会影响假阴性检出频率。 PPA=A / (A+C)；根据PPA计算假阴性率，FN=1-PPA阴性符合率（NPA）NPA定义为检测到的真阴性（TN，true negative）结果除以变异检测到的总野生变异（wt，wild-type=TN+FP）数量，需要对每种变异类型计算NPA。NPA会影响假阳性检出频率。 NPA=D / (D+B)；根据NPA计算假阳性率，FP=1-NPA技术阳性预测值（TPPV，technical positive predictive value） TPPV=A / (A+B) 精密度（Precision）精密度指同一样本在多次检测中结果的一致程度，即在相同产品及实验条件下，相同样本相同批次以及不同批次的测序实验得到的结果之间的一致性程度，主要利用重复性（repeatability）和重现性（reproducibility）进行分析。以上检测不需要金标准测序结果，只需要测试者计算每个样本相同检测结果的重复比例即可。无检出或者无效检出重复比例也需要进行统计。数据精确性可通过生物学重复（至少三个样本）和技术重复（相同样本不同barcode）来评估；另有标准性材料建议使用3个参考样本，并且每个检测3-5次，在相同以及不同的测试批次中，这个建议作为评估平台精确度的最低要求。评价重复性或重现性可以计算突变定性结果（阳性/阴性）的一致率，也可计算突变频率的变异系数，检测质量指标是否存在较大波动也可作为判断精密度的参考。 重复性（repeatability）重复性指在同一条件下（相同环境、相同操作人员、相同检测流程、相同仪器）对相同或者短期内相似样本进行多次检测，评估变异检测结果的一致性程度。 重现性（reproducibility）重现性指由不同操作人员、不同仪器（相同型号）进行相同或者短期内相似样本进行检测。评估变异检测结果的一致性程度。建议同一样本应包括不同操作人员、不同试剂批号、不同测序仪的检测。 检测限（limit of detection）通常情况下，最低检测限定义为至少可以检出95%阳性变异的最低检测浓度以及变异频率。可以通过进行样本混合构建具有不同频率百分比范围的梯度稀释样本来评估对于不同变异类型的最低检测限。每种变异类型均需对LoD进行性能确认。 考虑到不同方法对特殊样本（肿瘤样本，ctDNA，嵌合体）检测局限性，有时Sanger测序并不是所有变异检测的金标准。所以在这种情况下，可通过稀释样本混合实验（dilution of samples with known allele frequencies）来测试NGS技术检验变异能力的动态范围。 计算LoD可采用两种方式：①直接用符合率为100%的最低MAF作为LoD，这种方式相对比较简单；②采用统计学分析来计算95%的LoD水平。例如，Probit分析是最常见的分析方法，具体做法是：首先列出每个MAF水平的阳性结果数量和总测定样本数量，然后将每个MAF水平检测的阳性百分比转换至Probit，最后构建Probit值与MAF的回归线图，查Probit表确定C95。 建立LoD时，需同时建立空白限（Limit of blank，LoB），LoB的定义是一定概率下测量空白样本时可能得到的最高检测结果，在高通量测序检测中指对阴性结果（野生型位点）检测可能得到的最高检测结果。ps 在FDA审评中，检测下限也作为分析敏感性进行评价。 目前LOB、LoD概念整体上已经基本达成共识，但是在具体概念细节上，还存在歧义。因此在展示LOB指标的时候，还是尽量在结果展示阶段，说明具体的计算方法或参考资料。以避免引起一些不必要的歧义。]]></content>
      <categories>
        <category>NGS</category>
        <category>肿瘤检测</category>
        <category>标准化</category>
      </categories>
      <tags>
        <tag>基础概念</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[细胞遗传学位置]]></title>
    <url>%2F0001.%E7%9F%A5%E8%AF%86-01.NGS%2F%E7%BB%86%E8%83%9E%E9%81%97%E4%BC%A0%E5%AD%A6%E4%BD%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[细胞遗传学位置（Cytogenetic Location）：细胞遗传学定位是遗传学家描述基因的细胞遗传位置的一种标准方法。在大多数情况下，基因的位置是通过描述被染色的染色体上某一条“带”来定位。例如17q12。如果不知道精确位置，也可以描述为一组“带”，例如17q12-q21。 这一组数字和字母的组合提供可一个基因在染色体上的“地址”。这个地址由以下几部分组成： 基因所在的染色体编号。“地址”的第一位数字或者字母用来指示基因所在的染色体。第1-22号常染色体使用染色体的编号来指示染色体。性染色体则使用X或者Y。 染色体的臂。每条染色体上都有且仅有一个狭窄部位，称为着丝粒。着丝粒将染色体分为两部分又称为两条臂。习惯约定较短的臂（称为短臂）代码为p，较长的臂（称为长臂）代码为q。基因“地址”中的第二位是指示基因所在染色体的臂。例如5q指代的是5号染色体长臂，Xp则指代X染色体短臂。 基因在短臂p或者长臂q上的位置。当染色体被某些方式染色时，会呈现出明暗相间的条带。基因定位，正是基于这些条带。基因在染色体臂上的定位一般由两位数字组成，分别指代基因所在的“区”和“带”。有时在两位数字之后还会有小数点和其后的数字，指示的是在“带”区域中的“亚带”。数字随着基因与着丝粒距离的增加而增大。例如：14q21指示为14号染色体2区1带，比14q22指示的14号染色体2区2带距离着丝粒更近。 有些时候，细胞遗传学定位中还会出现“cen”和“ter”标识。“cen”代表基因非常接近着丝粒。例如，16pcen代表16号染色体近着丝粒位置。“ter”则代表基因非常接近臂的末端。例如14qter指示的是14号染色体长臂末端。（ 有时定位标识中还会出现“tel”。“tel”指代的是位于每条染色体臂末端的端粒。“ter”和“tel”指代的是相同的意思，均为染色体臂的末端）。 利用常规显带技术人类中期染色体显示的带纹数较少，一套单倍体染色体带纹总数仅有320条带。70年代后期，由于相关技术的改进，可以从早中期、前中期、晚前期细胞得到更长、带纹更多的染色体。一套单倍体染色体即可显示550～850条或更多的带纹，即在原有的带纹上分出更多的带，这种染色体称为高分辨显带染色体（high resolution banding chromosome，HRBC）。由于染色体高分辨显带能为染色体及其所发生的畸变的提供更多细节，所以有助于我们发现更多、更细微的染色体结构的异常，使染色体发生畸变的断裂点定位更准确，因此这一技术在临床细胞遗传学、分子细胞遗传学检查上，或在肿瘤染色体的研究和基因定位上都具有非常广泛的应用价值。 “人类细胞遗传学高分辨命名的国际体制（ISCN1981）”的模式图 ，标示了更多条带的高分辨带型。 高分辨显带的命名方法是在原带之后加“.”，并在“.”之后写新的带号，称为亚带 。 例如：原来的1p31带被分为三个亚带，命名为1p31.1，1p31.2，1p31.3，即表示1号染色体短臂3区1带第1亚带、第2亚带、第3亚带。1p31.3再分时，称为次亚带，则直接在 后面加序号，写为1p31.31，1p31.32，1p31.33。 染色体芯片分析技术是提高染色体核型分析精确度的一种新方法 ，可以结合 G 显带 、FISH 技术诊断亚显微结构的染色体重复或缺失。 ISCN2009介绍的常用方法有：aCGH（array based comparative genomic hybridization）和MLPA（Multiplex ligation dependent Probe amplification），这两种方法都是基于将目的基因作为芯片对全基因进行扫描，分析结果中提示的缺失或重复 DNA 片段进一步选择相关染色体区域探针FISH 确认分析，这样将染色体缺失或重复片段的分辨率提高到了 Mb 水平。 相应的描述方式发生了一些变化 。 如果阵列检测整个染色体结果正常 ，则可以如下方式描述。arr(1-22,X)×2表示正常女性 ；arr(1-22)×2,(XY)×1表示正常男性 ；如果阵列检测结果异常则只列出异常染色体。 性染色体异常要先列出，然后是其他异常的常染色体按照染色体序号排列，同时描述出现异常的核苷酸。需要注意的是在arr 和第一个异常染色体之间加一个空格。 如：arr 20q13.2q13.33(51,001,876.62,375,085)x1，阵列分析显示20号染色体长臂缺失部分节段，从1区3带2亚带到1区3带 3亚带3次亚带，括号内是缺失的核苷酸链]]></content>
      <categories>
        <category>知识沉淀</category>
        <category>NGS</category>
        <category>肿瘤检测</category>
        <category>缩略语</category>
      </categories>
      <tags>
        <tag>基础概念</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[肿瘤突变负荷检测及临床应用中国专家共识（2022年版）]]></title>
    <url>%2F0001.%E7%9F%A5%E8%AF%86-03.standard%2F%E5%85%B1%E8%AF%86-%20ctDNA%E9%AB%98%E9%80%9A%E9%87%8F%E6%B5%8B%E5%BA%8F%E4%B8%B4%E5%BA%8A%E5%AE%9E%E8%B7%B5%E4%B8%93%E5%AE%B6%E5%85%B1%E8%AF%86-2022%E5%B9%B4%E7%89%88%2F</url>
    <content type="text"><![CDATA[原文链接ctDNA高通量测序临床实践专家共识（2022年版）官网ctDNA高通量测序临床实践专家共识（2022年版）文档 随着液态活检的快速发展，采用体液对患者的分子特征进行分析成为可能，特别是基于循环肿瘤DNA（circulating tumor DNA，ctDNA）的高通量测序（nextgeneration sequencing，NGS）技术，因其无创或微创、检测时间短、能够反映瘤内和转移灶异质性、可动态监测治疗疗效等优势而在临床得到越来越广泛的应用。与肿瘤组织样本相比，采用ctDNA进行基因检测在样本收集和处理、检测技术要求、结果解读和临床应用等方面存在诸多不同，且目前国内尚缺少ctDNA基因检测标准，因此限制了其在临床上的规范应用。为此，中国抗癌协会肿瘤标志专业委员会组织国内肿瘤临床、病理、检验、生物信息分析和高通量检测领域专家，参考国内外 ctDNA 临床应用共识、指南和最新文献，结合国内外现有的高通量测序技术要求和临床实践，从ctDNA的生物学特征、临床应用价值和范围、高通量检测的标准要求及其未来发展趋势等方面提出本共识，以促进ctDNA NGS的健康规范发展。 专家共识 ctDNA是由肿瘤细胞主动分泌或肿瘤细胞在凋亡或坏死过程中释放入循环系统的 DNA片段；其丰度受多种因素影响，波动较大，在内外因素特别是治疗压力下，其携带的生物信息可能会发生演变，且受正常细胞胚系变异或克隆性造血细胞体系突变干扰，在临床检测和报告解读过程中应特别注意。 目前已有临床证据支持ctDNA NGS检测可应用于肺癌、乳腺癌、前列腺癌、卵巢癌等晚期实体肿瘤的伴随诊断，但涉及的驱动基因及其变异类型与相应分析体系均有严格限定，若超适应症应用时，建议与患者就检测必要性、检测费用以及局限性等内容进行充分知情。ctDNA NGS 检测已被国内外专家共识或指南建议作为多种晚期恶性肿瘤组织基因检测的替代方式，但依据其分析结果实时制定临床治疗策略时仍需高级别循证证据支持。 晚期实体肿瘤分子靶向或免疫检查点抑制剂治疗开始后，基于NGS检测的ctDNA水平定量和动态变化分析，有望成为新兴的疗效评估途径。ctDNA MRD 检测是全新的个性化技术应用领域，尚难以建立通用性技术标准，亟待通过大样本、多中心、前瞻性的临床试验验证其临床效用。ctDNA NGS 检测在临床上用于靶向或免疫治疗评估和分层时，建议就检测价值、局限性和费用等进行充分知情。 在临床环境中，ctDNA NGS检测可用于识别分子靶向治疗的耐药机制，尤其对于疑难复杂的肿瘤患者，该结果有助于后续的治疗选择决策。免疫检查点抑制剂治疗获得性耐药机制复杂，治疗选择压力下的肿瘤亚克隆演进仅为其部分原因，ctDNA靶向测序、全外显子和（或）全基因组检测仅作为其转化研究工具之一。 ctDNA NGS检测实验室质量管理需贯穿全程，ctDNA 收集、样本处理和自动化过程应按照标准化和临床验证程序进行，最大程度防范因操作差异而引发的假阴性可能。样本采集建议采用含细胞稳定剂的抗凝管，尽快完成血浆分离，提取的 cfDNA 建议在24 h内进行后续检测，否则，置于-30 ℃至-15 ℃下储存并避免反复冻融。 ctDNA NGS检测应根据项目需求选择技术路线，可依据检测基因数量及覆盖范围大小选择不同测序策略。在进行基于 ctDNA 的超高灵敏度突变检测时，建议使用分子标签技术和优化对应的生物信息分析设置，以降低由于测序平台随机误差导致的假阳性结果；建议通过建立测序噪音和克隆性造血背景库的方法降低克隆性造血及背景噪音带来的影响。 专家共识：ctDNA NGS临床检测报告应包含受检者基本信息、样本信息、实验室信息、检测项目、检测结果及变异解读、检测方法的实验室内部验证结果、检测局限性及不确定性以及进一步检测的建议等内容。实验室应建立报告 SOP，建议根据国内外文献、共识指南、临床试验证据和实践对检出的肿瘤基因突变进行分类或分级报告。 ctDNA检测优劣势ctDNA是由肿瘤细胞主动分泌或肿瘤细胞在凋亡或坏死过程中释放入循环系统的 DNA片段；其丰度受多种因素影响，波动较大，在内外因素特别是治疗压力下，其携带的生物信息可能会发生演变，且受正常细胞胚系变异或克隆性造血细胞体系突变干扰，在临床检测和报告解读过程中应特别注意。，长度 132~145 bp，半衰期较短（一般&lt;2 h）。 ctDNA 水平一般呈动态变化，且受多种因素影响： 肿瘤病理组织类型、部位、分期、肿瘤负荷等因素可影响ctDNA的释放。在肿瘤负荷较轻、特定部位（如颅内肿瘤）和特定组织学（如胶质瘤），以及增殖、凋亡和/或血管化水平较低的肿瘤患者中，ctDNA水平通常较低［1］。 大量其他来源的 DNA 干扰。如其他正常细胞或白细胞来源的DNA，与ctDNA一起被称为游离 DNA（cell free DNA，cfDNA）。多种生理和病理因素，如怀孕、剧烈运动、外伤、炎症、心肌梗死、自身免疫性疾病和急性中风等也会影响cfDNA的释放［2⁃3］。 克隆性造血细胞产生的cfDNA携带的基因突变信息可能会干扰ctDNA检测结果［4］。此外，其突变基因还受不同内外因素影响（包括年龄、吸烟、种族和肿瘤治疗等），如ASXL1突变在吸烟者中富集，DNA损伤应答（DNA⁃damaged response，DDR）基因（TP53、PPM1D、CHEK2）突变在接受放射、铂类药物和拓扑异构酶Ⅱ抑制剂治疗的肿瘤患者中更为常见［5］。 ctDNA 半衰期较短（一般&lt;2 h），不同采样时间可能影响 ctDNA 含量［6］。 药物治疗影响 ctDNA 含量。有研究显示，非小细胞肺癌（non⁃small cell lung cancer，NSCLC）患者接受酪氨酸激酶抑制剂（tyrosine kinase inhibitor，TKI）治疗后，ctDNA 含量在 24 h 达到顶峰，随后快速降低，提示药物也会影响ctDNA含量 与组织学检测相比，优势： ctDNA检测具有无创或微创，可反复取材，收集、处理和分析报告周转时间（turn⁃around time，TAT）短等优势； 能克服肿瘤空间异质性，可相对全面地实时反映患者的肿瘤分子特征； 与单独组织活检相比，血浆ctDNA还可增加驱动基因突变检出率。 劣势： 外周血 ctDNA含量较低，容易造成临床检测假阴性结果； 由于胚系变异或克隆性造血突变的存在，若未用白细胞作为对照，也可能导致假阳性结果； 不同肿瘤患者或同一患者不同时段释放入血的ctDNA量也存在差异，也给 ctDNA 的临床检测和结果解读带来困难。 ctDNA NGS检测的标准质控标准干实验”涵盖生物信息学分析流程各步骤，该阶段质控应对 最低测序深度（血浆cfDNA标本的NGS有效测序深度应达到1000×以上，并应在80%以上的目标区域达到这个深度 平均测序深度、 覆盖均一性、 鸟嘌呤和胞嘧啶（guanine and cytosine，GC）含量、 碱基识别质量值、 比对质量值、 在靶率等作出相应要求并遵照执行。 生信方法的要求基于分子标签技术的数据质控和数据分析ctDNA 检测常出现 PCR 重复序列比例高（50%~90%）、PCR 扩增和测序错误等引入的背景噪音高等问题。同时，血液中白细胞的克隆性造血突变也影响ctDNA变异的鉴定。基于分子标签技术的分析流程可帮助有效去除背景噪音，配对白细胞样本或背景库的建立可有效去除来自白细胞中克隆性造血突变引入的假阳性，提升ctDNA检测的准确性。UMI通过给每一条原始DNA片段加上一段特有的标签序列，经文库构建和PCR扩增后一同测序，根据不同的标签序列可以区分不同来源的 DNA 模板，分辨源于 PCR 扩增及测序过程中随机错误产生的假阳性突变，识别cfDNA中真正来源于肿瘤的突变，从而提高检测灵敏度和特异性，可达0.1%的检出限［71］。一般推荐1条UMI对应并至少3条reads。 测序噪音控制建立背景库过滤噪声的大致思路是通过使用相同测序流程的样本估计基因组坐标对应区域或碱基的测序错误率，然后使用假设检验等统计方法比较目标样本对应坐标的突变频率是否显著高于估计得出的测序错误率。通过对患者肿瘤样本建立背景库过滤噪声的常用方法有 SiNVICT 和OutLyzer。SiNVICT使用肿瘤样本计算出一段基因组区域的噪音水平，通过计算信噪比的方法过滤潜在的假阳性结果［72］。OutLyzer 通过计算目标突变附近200 bp 内的每个坐标的测序错误率，并通过多次Thompson’s Tau Test 过滤离群噪声［73］。在可用较为充足的情况下，使用多个患者的正常样本建立背景库能较准确地估计噪声水平。其中代表方法有Mutect1/Mutect2（GATK）［74］，该法使用不少于 40 个没有肿瘤细胞污染的正常样本构建背景库。在此基础上发展的基于不同算法的构建背景库算法，如基于二项分布的 TNER、基于高斯分布模拟的 IDES［75］、基于泊松分布的AmpliSolve［76］等，均可有效提升突变检出准确率和召回率，去除背景噪音。 克隆性造血变异过滤RAZAVI 等［77］发现ctDNA 存在大量未知来源的基因变异（variants ofunknown source，VUSo），大部分 VUSo 突变丰度&lt;1%，与样本测序深度、DNA 起始量、位点的测序深度、样本染色体拷贝数无相关性，且有很好的重复性结果。在白细胞样本中也检测到大量的 VUS，且与年龄呈正相关，证实了 ctDNA 中的大部分 VUSo 来自白细胞，并非来源于测序背景噪音，这部分 VUSo 突变被定义为克隆性造血突变。因此，在不同 ctDNA 高通量测序临床应用场景，需要考虑 ctDNA 中包含有源于白细胞的克隆性造血突变。ctDNA 检测时通过配对的白细胞样本，或者汇集整合检测到的克隆性造血突变，构建克隆性造血突变背景库，可有效帮助去除克隆性造血造成的假阳性。 数据的储存与管理随着测序深度的增加，ctDNA 下机数据有效安全的储存需要配备相应的基础设施。高通量测序会生成大量文件，下机的fastq或bam 原始文件、vcf结果文件等需长期保存，同时应保存好测序过程中完整的日志文件，以便区分高通量测序分析软件版本信息、追溯异常结果。诊断实验室应保存相关数据至少 3年，并在相关技术人员的支持下制定数据备份计划和恢复计划。 变异结果的命名规则对基因变异的描述应遵循一定的原则和规范，推荐参考人类基因组变异协会命名指南（www.hgvs.org，登录日期2022年6月22日），转录本的选择建议采用基因座参考基因组序列数据库（Locus reference genomic；www.lrg⁃sequence.org，登录日期2022年6月22日）界定的转录本或多个国际数据库公认的主要转录本。对遗传性肿瘤相关基因变异的说明，建议以美国医学遗传学学院（American college of medical genetics and genomics）指南为标准，在检测结果中列出具体的变异位点信息，包括基因名称、所参考的人类基因组版本号、转录本参考序列版本号、核苷酸变异、氨基酸变异、外显子/内含子序号、等位基因杂合性、染色体编号及坐标等。 ctDNA NGS检测临床应用展望临床研究表明基于 ctDNA NGS检测的bMSI （blood MSI）和bTMB（blood TMB）具有免疫检查点抑制剂疗效预测价值［35］。FDA于2020年8 月 26 日 批 准 基 于 ctDNA NGS 数 据 拟 合 bMSI 和bTMB 的试剂盒用于泛实体瘤多个靶向药物及免疫检查点抑制剂的伴随诊断。但 bMSI 和 bTMB 在临床应用中受多种因素影响，如样本采集时间、检测 panel基因覆盖范围、panel 大小、测序深度、生信算法及阈值设定等，因此还需要更多的前瞻性临床研究证据支持 ctDNA NGS 数据拟合的 bMSI 和 bTMB 的临床应用前景。 HRD 状态可通过同源重组相关基因突变（homologous recombinationrepair，HRR）检测和基因瘢痕（genomic scar，GS）检测两种方式判断。两者均采用 HRR 联合 GS 检测策略评估 HRD 状态。由于基于 ctDNA NGS 对 GS 进行评估技术上具有巨大挑战，所以目前基于 ctDNA NGS 检测进行HRD 评估主要集中在 HRR 信号通路基因 SNP 检测，但国内外对 HRR 基因的定义尚缺乏统一标准；另外，随着检测技术的进步，基于 ctDNA NGS 评估 GS逐渐成为可能，但目前基于 ctDNA NGS 检测评估HRD尚有巨大的探索空间。 ctDNA中包含核酸序列、位点突变、拷贝数、甲基化、MSI、序列长度、序列丰度、出现的时空模式、在基因组上的位置特征、起始位点和终止位点特征等丰富的生物信息，如肿瘤 cfDNA 片段的长度分布、核小体位置与正常细胞显著不同，这些信息可应用于肿瘤筛查和诊断中。CancerSEEK 的癌症早筛项目通过监测 ctDNA 中 16 个基因的突变及血清 8 个蛋白质生物标志物水平，并构建 Logistic回归模型，结果该模型在 8种肿瘤共包含 1 005例患者中的敏感性为69%~98%，特异性为 99%［84］。最新的应用机器学习分类器辅助的深度甲基化测序能检测到 52%~81%临床分期为ⅠA~Ⅲ期的患者，且检测限低至万分之一，显示了更高的敏感性和特异性（特异性为 96%，95%CI：93%~98%）［85］。 总之，基于 ctDNA NGS 检测的 bTMB 具有免疫检查点抑制剂疗效预测价值，但仍有诸多因素影响bTMB 检测在临床中的应用，未来还需开展更多的前瞻性临床研究。机器学习等人工智能算法不仅能降低假阳性、提高灵敏度，还能有效整合多维度的异质信息进行全面分析，大幅提高 ctDNA 数据效力，是ctDNA数据分析的主要方向]]></content>
      <categories>
        <category>NGS</category>
        <category>肿瘤检测</category>
        <category>共识</category>
      </categories>
      <tags>
        <tag>中国癌症防治杂志</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[c语言异常核查-gdb处理core文件]]></title>
    <url>%2F0002.%E5%BC%80%E5%8F%91-07.c%2Fc%E8%AF%AD%E8%A8%80%E5%BC%82%E5%B8%B8%E6%A0%B8%E6%9F%A5-gdb%E5%A4%84%E7%90%86core%E6%96%87%E4%BB%B6%2F</url>
    <content type="text"></content>
      <categories>
        <category>编程拾慧</category>
        <category>C++</category>
      </categories>
      <tags>
        <tag>编程语言</tag>
        <tag>C</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C语言程序异常处理]]></title>
    <url>%2F0002.%E5%BC%80%E5%8F%91-07.c%2Fc%E8%AF%AD%E8%A8%80%E7%A8%8B%E5%BA%8F%E7%9B%B8%E5%85%B3%E5%BC%82%E5%B8%B8%E5%A4%84%E7%90%86%E5%8A%9E%E6%B3%95%2F</url>
    <content type="text"></content>
      <categories>
        <category>编程拾慧</category>
        <category>C++</category>
      </categories>
      <tags>
        <tag>编程语言</tag>
        <tag>C</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python-包-python-openpyxl处理excel]]></title>
    <url>%2F0002.%E5%BC%80%E5%8F%91-02.python%2FPython-%E5%8C%85-python-openpyxl%E5%A4%84%E7%90%86excel%2F</url>
    <content type="text"><![CDATA[workbook = openpyxl.load_workbook(‘test.xlsx’) # 返回一个workbook数据类型的值print(workbook.sheetnames) # 打印Excel表中的所有表 sheet = workbook[‘Sheet1’] # 获取指定sheet表sheet = workbook.active # 获取活动表print(sheet) cell1 = sheet[‘A1’] # 获取A1单元格的数据cell2 = sheet[‘B7’] # 获取B7单元格的数据 cell2 = sheet[‘B7’].value # 另一种写法正确示范cell1.value获取单元格A1中的值cell2.value获取单元格B7中的值print(cell1.value,cell2.value) # 姓名 18]]></content>
      <categories>
        <category>编程拾慧</category>
        <category>python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Office处理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[微信小程序开发测试]]></title>
    <url>%2F0002.%E5%BC%80%E5%8F%91-06.miniprogram%2FWechat-miniprogram-%E5%85%A5%E9%97%A8%E6%B5%8B%E8%AF%95%2F</url>
    <content type="text"><![CDATA[参考资料开发者文档云函数 小程序的开发开发小程序的基础介绍小程序的代码构成1234.json 后缀的 JSON 配置文件.wxml 后缀的 WXML 模板文件.wxss 后缀的 WXSS 样式文件.js 后缀的 JS 脚本逻辑文件 JSON 配置JSON 是一种数据格式，并不是编程语言，在小程序中，JSON扮演的静态配置的角色。 我们可以看到在项目的根目录有一个 app.json 和 project.config.json，此外在 pages/logs 目录下还有一个 logs.json，我们依次来说明一下它们的用途。 小程序配置 app.jsonapp.json 是当前小程序的全局配置，包括了小程序的所有页面路径、界面表现、网络超时时间、底部 tab 等。QuickStart 项目里边的 app.json 配置内容如下：123456789101112&#123; &quot;pages&quot;:[ &quot;pages/index/index&quot;, &quot;pages/logs/logs&quot; ], &quot;window&quot;:&#123; &quot;backgroundTextStyle&quot;:&quot;light&quot;, &quot;navigationBarBackgroundColor&quot;: &quot;#fff&quot;, &quot;navigationBarTitleText&quot;: &quot;Weixin&quot;, &quot;navigationBarTextStyle&quot;:&quot;black&quot; &#125;&#125; 我们简单说一下这个配置各个项的含义: pages字段 —— 用于描述当前小程序所有页面路径，这是为了让微信客户端知道当前你的小程序页面定义在哪个目录。window字段 —— 定义小程序所有页面的顶部背景颜色，文字颜色定义等。其他配置项细节可以参考文档 小程序的配置 app.json 。 工具配置 project.config.json通常大家在使用一个工具的时候，都会针对各自喜好做一些个性化配置，例如界面颜色、编译配置等等，当你换了另外一台电脑重新安装工具的时候，你还要重新配置。 考虑到这点，小程序开发者工具在每个项目的根目录都会生成一个 project.config.json，你在工具上做的任何配置都会写入到这个文件，当你重新安装工具或者换电脑工作时，你只要载入同一个项目的代码包，开发者工具就自动会帮你恢复到当时你开发项目时的个性化配置，其中会包括编辑器的颜色、代码上传时自动压缩等等一系列选项。 其他配置项细节可以参考文档 开发者工具的配置 。 页面配置 page.json这里的 page.json 其实用来表示 pages/logs 目录下的 logs.json 这类和小程序页面相关的配置。 如果你整个小程序的风格是蓝色调，那么你可以在 app.json 里边声明顶部颜色是蓝色即可。实际情况可能不是这样，可能你小程序里边的每个页面都有不一样的色调来区分不同功能模块，因此我们提供了 page.json，让开发者可以独立定义每个页面的一些属性，例如刚刚说的顶部颜色、是否允许下拉刷新等等。 其他配置项细节可以参考文档 页面配置 。 JSON 语法这里说一下小程序里 JSON 配置的一些注意事项。JSON文件都是被包裹在一个大括号中 {}，通过 key-value 的方式来表达数据。JSON的 Key 必须包裹在一个双引号中，在实践中，编写 JSON 的时候，忘了给 Key 值加双引号或者是把双引号写成单引号是常见错误。JSON的值只能是以下几种数据格式，其他任何格式都会触发报错，例如 JavaScript 中的 undefined。123456数字，包含浮点数和整数字符串，需要包裹在双引号中Bool值，true 或者 false数组，需要包裹在方括号中 []对象，需要包裹在大括号中 &#123;&#125;Null 还需要注意的是 JSON 文件中无法使用注释，试图添加注释将会引发报错。 WXML 模板从事过网页编程的人知道，网页编程采用的是 HTML + CSS + JS 这样的组合，其中 HTML 是用来描述当前这个页面的结构，CSS 用来描述页面的样子，JS 通常是用来处理这个页面和用户的交互。 同样道理，在小程序中也有同样的角色，其中 WXML 充当的就是类似 HTML 的角色。打开 pages/index/index.wxml，你会看到以下的内容 和 HTML 非常相似，WXML 由标签、属性等等构成。但是也有很多不一样的地方，我们来一一阐述一下： 标签名字有点不一样 往往写 HTML 的时候，经常会用到的标签是 div, p, span，开发者在写一个页面的时候可以根据这些基础的标签组合出不一样的组件，例如日历、弹窗等等。换个思路，既然大家都需要这些组件，为什么我们不能把这些常用的组件包装起来，大大提高我们的开发效率。 从上边的例子可以看到，小程序的 WXML 用的标签是 view, button, text 等等，这些标签就是小程序给开发者包装好的基本能力，我们还提供了地图、视频、音频等等组件能力。 更多详细的组件讲述参考下个章节 小程序的能力 多了一些 wx:if 这样的属性以及 { { } } 这样的表达式 在网页的一般开发流程中，我们通常会通过 JS 操作 DOM (对应 HTML 的描述产生的树)，以引起界面的一些变化响应用户的行为。例如，用户点击某个按钮的时候，JS 会记录一些状态到 JS 变量里边，同时通过 DOM API 操控 DOM 的属性或者行为，进而引起界面一些变化。当项目越来越大的时候，你的代码会充斥着非常多的界面交互逻辑和程序的各种状态变量，显然这不是一个很好的开发模式，因此就有了 MVVM 的开发模式（例如 React, Vue），提倡把渲染和逻辑分离。简单来说就是不要再让 JS 直接操控 DOM，JS 只需要管理状态即可，然后再通过一种模板语法来描述状态和界面结构的关系即可。 小程序的框架也是用到了这个思路，如果你需要把一个 Hello World 的字符串显示在界面上。 WXML 是这么写 : 1&lt;text&gt;&#123; &#123;msg&#125; &#125;&lt;/text&gt; JS 只需要管理状态即可: 1this.setData(&#123; msg: &quot;Hello World&quot; &#125;) 通过 { { } } 的语法把一个变量绑定到界面上，我们称为数据绑定。仅仅通过数据绑定还不够完整的描述状态和界面的关系，还需要 if/else, for等控制能力，在小程序里边，这些控制能力都用 wx: 开头的属性来表达。 更详细的文档可以参考 WXML WXSS 样式WXSS 具有 CSS 大部分的特性，小程序在 WXSS 也做了一些扩充和修改。 新增了尺寸单位。在写 CSS 样式时，开发者需要考虑到手机设备的屏幕会有不同的宽度和设备像素比，采用一些技巧来换算一些像素单位。WXSS 在底层支持新的尺寸单位 rpx ，开发者可以免去换算的烦恼，只要交给小程序底层来换算即可，由于换算采用的浮点数运算，所以运算结果会和预期结果有一点点偏差。 提供了全局的样式和局部样式。和前边 app.json, page.json 的概念相同，你可以写一个 app.wxss 作为全局样式，会作用于当前小程序的所有页面，局部页面样式 page.wxss 仅对当前页面生效。 此外 WXSS 仅支持部分 CSS 选择器 更详细的文档可以参考 WXSS 。 JS 逻辑交互一个服务仅仅只有界面展示是不够的，还需要和用户做交互：响应用户的点击、获取用户的位置等等。在小程序里边，我们就通过编写 JS 脚本文件来处理用户的操作。12&lt;view&gt;&#123; &#123; msg &#125; &#125;&lt;/view&gt;&lt;button bindtap=&quot;clickMe&quot;&gt;点击我&lt;/button&gt; 点击 button 按钮的时候，我们希望把界面上 msg 显示成 “Hello World”，于是我们在 button 上声明一个属性: bindtap ，在 JS 文件里边声明了 clickMe 方法来响应这次点击操作：12345Page(&#123; clickMe: function() &#123; this.setData(&#123; msg: &quot;Hello World&quot; &#125;) &#125;&#125;) 响应用户的操作就是这么简单，更详细的事件可以参考文档 WXML - 事件 。 此外你还可以在 JS 中调用小程序提供的丰富的 API，利用这些 API 可以很方便的调起微信提供的能力，例如获取用户信息、本地存储、微信支付等。在前边的 QuickStart 例子中，在 pages/index/index.js 就调用了 wx.getUserInfo 获取微信用户的头像和昵称，最后通过 setData 把获取到的信息显示到界面上。更多 API 可以参考文档 小程序的API 。 小程序组件小程序提供了丰富的基础组件给开发者，开发者可以像搭积木一样，组合各种组件拼合成自己的小程序。 就像 HTML 的 div, p 等标签一样，在小程序里边，你只需要在 WXML 写上对应的组件标签名字就可以把该组件显示在界面上，例如，你需要在界面上显示地图，你只需要这样写即可：1&lt;map&gt;&lt;/map&gt; 使用组件的时候，还可以通过属性传递值给组件，让组件可以以不同的状态去展现，例如，我们希望地图一开始的中心的经纬度是广州，那么你需要声明地图的 longitude（中心经度） 和 latitude（中心纬度）两个属性:1&lt;map longitude=&quot;广州经度&quot; latitude=&quot;广州纬度&quot;&gt;&lt;/map&gt; 组件的内部行为也会通过事件的形式让开发者可以感知，例如用户点击了地图上的某个标记，你可以在 js 编写 markertap 函数来处理：1&lt;map bindmarkertap=&quot;markertap&quot; longitude=&quot;广州经度&quot; latitude=&quot;广州纬度&quot;&gt;&lt;/map&gt; 当然你也可以通过 style 或者 class 来控制组件的外层样式，以便适应你的界面宽度高度等等。 更多的组件可以参考 小程序的组件。 小程序api为了让开发者可以很方便的调起微信提供的能力，例如获取用户信息、微信支付等等，小程序提供了很多 API 给开发者去使用。 要获取用户的地理位置时，只需要：1234567wx.getLocation(&#123; type: &apos;wgs84&apos;, success: (res) =&gt; &#123; var latitude = res.latitude // 纬度 var longitude = res.longitude // 经度 &#125;&#125;) 调用微信扫一扫能力，只需要：12345wx.scanCode(&#123; success: (res) =&gt; &#123; console.log(res) &#125;&#125;) 需要注意的是：多数 API 的回调都是异步，你需要处理好代码逻辑的异步问题。 更多的 API 能力见 小程序的API。 不重复造轮子一些微信小程序项目的开源代码。可以进行代码参考和开发逻辑的学习。 一些开源Demohttps://github.com/caochangkui/miniprogram-project1：仿豆瓣电影微信小程序2：微信小程序移动端商城3：Gank微信小程序4：微信小程序高仿QQ应用5：微信中的知乎6：实现一个移动端小商城7：微信小程序demo8： 跑步微信小程序Demo9：简单的v2ex微信小程序10：腾讯云微信小程序11：微信小程序-微票12：微信小程序demo 仿手机淘宝13：一个为微信小程序开发准备的基础骨架14：巴爷微信商城的简单版本15：微信小程序 - 电影推荐16：微信小程序-知乎日报17：微信小程序： 音乐播放器18：使用微信小程序实现分答这款APP的基础功能19：微信小程序开发demo-地图定位20：微信小程序 - 豆瓣电影21：wepy仿微信聊天界面22：仿 「ONE · 一个」 的微信小程序23：微信小程序集成Redux实现的Todo list24： 基于Zhihu Live数据的微信小程序25：微信小程序之小熊の日记26：仿网易云音乐APP的微信小程序27：微信小程序的Flex布局demo28：番茄时钟微信小程序版29：Wafer 服务端 Demo30：微信小程序版聊天室31：微信小程序版简易计算器，适合入门练手32：微信小程序示例一笔到底33：基于面包旅行 API 制作的微信小程序示例34：新闻阅读器35：一个简单的微信小程序购物车DEMO36：微信小程序-公众号热门文章信息流37：通过Node.js实现的妹子照片爬虫微信小程序38：从FlexLayout布局开始学习微信小程序39：HiApp 微信小程序版40：微信小程序的简单尝试41：集美大学图书馆的便捷工具42：微信小程序版妹纸图43：V2ex 微信小程序版44：微信小程序仿百思不得姐45：微信小程序音乐播放器应用46：医药网原生APP的微信小程序DEMO47：微信小程序跟读48：微信小程序瀑布流布局模式49：微信小程序HotApp云笔记50：小程序模仿——网易云音乐51：微信小程序商城demo52：微信小程序版的扫雷53：专注管理时间的微信小程序54：微信小程序版干货集中营55：英雄联盟(LOL)战绩查询56：微信小程序首字母排序选择表57：微信小程序版豆瓣电影58：简单的实现了1024的游戏规则59：微信小程序试玩60：微信小程序逗乐61：一步步开发微信小程序62：一个 meteor 的 React todo list 例子63：微信小程序健康菜谱64： jspapa微信小程序版本65：微信小程序版的CNodeJs中文社区66：LeanCloud 的微信小程序用户登陆Demo67： 微笑话微信小程序68：微信小程序开发的App69：体育新闻微信小程序70：基于Labrador和mobx构建的小程序开发demo]]></content>
      <categories>
        <category>编程拾慧</category>
        <category>miniprogram</category>
      </categories>
      <tags>
        <tag>微信小程序</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基本功能-页面跳转]]></title>
    <url>%2F0002.%E5%BC%80%E5%8F%91-06.miniprogram%2FWechat-miniprogram-%E5%9F%BA%E6%9C%AC%E5%8A%9F%E8%83%BD-%E9%A1%B5%E9%9D%A2%E8%B7%B3%E8%BD%AC%2F</url>
    <content type="text"><![CDATA[参考资料开发者文档云函数 小程序的开发开发小程序的基础介绍小程序的代码构成12345678910111213141516171819202122232425262728293031323334// 保留当前页面，跳转到应用内的某个页面，使用wx.navigateBack可以返回到原页面。// 注意：调用 navigateTo 跳转时，调用该方法的页面会被加入堆栈，但是 redirectTo wx.navigateTo(&#123; url: &apos;page/home/home?user_id=111&apos;&#125;)// 关闭当前页面，返回上一页面或多级页面。可通过 getCurrentPages() 获取当前的页面栈，决定需要返回几层。wx.navigateTo(&#123; url: &apos;page/home/home?user_id=111&apos; // 页面 A&#125;)wx.navigateTo(&#123; url: &apos;page/detail/detail?product_id=222&apos; // 页面 B&#125;)// 跳转到页面 Awx.navigateBack(&#123; delta: 2&#125;)// 关闭当前页面，跳转到应用内的某个页面。wx.redirectTo(&#123; url: &apos;page/home/home?user_id=111&apos;&#125;)// 跳转到tabBar页面（在app.json中注册过的tabBar页面），同时关闭其他非tabBar页面。wx.switchTab(&#123; url: &apos;page/index/index&apos;&#125;)// 关闭所有页面，打开到应用内的某个页面。wx.reLanch(&#123; url: &apos;page/home/home?user_id=111&apos;&#125;) 2. wxml 页面组件跳转（可以通过设置open-type属性指明页面跳转方式）：1234567891011121314// navigator 组件默认的 open-type 为 navigate &lt;navigator url=&quot;/page/navigate/navigate?title=navigate&quot; hover-class=&quot;navigator-hover&quot;&gt;跳转到新页面&lt;/navigator&gt;// redirect 对应 API 中的 wx.redirect 方法&lt;navigator url=&quot;../../redirect/redirect/redirect?title=redirect&quot; open-type=&quot;redirect&quot; hover-class=&quot;other-navigator-hover&quot;&gt;在当前页打开&lt;/navigator&gt;// switchTab 对应 API 中的 wx.switchTab 方法&lt;navigator url=&quot;/page/index/index&quot; open-type=&quot;switchTab&quot; hover-class=&quot;other-navigator-hover&quot;&gt;切换 Tab&lt;/navigator&gt;// reLanch 对应 API 中的 wx.reLanch 方法&lt;navigator url=&quot;../../redirect/redirect/redirect?title=redirect&quot; open-type=&quot;redirect&quot; hover-class=&quot;other-navigator-hover&quot;&gt;关闭所有页面，打开到应用内的某个页面&lt;/navigator&gt;// navigateBack 对应 API 中的 wx.navigateBack 方法&lt;navigator url=&quot;/page/index/index&quot; open-type=&quot;navigateBack&quot; hover-class=&quot;other-navigator-hover&quot;&gt;关闭当前页面，返回上一级页面或多级页面&lt;/navigator&gt;]]></content>
      <categories>
        <category>编程拾慧</category>
        <category>miniprogram</category>
      </categories>
      <tags>
        <tag>微信小程序</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python包-time]]></title>
    <url>%2F0002.%E5%BC%80%E5%8F%91-02.python%2FPython-%E5%8C%85-time%2F</url>
    <content type="text"><![CDATA[Python中的时间间隔是以秒为单位的浮点小数。每个时间戳都以自从1970年1月1日午夜（历元）经过了多长时间来表示。时间戳单位最适于做日期运算。但是1970年之前的日期就无法以此表示了。太遥远的日期也不行，UNIX和Windows只支持到2038年。 常见用法 获取当前时间 123$ localtime = time.localtime(time.time())$ print "本地时间为 :", localtime本地时间为 : time.struct_time(tm_year=2016, tm_mon=4, tm_mday=7, tm_hour=10, tm_min=3, tm_sec=27, tm_wday=3, tm_yday=98, tm_isdst=0) 格式化时间 12345$ import time$ localtime = time.asctime( time.localtime(time.time()) )$ print "本地时间为 :", localtime本地时间为 : Thu Apr 7 10:05:21 2016]]></content>
      <categories>
        <category>编程拾慧</category>
        <category>python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python包-sqlite3]]></title>
    <url>%2F0002.%E5%BC%80%E5%8F%91-02.python%2FPython-%E5%8C%85-sqlite3%2F</url>
    <content type="text"><![CDATA[官方文档sqlite3 Documentation 123456789101112131415161718import sqlite3con = sqlite3.connect(&apos;example.db&apos;)cur = con.cursor()# Create tablecur.execute(&apos;&apos;&apos;CREATE TABLE stocks (date text, trans text, symbol text, qty real, price real)&apos;&apos;&apos;)# Insert a row of datacur.execute(&quot;INSERT INTO stocks VALUES (&apos;2006-01-05&apos;,&apos;BUY&apos;,&apos;RHAT&apos;,100,35.14)&quot;)# Save (commit) the changescon.commit()# We can also close the connection if we are done with it.# Just be sure any changes have been committed or they will be lost.con.close()]]></content>
      <categories>
        <category>编程拾慧</category>
        <category>python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python包-PyMongo]]></title>
    <url>%2F0002.%E5%BC%80%E5%8F%91-02.python%2FPython-%E5%8C%85-PyMongo%2F</url>
    <content type="text"><![CDATA[官方文档PyMongo 4.1.1 Documentation 仍然需要启动MongoDB的服务。不适用跨集群的迁移]]></content>
      <categories>
        <category>编程拾慧</category>
        <category>python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Database-TCGA]]></title>
    <url>%2F0001.%E7%9F%A5%E8%AF%86-04.Database%2FDatabase-TCGA%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[pipeline-Serverless_Cloud_Function-入门简介及测试]]></title>
    <url>%2F0003.%E5%BA%94%E7%94%A8-01.cloud_computing%2Fpipeline-SCF-Hello_world%2F</url>
    <content type="text"><![CDATA[测试整体测试过程参考 腾讯云-官方文档 ；腾讯云-控制台 ，进行函数的创建、和函数的配置及触发器配置。测试demo的代码编辑(https://console.cloud.tencent.com/scf/list-detail?rid=1&amp;ns=default&amp;id=helloworld-1650789454&amp;tab=codeTab) 触发服务访问控制台中配置的触发api，启动函数服务。 直接在浏览器地址输入链接，调用服务helloworld-1650789454 在Linxu集群通过访问链接可以调用服务]]></content>
      <categories>
        <category>pipeline</category>
        <category>SCF</category>
      </categories>
      <tags>
        <tag>流程开发</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pipeline-Serverless_Cloud_Function-入门简介及测试]]></title>
    <url>%2F0003.%E5%BA%94%E7%94%A8-01.cloud_computing%2Fpipeline-SCF-%E5%85%A5%E9%97%A8%E7%AE%80%E4%BB%8B%E5%8F%8A%E6%B5%8B%E8%AF%95%2F</url>
    <content type="text"><![CDATA[基本概念无服务器无服务器架构说法的来源可以根据 Mike Roberts 在 Martin Fowler 的博客网站上发表的 无服务器架构 一文中得到解释。 无服务器并不是没有服务器就能够进行计算，而是对于开发者来说，无需了解底层的服务器情况，也能使用到相关资源，因此称为无服务器。 无服务器也可以从更广的角度来识别，针对无需配置和了解底层的服务器就可以直接使用的云服务，在一定程度上也可以称为无服务器。 在云函数 SCF 产品中，我们针对的是无服务器场景中的计算场景。云函数产品提供的是无服务器模式下的 FaaS 能力。 函数即服务函数即服务提供了一种直接在云上运行无状态的、短暂的、由事件触发的代码的能力。 函数即服务和传统应用架构不同，函数服务提供的是事件触发式的运行方式，云函数不是始终运行的状态，而是在事件发生时由事件触发运行，并且在一次运行的过程中处理这一次事件。因此在云函数的代码中，仅需考虑针对一个事件的处理流程，而针对大量事件的高并发处理，由平台实现云函数的多实例并发来支持。 为了实现对高并发的支持，云函数平台提供了自动的弹性伸缩能力，会在有大量请求到来时启动更多实例来处理事件请求，也会在没有事件到来时缩减函数实例甚至到零实例。因此为了匹配自动扩缩能力，需要函数代码使用的是无状态开发方式，即不在云函数的运行内存中保留相关的状态数据并在多次运行时依赖这些状态数据。云函数的状态数据，可以依赖外部的持久存储能力例如云缓存、云数据库、云存储来进行。 触发器和触发源任何可以产生事件，触发云函数执行的均可以被称为触发器或触发源。触发器在本身产生事件后，通过将事件传递给云函数来触发函数运行。 触发器在触发函数时，可以根据自身特点，使用同步或异步方式触发函数。同步方式触发函数时，触发器将等待函数执行完成并获取到函数执行结果；异步方式触发函数时，触发器将仅触发函数而忽略函数执行结果。 腾讯云云函数在和腾讯云的某些产品或服务对接时，也有自身实现的一些特殊方式，例如推（PUSH）模式和拉（PULL）模式。 推模式：触发器主动将事件推送至云函数平台并触发函数运行。 拉模式：云函数平台通过拉取模块，从触发器中拉取到事件并触发云函数运行。 触发事件触发器在触发函数时会将事件传递给云函数。事件在传递时以一个特定的数据结构体现，数据结构格式在传递时均为 JSON 格式，并以函数 event 入参的方式传递给云函数。 触发事件的 JSON 数据内容，在不同的语言环境下将会转换为各自语言的数据结构或对象，无需在代码中自行进行从 JSON 结构到数据结构的转换。例如，在 Python 环境中，JSON 数据内容会转变为一个复杂 dict 对象，即函数的入参 event 就是一个 Python 的复杂 dict 对象。而在 Golang 或 Java 中，入参是一个需要和 event 数据结构可以匹配的对象。更具体的实现方式可以见 开发语言说明。 工作原理函数运行时的实例模型云函数 SCF 将在函数接收到触发请求时为您执行函数。SCF 执行请求的资源为实例，根据函数的配置信息（如内存大小等）进行资源分配，并启动一个或多个实例处理函数请求。SCF 平台负责所有函数运行实例的创建、管理和删除清理操作，用户没有权限对其进行管理。 Serverless 的价值首先，从开发者使用的来说，不用更多的去考虑服务器的相关内容，无需再去考虑服务器的规格大小、存储类型、网络带宽、自动扩缩容问题；同时，也无需再对服务器进行运维了，无需不断的打系统补丁、应用补丁、无需进行数据备份、软件配置等工作了。 其次，Serverless 产品是完全自动化的弹性扩缩容的；在业务高峰时，产品的计算能力、容量自动扩容，承载更多的用户请求，而在业务下降时，所使用的资源也会同时收缩，避免资源浪费。 再次，跟随着完全自动化的弹性所带来的，是全新的计量计费模式；开发者仅需根据使用量来付费，而在深夜无业务量的情况下，不会有空闲资源占用，因此也不会有费用产生。 1、降低运维需求： Serverless 使得应用与服务器解耦，业务上线前无需预估资源，无需进行服务器购买、配置； Serverless 也使得底层运维工作量进一步降低，业务上线后，也无需担忧服务器运维，而是全部交给了云平台或云厂商； 2、降低运营成本： Serverless 的应用是按需执行的。应用只在有请求需要处理或者事件触发时才会被加载运行，在空闲状态下 Serverless 架构的应用本身并不占用计算资源； 而在使用 Serverless 产品时，用户只需要为处理请求的计算资源付费，而无须为应用空闲时段的资源占用付费； 3、缩短迭代周期、上线时间： Serverless 架构带来的是进一步的业务解耦，应用功能被解构成若干个细颗粒度的无状态函数，开发可以聚焦在单功能的快速开发和上线上； 同时拆解后的云函数，也都可以进行独立的迭代升级，更快速的实现业务迭代，缩减功能的上市时间； 4、快速试错 利用 Serverless 架构的简单运维、低成本及快速上线能力，可以来快速尝试业务的新形态、新功能； 利用 Serverless 产品的强弹性扩容能力，在业务获得成功时，也无需为资源扩容而担心； Serverless 的技术特点这里提到的技术特点的对象，特指 Serverless 产品中的计算产品，也就是云函数。云函数包含了如下的技术特性： 事件驱动 云函数的运行，是由事件驱动起来的，在有事件到来时，云函数会启动运行 Serverless 应用不会类似于原有的监听-处理类型的应用一直在线，而是按需启动 事件的定义可以很丰富，一次 http 请求，一个文件上传，一次数据库条目修改，一条消息发送，都可以定义为事件 单事件处理 云函数由事件触发，而触发启动的一个云函数实例，一次仅处理一个事件 无需在代码内考虑高并发高可靠性，代码可以专注于业务，开发更简单 通过云函数实例的高并发能力，实现业务高并发 自动弹性伸缩 由于云函数事件驱动及单事件处理的特性，云函数通过自动的伸缩来支持业务的高并发 针对业务的实际事件或请求数，云函数自动弹性合适的处理实例来承载实际业务量 在没有事件或请求时，无实例运行，不占用资源 无状态开发 云函数运行时根据业务弹性，可能伸缩到 0，无法在运行环境中保存状态数据 分布式应用开发中，均需要保持应用的无状态，以便于水平伸缩 可以利用外部服务、产品，例如数据库或缓存，实现状态数据的保存 Serverless 的应用场景Serverless 架构或者技术，可以用在什么样的场景下，来充分发挥它的优势呢？如下的场景，都适合使用 Serverless 架构或产品，来实现所需的业务逻辑。 WEB 及移动后端 通过结合使用云函数和 API 网关或 HTTP 触发器，可以对外提供 URL 访问地址，成为 Web、小程序、或移动应用等的后端服务。Serverless 架构既可以直接用于构建后台来服务应用，也可以通过类似 BFF 模式，构建中台和应用间的桥梁。 Serverless 架构提供的强弹性能力，使得可以支撑业务或应用的暴涨；而提供的低运维需求，使得开发者可以专注于业务实现和优化；同时，按实际使用量的付费方式，使得开发者无需预配置资源，无需担心预配置资源的浪费。 消息处理 Serverles 架构的应用本身是由事件触发的，因此极其适合于进行消息处理。无论是消息队列中传递的业务消息，还是 Kafka 中采集应用日志，均可以对接到云函数上，进行实时的消息处理、分析。 对象存储文件处理 在 Serverless 应用场景中，由对象存储中的文件上传事件，来触发云函数的运行，也是一种常见场景。针对图片文件的上传，可以借助云函数完成图片的缩略图生成、二维码或水印标记、图片优化处理；而针对数据文件的上传，可以启动数据的自动化分析， 物联网 物联网意味着成千上万的设备会连入网络，时刻在不断的产生数据，这对数据的分析、处理的及时性提出了很高的挑战。通过使用 Serverless 架构，物联网设备所采集的数据将可以作为云函数的触发事件，而实现数据的实时处理、分析和应用。 随着物联网设备计算能力的进一步提升，云函数作为最小粒度的计算单元，有机会被调度到设备端运行，实现边缘计算，达到端-云联合的 Serverless 架构。 运维及集成 通过对接云函数以及云上的各个产品、日志服务、监控告警系统，云时代的运维也都可以用云函数来构建。定时触发的云函数，将可以方便的替代需要在主机上来运行的定时任务；而日志或告警触发的云函数，将可以对云中的事件作出立刻回应及处理。 Serverless - 云原生时代的应用架构云计算已经进入了新的时期，目前上云已经不是应不应该，而是如何上云的问题。在这种情况下，云原生的概念也随之而生。云原生的架构或应用，是基于云而设计的，充分的利用现代云计算平台所具备的弹性和分布式特性来实现应用架构。 而 Serverless 架构、产品、以及应用，均是完全依托于云而构建的，是典型且完全的云原生的架构、产品或应用。 Serverless 产品所具备的产品特性优势、技术优势、费用优势，将成为新一代云产品的发展方向；而基于 Serverless 架构推进完成开发的应用或架构，将充分享受到云时代带来的强大助力，使得云计算能真正成为业务的助推器。 Serverless 的计算产品-云函数，作为云虚拟机、容器技术之后的下一代计算形态，将引来云计算中新的热潮。围绕着云函数而建设的产品能力、工具、生态、以及应用开发，也将引来新的一轮发展。随着无服务器的产品和生态走向成熟，将逐步承载起企业核心业务。 在这个持续向前高速发展的过程中，腾讯云的云函数，将作为腾讯云云原生的重点发力领域，跟随客户需求、行业发展、技术发展，为用户提供完整全套的 Serverless 解决方案。]]></content>
      <categories>
        <category>pipeline</category>
        <category>SCF</category>
      </categories>
      <tags>
        <tag>流程开发</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux-配置文件-vim配置]]></title>
    <url>%2F0003.%E5%BA%94%E7%94%A8-02.Linux%2FLinux-%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6-vimrc%E8%AE%BE%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[Vimdiff背景颜色太重无法看清代码方案11234567891011121314set t_Co=256if ! has("gui_running") set t_Co=256 endifif &amp;diff colors blueendifset nuset tabstop=4set history=100set icset syntax=cset autoindentsyntax on 示例如下：ps: .vimrc的注释标记符为半角双引号，在本代码中可以看到&quot; colors delek 是用半角双引号注释掉的。 方案2123456if &amp;diff hi DiffAdd cterm=bold ctermfg=12 guibg=LightBlue hi DiffDelete cterm=bold ctermfg=13 ctermbg=14 gui=bold guifg=blue guibg=LightCyan hi DiffChange cterm=bold ctermbg=green ctermfg=15 guibg=Magenta hi DiffText term=reverse cterm=bold ctermfg=9 gui=bold guibg=Redendif 示例如下：]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>vim</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[文件格式说明 - yaml]]></title>
    <url>%2F0001.%E7%9F%A5%E8%AF%86-03.standard%2Ffileformat-yaml%2F</url>
    <content type="text"><![CDATA[官方资料https://www.jianshu.com/p/413576dc837ehttps://yaml.org/spec/1.2.2/https://www.redhat.com/en/topics/automation/what-is-yamlhttps://www.runoob.com/w3cnote/yaml-intro.html yaml格式规范YAML 与 XML、JSON YAML 与 XML 具有 XML 同样的优点，但比 XML 更加简单、敏捷等 YAML 与 JSON JSON 可以看作是 YAML 的子集，也就是说 JSON 能够做的事情，YAML 也能够做 YAML 能表示得比 JSON 更加简单和阅读，例如“字符串不需要引号”。所以 YAML 容易可以写成 JSON 的格式，但并不建议这种做 YAML 能够描述比 JSON 更加复杂的结构，例如“关系锚点”可以表示数据引用（如重复数据的引用）。 YAML 编写规范它的基本语法规则如下： 12341）大小写敏感2）使用缩进表示层级关系3）缩进时不允许使用Tab键，只允许使用空格。4）缩进的空格数目不重要，只要相同层级的元素左侧对齐即可 规范一：文档使用 Unicode 编码作为字符标准编码，例如 UTF-8规范二：使用“#”来表示注释内容1234567# 客户订单date: 2015-02-01customer: - name: Jaiitems: - no: 1234 # 订单号 - descript: cpu 规范三：使用空格作为嵌套缩进工具。通常建议使用两个空格缩进，不建议使用 tab （甚至不支持） 规范四：序列表示 使用“-”（横线） + 单个空格表示单个列表项 123--- # 文档开始- 第一章 简介- 第二章 设计目录 使用”[]”表示一组数据 12--- # 文档开始[blue, red, green] 组合表示。每个结构都可以嵌套组成复杂的表示结构。 1234--- # 文档开始- [blue, red, green] # 列表项本身也是一个列表- [Age, Bag]- site: &#123;osc:www.oschina.net, baidu: www.baidu.com&#125; # 这里是同 键值表 组合表示 规范五：键值表 使用 “:”（冒号） + 空格表示单个键值对 12345678# 客户订单date: 2015-02-01customer: - name: Jaiitems: - no: 1234 # 订单号 - descript: cpu - price: ￥800.00 使用”{}”表示一个键值表 12345# 客户订单date: 2015-02-01customer: - name: Jaiitems: &#123;no: 1234, descript: cpu, price: ￥800.00&#125; “? “ 问号+空格表示复杂的键。当键是一个列表或键值表时，就需要使用本符号来标记。 1234567# 使用一个列表作为键? [blue, reg, green]: Color# 等价于? - blue - reg - gree: Color 组合表示。每个结构都可以嵌套组成复杂的表示结构。 123456789101112131415161718192021222324 Color: - blue - red - green# 相当于 (也是 JSON 的表示)&#123;Color: [blue, red, green]&#125;div: - border: &#123;color: red, width: 2px&#125; - background: &#123;color: green&#125; - padding: [0, 10px, 0, 10px]# 使用缩进表示的键值表与列表项items: - item: cpu model: i3 price: ￥800.00 - item: HD model: WD price: ￥450.00# 上面使用 “-” 前导与缩进来表示多个列表项，相当于下面的JSON表示items: [&#123;item:cpu, model:i3, price:￥800.00&#125;, &#123;item:HD, model:WD, price: ￥450.00&#125;] 规范六：文本块 使用 “|” 和文本内容缩进表示的块：保留块中已有的回车换行。相当于段落块 12yaml: | # 注意 &quot;:&quot; 与 &quot;|&quot; 之间的空格 JSON的语法其实是YAML的子集，大部分的JSON文件都可以被YAML的解释器解释。 使用 “&gt;” 和文本内容缩进表示的块：将块中回车替换为空格，最终连接成一行。 123yaml: &gt; # 注意 &quot;:&quot; 与 &quot;&gt;&quot; 之间的空格，另外可以使用空行来分段落 JSON的语法其实是YAML的子集， 大部分的JSON文件都可以被YAML的解释器解释。 使用定界符“”（双引号）、‘’（单引号）或回车表示的块：最终表示成一行。 1234567yaml: # 使用回车的多行，最终连接成一行。 JSON的语法其实是YAML的子集， 大部分的JSON文件都可以被YAML的解释器解释。yaml: # 使用了双引号，双引号的好处是可以转义，即在里面可以使用特殊符号 &quot;JSON的语法其实是YAML的子集， 大部分的JSON文件都可以被YAML的解释器解释。&quot; 使用 “—“ 来分割一个文本文件中的多个yaml配置 12345678910111213141516​spring:profiles:#激活开发环境active: dev---#开发环境配置spring:profiles: devserver:port: 8080---#生产环境配置spring:profiles: prodserver:port: 8082 使用 “…” 来表示一个yaml文件的结束 虽然在大多数yaml解析的情况下，结束符可以省略，但是通过添加结束符可以在数据读取过程中明确文件被完整读取。123456789101112#开发环境配置spring:profiles: devserver:port: 8080---#生产环境配置spring:profiles: prodserver:port: 8082... # yaml文件结束符 规范七：数据类型的约定 对一些常用数据类型的表示格式进行了约定，包括: 1234567891011121314151617integer: 12345 # 整数标准形式octal: 0o34 # 八进制表示，第二个是字母 ohex: 0xFF # 十六进制表示float: 1.23e+3 # 浮点数fixed: 13.67 # 固定小数minmin: -.inf # 表示负无穷notNumber: .NaN # 无效数字null: # 空值boolean: [true, false] # 布尔值string: &apos;12345&apos; # 字符串date: 2015-08-23 # 日期datetime: 2015-08-23T02:02:00.1z # 日期时间iso8601: 2015-08-23t21:59:43.10-05:00 # iso8601 日期格式spaced: 2015-08-23 21:59:43.10 -5 # ? “!”（叹号）显式指示类型，或自定义类型标识。单叹号通常是自定义类型，双叹号是内置类型 123456789101112131415161718192021222324252627282930313233343536373839404142434445isString: !!str 2015-08-23 # 强调是字符串不是日期数据 picture: !!binary | # Base64 图片 R0lGODlhDAAMAIQAAP//9/X 17unp5WZmZgAAAOfn515eXv Pz7Y6OjuDg4J+fn5OTk6enp 56enmleECcgggoBADs= #下面是内置类型 !!int # 整数类型 !!float # 浮点类型 !!bool # 布尔类型 !!str # 字符串类型 !!binary # 也是字符串类型 !!timestamp # 日期时间类型 !!null # 空值 !!set # 集合 !!omap, !!pairs # 键值列表或对象列表 !!seq # 序列，也是列表 !!map # 键值表 #下面是一些例子： --- !!omap - Mark: 65 - Sammy: 63 - Key: 58 --- !!set # 注意，“?”表示键为列表，在这里列表为 null ? Mark ? Sammy ? Key # 下面是自定义的类型或标识 %TAG ! tag:clarkevans.com,2002: # % 是指令符号 --- !shape # Use the ! handle for presenting # tag:clarkevans.com,2002:circle - !circle center: &amp;ORIGIN &#123;x: 73, y: 129&#125; radius: 7 - !line start: *ORIGIN finish: &#123; x: 89, y: 102 &#125; - !label start: *ORIGIN color: 0xFFEEBB text: Pretty vector drawing. 规范八：锚点与引用，定义数据的复用。 第一步：使用 “&amp;” 定义数据锚点（即要复制的数据） 第二步：使用 “*” 引用上述锚点数据（即数据的复制目的地） 12345678---hr: - Mark McGwire # Following node labeled SS - &amp;SS Sammy Sosa # 定义要复制的数据rbi: - *SS # Subsequent occurrence 这里是数据复制目标 - Ken Griffey yaml 格式相关api/包python yaml123import yamlConfig = yaml.load(open(self.driver_yaml), Loader=yaml.FullLoader) # 读取配置文件Config[&apos;vep2BGI&apos;] # 获取配置文件相关信息]]></content>
      <categories>
        <category>知识沉淀</category>
        <category>文件格式</category>
      </categories>
      <tags>
        <tag>文件格式</tag>
        <tag>yaml</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python-包-scikit-learn]]></title>
    <url>%2F0002.%E5%BC%80%E5%8F%91-02.python%2FPython-%E5%8C%85-sklearn%2F</url>
    <content type="text"><![CDATA[官方资料scikit-learnscikit-learn中文社区scikit-learn Git skleran安装使用pip安装，terminal直接执行即可1pip install -U scikit-learn 使用Anaconda安装，推荐Anaconda，因为里面已经内置了NumPy，SciPy等常用工具1conda install scikit-learn 安装完成后可以在python中检查一下版本，import sklearn不报错，则表示安装成功，我这里用的是老版本了，基本功能差不多。123&gt;&gt;import sklearn&gt;&gt;sklearn.__version__&apos;0.19.1&apos;]]></content>
      <categories>
        <category>编程拾慧</category>
        <category>python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Circulating cell-free DNA for cancer early detection]]></title>
    <url>%2F0001.%E7%9F%A5%E8%AF%86-05.Paper%2FPMC9133648-Circulating_cell-free_DNA_for_cancer_early_detection%2F</url>
    <content type="text"><![CDATA[摘要简介图形总结 cfDNA BIOLOGY cfDNA主要通过细胞凋亡和坏死释放，也可能通过活性分泌释放。 cfDNA的半衰期为16分钟至2.5小时。 在体内，cfDNA要么被核酸酶清除，然后被肾脏排泄到尿液中，要么被肝脏和脾脏吸收，随后被巨噬细胞降解。 体外分离cfDNA的保存温度不超过 -20 ℃，且不超过1次冻融循环，以防止cfDNA降解。 cfDNA片段的稳定性可能通过与细胞膜、细胞外囊泡或蛋白质的结合而提高。 cfDNA在正常情况下浓度通常较低(在100 ng/mL以内)，但在某些生理和病理条件下，如运动、炎症、糖尿病、、癌症等，cfDNA浓度会显著升高。健康个体中，cfDNA主要来自淋巴细胞，而在癌症患者中，来自癌症组织的cfDNA含量显著增加。 采用基于测序的方法测定cfDNA时，166bp的峰值显著，而肿瘤来源的cfDNA长度(约为144 bp)较正常cfDNA较短。 目前基于cfDNA进行肿瘤早筛的测序方法癌症早期检测的策略通常是基于检测癌细胞释放的cfDNA中与癌症相关的改变，即 ctDNA(circulating tumor DNA)。血浆中ctDNA的浓度较低，占cfDNA总浓度的不到0.01%，尤其是在早期癌症中，对癌症的早期检测提出了很大的挑战。目前，突变、甲基化和片段模式是cfDNA早期检测癌症的主要测序生物标志物。 Mutation-based sequencing approach and strategy基于突变的测序方法在早期癌症检测中的理想应用需要较低的检测下限(LOD)来区分在DNA提取、文库制备、靶标富集、杂交过程和测序本身过程中可能引入的背景噪声(如假阳性)，这使得检测真正的突变更加困难。一些背景噪声可以减轻，如在提取过程中氧化应激损伤引起的序列改变，碎片化和杂交过程。为了提高目标测序的分析灵敏度，独特分子标识符(uniquemolecular identifiers, UMIs)，即直接连接在文库DNA片段两端的独特分子条形码，被用于促进同一DNA片段序列的生物信息学比对。这种UMI策略可以帮助减少后续分析中的人为错误。深度测序(CAPP-Seq)方法结合double-umi进行癌症个性化分析时，cfDNA的LOD可达0.02%。 基于cfDNA突变的分析有几个局限性： 靶向测序的芯片设计主要基于现有的关于癌症基因改变的知识。 假阴性结果不仅在癌变突变超出定制检测范围时不可避免，而且血浆ctDNA浓度不足也可能导致假阴性结果。 ctDNA在晚期和转移癌患者中的浓度高于早期癌症患者，且随癌症类型的不同而不同。它在肝癌、胆道癌、食道癌和卵巢癌中的含量也较高，但在前列腺癌、乳腺癌和结肠癌中的含量相对较低此外，一些因素可能影响cfDNA浓度的变化，如年龄、体重指数、和生理参数基于突变的分析也受限于肿瘤肿块中最初出现的突变数量，这因癌症而异。黑色素瘤和肺癌平均有超过8.9个突变/Mbp，而低级别胶质瘤、乳腺癌、胰腺癌和前列腺癌平均有少于2.2个突变/Mbp此外，早期癌症患者的肿瘤突变负担比晚期癌症患者低，这使得利用基于突变的技术检测癌症更加困难。 假阳性结果可能发生在大量未患癌症的个体中，由于克隆造血(染色质免疫沉淀[CHIP])相关突变。在来自NSCLC和健康对照的cfDNA中检测到的变异中，58.0%和90.0%也在匹配的白细胞(WBC)中检测到，强调了对匹配的WBC DNA应用等效测序深度以排除CHIP干扰的重要性。 Methylation-based sequencing approach基于表观遗传改变的cfDNA测序方法被认为是有希望的癌症早期检测的替代方法。CpG位点甲基化是基因表达、组织分化、器官发育、衰老和肿瘤发生的重要表观遗传学调控机制。肿瘤特异性DNA甲基化的变化发生在肿瘤发生的早期，有时甚至在基因突变发生之前。先前的一项研究表明，在临床癌症诊断前四年，血浆中就检测到了甲基化的变化。此外，与只发生在少数基因组位置的典型癌症突变不同，近3000万个甲基化位点分布在人类基因组中，使它们无处不在，成为癌症检测的丰富信号。值得注意的是，甲基化模式通常是组织特异性的，这使得TOO(tissue of origin)分析成为可能。 全基因组亚硫酸氢盐测序(WGBS：Whole genomic bisulfite sequencing)是获取全基因组DNA甲基化图谱的金标准。然而，然而高昂的成本，输入DNA回收率低，测序深度有限，使其无法在临床应用。基于二代测序(NGS)的甲基化测序方法由于成本较低、测序深度较高，受到越来越多的关注。基于ngs的方法包括亚硫酸氢盐预处理(还原亚硫酸氢盐测序[RRBS])、酶切(甲基化敏感限制性内切酶测序)和亲和富集(甲基化DNA免疫沉淀和高通量测序)。亚硫酸氢盐转化后的DNA回收率低也是基于ngs方法的一个问题。cfMeDIP-Seq (cfMeDIP-Seq)是一种基于免疫沉淀的协议，采用传统的MeDIP-Seq，允许在无亚硫酸氢盐的过程中，低输入DNA的全基因组甲基化分析。 与需要100 - 2000 ng DNA输入的MeDIP-Seq、RRBS和WGBS相比，cfMeDIP-Seq在观察到的和预期的差异甲基化区域数量之间产生了近乎完美的线性关联(r2 = 0.99, p &lt;0.0001)，只有1 - 10ng的DNA输入。 近年来的技术进步，如methylBEAMing、单细胞RRBS和增强的线性分裂扩增测序(ELSA-Seq)，通过减少所需的DNA输入量和提高分析灵敏度来促进cfDNA甲基化测序的应用。例如，ELSA-Seq构建了一个单链文库，其甲基化覆盖深度高，扩增偏置小，可重复性极高，输入量可达500 pg。此外，相邻的CpG位点通常代表共甲基化状态，将多个基因组距离近、相关性高的CpG位点整合为甲基化单倍型区块，可进一步提高检测精度。 cfdna甲基化分析也有局限性： 癌症中存在的表观遗传改变也可能存在于其他非癌组织中，这可能导致假阳性。例如，食道癌和巴雷特食管有几个相似的甲基化改变。此外， 甲基化改变随着年龄的增长而积累，大约5%的CpG位点表现出与衰老和肿瘤发生共同的显著变化。同时， 当检测信号低于LOD时，可能会出现假阴性结果。同样，cfDNA的检出率与cfDNA的浓度有完整的相关性，而cfDNA的浓度也受癌症病理类型的影响。此外，这些 基于甲基化的测序方法可能无法检测主要由基因突变、体细胞拷贝数畸变或融合事件驱动的癌症，例如主要由EGFRL858R突变、EML4-ALK融合或ERBB2扩增驱动的几种肺癌亚型。 cfDNA fragmentation-based sequencing approachcfDNA的全基因组片段化为癌症的早期检测开辟了新的领域。全基因组测序(WGS)显示癌源cfDNA片段的长度比非癌源cfDNA片段的长度变化更大。cfDNA片段模式的差异反映了癌症中染色质结构的变化，以及其他基因组和表观基因组异常。最近，一种结合全基因组片段化的机器学习模型可以将多个癌症患者与健康对照组区分开来，敏感性从57%到99%以上不等，特异性为98%，并在75%的病例中识别出癌症的TOO。除了片段长度之外，cfDNA片段在基因组中的断裂位点还揭示了核小体占据的全基因组地图，提供了丰富的信号。局限性： 尽管WGS能够从极少量的cfDNA中同时分析数十到数百种肿瘤特异性异常，但其覆盖深度较低，成本较高。 基于cfDNA片段的方法与cfDNA甲基化分析有相同的局限性，如生理等病理条件导致的假阳性和技术限制导致的假阴性。 三种技术方法的整体比较 文献原文]]></content>
      <categories>
        <category>NGS</category>
        <category>文献</category>
      </categories>
      <tags>
        <tag>cancer early detection</tag>
        <tag>circulating cell-free DNA</tag>
        <tag>liquid biopsy</tag>
        <tag>methylation</tag>
        <tag>multi-cancer early detection</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[经典文献清单]]></title>
    <url>%2F0001.%E7%9F%A5%E8%AF%86-05.Paper%2FPaper_list%2F</url>
    <content type="text"><![CDATA[NGS-CDS检测参考Standard operating procedure for somatic variant refinement of sequencing data with paired tumor and normal samples Use of synthetic DNA spike-in controls (sequins) for human genome sequencing%20for%20human%20genome%20sequencing.pdf) MRD方向Enhanced detection of minimal residual disease by targeted sequencing of phased variants in circulating tumor DNA 早筛]]></content>
      <categories>
        <category>文献</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[肿瘤临床检测中的点突变（SNV&InDel）变异检测方案调研]]></title>
    <url>%2F0001.%E7%9F%A5%E8%AF%86-01.NGS%2FNGS-%E7%82%B9%E7%AA%81%E5%8F%98%E5%8F%98%E5%BC%82%E6%A3%80%E6%B5%8B%E6%96%B9%E6%A1%88%E8%B0%83%E7%A0%94%2F</url>
    <content type="text"><![CDATA[变异检测变异过滤一些可以获取的临床变异检测资料信息中检院TMB标准化项目采用多个变异检测软件进行交叉验证，而后进行人工审核确认。 全国实体肿瘤体细胞突变 高通量测序（大 Panel）检测借助质评自身的特殊性，通过多家检测机构的结果进行较差验证，按着特定标准筛选获得标准答案集合。 基于IGV的人工审核针对如果对变异结果进行人工审核，2019年有一篇发表在GENETICS IN MEDICINE (IF 9.108)的文章，可以为我们提供一些参考。Standard operating procedure for somatic variant refinement of sequencing data with paired tumor and normal samples其中针对突变审核包含如下过程： 步骤1：把突变可视化 使用IGV观察突变；确保IGV与插件中突变选取保持一致；通过插件的“S”使突变发生reads置顶；确认IGV、插件的reads展示一致； 步骤2：确认支持突变的数量 查看突变的①链方向、总覆盖度、②位点的突变频率、③非突变频率；考虑突变受到的其他因素影响，比如原发性肿瘤 DNA、复发性DNA、肿瘤RNA等； 步骤3：确认支持突变的质量 寻找多个不匹配或者与ref高度差异性的区域；寻找 半透明或透明的reads或碱基(比对质量低的)；①对有疑问reads进一步确认比对质量、碱基质量；②看突变对应的normal检出情况； 步骤4：检查测序误差 ①切换“成对查看”确认短插入片段情况；②IGV缩小确认是否在高度差异性区域，临近区域是否有indel存在；③看参考序列是否在低复杂性区域，是否存在串列重复序列； 步骤5：给突变选择符合哪个Call标签 通过突变质量、突变数量的信息，选择合适的Call标签； 步骤6：给突变选择符合哪个Tag标签 可以对每个突变进行tag标签标记，对Call标记 不确定、失败的突变，尤其重要； 步骤7：给突变写附加的备注信息 其中提供了19中tag，作为变异过滤过程中参考判断的指标 燃石检测流程实际比较难获得泛生子内部资料，只能借助一些发布文章的method，这也往往是一个非常有效的办法。在燃石的官网查到燃石的学术发表找到其中一篇文章从题目看是和NGS检测为主的Evaluating the analytical validity of circulating tumor DNA sequencing assays for precision oncology从文章的method中，我们可以看到泛生子流程的整体检测逻辑如下： 制作模拟数据 通过wgsim (v1.9) 进行数据模拟 变异的分析流程 首先对下机数据进行Trim TrimGalore 使用anaquin 进行sequin（spike-in controls）分析, 将sequin的Reads进行分离，然后矫正到样本相同的深度，并通过Anaquin somatic 进行变异检测。详细见参考文献Use of synthetic DNA spike-in controls (sequins) for human genome sequencing bwa mem (v0.7.16) 进行数据比对，比对到Hg38 剔除捕获区间外的Reads，并用gatk MarkDuplicates (v4.0). 标记重复 VarScan (v2.4.3) 用来进行 SNVs and indels 的检测（最少支持数是3条read-fragments) 泛生子检测流程介绍在泛生子官网查找泛生子的成果,2016:The genome-wide mutational landscape of pituitary adenomas的附件中提到了分析流程 2019:Detection of early-stage hepatocellular carcinoma in asymptomatic HBsAg-seropositive individuals by liquid biopsy 详细分析方法参考文档中提及的方法材料： Sequencing reads were primarily processed with our own program to extract tags and remove sequence adapters. Residual adapters and low-quality regions were subsequently removed using Trimmomatic (v0.36). The cleaned reads were mapped to the hg19 and HBV genomes using ‘bwa(v0.7.10) mem’ with the default parameters. Candidate somatic mutations, consisting of SNP and INDEL, were identified using samtools mpileup (9) across the targeted regions of interest. To ensure accuracy, reads with the same tags, and start and end coordinates were grouped into Unique Identifier families (UID families). UID families containing at least two reads and in which at least 80% of reads were the same type were defined as Effective Unique Identifier families (EUID families). Each mutation frequency was calculated by dividing the number of alternative EUID families by the sum of alternative and reference ones. The mutations were further manually reviewed in IGV. The candidate variations were annotated with Ensembl Variant Effect Predictor (VEP) (10). HBV integrations were identified using Crest (11) , and at least 4 soft-clip reads supports were needed. 世和参考文章Tumor-derived DNA from pleural effusion supernatant as a promising alternative to tumor tissue in genomic profiling of advanced lung cancer Sequencing and data processing Target enriched libraries were sequenced on the HiSeq4000 platform (Illumina) with 2×150bp pair-end reads. Sequencing data were demultiplexed by bcl2fastq (v2.19), analyzed by Trimmomatic 24 to remove low-quality (quality&lt;15) or N bases, mapped to the reference hg19 genome (Human Genome version 19) using the Burrows-Wheeler Aligner. PCR duplicates were removed by Picard. The Genome Analysis Toolkit (GATK) was used to perform local realignments around indels and base quality reassurance. SNPs and indels were called by VarScan2 and HaplotypeCaller/UnifiedGenotyper in GATK, with the mutant allele frequency (MAF) cutoff as 0.5% for tissue samples, 0.1% for liquid biopsy samples, and a minimum of three unique mutant reads. Common variants were removed using dbSNP and the 1000 Genome project. Germline mutations were filtered out by comparing to patient’s whole blood controls. The resulting somatic variants were further filtered through an in-house list of recurrent sequencing errors that was generated from over 10,000 normal control samples on the same sequencing platform. Gene fusions were identified by FACTERA. copy number variations (CNVs) were analyzed with ADTEx . The log2 ratio cut-off for copy number gain was defined as 2.0 for tissue samples and 1.6 for liquid biopsy samples. A log2 ratio cut-off of 0.67 was used for copy number loss detection in all sample types. The thresholds were determined from previous assay validation using the absolute CNVs detected by droplet digital PCR (ddPCR). Allele-specific CNVs were analyzed by FACETS 30 with a 0.2 drift cut-off for unstable joint segments. The proportion of chromosomal instability (CIN) was calculated by dividing the size of drifted segments by the total segment size. Tumor mutational burden (TMB) in this study was defined as the number of somatic synonymous mutations per megabase in each sample, with hotspot/fusion mutations excluded. 贝瑞参考文章Genetic profiling of primary and secondary tumors from patients with lung adenocarcinoma and bone metastases reveals targeted therapy options In brief, DNA extracted from FFPE tissue biopsies was fragmented to an average size of 300 bp, molecules were then end repaired and A-tailed and finally T tailed linkers were ligated on. The added linkers were a mix of 96 different molecular barcodes giving a high probability that each molecule was marked differently at both ends and thus uniquely barcoded. Libraries were amplified by PCR and resulting amplicons captured using biotinylated probes (120 nucleotides) for the 457 genes. Following elution, molecules were re-amplified using complementary sequencing primers and then paired end (PE) sequenced (2 × 150 bp) on the NovaSeq platform (Illumina). Fastq sequencing reads were aligned to the hg19 reference genome using the Burrows Wheeler algorithm (Li and Durbin 2009). The resulting SAM files were converted to BAM file format and then sorted on genome coordinates using Samtools. To remove PCR bias (reads with the same molecular barcodes and same start and same stop positions), only the unique coded molecules were used for copy number analysis. After filtering out low mapping quality reads (MAQ &lt; 20), the average depth of coverage (DoC) for each target was calculated using the GATK Depth Of Coverage algorithm (McKenna et al. 2010). After GC correction using LOESS regression method (Alkan et al. 2009), reads were normalized using the RPKM method (Chiang et al. 2019). For these steps, the tumor and matched normal sample was processed separately. Somatic SNVs and indels were finally identified by MutLoc (Berry Genomics in-house tools, unpublished), which maps the alternative base fraction compared to the hg19 reference genome.]]></content>
      <categories>
        <category>NGS</category>
        <category>变异检测</category>
      </categories>
      <tags>
        <tag>变异检测</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Use of synthetic DNA spike-in controls (sequins) for human genome sequencing]]></title>
    <url>%2F0001.%E7%9F%A5%E8%AF%86-05.Paper%2FPMID31217595-Use_of_synthetic_DNA_spike-in_controls_for_human_genome_sequencing%2F</url>
    <content type="text"><![CDATA[文献全文%20for%20human%20genome%20sequencing.pdf) 二代测序(NGS)已被广泛应用于识别基因变异及其相关性研究与疾病。然而，由于人类的复杂性，测序数据的分析仍然具有挑战性文库制备、测序和分析过程中引入的遗传变异和混淆错误。本文比较开创性的开发了一种方法，synthetic DNA spike-ins—termed ‘sequins’ (sequencing spike-ins)，直接添加到DNA中样品库准备前。sequins可以用来测量技术偏差，并作为内部定量以及整个流程的定性控制。sequins可以用于全基因组测序和靶向测序。在实验开始阶段将sequins添加到人类DNA样本中，然后按照生物信息学步骤分离sequins和样本衍生的测序reads，并评估检测的诊断性能。 由于sequins是参考人类 DNA 序列合成的镜像序列，因此可以轻松的和人类基因组序列进行区分，同时由于序列的镜像，可以最大可能的保证分子动力学的相似性，使得sequins可以模拟在整个实验中，各个环节引入的错误。]]></content>
      <categories>
        <category>NGS</category>
        <category>文献</category>
      </categories>
      <tags>
        <tag>变异检测</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Enhanced detection of minimal residual disease by targeted sequencing of phased variants in circulating tumor DNA]]></title>
    <url>%2F0001.%E7%9F%A5%E8%AF%86-05.Paper%2FPMC8678141-Enhanced_detection_of_minimal_residual_disease_by_targeted_sequencing_of_phased_variants_in_circulating_tumor_DNA%2F</url>
    <content type="text"><![CDATA[文献链接文章线上链接文章原文 文献主题内容最近的方法已经通过追踪多个具有错误抑制测序的体细胞突变来改善ctDNA MRD性能。这种方法允许使用现成的面板或个性化的检测方法对有限的cfDNA进行检测，检测限度低至2-10 / 100000。部分参考方法如下： Integrated digital error suppression for improved detection of circulating tumor DNA Phylogenetic ctDNA analysis depicts early stage lung cancer evolution An ultrasensitive method for quantitating circulating tumor DNA with broad patient coverage 先前旨在降低LOD的方法集中于在亲本DNA双链的互补链上检测到的体细胞变异。“Duplex sequencing”由于需要两个一致的测序结果，所以有效的降低了单个核苷酸变异(SNV)检测的错误率。在之前的研究中，利用双序列对ctDNA进行分析，分析LOD低至400000个分子。然而，这种方法能通过DNA双链恢复的效率非常低，只有少数人(通常为20-25%)能恢复原模板链。这种低效率使得双工测序在实际ctDNA检测中不是最理想的，因为实际血液容量的输入DNA有限(每标准10ml采血管约4000 - 8000个基因组)，最大限度的基因组恢复是至关重要的。为了改进MRD检测，仍然需要同时实现低分析检测限和高分子回收率的方法来检测多个突变 Integrated digital error suppression for improved detection of circulating tumor DNA Detection of ultra-rare mutations by next-generation sequencing Detecting ultralow-frequency mutations by Duplex Sequencing 作为一种降低背景错误率的双相测序的替代方法，涉及到检测 顺式的变异体，即在顺式中发生两个或多个突变(如图1a所示，扩展数据图1所示)。与双相测序类似，由于对单个分子中两个独立的非参考事件的一致检测，该方法提供了更低的误差剖面。 芯片设计We identified and summarized the frequency of these ‘putative phased variants,’ (Methods) controlling for the total number of SNVs, from 2538 tumors across 24 cancer histologies including solid tumors and hematological malignancies (Fig 1b, Extended Data Fig 2, Table S1). Interestingly, PVs were most significantly enriched in two B-cell lymphomas (DLBCL and follicular lymphoma, FL, P&lt;0.05 vs all other histologies).各癌种下PVs变异比例情况如下图b： 可以看到在DLBCL中，存在PVs的比例整体较高，同事研究也发现，这部分变异在基因组上存在一定的富集，所以使用了79例样本进行PVs的富集，进行定制化的探针设计针对PVs富集区域进行设计； This final Phased variant Enrichment and Detection Sequencing (PhasED-Seq) panel targeted ~115kb of genomic space focused on PVs, along with an additional ~200kb targeting genes known to be recurrently mutated in B-NHLs; this single panel was used for both identification of PVs from tumor and/or plasma samples and tracking residual disease (Fig 2b). While the 115kb of space dedicated to PV-capture targets only 0.0035% of the human genome, it captures 26% of phased variants observed by WGS (Extended Data Fig 4a), yielding a ~7500-fold enrichment over WGS. 性能验证片段含有多个突变对杂交捕获的影响When subjecting an equimolar mixture of these oligonucleotides to capture and sequencing, molecules with as high as 5% mutation rate were captured with nearly the same efficiency as wildtype counterparts (85% vs 100%), while molecules with 10% mutations were captured with only 27% relative efficiency (Fig 4a). Notably, only 7% of cases had any region exceeding 10% mutation frequency across the panel (Methods, Extended Data Fig 5b–c), and in all cases the 90th percentile mutation rate was &lt;5%, suggesting the majority of phased mutations are recoverable by hybrid capture. 性能效率上When considering unique molecular depth, duplex sequencing recovered only 19% of all unique cfDNA fragments (Fig 4c). In contrast, the unique depth of reads covering PVs within a genomic distance of &lt;20bp was nearly identical to the overall sample depth. Similarly, PVs up to 80bps in size had depth greater than 50% of the median unique depth for a sample. Importantly, almost half (48%) of all PVs were less than 80bp in length (Fig 4d). 准确性所有方法在0.01%(1 part in 10,000)的水平上检测性能类似，但是在更低的水平 (e.g., 0.001%, 0.0002%, 0.0001%, and 0.00005%), PhasED-Seq and duplex sequencing 的变现明显优于 single-strand UMI based SNV (P&lt;0.0001 for duplex, ‘2x’ PhasED-Seq, and ‘3x’ PhasED-Seq; Fig 5a) 在几个方法对比过程中，PhasED-Seq也表现除最低的错误率水平 (Fig 5b). PVs中表现的更低的错误率，也可以提高ctDNA的检测限。]]></content>
      <categories>
        <category>NGS</category>
        <category>文献</category>
      </categories>
      <tags>
        <tag>MRD</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Software-PennCNV-基于SNP位点进行CNV检测]]></title>
    <url>%2F0003.%E5%BA%94%E7%94%A8-03.software%2FSoftware-%E5%9F%BA%E4%BA%8ESNP%E4%BD%8D%E7%82%B9%E8%BF%9B%E8%A1%8CCNV%E6%A3%80%E6%B5%8B-PennCNV%2F</url>
    <content type="text"><![CDATA[软件相关资源信息 软件文献pennCNV Home]]></content>
      <categories>
        <category>NGS</category>
        <category>software</category>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>变异检测</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Software-varscan-变异检测]]></title>
    <url>%2F0003.%E5%BA%94%E7%94%A8-03.software%2FSoftware-%E5%8F%98%E5%BC%82%E6%A3%80%E6%B5%8B-varscan%2F</url>
    <content type="text"><![CDATA[软件相关资源信息 软件文献 VarScan 2: Somatic mutation and copy number alteration discovery in cancer by exome sequencingDaniel Software implementation 软件获取 varscan.sourceforgevarscan.githubThe VarScan 2 core software was developed in Java; the false-positive filter was implemented in Perl. Binary executables, scripts, and source code are free for noncommercial use and available at http://varscan.sourceforge.net.The false-positive filter requires the bam-readcount utility (D. Larson et al., https://github.com/genome/bam-readcount), which is written and compiled in C. Varscan的使用安装下载Varscan的Jar包 使用samtools mpileup -f ref.fasta sample.bam |java -jar VarScan.v2.3.3.jar mpileup2indel–output-vcf 1–vcf-sample-list sample_names.list sample.varscan.vcf]]></content>
      <categories>
        <category>NGS</category>
        <category>software</category>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>变异检测</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Software-终端工具-MobaxTerm]]></title>
    <url>%2F0003.%E5%BA%94%E7%94%A8-03.software%2FSoftware-%E7%BB%88%E7%AB%AF%E5%B7%A5%E5%85%B7-MobaxTerm%2F</url>
    <content type="text"><![CDATA[SSH 为 Secure Shell 的缩写，是目前较可靠，专为远程登录会话和其他网络服务提供安全性的协议。利用 SSH 协议可以有效防止远程管理过程中的信息泄露问题。 每天都需要与linux服务器打交道，有很多人直接使用linux系统，比如ubuntu和centos等。但是也有很多人不喜欢linux系统，虽然它对于做计算很方便，但是对于日常办公软件的支持极差，比如QQ、微信、office等。这个时候SSH客户端应运而生，它的作用就是帮助我们在windows下去连接并操作linux服务器。在学校使用更多的是xshell，但是到了公司因为版权等问题，xshell并不能成为一个好的选择。便改用了MobaxTerm，在这里记录下MobaxTerm的一些常见的配置内容。 安装可以在官网下载安装可以通过下载可执行文件(v21.5)直接运行。 解决 MobaxTerm 自动断开链接使用MobaXterm工具通过SSH连接Linux服务器，如果一段时间没有操作，MobaXterm会把连接自动断开，这个设定很是不方便。通过更改下面的设置可以使SSH保持长连接，不会自动断开。分别依次选择setting 》》》 configuration 》》》 ssh SETTINGS然后再配置界面选择 keepalive， 即可保持链接不中断。]]></content>
      <categories>
        <category>software</category>
        <category>Windows</category>
        <category>W</category>
      </categories>
      <tags>
        <tag>软件工具</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Software-transvar_变异坐标转换]]></title>
    <url>%2F0003.%E5%BA%94%E7%94%A8-03.software%2FSoftware-%E5%8F%98%E5%BC%82%E5%9D%90%E6%A0%87%E8%BD%AC%E6%8D%A2-transvar%2F</url>
    <content type="text"><![CDATA[坐标转换基因组学研究中经常会进行的操作是将测序检测得到的染色体侧面的变异检测结果（SNV、InDel等），注释到基因上，因为大多数功能研究和蛋白研究都是针对基因进行的，将变异注释到基因上，可以更好的帮助我们预测变异对基因表达，和蛋白合成过程中带来的影响，而这个层级的影响可以帮助我们更好的进行功能组学的相关研究。因此一些发表的论文或数据库中经常提到的变异，一般有三种格式：1）基因组坐标：2）cDNA 坐标；3）蛋白氨基酸坐标。举个例子TP53上的某个变异的基因组坐标是g.chr17:74026C&gt;A，cDNA坐标是c.1001G&gt;T，蛋白氨基酸坐标是p.G334V。当然这几种注释的写法都是有标准规范可以参考的，可以参考文章 在数据分析的过程中经常会遇到这三种坐标相关转换的情况，例如你从文献或者某个数据库中收集到了几百个肿瘤靶向药的用药位点，而你在你样本中检测到了很多变异，想知道你的样本中包含多少收集到的已知的用药位点。但通常文献或者数据库会以第二种或者第三种形式表示变异，而我们自己检测的变异通常会以vcf格式存储，这样就无法直接匹配。当然可以对vcf格式的变异进行ANNOVAR注释，然后对cDNA或者蛋白氨基酸坐标形式的变异进行比较，但尝试过的人都表示特别痛苦：需要考虑的规则太多！尝试两次，还是放弃了：一是匹配规则不通用；二是总担心有没有考虑到过的情况。所以急需一个能完成这种坐标转换的工具。15年发表在NATURE METHODS上的题为：TransVar: a multilevel variant annotator for precision genomics的文章中推出了一款名为TransVar的软件成了解决不同层面变异坐标转换的神器。 文献下载 TransVar软件简介Transvar 是一款多种方向的突变/坐标转换工具，它支持基因组坐标、cDNA 坐标以及蛋白氨基酸坐标之间的转换。 如上图所示，该软件的功能可细分为下面3种：1）正向注释：对于基因组坐标的变异进行mRNA（cDNA）和蛋白注释，这款工具会提供所有的可能结果；2）反向注释：将mRNA（cDNA）坐标和蛋白坐标的变异转换成所有可能基因组坐标形式的变异；3）等价注释：对于某一给定的蛋白坐标的变异，搜索所有可能的与其为相同基因组坐标，但在不同转录本上的蛋白坐标变异。 软件的官方文档 ReadtheDoc 软件的使用Linux版本安装软件下载可以从github仓库获取 通过python 安装12345sudo pip install transvar ## 全局安装，需要root权限或者：pip install --user transvar ##用户安装，没有root权限的用此方法软件更新：pip install -U transvar 数据库的配置链接数据库，可通过命令行添加。最开始，不存在transvar.cfg这个文件，在第一次链接后，会创建transvar.cfg文件，并将你创建的对应关系写入文件中，transvar.cfg 存放的路径：os.path.dirname({PYTHON_PATH})/lib/python3.7/site-packages/transvar/transvar.cfg1234567891011121314151617181920212223242526272829303132# set up databasestransvar config --download_anno --refversion hg19 #默认的hg19的 dbSNP 数据库是2016年的，部分数据库如dbSNP新版数据库收录内容有很大变化（主要是数量的提升），所以建议自行重新下载# in case you don&apos;t have a referencetransvar config --download_ref --refversion hg19# in case you do have a reference to linktransvar config -k reference -v [path_to_hg19.fa] --refversion hg19transvar config -k aceview -v $PATH/hg19.aceview.gff.gz.transvardb --refversion hg19transvar config -k ccds -v $PATH/hg19.ccds.txt.transvardb --refversion hg19transvar config -k ensembl -v $PATH/hg19.ensembl.gtf.gz.transvardb --refversion hg19transvar config -k gencode -v $PATH/hg19.gencode.gtf.gz.transvardb --refversion hg19transvar config -k kg -v $PATH/transvar.download/hg19.knowngene.gz.transvardb --refversion hg19transvar config -k refseq -v $PATH/hg19.refseq.gff.gz.transvardb --refversion hg19transvar config -k ucsc -v $PATH//hg19.ucsc.txt.gz.transvardb --refversion hg19cat lib/python3.7/site-packages/transvar/transvar.cfg[DEFAULT]refversion = hg19[hg19]reference = $PATH/ucsc.hg19.fastarefseq = $PATH/hg19.refseq.gff.gz.transvardbccds = $PATH/hg19.ccds.txt.transvardbucsc = $PATH/hg19.ucsc.txt.gz.transvardbgencode = $PATH/hg19.gencode.gtf.gz.transvardbaceview = $PATH/hg19.aceview.gff.gz.transvardbensembl = $PATH/hg19.ensembl.gtf.gz.transvardbkg = $PATH/hg19.knowngene.gz.transvardb 使用这款软件即可以单点注释，也可以批量处理，下面分别介绍一下： 单点注释用 -i传入待注释位点，包括3种： 1234567891011121314# 基因组正向注释transvar ganno --ccds -i &apos;chr3:g.178936091G&gt;A&apos; # cDNA反向注释transvar canno --ccds -i &apos;PIK3CA:c.1633G&gt;A&apos;# 氨基酸反向注释transvar panno -i &apos;PIK3CA:p.E545K&apos; --ensembl # 已知 p. 进行注释，可以一次只注释一个数据库，也可以同时注释多个数据库transvar panno -i &apos;ERBB2:p.Leu755_Thr759del&apos; --aceview --ccds --ensembl --gencode --kg --refseq --ucsc# 其中--ccds、--ensembl为使用不同的数据库，如网页版，可以同时多选，\# 如 --ccds --ensembl --refseq --ucsc 来进行多选 批量注释 -l传入待注释位点 12345678910/*/software/anaconda3/bin/transvar canno -l mutiation.canno.list -m 1 -o 2 --refseq --longestcoding --gseq ###canno：指cDNA反向注释，备选包括panno（ 蛋白氨基酸反向注释）和ganno（基因组正向注释）-l：输入文件，变异与canno、panno、ganno对应。格式示例如下：![image.png](https://upload-images.jianshu.io/upload_images/22041438-ba466242c2050f60.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)-m：-l指定的输入文件可以有多列，通过-m指定哪列是待注释列，不加-m参数默认是第一列-o：同时可以通过-o来指定-l中的那一列作为输出文件的首列，不加-o，默认是第一列--refseq：使用哪个数据库的转录本进行注释，还有其他数据库可选如 ensembl/gencode/ucsc/ccds/aceview等。--longestcoding： 有多个转录本时，仅选择最长的转录本。如果不加这个参数会把涉及到的所有转录本都输出出来，这时候你就要自己制定标准进行筛选了--gseq ：在输出文件中增加类似VCF格式的变异信息，包括染色体，起始位置，终止位置，参考基因组序列，突变后的序列。 网页版Transvar Web版使用相对比较简单，界面也非常清晰]]></content>
      <categories>
        <category>NGS</category>
        <category>software</category>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>格式转换</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Software-编程终端-VSCode-1.基本配置]]></title>
    <url>%2F0003.%E5%BA%94%E7%94%A8-04.windows%2FSoftware-%E7%BC%96%E7%A8%8B%E7%BB%88%E7%AB%AF-VSCode-1.%E5%9F%BA%E6%9C%AC%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[更改扩展的缓存位置vscode的默认扩展均存储在用户家目录的隐藏目录（./.vscode/extensions）中，因为一般该目录在C盘，且扩展文件较大，会导致C盘存储激增。12#在vscode的Terminal界面执行如下命令，可以更改扩展的默认缓存位置code.cmd --extensions-dir D:\vscode.extensions 创建特定类型的文件模板在进行代码开发时，每次我们都需要手动填写名称，时间，作者等一些重复的基本信息，其实非常浪费时间，而且通过使用模板，可以帮我们节省这部分时间，更多的时间用在编程本身。使用vs code开发python代码的时候，可以建立自定义的模板，大大的提高效率。File-&gt;Preferences-&gt;User Snippets（用户-&gt;首选项-&gt;用户片段）可以对应的编程语言，以python为例，选择后会打开一个python.json的文件进行编辑。把原来的内容删除，输入下面的内容：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950&#123; &quot;HEADER&quot;:&#123; &quot;prefix&quot;: &quot;template&quot;, &quot;body&quot;: [ &quot;#!/usr/bin/python&quot;, &quot;# -*- encoding: utf-8 -*-&quot;, &quot;&apos;&apos;&apos;&quot;, &quot;@File : $TM_FILENAME&quot;, &quot;@Time : $CURRENT_YEAR/$CURRENT_MONTH/$CURRENT_DATE $CURRENT_HOUR:$CURRENT_MINUTE:$CURRENT_SECOND&quot;, &quot;@Author : Liu.Bo &quot;, &quot;@Version : 1.0.0.0&quot;, &quot;@Contact : liubo4@genomics.cn/614347533@qq.com&quot;, &quot;@WebSite : http://www.ben-air.cn/&quot;, &quot;&apos;&apos;&apos;&quot;, &quot;import logging&quot;, &quot;from logging.handlers import RotatingFileHandler&quot;, &quot;from argparse import ArgumentParser&quot;, &quot;&quot;, &quot;program = &apos;$TM_FILENAME&apos;&quot;, &quot;version = &apos;1.0.0.0&apos;&quot;, &quot;&quot;, &quot;parser = ArgumentParser(prog=program)&quot;, &quot;parser.add_argument(&apos;-in&apos; , dest=&apos;input&apos; , required=True, action=&apos;store&apos;, type=str, help=&apos;input File&apos;)&quot;, &quot;parser.add_argument(&apos;-out&apos;, dest=&apos;output&apos;, required=True, action=&apos;store&apos;, type=str, help=&apos;output File&apos;)&quot;, &quot;args = parser.parse_args()&quot;, &quot;&quot;, &quot;def log_config():&quot;, &quot; LOG_FORMAT = &apos;[%(asctime)s][%(levelname)s]: %(message)s&apos;&quot;, &quot; level = logging.INFO&quot;, &quot; logging.basicConfig(level=level, format=LOG_FORMAT)&quot;, &quot; #创建RotatingFileHandler对象,满2MB为一个文件，共备份3个文件&quot;, &quot; log_file_handler = RotatingFileHandler(filename=&apos;test.log&apos;, maxBytes=2*1024*1024, backupCount=3)&quot;, &quot; # 设置日志打印格式&quot;, &quot; formatter = logging.Formatter(LOG_FORMAT)&quot;, &quot; log_file_handler.setFormatter(formatter)&quot;, &quot; logging.getLogger(&apos;&apos;).addHandler(log_file_handler)&quot;, &quot;&quot;, &quot;def main():&quot;, &quot; Input = args.input&quot;, &quot; Output = args.output&quot;, &quot; log_config()&quot;, &quot;&quot;, &quot;if __name__ == &apos;__main__&apos;:&quot;, &quot; main()&quot;, &quot;&quot;, &quot;$0&quot; ], &#125; &#125; 然后再次创建python脚本后，输入template（prefix 对应的内容） 就可以将编程模板填充到代码中]]></content>
      <categories>
        <category>software</category>
        <category>Windows</category>
      </categories>
      <tags>
        <tag>环境配置</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[VS_Code-提升效率的配置]]></title>
    <url>%2F0003.%E5%BA%94%E7%94%A8-04.windows%2FSoftware-%E7%BC%96%E7%A8%8B%E7%BB%88%E7%AB%AF-VSCode-3.%E5%B8%B8%E7%94%A8%E5%8A%9F%E8%83%BD%2F</url>
    <content type="text"><![CDATA[Python解析器选择Ctrl+Shift+p ==》 python: Select Interpreter ==》选择所需环境]]></content>
      <categories>
        <category>software</category>
        <category>Windows</category>
      </categories>
      <tags>
        <tag>环境配置</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[VS_Code-提升效率的配置]]></title>
    <url>%2F0003.%E5%BA%94%E7%94%A8-04.windows%2FSoftware-%E7%BC%96%E7%A8%8B%E7%BB%88%E7%AB%AF-VSCode-2.%E5%B8%B8%E7%94%A8%E6%89%A9%E5%B1%95%2F</url>
    <content type="text"><![CDATA[所有常用插件可以通过账号进行多机器的自动同步！ 编程相关插件通用扩展Code Runner在编辑器里运行js代码，同时可在terminal里显示打印结果的工具，方便调试代码,支持多种语言比如 C++, Python, Java等等 guides显示代码对齐辅助线，很好用 SnippetsSnippets 是节约时间提高生产力的最好办法。这并不是单单某一个语言的扩展，而是多种语言的各种扩展。下面是一些流行的 Snippets 扩展： Angular Snippts (version 11) Python JavaScript (ES6) code snippets HTML Snippets ES7 React/Redux/GraphQL/React-Native snippets Vue 3 Snippets 语言专属扩展Snakemake Language支持snakemake语言的语法高亮 ；Basic syntax, language, and snippet support for Snakefiles (Snakemake workflow definition files) WDL开发相关扩展WDL Syntax HighlighterWDL DevToolspythonpython可以一键进行python代码的格式化。A Visual Studio Code extension with rich support for the Python language (for all actively supported versions of the language: &gt;=3.7), including features such as IntelliSense (Pylance), linting, debugging, code navigation, code formatting, refactoring, variable explorer, test explorer, and more! C__相关Better C++ Syntax 效率工具MarkdownMarkdown All in One :MarkdownAll in One可以处理所有的markdown需求，例如自动预览、快捷键、自动完成等。从2004年发布以来，Markdown已成为最流行的标记语言之一。技术作者广泛使用Markdown转写文章、博客、文档等，因为它十分轻便、简单，而且可以在多个平台上使用。它的流行带动了许多Markdown变体的出现，如GitHub Flavored markdown、MDX等。例如，要在Markdown中加粗字体，只需要选中文字按快捷键Ctrl+B即可，这样可以提高生产力。 Icons描述性的图标可以帮你区分不同的文件和文件夹。图标也让开发过程更有趣。 下面是两个VSCode标签页的比较。一个有图标，另一个没有。有许多图标扩展可供选择。流行的图标扩展有： vscode-icons Material Icon ThemeMaterial Theme Icons Simple icons Settings Sync配置同步，使用多台机器时，该扩展将大有帮助。Settings sync让所有电脑/笔记本电脑依照visual studio的设置方式实现同步。同时在办公室电脑和家用电脑上工作的开发人员，基本上会在不同的地点工作。手动更改两端设置极为耗时，因为需要根据正在做的项目不时更改设置以便缓解编程压力。建议使用这个扩展,以便将所有所作更改都自动同步到个人电脑和工作点。 ESLint/TSLint(未测试使用)此扩展的主要功能是自动格式化代码，以便在整个团队中实现一致的格式化。ESLint也可以配置为自动格式化代码，无论何时出现错误，它都会发出一连串的警告。 Docker相关Dev Containers ： 在一个容器里面打开一个目录或仓库，同时可以利用VScode的所有特性。]]></content>
      <categories>
        <category>software</category>
        <category>Windows</category>
      </categories>
      <tags>
        <tag>环境配置</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Standard operating procedure for somatic variant refinement of sequencing data with paired tumor and normal samples]]></title>
    <url>%2F0001.%E7%9F%A5%E8%AF%86-05.Paper%2FPMC6450397-Standard%20operating%20procedure%20for%20somatic%20variant%20refinement%20of%20sequencing%20data%20with%20paired%20tumor%20and%20normal%20samples%2F</url>
    <content type="text"><![CDATA[该文献结合IGV可视化视图，对突变的人工审核方案提供了一些指导性的说明。 参考材料参考文献参考附件参考PPT]]></content>
      <categories>
        <category>NGS</category>
        <category>肿瘤检测</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[常用数据库 - HGNC]]></title>
    <url>%2F0001.%E7%9F%A5%E8%AF%86-04.Database%2FDatabase-HGNC%2F</url>
    <content type="text"><![CDATA[常见的 Gene ID 官网这个标准比较多，有Ensembl ID，HGNC ID，Entrez ID（NCBI），Refseq ID 数据库 链接地址 Ensembl https://asia.ensembl.org/index.html HGNC https://www.genenames.org/ Entrez https://www.ncbi.nlm.nih.gov/gene/672 【案例】 Refseq https://www.ncbi.nlm.nih.gov/nuccore/NM_031991.4【所有物种，很少用】 https://www.genenames.org/about/guidelines/ 基因命名映射关系建立由于面向临床生产过程中的基因名称，参考相关指南和质评规范，统一以HGNC命名为统一参考。HGNC官网是随时都在进行更新的，没有固定的更新周期。但是本身每个月会保留一次版本镜像快照。所以日常更新建议使用快照版本，以便后续数据的相关回溯。 快照记录索引链接 快照记录的包含所有基因的名称映射（包含假基因，非编码RNA等）可以根据需要进行筛选以提高后续效率。 关注基因的基因ID获取针对关注的转录本及基因信息，获取现有注释体系下，基因ID和注释基因名称之间的对应关系 123456# 基于目前版本提报的基因和转录本信息，通过注释配置文件，获取基因ID和基因名称的对应关系总列表文件grep -w -f /jdfstj1/B2C_COM_P1/PipeAdmin/04.Pipeline/aio.v2.Dev.liubo/chip_info/PanCancer_IDT_v1/PanCancer.v1.Trans.list /jdfstj1/B2C_COM_P1/PipeAdmin/04.Pipeline/aio.v2.Dev.liubo/database/Ref104/ncbi_anno_rel104.dbref &gt; ncbi_ref104.geneSymble2ID#基于NCBI生成的ncbi_ref104.geneSymble2ID 文件中，其中第9、10两列为后续用于映射的列第9列： ncbi注释的基因symble第10列：entrez_id HGNC数据整理准备12345678910111213141516171819# 首先下载HGNC官方对应名称信息，根据需要调整相应的文件日期wget http://ftp.ebi.ac.uk/pub/databases/genenames/hgnc/archive/monthly/tsv/hgnc_complete_set_2021-06-01.txt其中重要的列为第1,2,9,19,24列。# 提取其中的映射相关的重要信息示例如下：awk -F &apos;\t&apos; &apos;&#123;print $1&quot;\t&quot;$19&quot;\t&quot;$2&quot;\t&quot;$9&quot;\t&quot;$24&#125;&apos; /jdfstj1/B2C_COM_P1/Research_and_Development/Database/HGNC/hgnc_complete_set_2021-06-01.txt |head head hgnc_complete_set_2021-06-01.fit.tsvhgnc_id entrez_id symbol alias_symbol refseq_accessionHGNC:5 1 A1BG NM_130786HGNC:37133 503538 A1BG-AS1 FLJ23569 NR_015380HGNC:24086 29974 A1CF &quot;ACF|ASP|ACF64|ACF65|APOBEC1CF&quot; NM_014576#其中比较主要的信息分别如下#第1洌hgnc_id：HGNC的基因ID#entrez_id：NCBI的基因ID#symbol：基因统一命名#alias_symbol：基因历史别名# 所在列数 字段头 官方含义 字段信息 1 hgnc_id ID used to designate a gene family or group the gene has been assigned to. HGNC对应的基因ID 2 symbol Status of the symbol report, which can be either “Approved” or “Entry Withdrawn”. 基因命名 3 name miRBase ID 4 locus_group Same as “location” but single digit chromosomes are prefixed with a 0 enabling them to be sorted in correct numerical order (e.g. 02q34). 5 locus_type A group name for a set of related locus types as defined by the HGNC (e.g. non-coding RNA). 6 status snoRNABase ID 7 location lncRNA Database ID 8 location_sortable Cytogenetic location of the gene (e.g. 2q34). 9 alias_symbol Other names used to refer to this gene as seen in the “SYNONYMS” field in the gene symbol report. 基因曾用名 10 alias_name The HGNC ID that the Alliance of Genome Resources (AGR) have linked to their record of the gene. Use the HGNC ID to link to the AGR. 11 prev_symbol Gene names previously approved by the HGNC for this gene. Equates to the “PREVIOUS SYMBOLS &amp; NAMES” field within the gene symbol report. 12 prev_name Orphanet ID 13 gene_group The HGNC ID used within the GenCC database as the unique identifier of their gene reports within the GenCC database. 14 gene_group_id Name given to a gene family or group the gene has been assigned to. Equates to the “GENE FAMILY” field within the gene symbol report. 15 date_approved_reserved Symbol used within the Catalogue of somatic mutations in cancer for the gene. 16 date_symbol_changed The date the gene name was last changed. 17 date_name_changed Date the entry was last modified. 18 date_modified The date the entry was first approved. 19 entrez_id Ensembl gene ID. Found within the “GENE RESOURCES” section of the gene symbol report. 20 ensembl_gene_id International Nucleotide Sequence Database Collaboration (GenBank, ENA and DDBJ) accession number(s). Found within the “NUCLEOTIDE SEQUENCES” section of the gene symbol report. 21 vega_id UniProt protein accession. Found within the “PROTEIN RESOURCES” section of the gene symbol report. 22 ucsc_id The HGNC approved gene symbol. Equates to the “APPROVED SYMBOL” field within the gene symbol report. 23 ena The date the gene symbol was last changed. 24 refseq_accession Pubmed and Europe Pubmed Central PMID(s). HGNC提供的转录本ID 25 ccds_id Symbol used to link to the SLC tables database at bioparadigms.org for the gene 26 uniprot_ids UCSC gene ID. Found within the “GENE RESOURCES” section of the gene symbol report. 27 pubmed_id Pseudogene.org 28 mgd_id ID used to link to the MEROPS peptidase database 29 rgd_id RefSeq nucleotide accession(s). Found within the “NUCLEOTIDE SEQUENCES” section of the gene symbol report. 30 lsdb The locus type as defined by the HGNC (e.g. RNA, transfer). 31 cosmic Symbol used within the Human Cell Differentiation Molecule database for the gene 32 omim_id HGNC approved name for the gene. Equates to the “APPROVED NAME” field within the gene symbol report. 33 mirbase Mouse genome informatics database ID. Found within the “HOMOLOGS” section of the gene symbol report. 34 homeodb HGNC ID. A unique ID created by the HGNC for every approved symbol. 35 snornabase Rat genome database gene ID. Found within the “HOMOLOGS” section of the gene symbol report. 36 bioparadigms_slc Other symbols used to refer to this gene as seen in the “SYNONYMS” field in the symbol report. 37 orphanet Online Mendelian Inheritance in Man (OMIM) ID 38 pseudogene.org Symbols previously approved by the HGNC for this gene. Equates to the “PREVIOUS SYMBOLS &amp; NAMES” field within the gene symbol report. 39 horde_id Homeobox Database ID 40 merops NCBI and Ensembl transcript IDs/acessions including the version number for one high-quality representative transcript per protein-coding gene that is well-supported by experimental data and represents the biology of the gene. The IDs are delimited by . 41 imgt Symbol used within HORDE for the gene 42 iuphar ID used to link to the Human Intermediate Filament Database 43 kznf_gene_catalog The objectId used to link to the IUPHAR/BPS Guide to PHARMACOLOGY database. To link to IUPHAR/BPS Guide to PHARMACOLOGY database only use the number (only use 1 from the result objectId:1) 44 mamit-trnadb The name of the Locus Specific Mutation Database and URL for the gene separated by a character 45 cd Consensus CDS ID. Found within the “NUCLEOTIDE SEQUENCES” section of the gene symbol report. 46 lncrnadb The gene symbol used to link to LNCipedia - a comprehensive compendium of human long non-coding RNAs. 47 enzyme_id Entrez gene ID. Found within the “GENE RESOURCES” section of the gene symbol report. ncbi的基因symbol 48 intermediate_filament_db Symbol used within international ImMunoGeneTics information system 49 rna_central_ids Rat genome database gene ID. Found within the “HOMOLOGS” section of the gene symbol report. 50 lncipedia ID used to link to the Human KZNF Gene Catalog 51 gtrnadb ID used to designate a gene family or group the gene has been assigned to. 52 agr The HGNC ID that the Alliance of Genome Resources (AGR) have linked to their record of the gene. Use the HGNC ID to link to the AGR. 53 mane_select ID to link to the Mamit-tRNA database 54 gencc ENZYME EC accession number 信息来源 进行HGNC映射文件更新12345# 生成HGNC基因名称转换文件 perl /jdfstj1/B2C_COM_P1/Research_and_Development/Database/HGNC/creat.change_genelist.pl -hgnc hgnc_complete_set_2021-06-01.txt -ncbi ncbi_ref104.geneSymble2ID -o change_gene.list.tmp 结果文件示例如下（change_gene.list.tmp） ： entrez_id HGNC_symble NCBI_symble Tag 1 A1BG A1BG Match 29974 A1CF A1CF Match 2 A2M A2M Match 144568 A2ML1 A2ML1 Match 53947 A4GALT A4GALT Match 51146 A4GNT A4GNT Match 8086 AAAS AAAS Match 65985 AACS AACS Match 13 AADAC AADAC Match 前三列为基因对应的信息，第四列tag为示例信息，提示是否能匹配上： 如果标记为match，则表明HGNC和NCBI的对应基因ID可以匹配。 如果标记为MisMatch，则表明两个数据对应的基因ID无法进行匹配， 更新后，在目录下readme中记录数据库的更新日期和操作人员。12eg：# liubo4 @ 20210616]]></content>
      <categories>
        <category>知识沉淀</category>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>数据库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[常用数据库-Uniprot]]></title>
    <url>%2F0001.%E7%9F%A5%E8%AF%86-04.Database%2FDatabase-Uniport%2F</url>
    <content type="text"><![CDATA[最近由于临床报告中需要展示一些Uniprot的相关信息字段，因此需要对Uniprot中的部分重要信息进行获取，从而后续实现数据的本地化。因此需要对Uniprot数据库的使用方法和数据查询的机制进行了一些了解，记录如下 数据库简介维护机构通用蛋白质资源(UniProt)是蛋白质序列和注释数据的综合资源。 UniProt数据库包括UniProtKB (UniProt知识库)、UniRef (UniProt Reference Clusters)和UniProt Archive (UniParc)。 UniProt是欧洲生物信息学研究所(EMBL-EBI)、SIB瑞士生物信息学研究所(SIB Swiss Institute of Bioinformatics)和蛋白质信息资源研究所(PIR)的合作项目。UniProt联盟和主办机构EMBL-EBI、SIB和PIR都致力于长期保存UniProt数据库。 EMBL-EBI和SIB共同用于生成Swiss-Prot**和TrEMBL**, PIR生成Protein Sequence Database (PIR- psd)。 这两个数据集同时存在，但蛋白质序列覆盖范围和注释优先级不同。 TrEMBL(翻译后的EMBL核苷酸序列数据库)最初创建是因为序列数据的生成速度超过了Swiss-Prot的能力。 与此同时，PIR维护了PIR- psd及相关数据库，包括蛋白质序列数据库iProClass和整理的家族库。 2002年，这三家机构决定集中他们的资源和专业知识，成立了UniProt联盟。 EBI（ European Bioinformatics Institute）：欧洲生物信息学研究所（EMBL-EBI）是欧洲生命科学旗舰实验室EMBL的一部分。位于英国剑桥欣克斯顿的惠康基因组校园内，是世界上基因组学领域最强地带之一。 SIB（the Swiss Institute of Bioinformatics）：瑞士日内瓦的SIB维护着ExPASy（专家蛋白质分析系统）服务器，这里包含有蛋白质组学工具和数据库的主要资源。 PIR（Protein Information Resource）：PIR由美国国家生物医学研究基金会（NBRF）于1984年成立，旨在协助研究人员识别和解释蛋白质序列信息。 数据库组成截至目前数据库共包含4个subDatabase The UniProt Knowledgebase (UniProtKB)UniProt知识库， 特别是UniProtKB/Swiss-Prot，被用来访问蛋白质的功能信息。 每个UniProtKB条目都包含了氨基酸序列、蛋白质名称或描述、分类数据和引文信息，除此之外，我们还添加了尽可能多的注释。 这包括广泛接受的生物本体、分类和交叉引用，以及以实验和计算数据的证据归因的形式明确标注标注质量。 UniProt Reference Clusters (UniRef)UniRef数据库提供来自UniProtKB和UniParc记录的聚类序列集，以提供多个分辨率的序列空间的完整覆盖。 UniRef90和UniRef50的数据库大小分别减少了约40%和65%，提供了显著更快的序列搜索。 UniProt Archive (UniParc)UniParc是最全面的公开可访问的非冗余蛋白质序列数据库，提供这些序列的所有潜在来源和版本的链接。 你可以立即发现一个感兴趣的序列是否已经在公共领域，如果不是，就找出它最近的亲属。 UniProt Metagenomic and Environmental Sequences (UniMES)UniMES是一个专门存储宏基因组和环境数据的数据库。 更多详细信息可以参考官方说明文档 数据库使用UniProtKB 的使用由于本次数据的获取信息，均来自于Uniport的知识库，目前主要针对知识库进行介绍。进入知识库的主页,看到的信息如下图： 最上面是 搜索框， 左侧可以进行数据的过滤，例如肿瘤数据关注点主要是人的基因信息，可以直接选择Human剔除掉一些非人源的数据信息。 中间就是整个数据库数据的展示了。 重点介绍数据内容主题框的上面两个功能 Download 和 Columns Download字面意思，进行数据的下载，可以选择多种数据格式进行数据下载，tsv、gff、xml、fasta等等，我们可以根据需要选择相关格式进行下载 Columns这个功能可以说是一个非常人性化的功能，尤其是结合Download，可以完全不适用爬虫获取该数据库的所有需要的信息，点击进入Columns后，可以筛选在汇总表格中需要展示的字段信息（具体那些字段需要，可以在详细表中进行获取，毕竟下数据了，我们首先要知道获取什么数据）。示例如下图勾选我们需要的信息后，点击下方的 save, 就可以在内容中现实特定的信息，结合Download，可以实现快速的数据获取。 获取信息后，在进行简单的格式整理，就可以直接使用了，相比那些验收、IP检测、流量限制等方案层出不确定网站，可以说是非常友好了。 BGI的下载与处理需要的信息列： Entry (Names &amp; Taxonomy) Entry name (Names &amp; Taxonomy) Gene names (Names &amp; Taxonomy) Organism (Names &amp; Taxonomy) Length (Sequences) Repeat (Family &amp; Domains) Region (Family &amp; Domains) Zinc finger (Family &amp; Domains) Domain [FT] (Family &amp; Domains) Nucleotide binding (Function) Cross-reference (GeneID) DNA binding (Function) 基于肿瘤2022.3.1的解读需求，可以参考进行下载%20[9606]%22&amp;format=tab&amp;force=true&amp;columns=id,entry%20name,genes,organism,length,feature(REPEAT),feature(REGION),feature(ZINC%20FINGER),feature(DOMAIN%20EXTENT),feature(NP%20BIND),database(GeneID)&amp;sort=score&amp;compress=yes) 下载后，流程处理脚本使用 toolkits/07.DealWithDatabase/UniprotKB_DataClean.py（GitHub仓库） 对数据进行处理。处理后的文件结果示例如下： Gene GeneID GeneLength feature_key Position_region Uniport_description BLK 640; 505 Region 1..37; “Disordered” BLK 640; 505 Domain [FT] 58..118; “SH3” BLK 640; 505 Domain [FT] 124..220; “SH2” BLK 640; 505 Domain [FT] 241..494; “Protein kinase” BLK 640; 505 Nucleotide binding 247..255; “ATP” BDP1 KIAA1241 KIAA1689 TFNR 55814; 2624 Repeat 823..877; “1; approximate” BDP1 KIAA1241 KIAA1689 TFNR 55814; 2624 Repeat 878..932; “2” BDP1 KIAA1241 KIAA1689 TFNR 55814; 2624 Repeat 933..987; “3” BDP1 KIAA1241 KIAA1689 TFNR 55814; 2624 Repeat 988..1040; “4” BDP1 KIAA1241 KIAA1689 TFNR 55814; 2624 Repeat 1041..1094; “5” BDP1 KIAA1241 KIAA1689 TFNR 55814; 2624 Repeat 1095..1148; “6” BDP1 KIAA1241 KIAA1689 TFNR 55814; 2624 Repeat 1149..1203; “7” BDP1 KIAA1241 KIAA1689 TFNR 55814; 2624 Repeat 1204..1257; “8; approximate” BDP1 KIAA1241 KIAA1689 TFNR 55814; 2624 Repeat 1258..1327; “9; approximate” BDP1 KIAA1241 KIAA1689 TFNR 55814; 2624 Region 1..299; “Interaction with ZBTB43” BDP1 KIAA1241 KIAA1689 TFNR 55814; 2624 Region 1..142; “Disordered” BDP1 KIAA1241 KIAA1689 TFNR 55814; 2624 Region 193..241; “Disordered” BDP1 KIAA1241 KIAA1689 TFNR 55814; 2624 Region 355..470; “Required for phosphorylation by CSNK2A1” BDP1 KIAA1241 KIAA1689 TFNR 55814; 2624 Region 379..449; “Disordered”]]></content>
      <categories>
        <category>知识沉淀</category>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>数据库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[常用数据库-dbNSFP]]></title>
    <url>%2F0001.%E7%9F%A5%E8%AF%86-04.Database%2FDatabase-dbNSFP%2F</url>
    <content type="text"><![CDATA[数据库简介dbNSFP is a database developed for functional prediction and annotation of all potential non-synonymous single-nucleotide variants (nsSNVs) in the human genome. Its current version is based on the Gencode release 29 / Ensembl version 94 and includes a total of 84,013,490 nsSNVs and ssSNVs (splicing-site SNVs). It compiles prediction scores from 37 prediction algorithms (SIFT, SIFT4G, Polyphen2-HDIV, Polyphen2-HVAR, LRT, MutationTaster2, MutationAssessor, FATHMM, MetaSVM, MetaLR, CADD, CADD_hg19, VEST4, PROVEAN, FATHMM-MKL coding, FATHMM-XF coding, fitCons x 4, LINSIGHT, DANN, GenoCanyon, Eigen, Eigen-PC, M-CAP, REVEL, MutPred, MVP, MPC, PrimateAI, GEOGEN2, BayesDel_addAF, BayesDel_noAF, ClinPred, LIST-S2, ALoFT), 9 conservation scores (PhyloP x 3, phastCons x 3, GERP++, SiPhy and bStatistic) and other related information including allele frequencies observed in the 1000 Genomes Project phase 3 data, UK10K cohorts data, ExAC consortium data, gnomAD data and the NHLBI Exome Sequencing Project ESP6500 data, various gene IDs from different databases, functional descriptions of genes, gene expression and gene interaction information, etc.Some dbNSFP contents (may not be up-to-date though) can also be accessed through variant tools, ANNOVAR, KGGSeq, VarSome, UCSC Genome Browser’s Variant Annotation Integrator, Ensembl Variant Effect Predictor, SnpSift and HGMD. 参考官网 发表文献 Liu X, Jian X, and Boerwinkle E. 2011. dbNSFP: a lightweight database of human non-synonymous SNPs and their functional predictions. Human Mutation. 32:894-899. Liu X, Jian X, and Boerwinkle E. 2013. dbNSFP v2.0: A Database of Human Non-synonymous SNVs and Their Functional Predictions and Annotations. Human Mutation. 34:E2393-E2402. Liu X, Wu C, Li C and Boerwinkle E. 2016. dbNSFP v3.0: A One-Stop Database of Functional Predictions and Annotations for Human Non-synonymous and Splice Site SNVs. Human Mutation. 37:235-241. Liu X, Li C, Mou C, Dong Y, and Tu Y. 2020. dbNSFP v4: a comprehensive database of transcript-specific functional predictions and annotations for human nonsynonymous and splice-site SNVs. Genome Medicine. 12:103.]]></content>
      <categories>
        <category>知识沉淀</category>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>数据库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[math-百分位数的计算说明]]></title>
    <url>%2F0001.%E7%9F%A5%E8%AF%86-02.Math%2Fmath-%E7%99%BE%E5%88%86%E4%BD%8D%E6%95%B0%E7%9A%84%E8%AE%A1%E7%AE%97%E8%AF%B4%E6%98%8E%2F</url>
    <content type="text"><![CDATA[定义统计学术语，如果将一组数据从小到大排序，并计算相应的累计百分位，则某一百分位所对应数据的值就称为这一百分位的百分位数。可表示为：一组n个观测值按数值大小排列。如，处于p%位置的值称第p百分位数。 说明一：用99个数值或99个点，将按大小顺序排列的观测值划分为100个等分，则这99个数值或99个点就称为百分位数，分别以Pl，P2，…，P99代表第1个，第2个，…，第99个百分位数。第j个百分位数j=1,2…100。式中Lj，fj和CFj分别是第j个百分位数所在组的下限值、频数和该组以前的累积频数，Σf是观测值的数目。百分位通常用第几百分位来表示，如第五百分位，它表示在所有测量数据中，测量值的累计频次达5%。以身高为例，身高分布的第五百分位表示有5%的人的身高小于此测量值，95%的身高大于此测量值。百分位数则是对应于百分位的实际数值。= 说明二：第25百分位数又称第一个四分位数（First Quartile），用Q1表示；第50百分位数又称第二个四分位数（Second Quartile），用Q2表示，该值对应的也是中位数；第75百分位数又称第三个四分位数（Third Quartile）,用Q3表示。若求得第p百分位数为小数，可完整为整数。分位数是用于衡量数据的位置的量度，但它所衡量的，不一定是中心位置。百分位数提供了有关各数据项如何在最小值与最大值之间分布的信息。对于无大量重复的数据，第p百分位数将它分为两个部分。大约有p%的数据项的值比第p百分位数小；而大约有(100-p)%的数据项的值比第p百分位数大。对第p百分位数，严格的定义如下。第p百分位数是这样一个值，它使得至少有p%的数据项小于或等于这个值，且至少有(100-p)%的数据项大于或等于这个值。高等院校的入学考试成绩经常以百分位数的形式报告。比如，假设某个考生在入学考试中的语文部分的原始分数为54分。相对于参加同一考试的其他学生来说，他的成绩如何并不容易知道。但是如果原始分数54分恰好对应的是第70百分位数，我们就能知道大约70%的学生的考分比他低，而约30%的学生考分比他高。 计算原理下面的步骤来说明如何计算第p百分位数。 方法一第1步：以递增顺序排列原始数据（即从小到大排列）。第2步：计算指数i=n * p%第3步：l）若 i 不是整数，将 i 向上取整。大于i的毗邻整数即为第p百分位数的位置。2) 若i是整数，则第p百分位数是第i项与第(i+l)项数据的平均值。 方法二除了以上方法，再介绍另外一种方法，这种方法是SPSS所用方法，也是SAS所用方法之一。第一步：将n个变量值从小到大排列，X(j)表示此数列中第j个数。第二步：计算指数，设(n+1)P%=j+g，j为整数部分，g为小数部分。第三步：1)当g=0时：P百分位数=X(j);2)当g≠0时：P百分位数=gX(j+1)+(1-g)X(j)=X(j)+g*[X(j+1)-X(j)]。 相关代码函数pyhton123456789101112131415import numpy as np a = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])# 中位数print(np.median(a))# 25%分位数print(np.percentile(a, 25))# 75%分位数print(np.percentile(a, 75))# 输出结果：5.53.257.75]]></content>
      <categories>
        <category>知识沉淀</category>
      </categories>
      <tags>
        <tag>统计学</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Git.常见问题]]></title>
    <url>%2F0003.%E5%BA%94%E7%94%A8-99.other%2FGit-%E5%BC%82%E5%B8%B8%E9%97%AE%E9%A2%98%E5%A4%84%E7%90%86%E8%AE%B0%E5%BD%95%2F</url>
    <content type="text"><![CDATA[终端使用git 命令中文显示异常执行如下配置git config配置，可以正常显示中文1git config --global core.quotepath false CAfile &amp;Capath none报错内容123server certificate verification failed. CAfile: /etc/ssl/certs/ca-certificates.crt CRLfile: none” 这种情况一般使用ssh：链接可以同步，但是不能正常使用https ； 可以通过取消验证解决该问题12git config --global http.sslverify falsegit config --global https.sslverify false]]></content>
      <categories>
        <category>编程拾慧</category>
        <category>Git</category>
      </categories>
      <tags>
        <tag>问题处理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Survival Analysis]]></title>
    <url>%2F0001.%E7%9F%A5%E8%AF%86-01.NGS%2FSurvival_Analysis%2F</url>
    <content type="text"><![CDATA[缩略语 删失数据： 在实验过程中丢失的、失去跟踪的数据。部分纳入实验的样本，在试验过程中会无法观测到死亡事件的发生，比如无法联系到、或主动退出、或其他需要紧急处理退出临床试验的情况、以及试验结束时还未发生死亡等，这些数据就称作删失数据 右删失： 一些患者记录了从一开始到删失前的进展，而丢失了后续的结局，我们将这类删失称作右删失。 左删失： 我们要统计从初次患病到最终死亡的生存时间的分析，有些病人已知患有疾病且知道其死亡时间，但无法确定初次患病的时间，这样的删失则称为左删失。 生存概率(Survival probability)：指的是研究对象从试验开始直到某个特定时间点仍然存活的概率，可见它是一个对时间t的函数，我们定义之为 S(t)。 风险概率(Hazard probability)：指的是研究对象从试验开始到某个特定时间 t 之前存活，但在 t 时间点发生观测事件如死亡的概率，它也是对时间 t 的函数，定义为 H(t)。]]></content>
      <categories>
        <category>知识沉淀</category>
        <category>NGS</category>
        <category>肿瘤检测</category>
        <category>缩略语</category>
      </categories>
      <tags>
        <tag>肿瘤</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[肿瘤体系检测过程中的胚系过滤方案]]></title>
    <url>%2F0001.%E7%9F%A5%E8%AF%86-01.NGS%2FNGS-%E8%82%BF%E7%98%A4%E4%BD%93%E7%B3%BB%E6%A3%80%E6%B5%8B%E8%BF%87%E7%A8%8B%E4%B8%AD%E7%9A%84%E8%83%9A%E7%B3%BB%E8%BF%87%E6%BB%A4%E6%96%B9%E6%A1%88%2F</url>
    <content type="text"><![CDATA[他们是怎么做的？目前已经发表的一些体系检测软件，引用了一些检测方法，为我们提供了参考。主要有3个相对比较主流的方案： 从体系检测结果中减去胚系检出结果 体细胞变异调用者使用贝叶斯方法 Fisher精确统计方法 Fisher 检验（目前产品在用）参考文献[1] Hansen N F , Gartner J J , Lan M , et al. Shimmer: detection of genetic alterations in tumors using next-generation sequence data.[J]. Bioinformatics, 2013(12):1498-1503.[2] Koboldt D C , Zhang Q , Larson D E , et al. VarScan 2: Somatic mutation and copy number alteration discovery in cancer by exome sequencing[J]. Genome Research, 2012, 22(3):568-576. 介绍以Varscan为例，基于Fisher检验是否存在显著性（Pvalue &lt; 0.1) ，及双端情况将数据划分为3类（LOH、Germline、Somatic）。 补充说明因为目前监测体系变异检测，会涉及到一些低频检测需求(1%甚至更低的频率检测需求)，在临床使用中，发现一个难以避免的问题，会存在由于对照深度不足导致的P值永远难以显著（即使对照纯阴性也无法存在显著性差异）。模拟统计计算如下：组织深度1200x； 对照纯阴性。（对照阈值300x）对于检测限 1% 的突变（组织/血浆测序深度1200x时)，对照只有达到 389x 以上时，才可能有显著性。对于一个 3% 的突变（组织/血浆测序深度1200x时)，对照只有达到 127x 以上时，才可能有显著性。对于一个 0.5% 的突变（组织/血浆测序深度1200x时)，对照只有达到 843x 以上时，才可能有显著性。 WES产品 500x；对照纯阴性。（对照阈值200x）针对检测限 3% 的突变，纯阴对照需要达到 133x 才能存在显著性。针对一个 1% 的突变，纯阴对照需要达到 506x 才能存在显著性。 做减法参考文献[1] A comparative analysis of algorithms for somatic SNV detection in cancer.[J]. Bioinformatics, 2013.[2] GATK mutect2 介绍以GATK为例 A variant allele in the case sample is not called if the site is variant in controls.We explain an exception for GATK4 Mutect2 in a bit.Historically, somatic callers have called somatic variants at the site-level. That is, if a variant site in the case is also variant in the matched control or in a population resource, e.g. dbSNP, even if the variant allele is different than the control or resource it is discounted from the somatic callset. This practice stems in part from cancer study designs where the control normal sample is sequenced at much lower depth than the case tumor sample. Because of the assumption mutations strike randomly, cancer geneticists view mutations at sites of common germline variation with skepticism. Remember for humans, common germline variant sites occur roughly on average one in a thousand reference bases. So if a commonly variant site accrues additional mutations, we must weigh the chance of it having arisen from a true somatic event or it being something else that will likely not add value to downstream analyses. For most sites and typical analyses, the latter is the case. The variant is unlikely to have arisen from a somatic event and more likely to be some artifact or germline variant, e.g. from mapping or cross-sample contamination.GATK4 Mutect2 still applies this practice in part. The tool discounts variant sites shared with the panel of normals or with a matched normal control’s unambiguously variant site. If the matched normal’s variant allele is supported by few reads, at low allele fraction, then the tool accounts for the possibility of the site not being a germline variant.When it comes to the population germline resource, GATK4 Mutect2 distinguishes between the variant alleles in the germline resource and the case sample. That is, Mutect2 will call a variant site somatic if the allele differs from that in the germline resource. [1] A comparative analysis of algorithms for somatic SNV detection in cancer.[J]. Bioinformatics, 2013.[2] GATK mutect2 体细胞变异调用者使用贝叶斯方法参考文献[1] Cibulskis K , Lawrence M S , Carter S L , et al. Sensitive detection of somatic point mutations in impure and heterogeneous cancer samples[J]. Nature Biotechnology, 2013, 31(3):213-219.[2] Christopher, T, Saunders, et al. Strelka: accurate somatic small-variant calling from sequenced tumor-normal sample pairs.[J]. Bioinformatics, 2012.[3] Yuichi S , Yusuke S , Kenichi C , et al. An empirical Bayesian framework for somatic mutation detection from cancer genome sequencing data[J]. Nucleic Acids Research, 2013(7):e89-e89.[4] SomaticSniper[J]. Bioinformatics, 2012.[5] Identification of somatic mutations in cancer through Bayesian-based analysis of sequenced genome pairs[J]. Bmc Genomics, 2013, 14.]]></content>
      <categories>
        <category>NGS</category>
        <category>标准化</category>
        <category>变异检测</category>
      </categories>
      <tags>
        <tag>变异检测</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[解读癌症研究的里程碑文献：Hallmarks of Cancer]]></title>
    <url>%2F0001.%E7%9F%A5%E8%AF%86-05.Paper%2FSeries-Hallmarks_of_cancer%2F</url>
    <content type="text"><![CDATA[概述2000年1月7日，瑞士Agora转化癌症研究中心的Douglas Hanahan教授和麻省理工学院的Robert A Weinberg教授在Cell杂志合作发表了一篇开创性综述“Hallmarks of Cancer”[1]，该综述的发表改变了以往对癌症研究的散乱认知，将众多艰涩难懂的概念归纳到可以简短表述的特征中来。自此，癌症特征（Hallmarks of Cancer）形成一种启发式工具，为人们理解和探索癌症浩如烟海的表型与机制提供了基本的逻辑框架。更为可贵的是，“Hallmarks of Cancer”与时俱进，后又经历了2次“十年磨一剑”的更新，每次更新版本均可谓癌症研究领域的里程碑。在首版“Hallmarks of Cancer”综述中，充分梳理了过去近百年癌症研究的进展和发现，提炼出了癌症的6个特征，其成为Cell杂志历史上被引用次数最多（超过37 800次）的综述之一[1]。“Hallmarks of Cancer”中提出的恶性肿瘤细胞的6个特征分别是：自给自足的生长信号（self-sufficiency in growth signals）、对生长抑制信号不敏感（insensitivity to anti-growth signals）、逃避细胞凋亡（evading apoptosis）、无限复制的潜力（limitless replicative potential）、持续的血管新生（sustained angiogenesis）及组织侵袭转移（tissue invasion &amp; metastasis）。癌症的发病机制是多步骤的，正常细胞转变为肿瘤的过程中会获得这些特征性功能并最终发展成恶性病变。几乎所有的癌症都包括这6个特征，而这些特征在不同肿瘤中的优先级又因肿瘤的阶段和类型而不同，体现出肿瘤的异质性。 2011年，Douglas Hanahan和Robert A Weinberg 2位教授再次在Cell上发表了“Hallmarks of cancer：the next generation”[2]，将首版综述中的6个特征的概念描述进行了些许调整，同时在原有基础上增加了4个特征，分别是：细胞能量代谢的失控（deregulating cellular energetics）、逃避免疫清除（avoiding immune destruction）、肿瘤促炎症作用（tumor-promoting inflammation）及基因组的不稳定性和突变（genome instability and mutation）；此外，进一步明确了一个观点，即肿瘤不仅是肿瘤细胞数量上的增加，而且需要围绕肿瘤微环境进行理解。在该版本中，2位教授用10类药物作为例子佐证了肿瘤10个标志性特征具有的现实意义，如表皮生长因子受体抑制剂对抗“持续的增殖信号”、细胞周期依赖性激酶抑制剂对抗“逃避生长抑制”、血管内皮生长因子抑制剂对抗“持续的血管新生”、抗细胞毒性T淋巴细胞相关蛋白4单克隆抗体对抗“逃避免疫清除”等。 2022年1月，Douglas Hanahan教授在Cancer Discovery杂志发表了第3版“Hallmarks of Cancer：new dimensions”[3]，再增加了4个新的肿瘤标志性特征，分别是：解锁表型可塑性（unlocking phenotypic plasticity）、衰老细胞（senescent cells）、非突变表观遗传重编程（non-mutational epigenetic reprogramming）及多态性的微生物组（polymorphic microbiomes），这4个新增加特征中的前2个被定义为“新出现的特征”（emerging hallmarks），指仍需进一步研究和验证；后2个被定义为“赋能特征”（enabling hallmarks），表明其是肿瘤发展和其他标志性特征获得的驱动性因素。 Hallmarks of Cancer: New Dimensions参考资料原文链接Hallmarks of CancerHallmarks of Cancer：the next generationHallmarks of Cancer: New Dimensions PDFHallmarks of Cancer: New Dimensions.pdf 解析资料http://www.hxyx.com/article/10.7507/1007-9424.202202033]]></content>
      <categories>
        <category>NGS</category>
        <category>文献</category>
      </categories>
      <tags>
        <tag>肿瘤</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[顺反式突变]]></title>
    <url>%2F0001.%E7%9F%A5%E8%AF%86-01.NGS%2F%E9%A1%BA%E5%8F%8D%E5%BC%8F%E7%AA%81%E5%8F%98%2F</url>
    <content type="text"></content>
      <categories>
        <category>NGS</category>
        <category>肿瘤检测</category>
        <category>标准化</category>
      </categories>
      <tags>
        <tag>数据积累</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[靶向测序策略-扩增法和杂交捕获]]></title>
    <url>%2F0001.%E7%9F%A5%E8%AF%86-01.NGS%2F%E9%9D%B6%E5%90%91%E6%B5%8B%E5%BA%8F%E7%AD%96%E7%95%A5-%E6%89%A9%E5%A2%9E%E6%B3%95%E5%92%8C%E6%9D%82%E4%BA%A4%E6%8D%95%E8%8E%B7%2F</url>
    <content type="text"><![CDATA[NGS技术正逐年成熟，这使得全基因组测序的成本越来越低，但是对全基因组进行测序后得到的极其庞大、繁杂的数据量的分析工作并没有随之一起变得更加简单，恰恰相反，更高的测序深度反而导致最后的数据量变得更加庞大了,分析工作也变得更加困难。于是，测序技术的发展出现了两个极端的方向：一种是大而全的全基因组测序，一种是小而精的靶向重测序。 相比于全基因组测序（WGS），靶向重测序技术直接从样品中对感兴趣的基因组区域进行分离测序。这种方式能够更加高效且经济地发挥NGS技术的优势，后续的分析速度也会有跨越性的提升。比如外显子组仅占基因组的1%左右，但却包含了绝大部分的已知致病突变，将外显子区域分离出来后单独进行测序，后续的分析就能降低99%的工作量，极大的加快了分析的速度。 在遗传突变、肿瘤筛查等领域，靶向重测序所能达到的灵敏度也是全基因组测序完全无法实现的。由于靶向重测序在测序前就对基因的目标区域进行了分离与富集，目标区域的大幅减少可实现5000×甚至更高的测序深度。测序深度的提高意味着更高的灵敏度（能够检测低频率的变异），其检测极限低至0.1%。 杂交捕获技术通过设计与目标片段互补的生物素化探针，使其与含目的基因的片段进行杂交，以达到将目的基因片段富集后进行高通量测序的目的。根据支持物的不同，探针杂交捕获技术分为液相杂交与固相杂交两种。固相杂交由于其在花费与操作上的劣势，已基本被淘汰；液相杂交是在溶液中，目标片段和带有生物素标记的探针直接杂交，然后利用被链霉亲和素包裹的磁珠对杂交了生物素探针的片段进行吸附。洗去游离DNA后，将富集得到的DNA进行扩增，构建高通量测序文库。 优势 对目标序列有一定的容错率，75%的相似度就可以捕获。（doi:10.1128/mBio.01491-15）； 对DNA 完整性要求低，捕获法基于接头互补链接的方式构建文库，理论上可以容忍DNA片段长度比扩增法短； 可以处理cfDNA；劣势 捕获法对DNA起始量的要求是相对较高的，原因是需要先打断，而这个打断的过程通常有损失。损失最大的应该是超声法，其次是酶切法，损失最小的是转座酶法。扩增子捕获技术 扩增子捕获测序技术是一种目标区域高通量测序技术，利用特异性引物来对感兴趣的DNA区域进行PCR扩增，形成高度富集的DNA库，将PCR产物纯化后再进行文库构建和高通量测序。篇首所述的Ion Ampliseq技术便归属于扩增子捕获技术。 优势 PCR扩增这个灵敏度很高，对DNA起始量需求低；劣势 CNV、SV的检测受限（PCR效率会淹没基因组本身的情况） 对序列的相似度要求高，尤其是3’端，必须100%匹配否则无法扩增。 对片段完整性要求高，DNA过度片段化，会导致没有设计扩增子的空间。cfDNA（170bp左右）的设计已经困难； 不适用于甲基化（亚硫酸氢盐处理签不能PCR)参考参考信息参考信息]]></content>
      <categories>
        <category>NGS</category>
        <category>肿瘤检测</category>
      </categories>
      <tags>
        <tag>实验原理</tag>
        <tag>靶向测序</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[实验原理-Barcode]]></title>
    <url>%2F0001.%E7%9F%A5%E8%AF%86-01.NGS%2F%E5%AE%9E%E9%AA%8C%E5%8E%9F%E7%90%86-Barcode%2F</url>
    <content type="text"><![CDATA[barcode是做什么的$\qquad$前NGS检测在精准医疗领域广泛应用，但是目前主要测序机型和实际样本数据需求量之前仍然存在较大的距离，以MGISeq-2000为例，一次测序下机数据量约320G（80G*4Lane），但是目前临床产品所需的数据量普遍达不到这样的需求（一般在15G~40G不等），有一些小的靶向捕获芯片，所需数据量甚至不足1G。 $\qquad$在这种测序仪器的测序能力远大于单一测试样本需求数据量的情况下，为避免仪器浪费，一个lane同时测定多个样品成为很自然的思路。然而为了区分多种样品的序列，就必须要给不同样品加上特定的“标签”，从而可以在后续数据分析时将不同样品数据分开，而这个“标签”就是barcode。 $\qquad$简言之，barcode就是测序中混合样品的”身份证“，用于区分不同样品。 如何选择好的barcodebarcode的选择有两个原则：碱基平衡和激光平衡。所有的原则，都是尽可能保证数据的分布均匀，不会给测序过程带来严重的干扰。 碱基平衡碱基平衡是指的需要兼顾barcode序列的平衡度与复杂度，平衡度是指的碱基的比例是均衡的（1:1是最均衡的），而复杂度是指的碱基的种类是多样的（四种碱基同时存在是最多样的）。 所以最好的barcode序列应该是同时有A、T、G、C四种碱基，且各碱基所占比例近似均为25%。 此处所说的碱基平衡是指的多个barcode之间的平衡，并非一个barcode内部的碱基平衡。举例来说，有12个转录组样品需要测定，那么就需要12个barcode（假定每个barcode长度为6位），根据碱基平衡原则，第一位barcode碱基应该尽量同时存在A、T、G、C四种碱基，且各碱基所占比例近似均为25%，也就是这12个barcode序列最佳情况应该是以A、T、G、C开头各3个。剩余5个碱基位的barcode以此类推。 激光平衡在illumina测序仪中，A和C两种碱基共用一种激光，由波长660nm的红激光激发；G和T共用一种激光，由波长532 nm的绿激光激发。因此假使不能满足碱基平衡的情况下，可以退而求其次，尽量满足激光平衡。 简单来说，激光平衡就是尽量在使用的一组barcode中满足每个碱基位都是A+C=G+T。 既不满足碱基平衡，又不满足激光平衡的barcode将会有很大的数据分离隐患，或者无法分离开样品，或者无法识别某些测序片段。]]></content>
      <categories>
        <category>NGS</category>
        <category>肿瘤检测</category>
      </categories>
      <tags>
        <tag>实验原理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[NGS检测相关缩略语说明表]]></title>
    <url>%2F0001.%E7%9F%A5%E8%AF%86-01.NGS%2F2022-01-10.NGS%E6%A3%80%E6%B5%8B%E7%9B%B8%E5%85%B3%E7%BC%A9%E7%95%A5%E8%AF%AD%E8%AF%B4%E6%98%8E%E8%A1%A8%2F</url>
    <content type="text"><![CDATA[NGS检测相关实验方向 缩略语 全称 含义说明 PEC 延伸型探针捕获(Probe Extension Capture) 一种先使探针与目标区域上游结合，然后以探针为引物通过PCR延伸获取目标区域DNA序列的捕获技术。 PE测序 双端(Pair-End)测序 - 。 信息分析方向 缩略语 全称 含义说明 VAF 突变频率(Variant Allele Frequency) 基因组某个位点支持突变的reads覆盖深度占这个位点总reads覆盖深度的比例。 Dup Duplication 为了提高丰度在实验前期进行PCR方法对模板进行扩增，扩增后一个模板可能会有多个不同的PCR产物被测到，一个模板被重复检测的测序数据成为Dup 肿瘤检测相关 缩略语 全称 含义说明 UMI Unique molecular identifiers 对原始样本基因组打断后的每一个片段都加上一段特有的标签序列，用于区分同一样本中成千上万的不同的片段，在后续的数据分析中可以通过这些标签序列来排除由于 DNA 聚合酶和扩增以及测序过程中所引入的错误。 MSI Microsatellite Instability 与正常组织相比，肿瘤中某个微卫星位点由于重复单元的插入或缺失而出现新的微卫星等位基因的现象。 MSI的发生是由于肿瘤组织的DNA错配修复出现功能性缺陷导致。 伴随着DNA错配修复缺陷的MSI现象是临床上的一项重要的肿瘤标志物。 临床医学相关 缩略语 全称 含义说明 DLT 剂量限制性毒性 是基于系统性抗癌症治疗在第一个周期中出现严重毒性来定义的。 此类毒性是根据美国国家癌症研究所的不良事件通用术语标准（CTCAE）进行评估的，通常涵盖所有3级或更高的毒性，定义时一般会将3级非发热性中性粒细胞减少症和脱发作为例外。 MTD 最大耐受剂量 RP2D II期推荐剂量 DOR 缓解持续时间 CR 完全缓解(complete response) 所有靶病灶消失，无新病灶出现，且肿瘤标志物正常，至少维持4周。 PR 部分缓解(partial response) 靶病灶最大径之和减少≥30%，至少维持4周。 PD 疾病进展(progressive disease) 靶病灶最大径之和至少增加≥20%，或出现新病灶。 SD 疾病稳定(stable disease) 靶病灶最大径之和缩小未达PR，或增大未达PD。 OS 总生存期(overall survival) 从随机化（random assignment）开始至因任何原因引起死亡的时间（失访患者为最后一次随访时间；研究结束时仍然存活患者，为随访结束日）。 MST 中位生存期(Median Survival Time) 又称为半数生存期，表示有且只有50%的个体可以活过这个时间。评估某个癌种的中位生存期，一般从发现该肿瘤开始计算；如果是评估某项临床试验的中位生存期，一般从给药或随机开始。 DFS 无病生存期/无疾病生存时间 (Disease Free Survival) 指从随机化开始至第一次肿瘤复发/转移或由于任何原因导致受试者死亡的时间(失访患者为最后一次随访时间；研究结束时仍然存活患者，为随访结束日)。 中位DFS 中位DFS 又称半数无病生存期，表示恰好有50％的个体未出现复发/转移的时间。 TTP 疾病进展时间 (，Time To Progression) 指从随机分组开始到第一次肿瘤客观进展的时间。(TTP与PFS唯一不同在于PFS包括死亡，而TTP不包括死亡。因此PFS更能预测和反应临床受益，与OS一致性更好；而TTP在预测临床受益方面则较差，因其仅考虑抗肿瘤活性，在分析时较早时期的死亡情况被删失，导致一些重要信息的丢失。在导致死亡的非肿瘤原因多于肿瘤原因的情况下，TTP是一个合适的指标。TTP同样也需有明确的定义。) PFS 无进展生存期(Progress Free Survival) 指从随机分组开始到第一次肿瘤进展或死亡时间。通常作为晚期肿瘤疗效评价的重要指标。 ORR 客观缓解率(Objective Response Rate) 是指肿瘤缩小达到一定量并且保持一定时间的病人的比例(主要针对实体瘤)，包含完全缓解(CR，Complete Response)和部分缓解(PR，Partial Response)的病例。(客观缓解率是II期试验的主要疗效评价指标，可提供药物具有生物活性的初步证据。但一般不作为III期临床试验的主要疗效指标。) DCR 疾病控制率 (DCR，Disease Control Rate) 是指肿瘤缩小或稳定且保持一定时间的病人的比例(主要针对实体瘤)，包含完全缓解(CR，Complete Response)、部分缓解(PR，Partial Response)和稳定(SD，Stable Disease)的病例 DOR 缓解持续时间 (DOR，Duration of Response) 是指肿瘤第一次评估为CR或PR开始到第一次评估为PD(Progressive Disease)或任何原因死亡的时间。 TTF 治疗失败时间(TTF，Time To Failure) 是指从随机化开始至治疗中止/终止的时间，包括任何中止/终止原因，如疾病进展、死亡、由于不良事件退出、受试者拒绝继续进行研究或者使用了新治疗的时间。(TTF综合了有效性与毒性的评价，是一个具有综合特性的指标，不推荐作为单独支持药物批准的疗效指标。) DDC 疾病控制时间 (DDC，duration of disease control) 是指肿瘤第一次评估为CR、PR或SD开始到第一次评估为PD(Progressive Disease)或任何原因死亡的时间。 OS 总生存期（OS，overall survival） 从随机化（random assignment）开始至因任何原因引起死亡的时间（失访患者为最后一次随访时间；研究结束时仍然存活患者，为随访结束日）。 DOR 总缓解期（Duration of overall response） 从第一次出现CR或PR，到第一次诊断PD或复发的时间。 DSD 疾病稳定期（duration of stable disease） 是指从治疗开始到评价为疾病进展时的这段时间。 ORR 总缓解率 （ORR，overall response rate） 经过治疗CR+PR病人总数占对于总的可评价病例数的比例。 RR 缓解率（RR, response rate） 达到CR、PR的病人占同期病人总数的百分比。 CBR 临床获益率（CBR，clinical benefit rate） CR+PR+SD。 一线治疗 是指诊断以后的首轮治疗，这时的治疗方案效果最好、副作用最小，也称为基本治疗或疗法。一线治疗的目的是在可能的情况下“治愈”癌症。 二线治疗 指的是在一线治疗后，患者再次出现疾病进展，对一线治疗方案产生耐药，需要更换抗癌机理不同的方案。和一线相比，二线治疗方案或疗效劣于一线，或副作用偏大，或价格偏高。 三线治疗 指的是二线治疗失败后，再次换用的其它治疗方案。一般到三线治疗时，可选择的药物和有效的治疗方案就会越少。]]></content>
      <categories>
        <category>知识沉淀</category>
        <category>NGS</category>
        <category>肿瘤检测</category>
        <category>缩略语</category>
      </categories>
      <tags>
        <tag>数据积累</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[肿瘤检测数据知识点]]></title>
    <url>%2F0001.%E7%9F%A5%E8%AF%86-01.NGS%2F%E6%A6%82%E5%BF%B5-%E8%82%BF%E7%98%A4NGS%E7%9B%B8%E5%85%B3%E7%9F%A5%E8%AF%86%E7%82%B9%2F</url>
    <content type="text"><![CDATA[cfDNA片段长度cfDNA片段长度约170bp 参考文献 Lengthening and shortening of plasma DNA in hepatocellular carcinoma patients 中靶率（on target rate）液相杂交捕获是一种容错型富集方法，无法做到100%中靶（On target）。在测序下机数据中，总存在一定比例的脱靶数据（Off target），即非靶向区域数据，若该部分数据占比过多，会造成有效利用数据减少。通常使用碱基或者读长片段来计算中靶率，中靶率代表了靶向捕获区域范围内的碱基数或读长片段数占有效测序下机数据中碱基总数或读长片段总数的比例。本次评估中使用碱基数作为评估标准，即%中靶=中靶的碱基数/总有效碱基数。 覆盖度（coverage）覆盖度评估包括读长深度（Read depth）以及完整性（completeness）。需要评估并明确平均以及最小覆盖深度、覆盖均一性以及超过最小覆盖深度的目标区域碱基比例的阈值。需要评估产品宣传检测区域的覆盖度或者完整度比例。平均覆盖深度（average coverage depth）覆盖深度指被测序的DNA片段比对（mapping）到基因组靶向区域的次数，平均覆盖深度指整个检测区域中，各靶向区域覆盖深度的均值，靶向区域被覆盖的越深，其测序结果的可靠性和灵敏度越高。当评估检测所使用的适合的覆盖深度，可以使用标准品或者前期特征化的样本来进行深度定义，即适当平均深度条件下，额外的测序深度覆盖度不能显著的提高测序的准确性。 覆盖均一性（coverage uniformity）在液相杂交捕获体系中，针对不同GC含量、碱基重复性区域等，探针经过捕获以及PCR扩增后会产生不同程度的数据偏好性，体现在不同目标区域检测到的覆盖深度不完全相同，标准差与平均数的比值越小，说明探针的捕获均一性越好。ps:GC含量高的区域经常会降低覆盖均一性。 Fold80芯片捕获区 80%的区域可以达到平均深度的所需要增加的数据量； 一般经验上要求Fold-80要小于2。一个比较直观的示意图如下：Evenness of coverage can be evaluated by the fold80 measure which represents the amount of additional sequencing needed to have 80% of all targets covered at the currently observed mean. It is computed as the mean coverage divided by the 20th percentile. Smaller values indicate tighter coverage distributions. Left, large fold80 values correspond to a wide distribution and uneven coverage; Right, small values correspond to a narrow distribution and even coverage. 同样Twist也提供了一个比较详细的介绍说明CD: desired coverageCM: the mean coverage actually observed in the experiment.引自： TWIST: The Importance of Coverage Uniformity Over On-Target Rate for Efficient Targeted NGS]]></content>
      <categories>
        <category>NGS</category>
        <category>肿瘤检测</category>
      </categories>
      <tags>
        <tag>基础概念</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[肿瘤产品资质相关概念]]></title>
    <url>%2F0001.%E7%9F%A5%E8%AF%86-01.NGS%2F%E6%A6%82%E5%BF%B5-%E4%BA%A7%E5%93%81%E8%B5%84%E8%B4%A8%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5%2F</url>
    <content type="text"><![CDATA[资质各阶段环节 Reseach Use Only (RUO) products :Investigational Use Only (IUO) Products :General Purpose Reagent (GPR) Products :Analyte Specific Reagent (ASR) Products :ReferenceRegulatory &amp; More]]></content>
      <categories>
        <category>NGS</category>
        <category>肿瘤检测</category>
      </categories>
      <tags>
        <tag>基础概念</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[肿瘤相关融合人群检出率报道]]></title>
    <url>%2F0001.%E7%9F%A5%E8%AF%86-01.NGS%2F2022-01-07.%E8%82%BF%E7%98%A4%E7%9B%B8%E5%85%B3%E8%9E%8D%E5%90%88%E4%BA%BA%E7%BE%A4%E6%A3%80%E5%87%BA%E7%8E%87%E6%8A%A5%E9%81%93%2F</url>
    <content type="text"><![CDATA[NTRK 相关融合参考文献Genomic context of NTRK1/2/3 fusion-positive tumours from a large real-world population 内容本研究旨在查询综合基因组分析数据的大型真实世界数据库，以描述NTRK基因融合的基因组景观和流行情况。NTRK融合阳性肿瘤是从超过 295,000 名癌症患者的 FoundationCORE ®数据库中确定的。我们调查了NTRK融合的患病率和伴随的基因组景观，预测了患者血统，并将 FoundationCORE 队列与 entrectinib 临床试验队列进行了比较 (ALKA-372-001 [EudraCT 2012-000148-88]; STARTRK-1 [NCT02097810-2]; [NCT02568267]）。整体NTRK在具有 88 个独特融合伴侣对的 45 种癌症中，融合阳性肿瘤的患病率为 0.30%，其中 66% 以前未报告过。在所有病例中，≥18 岁和 &lt;18 岁患者的患病率分别为 0.28% 和 1.34%；5 岁以下患者的患病率最高 (2.28%)。在唾液腺肿瘤中观察到NTRK融合的最高流行率(2.62%)。存在NTRK基因融合并没有与其他临床生物标志物可操作相关成分; 在乳腺癌或结直肠癌 (CRC) 中没有与已知的致癌驱动因素同时发生。然而，在 CRC 中，NTRK融合阳性与自发性微卫星不稳定性 (MSI) 相关；在这个 MSI CRC 子集中，与BRAF互斥观察到突变。NTRK融合阳性肿瘤类型在 FoundationCORE 和 entrectinib 临床试验中具有相似的频率。NTRK基因融合患病率因年龄、癌症类型和组织学而异。询问大型数据集有助于更好地了解癌症非常罕见的分子亚群的特征，并允许识别基因组模式和以前未报告的融合伙伴，在较小的数据集中不明显。 数据展示：]]></content>
      <categories>
        <category>知识沉淀</category>
        <category>NGS</category>
        <category>肿瘤检测</category>
        <category>数据积累</category>
      </categories>
      <tags>
        <tag>数据积累</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[肿瘤相关融合人群检出率报道]]></title>
    <url>%2F0001.%E7%9F%A5%E8%AF%86-01.NGS%2F%E6%A6%82%E5%BF%B5-%E8%82%BF%E7%98%A4-%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5%2F</url>
    <content type="text"><![CDATA[概念tumorTumors 更多的是指肿块，但这些肿块并非全部都是恶性的，其中存在一些是良性，所以需要记住 tumors 和 cancers 并非同义词。 cancers是tumors中的一部分（恶性部分）。 cancer是tumor中的一种，特指恶性肿瘤（malignant tumor），癌症是用来形容具有异常的失控的细胞分裂，并能侵入其他组织的疾病的一个术语。癌细胞可通过血液和淋巴系统扩散到身体的其他部位。cancer本身也不是一个单一的疾病而是100多种癌症的总称。大多数癌症以其出现的器官或细胞类型命名， 例如，始发于结肠的癌症，被称为结肠癌，始发于皮肤基底细胞的癌症称为基底细胞癌。 癌症类型可归为广义范畴。癌症的种类主要包括： Carcinomacancer that begins in the skin or in tissues that line or cover internal organs. 癌-起始于皮肤或组织，呈线条状或覆于内脏的癌症。 Sarcoma（肉瘤）cancer that begins in bone, cartilage, fat, muscle, blood vessels, or other connective or supportive tissue.起始于骨，软骨，脂肪，肌肉，血管，或其他结缔组织或支持组织的癌症。 Leukemia（白血病）cancer that starts in blood-forming tissue such as the bone marrow and causes large numbers of abnormal blood cells to be produced and enter the blood.起始于造血组织，如骨髓，可生成大量异常血细胞并进入血液的癌症。 Lymphoma and myeloma（淋巴瘤和骨髓瘤）cancers that begin in the cells of the immune system.起始于免疫系统细胞的癌症 Central nervous system cancers（中枢神经系统癌症）cancers that begin in the tissues of the brain and spinal cord.起始于大脑和脊髓组织的癌症。 癌症的起源All cancers begin in cells, the body’s basic unit of life. To understand cancer, it’s helpful to know what happens when normal cells become cancer cells. 所有的癌症起始于细胞---人体基本的生命单位。为了理解癌症，了解正常细胞何时会变为癌细胞非常有用。 The body is made up of many types of cells. These cells grow and divide in a controlled way to produce more cells as they are needed to keep the body healthy. When cells become old or damaged, they die and are replaced with new cells. 身体是由许多类型的细胞构成。因为他们需要保持身体健康，这些细胞在一个可控制的方式下生长和分化，以产生更多的细胞。当细胞衰老或损坏时，细胞死亡并被新的细胞所取代。 However, sometimes this orderly process goes wrong. The genetic material (DNA) of a cell can become damaged or changed, producing mutations that affect normal cell growth and division. When this happens, cells do not die when they should and new cells form when the body does not need them. The extra cells may form a mass of tissue called a tumor. 但是，有时这种有序的过程也会出现错误。一个细胞的遗传物质（DNA），可受损或改变，产生基因突变，从而影响正常的细胞生长和分裂。在这种情况下，细胞应该死亡但并未死亡，身体并不需要时新细胞形成。额外的细胞可能形成一个所谓的肿瘤组织肿块。 Not all tumors are cancerous; tumors can be benign or malignant. 并非所有的肿瘤都是癌症性的，肿瘤也可是良性或恶性。 Benign tumors aren’t cancerous. They can often be removed, and, in most cases, they do not come back. Cells in benign tumors do not spread to other parts of the body. 良性肿瘤不是癌性的。它们通常可以切除，并且，在大多数情况下，不会复发。良性肿瘤的细胞不会扩散到身体的其他部位。 Malignant tumors are cancerous. Cells in these tumors can invade nearby tissues and spread to other parts of the body. The spread of cancer from one part of the body to another is called metastasis. 恶性肿瘤是癌性的。这些肿瘤细胞可侵入附近组织并扩散到身体的其他部位。癌症从身体的一部分扩散至另一部分称为转移。 Some cancers do not form tumors. For example, leukemia is a cancer of the bone marrow and blood. 有些癌症不形成肿瘤。例如，白血病是骨髓和血液的一种癌症。 理解癌症Cancer begins in cells, the building blocks that form tissues. Tissues make up the organs of the body. 癌症细胞起始于细胞，并形成肿块。 Normally, cells grow and divide to form new cells as the body needs them. When cells grow old, they die, and new cells take their place. 通常情况下，细胞生长、分裂，形成新的细胞，这是人体所需要的。当细胞衰老时，细胞死亡，新细胞取代其位置。 Sometimes, this orderly process goes wrong. New cells form when the body does not need them, and old cells do not die when they should. These extra cells can form a mass of tissue called a growth or tumor. 有时，这种有序的过程中出现错误。当身体并不需要时，新细胞形成，衰老的细胞应该死亡但并未死亡。这些额外的细胞可以形成一个组织团块称为增生或肿瘤。 Tumors can be benign or malignant: 肿瘤可以是良性或恶性： Benign tumors are not cancer: 良性肿瘤不是癌性的： Benign tumors are rarely life-threatening. 良性肿瘤很少危及生命。 Generally, benign tumors can be removed, and they usually do not grow back. 一般来说，良性肿瘤可以切除，而他们通常不会复发。 Cells from benign tumors do not invade the tissues around them. 良性肿瘤细胞不侵入周围组织。 Cells from benign tumors do not spread to other parts of the body. 良性肿瘤细胞不会扩散到身体的其他部位。 Malignant tumors are cancer: 恶性肿瘤是癌性的： Malignant tumors are generally more serious than benign tumors. They may be life-threatening. 恶性肿瘤一般都比良性肿瘤严重。他们可能会危及生命。 Malignant tumors often can be removed, but sometimes they grow back. 恶性肿瘤往往可以切除，但有时它们重新生长出来（复发）。 Cells from malignant tumors can invade and damage nearby tissues and organs. 恶性肿瘤细胞可侵入和破坏邻近组织和器官。 Cells from malignant tumors can spread (metastasize) to other parts of the body. Cancer cells spread by breaking away from the original (primary) tumor and entering the bloodstream orlymphatic system. The cells can invade other organs, forming new tumors that damage these organs. The spread of cancer is called metastasis. 恶性肿瘤细胞可以扩散（转移）到身体的其他部位。癌细胞经脱离原（主）肿瘤并进入血液系统或淋巴系统发生扩散。细胞可以侵入其他器官，形成新的肿瘤并损害这些器官。癌症的扩散称为转移。 Most cancers are named for where they start. For example, lung cancer starts in the lung, and breast cancer starts in the breast. Lymphoma is cancer that starts in the lymphatic system. And leukemia is cancer that starts in white blood cells (leukocytes). 大多数癌症以其起始部位命名。例如，肺癌起始于肺，乳腺癌起始于乳腺。淋巴瘤是起始于淋巴系统的癌症。白血病是起始于白细胞的癌症。 When cancer spreads and forms a new tumor in another part of the body, the new tumor has the same kind of abnormal cells and the same name as the primary tumor. For example, if prostate cancer spreads to the bones, the cancer cells in the bones are actually prostate cancer cells. The disease is metastatic prostate cancer, not bone cancer. For that reason, it is treated as prostate cancer, not bone cancer. Doctors sometimes call the new tumor “distant” or metastatic disease. 当癌细胞扩散，并在身体的其他部位形成一种新的肿瘤时，新的肿瘤具有同一种异常细胞，名称与原发肿瘤相同。例如，如果前列腺癌扩散到骨，骨头的癌细胞其实是前列腺癌细胞。这种疾病是转移性前列腺癌，而不是骨癌。出于这个原因，被视为前列腺癌进行治疗，而不是骨癌。医生有时称之为“远部”新肿瘤或转移性疾病。 参考tumor，cancer，carcinoma…你弄清楚了吗？]]></content>
      <categories>
        <category>NGS</category>
        <category>肿瘤检测</category>
      </categories>
      <tags>
        <tag>基础概念</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Fish验证简介]]></title>
    <url>%2F0001.%E7%9F%A5%E8%AF%86-02.Math%2F2022-01-05-Fish%E9%AA%8C%E8%AF%81%E7%AE%80%E4%BB%8B%2F</url>
    <content type="text"><![CDATA[By 赵东晓 NGS检测过程中经常遇到需要第三方验证的需求，不管是为了证明检测结果准确性，还是前期方法学检测的需求，都不可避免的需要面对各种第三方验证。而Fish检验就是一个常用的用来检验融合的方法。最近同事对Fish进行了一些调研，简单梳理以备后用。 Fish调研NGS报告融合推药逻辑 1类融合多见，为常见伴侣融合，是指南里面提到的，且有功能证据证明激活的融合亚型； 针对1类融合，相同基因，不同断点和亚型，现在药物推荐还没有区分差别。 2类融合少见，是指那些没有指南支持，仅有少量文献证据的罕见融合； 3类融合不推药； 通过FISH（荧光原位杂交）检测融合，目前有2种方法： 第1种（应用较广）：可以设计已知融合基因的探针，比如针对ALK的，用红色和黄色覆盖ALK基因的上下游区域，正常就是红黄在一起，融合就是ALK发生了断裂就是红黄分开了。此方法检出的融合，不能确定是何种融合亚型，推药只能按照广义层面上的融合大类来推荐用药。 第2种（不经济实用）：可以设计已知融合基因和伴侣基因的探针，比如针对ALK和EML4的，ALK全部用红色探针覆盖，EML4用黄色探针覆盖，正常就是红黄分开，融合就是红黄在一起。此方法检出的融合，为常见伴侣基因的融合，为1类融合，推药逻辑没有差异。 以ALK探针为例：Vysis ALK break-apart probe (Abbott Molecular)美国食品药品监督管理局(FDA)已批准的FISH分离探针试剂盒(Vysis ALK Break Apart FISH Probe Kit; Abbott Molecular, Inc.)可用于检测ALK融合基因的表达。该试剂盒设计的两种探针分别标记ALK基因第20号外显子断裂点的两端，在5 ‘(着丝粒)侧有一个约442 kb的绿色探针，在3 ‘(端粒)侧有一个约300 kb的橙色探针，橙色区域包括了ALK激酶活性区。 ALK基因：chr2:29,415,640-30,144,452 (GRCh37/hg19 by Entrez Gene)，Size:728,813 bases. 结果判读阴性信号： 一个癌细胞核内至少有一个橙色和一个绿色信号，橙色信号与绿色信号相互邻近或叠加，其问距小于两个信号直径。 一个癌细胞核内至少有一个橙色和一个绿色信号，有单独的绿色信号，但无相应的橙色信号。 阳性信号： 一个癌细胞核内至少有一个橙色和一个绿色信号，橙色和绿色信号的间距大于两个信号直径。 一个癌细胞核内至少有一个橙色和一个绿色信号，有单独的橙色信号，但无相应的绿色信 阳性判定： 计数50个肿痛细胞，若阳性肿指细胞数多于25个，该样本为阳性； 若阳性肿指细胞数小于5个，该样本为阴性； 阳性肿指细胞数介于5-25个，为可疑阳性。需要另计数50个肿指细胞，将前后两次的合计100个肿瘤细胞的信号状况汇总。 若阳性肿痛细胞比例少于1598(15/100),该样本为阴性。 着阳性肿痛细胞比例多于159(15/100,该样本为阳性。 该方法只能判断ALK基因是否断裂，可以检测所有的融合型，但不能区分与其发生融合的基因是什么。阳性细胞为存在橙绿信号分离或单独橙色信号的细胞。 ALK基因的检测方法ALK基因的检测方法有荧光原位杂交（FISH）、显色原位杂交（CISH）、免疫组化（IHC）、基于PCR的各种方法、NGS二代测序等。 检测ALK融合的各种技术的特点总结参考：中国间变性淋巴瘤激酶(ALK)阳性非小细胞肺癌诊断专家共识(2013版) 参考：Int J Mol Sci. 2019 Aug 13;20(16):3939. doi: 10.3390/ijms20163939. 总结ALK基因融合的检测要尽可能采用两种以上的方法相互印证，以免出现漏检或假阴性结果，使得部分病友失去从靶向药中获益的机会。检测实验室应该根据组织标本类型选择合适的检测技术。当怀疑一种技术的可靠性时（如FISH的肿瘤细胞融合率接近15％时），可以考虑采用另一种技术加以验证。]]></content>
      <categories>
        <category>NGS</category>
        <category>肿瘤检测</category>
      </categories>
      <tags>
        <tag>肿瘤</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[IGV的使用]]></title>
    <url>%2F0003.%E5%BA%94%E7%94%A8-03.software%2FSoftware-%E5%9F%BA%E5%9B%A0%E7%BB%84%E5%8F%AF%E8%A7%86%E5%8C%96-IGV%2F</url>
    <content type="text"><![CDATA[官网资料IGV]]></content>
      <categories>
        <category>software</category>
        <category>Linux</category>
        <category>可视化</category>
      </categories>
      <tags>
        <tag>可视化</tag>
        <tag>IGV</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Jbrowse部署安装]]></title>
    <url>%2F0003.%E5%BA%94%E7%94%A8-03.software%2FSoftware-%E5%8F%AF%E8%A7%86%E5%8C%96-Jbrowse%E9%83%A8%E7%BD%B2%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[腾讯云服务如果服务不存在，登陆腾讯云服务器执行如下命令12cd /var/www/html/jbrowsenohup npx serve . &amp; 前言JBrowse 是今天要介绍的主角。它是GMOD( Generic Model Organism Database) 开源项目的一部分,这个项目收集了非常多的开源软件工具，用来管理、可视化、存储、展示基因组学数据。 the Generic Model Organism Database project, a collection of open source software tools for managing, visualising, storing, and disseminating genetic and genomic data. ---GMOD官网在GMOD的众多项目中最出名的应该是Galaxy，而JBrowse则是GMOD之前一款基因浏览器JGBrowse的继任者。 JBrowse资料JBrowse官网GMOD-JBrowse WIkiJBrowse2 Github项目 JBrowse 部署安装镜像调整设置yum镜像123456789cd /etc/yum.repos.d/mkdir centos.backmv *.repo centos.back# 获取新镜像wget http://mirrors.aliyun.com/repo/Centos-7.repo 阿里的源wget http://mirrors.163.com/.help/CentOS7-Base-163.repo 163的源yum clean all # 清空缓存yum makecache # 生成缓存 设置npm镜像123456789# 设置npm源为淘宝NPM镜像npm config set registry https://registry.npm.taobao.org# 查看是否设置成功npm config get registry# 直接使用cnpm命令行工具代替默认的npmnpm install -g cnpm --registry=https://registry.npm.taobao.org# 设置回默认的官方源npm config set registry https://registry.npmjs.org/ 环境需求Node version must be &gt;=12.0.0 to use this CLI需要安装 redhat-lsb1yum -y install redhat-lsb 升级Node版本安装npm的n模块(专门用来管理nodejs的版本)12npm install -g n # 安装n模块n stable # 升级到最新的稳定版本 安装jbrowse123### operate under a normal user so this guide does not use thisnpm install -g @jbrowse/clijbrowse create /var/www/html/jbrowse2 也可以直接下载软件包： 下载最新版JBrowse2软件包 选择其中的Web版本(jbrowse-web-v1.5.3.zip)，解压后进入目录,运行npm服务即可 123456789101112131415unzip jbrowse-web-v1.5.3.zipcd jbrowsenpx serve .弹出如下信息，则证明服务已经成功启动。 ┌──────────────────────────────────────────────────┐ │ │ │ Serving! │ │ │ │ - Local: http://localhost:39963 │ │ - On Your Network: http://172.21.0.8:39963 │ │ │ │ This port was picked because 3000 is in use. │ │ │ └──────────────────────────────────────────────────┘ 配置jbrowse完成了jbrowse以后，下一步就是Jbrowse的配置，有两种方案可以进行配置 命令行的方法 图形界面 使用命令行添加数据添加基因组12# 添加Hg19的基因组，由于添加过程会在执行目录更新config.json文件，因此记得在jbrowse目录下执行！！jbrowse add-assembly /root/Database/hg19.fa --load copy --displayName Hg19 添加 bam 文件123jbrowse add-track /data/volvox.bam --load copy# 如果bam缺少索引文件，需要建立索引。samtools index volvox.bam 添加 BigWig/BigBed 文件因为jbrowse添加的bed文件必须是BigBed文件(二进制的bed文件)，因此bed文件添加前要先通过bedToBigBed 进行处理12345sort -k1,1 -k2,2n unsorted.bed &gt; input.bed #bed文件必须先排序bedToBigBed input.bed chrom.sizes myBigBed.bb## Download bigwig or bigbed filejbrowse add-track volvox-sorted.bam.coverage.bw --load copy 除了bed文件外，jbrowse还支持 BigWig有多种方式可以生成BigWig文件以下介绍的方式有基于wiggle (wig) 格式的文件，通过 软件：wigToBigWig 生成）， BigWig(.bw)的生成需要准备一个BedGraph文件格式如下： 123456#Chr Start End Value(可以自行定义)chr1 10270744 10270745 0.278208333333333chr1 10270745 10270746 0.275666666666667chr1 10270746 10270747 0.276833333333333chr1 10270747 10270748 0.27675chr1 10270748 10270749 0.279083333333333 排序后，通过bedGraphToBigWig工具，将BedGraph文件转换成.bw文件，直接加载12sort -k1,1 -k2,2n $file.bedGraph &gt; $file.sorted.bedGraph bedGraphToBigWig $file.sorted.bedGraph hg19.fa.fai $file.bw Adding a variant track12345678#首先对vcf进行排序bcftools sort file.vcf &gt; file.sorted.vcf#vcf需要进行压缩和建立索引bgzip yourfile.vcftabix yourfile.vcf.gzjbrowse add-track /data/yourfile.vcf.gz --load copy Adding a GFF3 file with GFF3Tabix1234gt gff3 -sortlines -tidy -retainids yourfile.gff &gt; yourfile.sorted.gffbgzip yourfile.sorted.gfftabix yourfile.sorted.gff.gzjbrowse add-track yourfile.sorted.gff.gz --load copy]]></content>
      <categories>
        <category>software</category>
        <category>Linux</category>
        <category>可视化</category>
      </categories>
      <tags>
        <tag>可视化</tag>
        <tag>Jbrowse</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[NGS检测中的数据模拟]]></title>
    <url>%2F0001.%E7%9F%A5%E8%AF%86-01.NGS%2F2021-12-23-NGS%E6%A3%80%E6%B5%8B%E4%B8%AD%E7%9A%84%E6%95%B0%E6%8D%AE%E6%A8%A1%E6%8B%9F%2F</url>
    <content type="text"></content>
      <categories>
        <category>NGS</category>
        <category>肿瘤检测</category>
      </categories>
      <tags>
        <tag>NGS</tag>
        <tag>肿瘤</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[芯片设计(Panel-Design)]]></title>
    <url>%2F0001.%E7%9F%A5%E8%AF%86-01.NGS%2F2021-12-12-Panel-Design%2F</url>
    <content type="text"><![CDATA[前言目前探针的靶向捕获测序，已经发展临床检测应用的常规技术手段。因此在进行靶向捕获测序时，我们需要面临和解决的第一个问题，就是我们应该如何设计芯片探针。2014年，发表的一篇文章An ultrasensitive method for quantitating circulating tumor DNA with broad patient coverage为我们进行靶向捕获区域设计提供了一个可参考的方案CAPP-Seq Selector。 名词概念 RI值 ：Recurrence Index (RI), is defined as the number of unique patients (i.e., tumors) with somatic mutations per kilobase of a given genomic unit (here, exon)。RI =（n×1000）÷L，其中n为所述外显子区间的患者数目，L为外显子区间的序列长度（bp）。 方法介绍 本方法是针对NSCLC设计的，但是也可以推广应用到其他高频突变已经明确的癌种。 首先，我们挑选出一些重要的外显子，这些外显子包含了COSMIC和其他来源（Somatic mutations affect key pathways in lung adenocarcinoma.[PubMed: 18948947]；Identifying cancer driver genes in tumor genome sequencing studies. [PubMed: 21169372]）的潜在驱动基因中反复出现的突变。 然后使用TCGA数据库，获取NSCLC的407例样本的WES测序数据。 最后应用了一种迭代算法来最大化每个患者的错义突变数量，同时最小化整个芯片大小。 Most human cancers are relatively heterogeneous for somatic mutations in individual genes. Specifically, in most human tumors, recurrent somatic alterations of single genes account for a minority of patients, and only a minority of tumor types can be defined using a small number of recurrent mutations (&lt;5-10) at predefined positions. Therefore, the design of the selector is vital to the CAPP-Seq method because (1) it dictates which mutations can be detected with high probability for a patient with a given cancer, and (2) the selector size (in kb) directly impacts the cost and depth of sequence coverage. For example, the hybrid selection libraries available in current whole exome capture kits range from 51-71 Mb, providing ~40-60 fold maximum theoretical enrichment versus whole genome sequencing. The degree of potential enrichment is inversely proportional to the selector size such that for a ~100 kb selector, &gt;10,000 fold enrichment should be achievable. We employed a six-phase design strategy to identify and prioritize genomic regions for the CAPP-Seq NSCLC selector as detailed below. Three phases were used to incorporate known and suspected NSCLC driver genes, as well as genomic regions known to participate in clinically actionable fusions (phases 1, 5, 6), while another three phases employed an algorithmic approach to maximize both the number of patients covered and SNVs per patient (phases 2–4). The latter relied upon a metric that we termed “Recurrence Index” (RI), defined as the number of NSCLC patients with SNVs that occur within a given kilobase of exonic sequence (i.e., No. of patients with mutations / exon length in kb). RI thus serves to measure patient-level recurrence frequency at the exon level, while simultaneously normalizing for gene or exon size. As a source of somatic mutation data uniformly genotyped across a large cohort of patients, in phases 2–4, we analyzed non-silent SNVs identified in TCGA whole exome sequencing data from 178 patients in the Lung Squamous Cell Carcinoma dataset (SCC)10 and from 229 patients in the Lung Adenocarcinoma (LUAD) datasets (TCGA query date was March 13, 2012). Thresholds for each metric (i.e. RI and patients per exon) were selected to statistically enrich for known/suspected drivers in SCC and LUAD data (Supplementary Fig. 1). RefSeq exon coordinates (hg19) were obtained via the UCSC Table Browser (query date was April 11, 2012) The following algorithm was used to design the CAPP-Seq selector (parenthetical descriptions match design phases noted in Fig. 1b). Phase 1 (Known drivers) Initial seed genes were chosen based on their frequency of mutation in NSCLCs. Analysis of COSMIC (v57) identified known driver genes that are recurrently mutated in ≥9% of NSCLC (denominator ≥500 cases). Specific exons from these genes were selected based on the pattern of SNVs previously identified in NSCLC. The seed list also included single exons from genes with recurrent mutations that occurred at low frequency but had strong evidence for being driver mutations, such as BRAF exon 15, which harbors V600E mutations in &lt;2% of NSCLC. Phase 2 (Max. coverage) For each exon with SNVs covering ≥5 patients in LUAD and SCC, we selected the exon withhighest RI that identified at least 1 new patient when compared to the prior phase. Amongexons with equally high RI, we added the exon with minimum overlap among patients alreadycaptured by the selector. This was repeated until no further exons met these criteria. Phase 3 (RI ≥ 30) For each remaining exon with an RI ≥ 30 and with SNVs covering ≥3 patients in LUAD andSCC, we identified the exon that would result in the largest reduction in patients with only 1SNV. To break ties among equally best exons, the exon with highest RI was chosen. This wasrepeated until no additional exons satisfied these criteria. Phase 4 (RI ≥ 20) Same procedure as phase 3, but using RI ≥ 20. Phase 5 (Predicted drivers) We included all exons from additional genes previously predicted to harbor driver mutations inNSCLC12,13. Phase 6 (Add fusions) For recurrent rearrangements in NSCLC involving the receptor tyrosine kinases ALK, ROS1,and RET, the introns most frequently implicated in the fusion event and the flanking exons wereincluded. All exons included in the selector, along with their corresponding HUGO gene symbols andgenomic coordinates, as well as patient statistics for NSCLC and a variety of other cancers, areprovided in Supplementary Table 1, organized by selector design phase. 参考资料文献An ultrasensitive method for quantitating circulating tumor DNA with broad patient coverage $\color{red}{ed}$]]></content>
      <categories>
        <category>NGS</category>
        <category>肿瘤检测</category>
      </categories>
      <tags>
        <tag>芯片设计</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python包-pyechart画web版图片]]></title>
    <url>%2F0002.%E5%BC%80%E5%8F%91-02.python%2FPython-%E5%8C%85-pyechart%E7%94%BBweb%E7%89%88%E5%9B%BE%E7%89%87%2F</url>
    <content type="text"><![CDATA[pyechart quickstart 12345678from pyecharts.charts import Barbar = Bar()bar.add_xaxis([&quot;衬衫&quot;, &quot;羊毛衫&quot;, &quot;雪纺衫&quot;, &quot;裤子&quot;, &quot;高跟鞋&quot;, &quot;袜子&quot;])bar.add_yaxis(&quot;商家A&quot;, [5, 20, 36, 10, 75, 90])# render 会生成本地 HTML 文件，默认会在当前目录生成 render.html 文件# 也可以传入路径参数，如 bar.render(&quot;mycharts.html&quot;)bar.render()]]></content>
      <categories>
        <category>编程拾慧</category>
        <category>python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[本地构建control集合进行过滤]]></title>
    <url>%2F0001.%E7%9F%A5%E8%AF%86-01.NGS%2F2021-12-08.Local_control%2F</url>
    <content type="text"><![CDATA[各个流程的处理过程大同小异 GATK PONA Panel of Normal or PON is a type of resource used in somatic variant analysis. Depending on the type of variant you’re looking for, the PON will be generated differently. What all PONs have in common is that : they are made from normal samples (in this context, $\color{red}{“normal”}$ means derived from healthy tissue that is believed to not have any somatic alterations) their main purpose is to capture recurrent technical artifacts in order to improve the results of the variant calling analysis. As a result, the most important selection criteria for choosing normals to include in any PON are the technical properties of how the data was generated. It’s very important to use normals that are as technically similar as possible to the tumor (same exome or genome preparation methods, sequencing technology and so on). Additionally, the samples should come from subjects that were young and healthy to minimize the chance of using as normal a sample from someone who has an undiagnosed tumor. Normals are typically derived from blood samples. There is no definitive rule for how many samples should be used to make a PON (even a small PON is better than no PON) but in practice we recommend aiming for a minimum of 40. At the Broad Institute, we typically make a standard PON for a given version of the pipeline (corresponding to the combination of all protocols used in production to generate the sequence data, starting from sample preparation and including the analysis software) and use it to process all tumor samples that go through that version of the pipeline. Because we process many samples in the same way, we are able to make PONs composed of hundreds of samples. MSKFiltering for high confidence mutations: Raw SNV and indel calls are subjected to a series of filtering steps to ensure only high-confidence calls are admitted to the final step of manual review. These parameters include (1) evidence of it being a somatic mutation (i.e., ratio between mutation frequencies in the tumor and normal samples to be ≥ 5.0); (2) whether the mutation is a known hotspot mutation (refer to Appendix 1a for details); (3) reference on in house ‘standard normal’ based on common artifacts; (4) technical characteristics that use coverage depth (DP), number of mutant reads (AD), mutation frequency (VF). The filtering scheme and threshold are shown in Figure 1 below. The threshold values for the filtering criteria were established based on paired-sample mutation analysis on replicates of normal FFPE samples, and optimized to reject all false positive SNVs and almost all false positive indel calls from the reference dataset. BGI 目前策略Control集合构建SOP reference GATK:PON EVALUATION OF AUTOMATIC CLASS III DESIGNATION FOR MSK-IMPACT]]></content>
      <categories>
        <category>NGS</category>
        <category>肿瘤检测</category>
      </categories>
      <tags>
        <tag>NGS</tag>
        <tag>肿瘤</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Pseudogene]]></title>
    <url>%2F0001.%E7%9F%A5%E8%AF%86-01.NGS%2F2023-06-09.pseudogene_in_NGS%2F</url>
    <content type="text"><![CDATA[什么是假基因假基因是与功能基因具有显着同源性的 DNA 序列，但它们缺乏用于转录的启动子序列或包含其他阻止功能产物形成的突变，不能产生功能性蛋白的序列[1]。同时由于假基因通常不面临选择压力，因此更容易出现变异信号的积累。根据 GENCODE统计数据，人类基因组中有14737个假基因（编码基因19393个），对应3391个 “parent gene”。当然其实截止目前，人类基因组的基因总数目其实也尚未在业界达成一个统一性的共识。但是通过GENCODE的数据，至少我们可以看到，在基因组中，假基因的数目占比非常巨大。 假基因的起源提到假基因的产生，其实就绕不过新基因的产生（假基因只是一类不能产生功能性蛋白的特殊新基因）。而新基因的产生过程会涉及到各种分子事件，同时这些事件必须发生在种系中才能遗传给下一代。当新产生的基因能进行遗传后，变回参与到和环境的互作（压力选择）中，最终取得优势得以保存，或者存在劣势被淘汰。假基因最初被定义为类似于已知基因但不能产生功能性蛋白质的序列，对假基因的研究不仅揭示了基因退化的频率，而且还揭示了许多曾经被认为是退化蛋白质编码基因的序列实际上是功能性 RNA 基因。多年来，科学家提出了几种产生新基因的机制。这些包括基因复制、转座子蛋白驯化、横向基因转移、基因融合、基因裂变和从头起源[4]。 Gene Duplication 基因复制 Transposable Element Protein Domestication 转座子蛋白驯化 Lateral Gene Transfer 横向基因转移 Gene Fusion and Fission 基因融合与裂变 De Novo Gene Origination 从头基因起源 假基因起源于与蛋白质编码基因相同的机制，随后是随后破坏阅读框或导致过早终止密码子插入的致残突变（例如，核苷酸插入、缺失和/或替代）的积累[2]。假基因可大致分为两类： - 未加工的假基因通常包含内含子，并且它们通常位于其旁系同源基因的旁边。 - 加工过的假基因被认为起源于逆转录转座；因此，它们缺少内含子和启动子区域，但它们通常包含聚腺苷酸化信号，并且两侧是同向重复序列。逆转录错误和缺乏适当的监管环境通常会导致基因转录的退化。 给定基因组中假基因的丰度通常取决于基因复制和丢失的速率。哺乳动物似乎有大量经过处理的假基因——大约 8,000 个[3]。 假基因对分析带来的影响基于假基因的产生机制，我们不难发现，假基因，尤其是通过基因复制、横向基因转移、基因融合等原因产生的假基因会和基因组上其他区域（假基因产生的母本）存在序列的高度同源性。而这些高度同源性的序列存在会使得NGS分析过程变得复杂化： 使用短读长（75-300 bp ），则序列同源性会导致假基因和母本片段区域难以进行区分。 序列同源使准确的reads比对（映射）变得复杂，如下图所示。映射到多个基因组位置的序列读数在分析中被丢弃，这会导致序列覆盖率出现缺口。差异加大时，可以进行有效的区分，但是随着序列同源性增加时，会出现假基因的错误比对（假基因序列变动会被错误识别为变异）。当序列读数与几个基因组位置对齐得同样好时，它们将被丢弃。而在NGS的靶向捕获中，假基因的存在还会影响杂交捕获过程。 如前所述，人类有上万个假基因（GENCODE 项目），如果我们的检测范围中存在假基因或者假基因对应的编码基因/同源区域，则会影响对应区域的检测准确性，这类区域的准确性会低于没有假基因影响的区域。 有什么方式可以减少假基因的影响 最简单直接的，是在进行芯片涉及阶段，尽可能剔除掉假基因和对应的同源区域及编码基因。从根本上消除假基因的干扰。想获取具体的假基因清单，可以在Ensemble中下载基因结构文件gff,其中基因的biotype 标签会记录基因是否属于假基因。 尽可能采用长读长测序（PE250&gt;PE100&gt;PE50)，长度长测序比短读长能更有效的区分进行同源区域的识别和划分[5]。 提高比对质量值的阈值，仅保留高比对质量的数据进行变异的检出。 涉及长Long-Range PCR和Sanger 引物进行验证，确定分析结果。 进行生物信息流程的定制化，以消除影响。 Reference[1]. Wilde CD. Pseudogenes. CRC Crit Rev Biochem. 1986;19(4):323-352.[2]. D’Errico I, Gadaleta G, Saccone C. Pseudogenes in metazoa: origin and features. Brief Funct Genomic Proteomic. 2004;3(2):157-167. doi:10.1093/bfgp/3.2.157[3]. Zhang Z, Carriero N, Gerstein M. Comparative analysis of processed pseudogenes in the mouse and human genomes. Trends Genet. 2004;20(2):62-67. doi:10.1016/j.tig.2003.12.005[4]. Chitra Chandrasekaran, Esther Betrán .Origins of New Genes and Pseudogenes[5]. Vahid Bahrambeigi and others, An Approach for Accurate Molecular Diagnosis of Highly Homologous SDHA Gene, American Journal of Clinical Pathology, Volume 146, Issue suppl_1, September 2016, 214[6]. ensembl]]></content>
      <categories>
        <category>NGS</category>
        <category>肿瘤检测</category>
      </categories>
      <tags>
        <tag>NGS</tag>
        <tag>分子诊断</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[NGS检测原理-实验]]></title>
    <url>%2F0001.%E7%9F%A5%E8%AF%86-01.NGS%2F2021-12-07.NGS%E6%A3%80%E6%B5%8B%E5%8E%9F%E7%90%86-%E5%AE%9E%E9%AA%8C%2F</url>
    <content type="text"><![CDATA[NGS在数据下机前需要对数据进行一些列的实验处理后才能进行上机测序，因此了解实验原理才能更好的从根源了解数据处理过程，及各个环节的不同处理方案可能带来的优劣影响。在下游数据处理过程中，更有针对性的对数据处理过程进行优化提升。 DNA的提取细胞裂解预处理加入保护液（如TE buffer）来溶解DNA并防止其降解；在破胞前加入去垢剂来去除杂质并破坏细胞（如SDS是一种表面活性剂，能破坏细胞膜上的脂质，并在低温下使其沉淀） 主要方法机械破碎法（振荡珠磨、液氮研磨、反复冻融等）和酶解法（如溶菌酶溶解细菌细胞壁） 去除杂质杂质如细胞碎片、蛋白质、RNA、腐殖酸等 方法化学法（加入抑制因子沉淀杂质、使用氯仿等有机溶剂溶解杂质）、酶解法（例如蛋白酶K、RNA酶降解蛋白质、RNA） 回收DNA主要方法 醇沉淀法：DNA不溶解于异丙醇、乙醇等 过柱收集法：DNA在高盐环境下可以吸附在硅胶滤膜上，这是大部分试剂盒采用的方法 磁珠吸附法：DNA分子通过氢键吸附到具有磁性的磁珠上，然后在磁场中分离磁珠，常见于自动提取仪 清洗溶解DNA方法 对沉淀的DNA、收集柱DNA以及磁珠吸附的DNA使用70%乙醇清洗； 待乙醇挥发后使用无菌水溶解DNA。 DNA提取原则 保证核酸一级结构的完整性； 核酸样品中不应存在对酶有抑制作用的有机溶剂和过高浓度的金属离子； 其他生物大分子如蛋白质、多糖和脂类分子的污染应降低到最低程度； 其他核酸分子，如RNA，也应尽量去除。 文库制备末端修饰 使用Taq聚合酶补齐不平的末端； 并在两个末端添加突出的碱基A，从而产生粘性末端（若使用Taq酶扩增，则无需末端修饰）； 产生粘性末端的片段可以添加接头（Adaptor）。 添加接头 经过末端修饰后的PCR片段末端具有突出的A尾，而接头具有突出的T尾，可以使用连接酶将接头添加到DNA片段两端。 NEB的接头为特殊的碱基U连接的环状结构（可以增强稳定性），因此连接接头后，还需要将碱基U删除从而形成“Y”形接头。 上一步添加的接头主要是为了后续PCR中作为引物扩增继续添加文库index和与测序平台互补的寡核苷酸序列（此外还作为测序引物Rd1 SP/Rd2 SP）。 之所以为“Y”型开叉结构，是因为每一端接头是两条不互补的序列（每一端都是Rd1 SP与Rd2 SP交错），连接酶没有选择性，每个接头都是只靠突出的T来与DNA连接，“Y”接头保证了每条单序列两端均为不同的测序引物，从而在后续PCR中可以连接不同的寡核苷酸序列（P5/P7）。 过程示意图如下： 磁珠纯化目的添加接头后的文库体系中含有聚合酶、连接酶等各种酶以及辅助物质，接头的添加也是过量的，而且由于末端的不稳定性，容易形成自连片段，鸟枪法打断的片段中也可能有大片段存在，所以需要特殊磁珠（AMPure XP Beads）纯化来去除大片段以及各种杂质，从而获得成功添加接头的文库片段。 原理磁珠可以通过氢键等作用力来吸附DNA片段，磁珠本身不具有片段大小选择的能力，但其储存的buffer里面含有20%的PEG 8000，PEG浓度越大则可以吸附的DNA片段越小。 注意事项磁珠纯化的时候要根据文库片段不同严格控制磁珠添加量（其实是PEG添加量）来实现片段选择。 PCR扩增 添加了接头的DNA片段，可以使用与接头互补的引物来扩增。 此外，片段还需要添加用于区分不同文库的特异性index，以及与测序仪芯片互补的两种寡核苷酸序列（P5/P7）。 第二次磁珠纯化 PCR后需要将产物DNA片段与聚合酶等杂质分离，因此再次进行磁珠纯化。 之后进行质量检测，包括DNA浓度检测、琼脂糖凝胶电泳和片段长度检测，完成建库。 NGS测序仪上机待补充 参考来源 https://mp.weixin.qq.com/s/zNFvod8B-VhX7Kq7OgoRMA ClarkeA C, Prost S, Stanton J a L, et al. From cheek swabs to consensus sequences: anA to Z protocol for high-throughput DNA sequencing of complete humanmitochondrial genomes[J]. Bmc Genomics, 2014, 15(1): 1-12. BowmanS K, Simon M D, Deaton A M, et al. Multiplexed Illumina sequencing librariesfrom picogram quantities of DNA[J]. Bmc Genomics, 2013, 14(1): 135-143. MardisE R. Next-Generation DNA Sequencing Methods[J]. Annual Review of Genomics &amp;Human Genetics, 2008, 9(9): 387-402]]></content>
      <categories>
        <category>NGS</category>
        <category>肿瘤检测</category>
        <category>标准化</category>
      </categories>
      <tags>
        <tag>NGS</tag>
        <tag>肿瘤</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DNA-Damage]]></title>
    <url>%2F0001.%E7%9F%A5%E8%AF%86-01.NGS%2F2021-12-07.DNA-Damage%2F</url>
    <content type="text"><![CDATA[背景概念生物学真实损伤和实验引入损伤间的差异。DNA易受多种损伤，其中之一就是氧化损伤。随着时间的推移，这种类型的损害会逐渐累积，破坏修复系统，导致健康问题，最终导致疾病。 在生理环境下，一旦某一个碱基发生改变如果没有被错配修复蛋白处理掉，这种错配可以传递给子代，形成突变，一个真实的突变正模板链和对应负模板链应同时被替换。 如果碱基改变是发生在实验阶段，那么其对应链不会发生改变，正链发生G&gt;8-oxoG的改变时，由于其可与A配对，易被测序仪读成T，但对应的负链C碱基，仍会被读为C，而不是A 正负链 真实突变 假突变 正链：5’-3’ 5’- ATC$\color{red}{G}$ATCG-3 5’- ATC$\color{red}{G}$ATCG-3 负链：3’-5’ 3’- TAG$\color{red}{A}$TAGC-5 3’- TAG$\color{red}{C}$TAGC-5 NGS识别氧化损伤的技术基础 处理方式参考文献[16] 参考资料文献 Characterization of background noise in capture-based targeted sequencing data FIREVAT: finding reliable variants without artifacts in human cancer samples using etiologically relevant mutational signatures Sequence Neighborhoods Enable Reliable Prediction of Pathogenic Mutations in Cancer Genomes Needlestack: an ultra-sensitive variant caller for multi-sample next generation sequencing data Location analysis of 8-oxo-7,8-dihydroguanine in DNA by polymerase-mediated differential coding Targeted Single Primer Enrichment Sequencing with Single End Duplex-UMI Analysis of error profiles in deep next-generation sequencing data The use of technical replication for detection of low-level somatic mutations in next-generation sequencing Allele balance bias identifies systematic genotyping errors and false disease associations Overview of Next-Generation Sequencing Technologies Detecting Somatic Mutations in Normal Cells Detecting Rare Mutations and DNA Damage with Sequencing-Based Methods IMPUTOR: Phylogenetically Aware Software for Imputation of Errors in Next-Generation Sequencing UDiTaS™, a genome editing detection method for indels and genome rearrangements Reference standards for next-generation sequencing Discovery and characterization of artifactual mutations in deep coverage targeted capture sequencing data due to oxidative DNA damage during sample preparation 网站 Science Direct: Oxidative Damage exploredna: DNA and Oxidative Damage Oxidative DNA damage: mechanisms, mutation, and disease The genomics of oxidative DNA damage, repair, and resulting mutagenesis]]></content>
      <categories>
        <category>NGS</category>
        <category>肿瘤检测</category>
      </categories>
      <tags>
        <tag>NGS</tag>
        <tag>肿瘤</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TMB简介及华大相关进展]]></title>
    <url>%2F0001.%E7%9F%A5%E8%AF%86-01.NGS%2F2021-11-29-TMB%2F</url>
    <content type="text"><![CDATA[TMB的出现及发展TMB 基本现状定义 WES： TMB是指肿瘤基因组内存在的体细胞突变位点数量，可以间接反映肿瘤产生新生抗原的能力。由于早期研究多基于WES检测，因此TMB通常是指单位基因组外显子编码区域（外显子组，exome）的突变数量（mutations, muts），单位为muts/exome。 Panel： 虽然WES是检测TMB的金标准，但WES时间成本和分析成本较高。经过多项大样本研究验证后，TMB检测从WES扩展到了更切合临床实际的靶向二代测序（next-generation sequencingpanel, NGS panel）。靶向测序的基因检测位点比外显子组少，由于不同平台检测方法和测序覆盖的外显子区域长度不同，TMB也被定义为肿瘤基因组区域中每兆碱基（megabase, Mb）发生的碱基替换突变和插入缺失突变的数量总和，单位为muts/Mb。 纳入计算的突变范围 WES TMB被定义为通过WES测序肿瘤组织样本中体细胞非同义突变数量的总和。 Panel 在NGSpanel检测TMB的研究中，纳入TMB计算的是体细胞编码区中碱基替换突变和插入缺失突变，部分NGS panel计算TMB也同时纳入了同义突变，而胚系变异、核苷酸多态性位点、明确的抑癌基因及驱动基因热点突变则不计算在内.(Analysis of 100,000 human cancer genomes reveals the landscape of tumor mutational burden、The COSMIC (Catalogue of Somatic Mutations in Cancer) database and website、ExAC:Analysis of protein-coding genetic variation in 60,706 humans ) TMB计算过程中分母的选择-2023年9月补充实际在进行TMB计算过程中，尤其是使用Panel进行捕获的产品，分母的选择也是成为了困扰。部分机构会直接使用Panel的全长（涵盖内含子之类的区域）作为分母进行计算，但是还有一些机构会选择使用编码区域全长作为分母。所以在这里也记录看到的一些处理方式以供参考。 使用编码区长度 FoundationOne®CDx June 7, 2022 技术文档中提到，The resulting mutation number is then divided by the coding region corresponding to the number of total variants counted; 其实略尴尬，TMB共识普遍建议应该使用的Panel（当然未明确说明编码区和内含子区域的比例）在1MB以上，但是FoundationOne的编码区大小仅753kb。 [肿瘤突变负荷应用于肺癌免疫治疗的专家共识] TMB 定义中提到，由于早期研究多基于WES检测，因此TMB通常是指单位基因组外显子编码区域（外显子组，exome）的突变数量（mutations, muts），单位为muts/exome。虽然WES是检测TMB的金标准，但WES时间成本和分析成本较高。经过多项大样本研究验证后，TMB检测从WES扩展到了更切合临床实际的靶向二代测序（next-generation sequencing panel, NGS panel）。靶向测序的基因检测位点比外显子组少，由于不同平台检测方法和测序覆盖的外显子区域长度不同，TMB也被定义为肿瘤基因组区域中每兆碱基（megabase, Mb）发生的碱基替换突变和插入缺失突变的数量总和，单位为muts/Mb。，明确说明了时编码区的变异数目，虽然对应区域同理应该选用编码区长度，但是共识未明确体积分母的计算方式。 [肿瘤突变负荷检测及临床应用中国专家共识（2020年版）] 中提到 以数百个基因的肿瘤靶向 Panel 检测是通过计算覆盖在产品外显子上的体细胞突变数目除以产品的外显子覆盖区域得到基于 Panel 的 TMB 结果，然而其计算得到的 TMB值与 WES 检测得到的 TMB 值存在偏差,说明了以产品的外显子覆盖区域作为分母。 [TMB标准化项目蓝皮书]在中检院组织的TMB标准化项目中，针对WES的计算提供了分母的计算方式 位于全外显子捕获区间区域上下游扩展50bp 与 CDS 区域上下游扩展 2bp 的交集区域。 ，但是同时针对Panel提供的计算方式则是 检测覆盖到的区域总长度。 其实目前国内大部分检测公司并没有开展临床试验，而目前评估Panel的性能也多为和WES结果进行相关性比较，但是显然，分母作为一个常数值，不管如何选择，都不会影响相关性表现。所以如果 Panel大小有一些证据[23,24]发现检测的基因数越多，TMB的检测结果与WES的一致性越高，NGS大panel可能更适合评估TMB。对于TMB较高的样本，不同大小的NGS panel检测结果差异可能不显著；而对于TMB较低的样本，检测结果的不一致性显著增加。目前一般认为NGS panel ≥0.8 Mb可以较好地评估肿瘤组织TMB水平(Implementing TMB measurement in clinical practice: considerations on assay requirements)。 参考材料：来源：历史中检院宣讲PPT材料 Transl Lung Cancer Res 2018;7(6):703-715Genes Chromosomes Cancer. 2019;58:578–588. 影响因素样本收集阶段、DNA处理阶段、测序阶段、生物信息分析阶段和报告生成阶段均会影响TMB检测的可靠性。样本收集阶段主要包括样本类型、肿瘤类型、肿瘤异质性和克隆进化等影响因素；DNA处理阶段包括DNA质量和数量、文库构建等影响因素；测序阶段包括DNA捕获区域、测序深度、覆盖读长、测序平台等影响因素；生物信息分析阶段包括突变类型、胚系突变过滤、等位基因突变频率等影响因素；报告生成阶段包括瘤种分类、患者人群、患者数量、TMB排序标准等影响因素。除了考虑技术因素，流程监管、样本收集和处理质控以及样本运送时长等因素可能也会影响样本质量，从而影响检测结果。 关联指标在一定程度上，TMB 水平反映的是肿瘤细胞内DNA 的修复损伤情况，与产生肿瘤新抗原能力密切相关。DNA错配修复基因（mismatch repair genes，MMR）负责修正DNA 复制错误，若MMR 存在突变往往会导致微卫星不稳定（microsatellite instability，MSI），因此高微卫星不稳定（MSI high，MSI鄄H）常作为MMR 功能缺陷（mismatch repair deficient，dMMR）的替代指标［6］。 此外，DNA 聚合酶着（DNA polymerase 着，POLE）和DNA 聚合酶啄1（polymerase delta 1,POLD1）对DNA复制的校对和保真至关重要，POLE/POLD1 基因突变（特别是外切酶活性域突变）也会导致肿瘤的高突变或超突变（TMB&gt;100 个突变/Mb）(Tumor and Microenvironment Evolution during Immunotherapy with Nivolumab) TMB发展重要节点历程TMB概念起源于2013年Nature发表的一项研究(Signatures of mutational processes in human cancer)。在30个癌种7,000多个标本中，研究者通过全基因组测序（whole genome sequencing,WGS）和全外显子测序（whole exome sequencing, WES）技术分析了突变图谱，描述了不同癌种样本中每百万碱基（megabase, Mb）的突变数量。 2014年的一项黑色素瘤研究（Genetic Basis for Clinical Response to CTLA-4 Blockade in Melanoma）发现，免疫治疗的响应率与肿瘤突变数目有一定的相关性，通过WES检出错义突变数量大于100的患者接受免疫治疗后具有更长的总生存期（overall survival, OS），这是首个验证TMB和免疫治疗疗效相关性的研究。 2015年，首个TMB与NSCLC免疫治疗疗效的研究（Mutational landscape determines sensitivity to PD-1 blockade in non–small cell lung cancer）发表于Science，该研究发现高于中位TMB的NSCLC患者具有更长的无进展生存期（progression-free survival, PFS）。 此后，CheckMate-026、CheckMate-227等多项大型研究证实了TMB对NSCLC免疫治疗疗效的预测作用。 2017年，Genome Medicine发表的一项10万例实体瘤患者研究(Analysis of 100,000 human cancer genomes reveals the landscape of tumor mutational burden)，探索了靶向捕获测序panel与WES检测TMB的相关性，证实了panel检测TMB的可行性与可靠性。 2019年中国临床肿瘤学会（Chinese Society of Clinical Oncology, CSCO）指南和美国国家综合癌症网络（National Comprehensive Cancer Network, NCCN）指南均将TMB纳入晚期肺癌的分子病理检测范围。 2020年，美国食品药品监督管理局（Food and Drug Administration, FDA）批准Pembrolizumab单药用于治疗高TMB且既往接受治疗后病情进展的不可手术或转移性实体瘤患者。Pembrolizumab成为全球首个以TMB作为标志物而获批的抗肿瘤药物。但临床实践中TMB的检测和评估缺乏统一的标准，这极大限制了其临床应用。虽然《肿瘤突变负荷检测及临床应用中国专家共识（2020年版）》已发布，但TMB在肺癌免疫治疗临床应用中的相关规范仍有待统一。 2021年，为促进TMB在肺癌免疫治疗中应用的规范化，协作组组织国内肺癌领域权威专家，综合国内外高质量文献，形成《肿瘤突变负荷应用于肺癌免疫治疗的专家共识》，对TMB的定义、临床意义和临床应用给出指导性建议。 TMB 重要资讯TMB是肿瘤NGS检测 TMB相关文章指南共识信息 发布时间 简介 机构 文章材料 2019.10.17 肿瘤突变负荷检测国家参考品说明书公示 中检院 肿瘤突变负荷检测国家参考品说明书公示 2020.06 肿瘤突变负荷检测国家参考品 中检院 肿瘤突变负荷检测国家参考品 2020.10 肿瘤突变负荷检测及临床应用中国专家共识（2020 年版） 中国癌症防治杂志 肿瘤突变负荷检测及临床应用中国专家共识（2020年版） 2021.11 TMB国内共识 - 面向肺癌 中国临床肿瘤学会 肿瘤突变负荷应用于肺癌免疫治疗的专家共识 TMB阈值确定方法标准化的阈值，应该使用临床疗效数据进行阈值的判断！ 目前部分文章是使用的人群比例占比进行的TMB High/Low的划分。 阈值确定参考素材 index TMB-High TMB-intermedia TMB-L 参考文献 1 Top 10% Top 10%-20% else https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7210182/ 2 Top 10% Top 10%-50% else https://mct.aacrjournals.org/content/molcanther/16/11/2598.full.pdf 免疫治疗响应疗效报道 BGI的TMB工作TMB计算逻辑1.5% 以上的SNV（含同义突变)和InDel的总突变数目除以芯片大小(2.79M)其中包含同义突变的参考依据如下： Chalmers ZR, et al. Analysis of 100,000 human cancer genomes reveals the landscape of tumor mutational burden. Genome Med. 2017;9:34. doi: 10.1186/s13073-017-0424-2. Rosenberg JE, et al. Atezolizumab in patients with locally advanced and metastatic urothelial carcinoma who have progressed following treatment with platinum-based chemotherapy: a single-arm, multicentre, phase 2 trial. Lancet. 2016;387(10031):1909–1920 Han Chang, Ariella Sasson, Sujaya Srinivasan, Ryan Golhar, Danielle M. Greenawalt, William J. Geese, George Green, Kim Zerba, Stefan Kirov, Joseph Szustakowski bioRxiv 626143; doi: https://doi.org/10.1101/626143 FM 原文参考: TMB by F1CDx is defined based on counting the total number of all synonymous and non-synonymous variants present at 5% allele frequency or greater (after filtering) and reported as mutations per megabase (mut/Mb) unit. The clinical validity of TMB defined by this panel has not been established TMB标准化项目整体时间规划 第一阶段结果基于华大数据进行Panel和WES的相关性拟合基于TCGA公共数据，进行拟合，获取各公司Panel计算TMB值和原WES水平TMB的回归拟合公式。y=1.371559x-0.735414 （x：Panel检测的TMB值； y：经过矫正后，对应WES水平的TMB值。） 使用该公式，对PanCancer（688芯片）检测临床样本获得的TMB值进行校正，并利用校正后的数据和WES检测TMB结果进行相关性比较，结果如下： 第一阶段总结TMB标准化第一阶段发布会-内容存档 华大数据基于三分段确定的阈值BGI历史数据阈值（三分段）基于该标准（index1）和华大历史样本确定的整体阈值（数据截止2020.8） 癌症类型 TMB high TMB median TMB low 组织泛癌种 ≥8.6Mut/Mb ＞5.02Mut/Mb且＜8.6Mut/Mb ≤5.02Mut/Mb 基于该标准（index1）和华大历史样本确定的各个癌种阈值（数据截止2021.9） 癌症类型 TMB high TMB median TMB low 胆管癌 Cholangiocarcinoma ≥5.73Mut/Mb ＞3.94Mut/Mb且＜5.73Mut/Mb ≤3.94Mut/Mb 胆囊癌 Carcinoma of Gallbladder ≥8.6Mut/Mb ＞4.66Mut/Mb且＜8.6Mut/Mb ≤4.66Mut/Mb 非小细胞肺癌 Non-Small Cell Lung Cancer ≥9.68Mut/Mb ＞6.09Mut/Mb且＜9.68Mut/Mb ≤6.09Mut/Mb 肝细胞癌 Hepatocellular Carcinoma ≥7.89Mut/Mb ＞5.38Mut/Mb且＜7.89Mut/Mb ≤5.38Mut/Mb 宫颈癌 Cervical Cancer ≥16.49Mut/Mb ＞8.96Mut/Mb且＜16.49Mut/Mb ≤8.96Mut/Mb 黑色素瘤 Melanoma ≥8.24Mut/Mb ＞3.94Mut/Mb且＜8.24Mut/Mb ≤3.94Mut/Mb 结直肠癌 Colorectal Cancer ≥27.24Mut/Mb ＞6.45Mut/Mb且＜27.24Mut/Mb ≤6.45Mut/Mb 卵巢癌 Ovarian Cancer ≥5.73Mut/Mb ＞3.94Mut/Mb且＜5.73Mut/Mb ≤3.94Mut/Mb 膀胱癌 Bladder Cancer ≥20.07Mut/Mb ＞10.75Mut/Mb且＜20.07Mut/Mb ≤10.75Mut/Mb 前列腺癌 Prostate Cancer ≥8.6Mut/Mb ＞6.09Mut/Mb且＜8.6Mut/Mb ≤6.09Mut/Mb 乳腺癌 Breast Cancer ≥6.81Mut/Mb ＞5.38Mut/Mb且＜6.81Mut/Mb ≤5.38Mut/Mb 软组织肉瘤 Soft Tissue Sarcoma ≥4.66Mut/Mb ＞2.87Mut/Mb且＜4.66Mut/Mb ≤2.87Mut/Mb 肾癌 Kidney Cancer ≥6.45Mut/Mb ＞3.94Mut/Mb且＜6.45Mut/Mb ≤3.94Mut/Mb 头颈癌 Head and Neck Cancer ≥8.6Mut/Mb ＞5.73Mut/Mb且＜8.6Mut/Mb ≤5.73Mut/Mb 胃癌 Gastric Cancer ≥12.19Mut/Mb ＞7.17Mut/Mb且＜12.19Mut/Mb ≤7.17Mut/Mb 胰腺癌 Pancreatic Adenocarcinoma ≥3.94Mut/Mb ＞2.51Mut/Mb且＜3.94Mut/Mb ≤2.51Mut/Mb 子宫肿瘤 Uterine Neoplasms ≥26.88Mut/Mb ＞17.92Mut/Mb且＜26.88Mut/Mb ≤17.92Mut/Mb 基于该标准（index1）和华大历史样本确定的各个癌种阈值（数据截止2022.04，质评用）|癌症类型|TMB high|TMB median|TMB low||-|-|-|-||泛癌种(15917例)|≥9.68Mut/Mb|＞5.73Mut/Mb且＜9.68Mut/Mb|≤5.73Mut/Mb||肺腺癌(2733例)|≥8.24Mut/Mb|＞5.02Mut/Mb且＜8.24Mut/Mb|≤5.02Mut/Mb||非小细胞肺癌(4249例) |≥10.04Mut/Mb|＞6.45Mut/Mb且＜10.04Mut/Mb|≤6.45Mut/Mb|]]></content>
      <categories>
        <category>NGS</category>
        <category>肿瘤检测</category>
        <category>标准化</category>
      </categories>
      <tags>
        <tag>Standards and guidelines</tag>
        <tag>Biomarker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[01.基因功能库构建]]></title>
    <url>%2F0001.%E7%9F%A5%E8%AF%86-06.%E8%A7%A3%E8%AF%BB%E6%95%B0%E6%8D%AE%E5%BA%93%E6%9E%84%E5%BB%BA%2F01.%E5%9F%BA%E5%9B%A0%E5%8A%9F%E8%83%BD%E5%BA%93%E6%9E%84%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[待补充]]></content>
      <categories>
        <category>知识沉淀</category>
        <category>NGS</category>
        <category>数据库</category>
        <category>解读</category>
      </categories>
      <tags>
        <tag>数据库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据分析模型 - 波士顿矩阵]]></title>
    <url>%2F0001.%E7%9F%A5%E8%AF%86-09.%E4%BA%A7%E5%93%81%E6%A8%A1%E5%9E%8B%2F01.%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E6%A8%A1%E5%9E%8B-%E6%B3%A2%E5%A3%AB%E9%A1%BF%E7%9F%A9%E9%98%B5%2F</url>
    <content type="text"><![CDATA[波士顿矩阵（BCG Matrix），又称市场增长率-相对市场份额矩阵，由美国著名的管理学家、波士顿咨询公司创始人布鲁斯·亨德森于1970年首创，它是通过销售增长率（反应市场引力的指标）和市场占有率（反应企业实力的指标）来分析决定企业的产品结构。 波士顿矩阵将产品类型分为四种： 1，明星类产品：高增长且高市占，发展前景好，竞争力强，需加大投资以支持其发展； 2，问题类产品：高增长但低市占，发展前景好但市场开拓不足，需谨慎投资； 3，现金牛产品：低增长但高市占，成熟市场的领导者，应降低投资，维持市占并延缓衰退； 4，瘦狗类产品：低增长且低市占，理论率低甚至亏损，应采取撤退战略。 参考链接： https://zhuanlan.zhihu.com/p/67544309]]></content>
      <categories>
        <category>知识沉淀</category>
        <category>数据分析模型</category>
      </categories>
      <tags>
        <tag>数据分析模型</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[什么是LLM大语言模型？Large Language Model]]></title>
    <url>%2F0001.%E7%9F%A5%E8%AF%86-10.%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2FLLM%E6%A6%82%E5%BF%B5%E5%85%A5%E9%97%A8%2F</url>
    <content type="text"></content>
      <categories>
        <category>知识沉淀</category>
        <category>数据分析模型</category>
      </categories>
      <tags>
        <tag>LLM</tag>
        <tag>Machine-Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[docker - 常用命令 - 运行任务是参数的传递]]></title>
    <url>%2F0002.%E5%BC%80%E5%8F%91-04.pipeline%2Fpipeline-container-docker-%E7%94%A8%E6%88%B7%E7%BB%84%E7%AE%A1%E7%90%86%2F</url>
    <content type="text"><![CDATA[docker 以进程为核心, 对系统资源进行隔离使用的管理工具. 隔离是通过 cgroups (control groups 进程控制组) 这个操作系统内核特性来实现的. 包括用户的参数限制、 帐户管理、 资源(cpu,内存,磁盘i/o,网络)使用的隔离等. docker 在运行时可以为容器内进程指定用户和组. 没有指定时默认是 root .但因为隔离的原因, 并不会因此丧失安全性. 传统上, 特定的应用都以特定的用户来运行, 在容器内进程指定运行程序的所属用户或组并不需要在 host 中事先创建. Docker容器的权限管理默认使用的root权限不管是以root用户还是以普通用户（有启动docker容器的权限）启动docker容器，容器进程和容器内的用户权限都是root！新建了sleep用户，以sleep用户权限启动容器并在有root权限的磁盘进行权限测试。1docker run -v /data/sleep:/sleep -d --name sleep-1 ubuntu sleep infinity 在宿主机中/data/sleep路径新建了leo_zhou文件并在文件写入“docker”，然后进入sleep-1容器1docker exec -it sleep-1 bash 依然可以正常操作拥有root权限的文件。 限制Docker容器启动的用户新增–user参数，使容器启动用户变成指定的sleep用户，发现并不能操作拥有root权限的文件了。会发现容器中的uid号和实际主机中的uid号一样，也验证了docker容器使用宿主机的内核。可以一定程度进行权限管理。 使用namespace隔离技术namespace是一种隔离技术，docker就是使用隔离技术开启特定的namespace创建出一些特殊的进程，不过使用namespace是有条件的。系统会创建dockremap，通过/etc/subuid和/etc/subuid对应的id值，映射到容器中去；实际情况还是使用的是dockremap普通权限，达到自动隔离的效果。①开启Centos内核中关闭的user namespace的功能。12grubby --args=&quot;namespace.unpriv_enable=1 user_namespace.enable=1&quot; --update-kernel=&quot;$(grubby --default-kernel)&quot;echo &quot;user.max_user_namespaces=15076&quot; &gt;&gt; /etc/sysctl.conf ②修改/etc/docker/daemon.json配置，新增”userns-remap”: “default”选项，default默认就是docker自动创建的用户dockremap，然后重启docker。修改此项配置需要慎重，如果是已经部署了一套docker环境，启用此选项后，会切换到隔离环境，以前的docker容器将无法使用！ ③Centos需要手动输入id值映射范围最后systemctl restart docker后再次测试效果，发现文件权限已经变成nobody，但docker容器内部依然是以”root”的权限管理，但实际只有普通用户的权限，从而达到权限隔离的效果。 进程控制组cgroups主要可能做以下几件事: 资源限制 组可以设置为不超过配置的内存限制, 其中还包括文件系统缓存 优先级 某些组可能会获得更大的 cpu 利用率份额或磁盘 i/o 吞吐量 帐号会计 度量组的资源使用情况, 例如, 用于计费的目的 控制 冻结组进程, 设置进程的检查点和重新启动 与 cgroups(控制进程组) 相关联的概念是 namespaces (命令空间)。命名空间主要有六种名称隔离类型: pid 命名空间为进程标识符 (pids) 的分配、进程列表及其详细信息提供了隔离。 虽然新命名空间与其他同级对象隔离, 但其 “父 “ 命名空间中的进程仍会看到子命名空间中的所有进程 (尽管具有不同的 pid 编号)。 网络命名空间隔离网络接口控制器 (物理或虚拟)、iptables 防火墙规则、路由表等。网络命名空间可以使用 “veth “ 虚拟以太网设备彼此连接。 uts 命名空间允许更改主机名。 mount(装载)命名空间允许创建不同的文件系统布局, 或使某些装入点为只读。 ipc 命名空间将 system v 的进程间通信通过命名空间隔离开来。 用户命名空间将用户 id 通过命名空间隔离开来。 普通用户 docker run 容器内 root如 busybox, 可以在 docker 容器中以 root 身份运行软件. 但 docker 容器本身仍以普通用户执行.考虑这样的情况1echo test | docker run -i busybox cat 前面的是当前用户当前系统进程,后面的转入容器内用户和容器内进程运行. 当在容器内 pid 以1运行时, linux 会忽略信号系统的默认行为, 进程收到 sigint 或 sigterm 信号时不会退出, 除非你的进程为此编码. 可以通过 dockerfile stopsignal signal指定停止信号. 普通用户 docker run 容器内指定不同用户 demo_user1docker run --user=demo_user:group1 --group-add group2 &lt;image_name&gt; &lt;command&gt; 这里的 demo_user 和 group1(主组), group2(副组) 不是主机的用户和组, 而是创建容器镜像时创建的. 当dockerfile里没有通过user指令指定运行用户时, 容器会以 root 用户运行进程. docker 指定用户的方式dockerfile 中指定用户运行特定的命令12user &lt;user&gt;[:&lt;group&gt;] #或user &lt;uid&gt;[:&lt;gid&gt;] docker run -u(–user)[user:group] 或 –group-add 参数方式12345678$ docker run busybox cat /etc/passwdroot:x:0:0:root:/root:/bin/sh...www-data:x:33:33:www-data:/var/www:/bin/falsenobody:x:65534:65534:nobody:/home:/bin/false$ docker run --user www-data busybox iduid=33(www-data) gid=33(www-data) docker 容器内用户的权限对比以下情况, host 中普通用户创建的文件, 到 docker 容器下映射成了 root 用户属主:123$ mkdir test &amp;&amp; touch test/a.txt &amp;&amp; cd test$ docker run --rm -it -v `pwd`:/mnt -w /mnt busybox /bin/sh -c &apos;ls -al /mnt/*&apos; -rw-r--r-- 1 root root 0 oct 22 15:36 /mnt/a.txt 而在容器内卷目录中创建的文件, 则对应 host 当前执行 docker 的用户:1234$ docker run --rm -it -v `pwd`:/mnt -w /mnt busybox /bin/sh -c &apos;touch b.txt&apos;$ ls -al-rw-r--r-- 1 xwx staff 0 10 22 23:36 a.txt-rw-r--r-- 1 xwx staff 0 10 22 23:54 b.txt docker volume 文件访问权限创建和使用卷, docker 不支持相对路径的挂载点, 多个容器可以同时使用同一个卷.1234567891011121314$ docker volume create hello #创建卷hello$ docker run -it --rm -v hello:/world -w /world busybox /bin/sh -c &apos;touch /world/a.txt &amp;&amp; ls -al&apos; #容器内建个文件total 8drwxr-xr-x 2 root root 4096 oct 22 16:38 .drwxr-xr-x 1 root root 4096 oct 22 16:38 ..-rw-r--r-- 1 root root 0 oct 22 16:38 a.txt$ docker run -it --rm -v hello:/world -w /world busybox /bin/sh -c &apos;rm /world/a.txt &amp;&amp; ls -al&apos; #从容器内删除total 8drwxr-xr-x 2 root root 4096 oct 22 16:38 .drwxr-xr-x 1 root root 4096 oct 22 16:38 .. 外部创建文件, 容器内指定用户去删除12$ touch c.txt &amp;&amp; sudo chmod root:wheel c.txt$ docker run -u 100 -it --rm -v `pwd`:/world -w /world busybox /bin/sh -c &apos;rm /world/c.txt &amp;&amp; ls -al&apos; 实际是可以删除的123456rm: remove &apos;/world/c.txt&apos;? ytotal 4drwxr-xr-x 4 100 root 128 oct 23 16:09 .drwxr-xr-x 1 root root 4096 oct 23 16:09 ..-rw-r--r-- 1 100 root 0 oct 22 15:36 a.txt-rw-r--r-- 1 100 root 0 oct 22 15:54 b.txt docker 普通用户的1024以下端口权限123456 $ docker run -u 100 -it --rm -p 70:80 busybox /bin/sh -c &apos;nc -l -p 80&apos;nc: bind: permission denied #用户id 100 时, 不能打开80端口 $ docker run -u 100 -it --rm -p 70:8800 busybox /bin/sh -c &apos;nc -l -p 8800&apos; #容器端口大于1024时则可以... $ docker run -it --rm -p 70:80 busybox /bin/sh -c &apos;nc -l -p 80&apos; #容器内是 root 也可以...]]></content>
      <categories>
        <category>镜像</category>
        <category>云计算</category>
        <category>pipeline</category>
        <category>container</category>
      </categories>
      <tags>
        <tag>流程开发</tag>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[docker - 挂载目录]]></title>
    <url>%2F0002.%E5%BC%80%E5%8F%91-04.pipeline%2Fpipeline-container-docker-%E9%95%9C%E5%83%8F%E4%BD%BF%E7%94%A8-%E6%8C%82%E8%BD%BD%E7%9B%AE%E5%BD%95%2F</url>
    <content type="text"><![CDATA[我们可以在创建容器的时候，将宿主机的目录与容器内的目录进行映射，这样我们就可以实现宿主机和容器目录的双向数据自动同步 挂载一个容器启动容器时挂载一个目录（推荐）Docker容器启动的时候，如果要挂载宿主机的一个目录，可以用-v参数指定。 譬如我要启动一个centos容器，宿主机的/test目录挂载到容器的/soft目录，可通过以下方式指定：1# docker run -it -v /test:/soft centos /bin/bash 这样在容器启动后，容器内会自动创建/soft的目录。通过这种方式，我们可以明确一点，即-v参数中，冒号”:”前面的目录是宿主机目录，后面的目录是容器内目录（ 容器目录不可以是相对目录，必须是绝对目录 ）。 默认挂载的目录是可以进行双向同步的，但是有些数据库文件，一般在镜像中我们只会进行数据的读取，可以挂载为只读目录。1docker run -it -v /宿主机目录:/容器目录:ro 镜像名 其中进行目录挂载时的几个执行的逻辑情况如下： 可以挂载多个目录（-v 参数可以重复使用） 1docker run -it -v /宿主机目录:/容器目录 -v /宿主机目录2:/容器目录2 镜像名 容器目录不可以是相对目录，必须是绝对目录 宿主机目录如果不存在，则会自动生成 宿主机目录如果是相对目录，则会基于/var/lib/docker/volumes/ 生成相对目录，与执行目录无关。（可以通过 docker inspect 查看） -v挂载时，只指定一个目录，则会在宿主机/var/lib/docker/volumes/下生成一个随机目录名，指定的目录为容器内的目录。 容器内如果对挂载目录的属主和属组进行了修改，会同步修改宿主对应目录的属主和属组信息，但是更改的是用户的UID，由于容器内和宿主机中UID标识的实际用户一般不一样，因此属主和属组会发生改变，但是用户会存在差异。 容器销毁后，挂载目录内的更改仍会保留，不会还原。 通过使用数据容器挂载 首先创建一个数据卷 命令: docker run -v 需挂载目录的路径:容器挂载路径 –name 数据卷名字 容器名字 /bin/bash 1docker run -v /home/dock/Database:/Database --name Database ubuntu64 /bin/bash 再创建一个新的容器，来使用这个数据卷(通过数据卷的名字 Database。 1docker run -it --volumes-from Database ubuntu64 /bin/bash 通过dockerfile创建挂载点上面介绍的通过docker run命令的-v标识创建的挂载点只能对创建的容器有效。 通过dockerfile的 VOLUME 指令可以在镜像中创建挂载点，这样只要通过该镜像创建的容器都有了挂载点。 还有一个区别是，通过 VOLUME 指令创建的挂载点，无法指定主机上对应的目录，是自动生成的 容器共享卷（挂载点）1docker run --name test1 -it myimage /bin/bash 上面命令中的 myimage是用前面的dockerfile文件构建的镜像。 这样容器test1就有了 /data1 和 /data2两个挂载点。 下面我们创建另一个容器可以和test1共享 /data1 和 /data2卷 ，这是在 docker run中使用 –volumes-from标记，如：12345可以是来源不同镜像，如：docker run --name test2 -it --volumes-from test1 ubuntu /bin/bash也可以是同一镜像，如：docker run --name test3 -it --volumes-from test1 myimage /bin/bash 上面的三个容器 test1 , test2 , test3 均有 /data1 和 /data2 两个目录，且目录中内容是共享的，任何一个容器修改了内容，别的容器都能获取到。 常见异常情况及处理挂载宿主机目录后，无操作权限挂载宿主机已存在目录后，在容器内对其进行操作，报“Permission denied”。 解决方式 关闭selinux。 临时关闭：# setenforce 0 永久关闭：修改/etc/sysconfig/selinux文件，将SELINUX的值设置为disabled。 以特权方式启动容器指定–privileged参数如：# docker run -it –privileged=true -v /test:/soft centos /bin/bash]]></content>
      <categories>
        <category>镜像</category>
        <category>云计算</category>
        <category>pipeline</category>
        <category>container</category>
      </categories>
      <tags>
        <tag>流程开发</tag>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[docker - 基于commit方法的镜像制作]]></title>
    <url>%2F0002.%E5%BC%80%E5%8F%91-04.pipeline%2Fpipeline-container-docker-%E9%95%9C%E5%83%8F%E5%88%B6%E4%BD%9C-commit%E4%BF%9D%E5%AD%98%2F</url>
    <content type="text"><![CDATA[参考资料官方指南知乎 制作自己的Docker镜像制作自己的Docker镜像主要有如下两种方式： 在运行一个已有镜像后，我们进入对应容器，进行了相关的操作（安装、配置等），想要保存相关的操作记录，则可以通过 docker commit将容器保存为一个新的镜像. 构建一个DockerFile（记录了镜像创建步骤），然后使用docker build创建一个新的镜像。 本文主要介绍通过commit进行镜像的创建，DockerFile的创建可以参考另外一篇文章。 使用docker commit 命令来扩展镜像docker commit的方案相对比较简单,而且在镜像创建过程中，允许通过交互式shell来进行容器的调试；但是仍然推荐使用DockerFile进行镜像的管理；docker commit 不会打包容器上挂载卷的数据；正在提交的容器和其进程将在提交镜像时暂停（关闭 –pause false）。 既然以容器为单位，所以首先我们需要获得一个运行状态的容器 12# 通过docker run 命令启动容器docker run -it -v /etc:/etc_test python:latest /bin/bash 进入容器，对容器内的环境（软件、文件、环境、用户等）进行更改。实际和正常使用容器的过程一样 12345docker run -it -v /etc:/etc_test ae9660359c2a /bin/bashroot@1fb4f27a9174:/# pip install pandasCollecting pandas Downloading pandas-2.0.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.3 MB) ━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.3/12.3 MB 72.4 kB/s eta 0:02:46 docker commit 提交修改的镜像1234567891011121314151617181920212223# docker commit [OPTIONS] CONTAINER [REPOSITORY[:TAG]] # 从容器创建一个新的镜像。-a :提交的镜像作者；-c :使用Dockerfile指令来创建镜像,更改dockerFile的元数据；-m :提交时的说明文字；-p :在commit时，将容器暂停。eg: docker commit -m "this is a test" --author='Ben' bbcad8aaf6f1 test:v1$ docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESc3f279d17e0a ubuntu:22.04 /bin/bash 7 days ago Up 25 hours desperate_dubinsky197387f1b436 ubuntu:22.04 /bin/bash 7 days ago Up 25 hours focused_hamiltondocker inspect -f "&#123;&#123; .Config.Env &#125;&#125;" c3f279d17e0a[HOME=/ PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin]# 更改DockerFile的 "ENV" 指令，制定DEBUG的参数为truedocker commit --change "ENV DEBUG=true" c3f279d17e0a svendowideit/testimage:version3f5283438590ddocker inspect -f "&#123;&#123; .Config.Env &#125;&#125;" f5283438590d[HOME=/ PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin DEBUG=true] 使用 docker commit 完成登记以后，新的镜像就创建完成了。 可以看到，通过在容器中进行操作，然后commit保存镜像的方法非常便捷，尤其是针对镜像环境/软件依赖没有明确的情况下，可以灵活的进行环境的调整和测试。而且不需要额外的去熟悉学习DockerFile的相关命令和特性，对刚接触Docker的人来说相对友好。 但是另一个方面，每一个运行的容器中的所有操作统一保存到镜像，一方面难免会存在一些冗余操作，另一方面，不如dockerFile方便进行共享。]]></content>
      <categories>
        <category>镜像</category>
        <category>云计算</category>
        <category>pipeline</category>
        <category>container</category>
      </categories>
      <tags>
        <tag>流程开发</tag>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[docker - 常用命令 - 运行任务是参数的传递]]></title>
    <url>%2F0002.%E5%BC%80%E5%8F%91-04.pipeline%2Fpipeline-container-docker-%E7%BC%96%E7%A0%81%E8%A7%84%E8%8C%83%2F</url>
    <content type="text"><![CDATA[运行容器时传参ENTRYPOINT 和 CMD我们在运行 docker 镜像时希望能用下面的命令向容器传递命令行参数1docker run &lt;image-name&gt; &lt;command&gt; arg1 arg2 如果要向 docker 容器传递参数时，Dockerfile 该如何写，这就有必要稍稍了解一下 Dockerfile 中 CMD 和 ENTRYPOINT这两个指令，并且它们有 exec 和 shell 两种格式的写法。 对于一个 docker 镜像，我们可以这么来理解 ENTRYPOINT 与 CMD 的关系如果没有定义 ENTRYPOINT， CMD 将作为它的 ENTRYPOINT 定义了 ENTRYPOINT 的话，CMD 只为 ENTRYPOINT 提供参数 CMD 可由 docker run \&lt;image> 后的命令覆盖，同时覆盖参数 对于 #1 和 #2 更精致的理解是容器运行的最终入口由 ENTRYPOINT 和实际的 CMD 拼接而成。ENTRYPOINT 和 CMD 需转换为实际镜像中的 exec 格式来拼接，合并后的第一个元素是命令，其余是它的参数。 举四个例子进行说明 一, 未定义 ENTRYPOINT, 定义了 CMD 12#ENTRYPOINT []CMD [&quot;echo&quot;, &quot;hello&quot;] 实际入口是它们拼接后还是 CMD 本身，[“echo”, “hello”] 二, 定义了 ENTRYPOINT 和 CMD 12ENTRYPOINT [&quot;echo&quot;, &quot;hello&quot;]CMD [&quot;echo&quot;, &quot;world&quot;] 实际入口是它们拼接起来，形成 [“echo”, “hello”, “echo”, “world”], 执行 docker run test 显示为 hello echo world 三, 定义了 ENTRYPOINT, CMD 由 docker run 提供 1ENTRYPOINT [&quot;echo&quot;, &quot;hello&quot;] 执行命令 docker run \ rm -rf /, 实际入口是由 [“echo”, “hello”] 与 [“rm”, “-rf”, “/“] 拼接而成的 [“echo”, “hello”, “rm”, “-rf”, “/“], 输出为 hello rm -rf /。看到 rm -rf / 也不用担心，用 ENTRYPOINT 就是让人放心 注：ENTRYPOINT 同样可以被覆盖，如 docker run –entrypoint ls test -l /，将会执行 ls -l / 命令。 四, 如果 ENTRYPOINT 用 shell 格式定义的 1234567891011121314151617181920212223 ENTRYPOINT java -jar /app.jar CMD [&quot;hello&quot;, &quot;world&quot;] ``` 通过 docker inspect 命令看到镜像中实际的 ENTRYPOINT 是 ENTRYPOINT [&quot;/bin/sh&quot;, &quot;-c&quot;, &quot;java -jar /app.jar&quot;] 所以与 CMD 连接起来的入口就是 [&quot;/bin/sh&quot;, &quot;-c&quot;, &quot;java -jar /app.jar&quot;, &quot;hello&quot;, &quot;world&quot;]，&quot;bin/sh&quot; 直接忽略掉后面的 &quot;hello&quot; 与 &quot;world&quot;，这就是为什么shell 命令方式无法获取参数。有了以上几点概念，以及四个实例作为感观认识后，想要怎么往容器传递参数应该很容易确定了。## 传参示例- 未定义 ENTRYPOINT没有定义 ENTRYPOINT 的镜像想怎么来就怎么来，docker run &lt;image&gt; 后面的输入你自己作主。- 有定义 ENTRYPOINT 定义了 ENTRYPOINT 的镜像，则是 CMD 或 docker run &lt;image&gt; 后的输入作为 ENTRYPOINT 中命令的附加参数。再次提醒 shell 格式的 ENTRYPOINT 和 CMD 务必要转换为相应 exec 格式来理解。- shell 格式的 ENTRYPOINT 如果是复杂的 shell 命令不容易拆解出一个个参数，而希望用 shell 格式来定义 ENTRYPOINT 的话，也有办法。shell 格式的 ENTRYPOINT 是由 &quot;/bin/sh -c&quot; 启动的，而它是可以解析变量的。另一方面 CMD 或 docker run &lt;image&gt; 的输入第一个元素存成了 $0，其他剩余元素存为 $@, 所以 shell 格式的 ENTRYPOINT 可以这么写## 使用环境变量传参### 使用ENTRYPOINT对于 shell 格式的 ENTRYPOINT, 或者显式由 &quot;/bin/sh -c&quot; 来启动的命令，可以通过环境变量传入参数 ENTRYPOINT java $JAVA_OPTS -jar app.jar $0 $@ #或显式的 ENTRYPOINT [“/bin/sh”, “-c”, “java $JAVA_OPTS -jar /app.jar $0 $@”]1启动容器时的命令用 docker run -e JAVA_OPTS=”-Xmx5G -Xms2G” aa bb1那么实际执行 java 的完整命令就是 java -Xmx5G -Xms2G -jar /app.jar aa bb12### 使用CMD- dockerfile 示例 FROM docker.io/python:3.6MAINTAINER tianye 设置容器时间 RUN cp /usr/share/zoneinfo/Asia/Shanghai /etc/localtime &amp;&amp; echo ‘Asia/Shanghai’ &gt;/etc/timezone ENV LANG C.UTF-8 # 设置编码 ENV PATH=$PATH:/usr/local/lib/python3.6/ ENV PYTHONPATH $PATH # 配置环境变量 ENV PARAMS=”” # 给我们要传的参数一个初始值 #代码添加到code文件夹 ADD ./tttt/ /test/code/tttt/ #设置code文件夹为工作目录 WORKDIR /test/code/tttt/ CMD python3 ttt.py $PARAMS 1- 创建镜像并启动容器 docker build -t my_image . docker run -it -d –name my_container -e PARAMS=”hahaha” my_image # my_image 放最后 这里hahaha 加不加引号 无所谓 docker logs -f –tail 200 my_container ` Dockerfile中 最后一行 $PARAMS 会解析为一个变量获取其值，也就是 docker run传入的参数 “hahaha”， 在python程序中通过 argv[1] 就可以获取到我们传入的”hahaha” ! 需要注意的一点是Dockerfile 中CMD的用法，如果我们不传参那么写法有很多如： CMD [“python3”, “ttt.py”] CMD [python3, ttt.py] CMD “python3” “ttt.py” CMD python3 ttt.py 但是要传参的话： 我们的参数 \$PARAMS 是万万不能用 “ “ 的，不然Dockerfile会认为是普通字符串 CMD [“python”, “ttt.py”, \$PARAMS] (×) 原因可能是字符串和变量放到一个列表时，字符串优先级高，直接将 $PARAMS当作一个字符串处理 CMD [python3, ttt.py, $PARAMS] (×) CMD “python3” “ttt.py” $PARAMS (√，推荐) CMD python3 ttt.py $PARAMS (√)CMD [] 形式，中括号中 必须用逗号分割； 如果不用中括号，不能用逗号分割！传参机制 容器运行的最终入口由 ENTRYPOINT 和实际的 CMD 拼接而成。 ENTRYPOINT 和 CMD 合并前需转换为 exec 格式(用 docker inspect 查看)，合并后(相当于数组) 第一个元素是命令，其他都为参数 CMD 可在 Dockerfile 中配置，在启动容器时会被 docker run 后的参数覆盖 CMD 的 exec 格式中，第一个元素是 shell 的 $0, 其余元素是 shell 的 $@。当 ENTRYPOINT 中用 shell 格式或显式的 sh(bash等)就可以引用 $0, $@ 环境变量的解析是通过 sh(bash 等) 来解析的，所以 ENTRYPOINT [“echo”, “$name”] 中的 $name 是不被解析的最能说明问题的是 docker inspect 看个究竟，Path 和 Args 说明了一切 参考资料 CSDN博客 博客]]></content>
      <categories>
        <category>镜像</category>
        <category>云计算</category>
        <category>pipeline</category>
        <category>container</category>
      </categories>
      <tags>
        <tag>流程开发</tag>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[docker - 常用命令 - 运行任务是参数的传递]]></title>
    <url>%2F0002.%E5%BC%80%E5%8F%91-04.pipeline%2Fpipeline-container-docker-%E8%BF%90%E8%A1%8C%E4%BC%A0%E5%8F%82%2F</url>
    <content type="text"><![CDATA[运行容器时传参ENTRYPOINT 和 CMD我们在运行 docker 镜像时希望能用下面的命令向容器传递命令行参数1docker run &lt;image-name&gt; &lt;command&gt; arg1 arg2 如果要向 docker 容器传递参数时，Dockerfile 该如何写，这就有必要稍稍了解一下 Dockerfile 中 CMD 和 ENTRYPOINT这两个指令，并且它们有 exec 和 shell 两种格式的写法。 对于一个 docker 镜像，我们可以这么来理解 ENTRYPOINT 与 CMD 的关系如果没有定义 ENTRYPOINT， CMD 将作为它的 ENTRYPOINT 定义了 ENTRYPOINT 的话，CMD 只为 ENTRYPOINT 提供参数 CMD 可由 docker run \&lt;image> 后的命令覆盖，同时覆盖参数 对于 #1 和 #2 更精致的理解是容器运行的最终入口由 ENTRYPOINT 和实际的 CMD 拼接而成。ENTRYPOINT 和 CMD 需转换为实际镜像中的 exec 格式来拼接，合并后的第一个元素是命令，其余是它的参数。 举四个例子进行说明 一, 未定义 ENTRYPOINT, 定义了 CMD 12#ENTRYPOINT []CMD [&quot;echo&quot;, &quot;hello&quot;] 实际入口是它们拼接后还是 CMD 本身，[“echo”, “hello”] 二, 定义了 ENTRYPOINT 和 CMD 12ENTRYPOINT [&quot;echo&quot;, &quot;hello&quot;]CMD [&quot;echo&quot;, &quot;world&quot;] 实际入口是它们拼接起来，形成 [“echo”, “hello”, “echo”, “world”], 执行 docker run test 显示为 hello echo world 三, 定义了 ENTRYPOINT, CMD 由 docker run 提供 1ENTRYPOINT [&quot;echo&quot;, &quot;hello&quot;] 执行命令 docker run \ rm -rf /, 实际入口是由 [“echo”, “hello”] 与 [“rm”, “-rf”, “/“] 拼接而成的 [“echo”, “hello”, “rm”, “-rf”, “/“], 输出为 hello rm -rf /。看到 rm -rf / 也不用担心，用 ENTRYPOINT 就是让人放心 注：ENTRYPOINT 同样可以被覆盖，如 docker run –entrypoint ls test -l /，将会执行 ls -l / 命令。 四, 如果 ENTRYPOINT 用 shell 格式定义的 1234567891011121314151617181920212223 ENTRYPOINT java -jar /app.jar CMD [&quot;hello&quot;, &quot;world&quot;] ``` 通过 docker inspect 命令看到镜像中实际的 ENTRYPOINT 是 ENTRYPOINT [&quot;/bin/sh&quot;, &quot;-c&quot;, &quot;java -jar /app.jar&quot;] 所以与 CMD 连接起来的入口就是 [&quot;/bin/sh&quot;, &quot;-c&quot;, &quot;java -jar /app.jar&quot;, &quot;hello&quot;, &quot;world&quot;]，&quot;bin/sh&quot; 直接忽略掉后面的 &quot;hello&quot; 与 &quot;world&quot;，这就是为什么shell 命令方式无法获取参数。有了以上几点概念，以及四个实例作为感观认识后，想要怎么往容器传递参数应该很容易确定了。## 传参示例- 未定义 ENTRYPOINT没有定义 ENTRYPOINT 的镜像想怎么来就怎么来，docker run &lt;image&gt; 后面的输入你自己作主。- 有定义 ENTRYPOINT 定义了 ENTRYPOINT 的镜像，则是 CMD 或 docker run &lt;image&gt; 后的输入作为 ENTRYPOINT 中命令的附加参数。再次提醒 shell 格式的 ENTRYPOINT 和 CMD 务必要转换为相应 exec 格式来理解。- shell 格式的 ENTRYPOINT 如果是复杂的 shell 命令不容易拆解出一个个参数，而希望用 shell 格式来定义 ENTRYPOINT 的话，也有办法。shell 格式的 ENTRYPOINT 是由 &quot;/bin/sh -c&quot; 启动的，而它是可以解析变量的。另一方面 CMD 或 docker run &lt;image&gt; 的输入第一个元素存成了 $0，其他剩余元素存为 $@, 所以 shell 格式的 ENTRYPOINT 可以这么写## 使用环境变量传参### 使用ENTRYPOINT对于 shell 格式的 ENTRYPOINT, 或者显式由 &quot;/bin/sh -c&quot; 来启动的命令，可以通过环境变量传入参数 ENTRYPOINT java $JAVA_OPTS -jar app.jar $0 $@ #或显式的 ENTRYPOINT [“/bin/sh”, “-c”, “java $JAVA_OPTS -jar /app.jar $0 $@”]1启动容器时的命令用 docker run -e JAVA_OPTS=”-Xmx5G -Xms2G” aa bb1那么实际执行 java 的完整命令就是 java -Xmx5G -Xms2G -jar /app.jar aa bb12### 使用CMD- dockerfile 示例 FROM docker.io/python:3.6MAINTAINER tianye 设置容器时间 RUN cp /usr/share/zoneinfo/Asia/Shanghai /etc/localtime &amp;&amp; echo ‘Asia/Shanghai’ &gt;/etc/timezone ENV LANG C.UTF-8 # 设置编码 ENV PATH=$PATH:/usr/local/lib/python3.6/ ENV PYTHONPATH $PATH # 配置环境变量 ENV PARAMS=”” # 给我们要传的参数一个初始值 #代码添加到code文件夹 ADD ./tttt/ /test/code/tttt/ #设置code文件夹为工作目录 WORKDIR /test/code/tttt/ CMD python3 ttt.py $PARAMS 1- 创建镜像并启动容器 docker build -t my_image . docker run -it -d –name my_container -e PARAMS=”hahaha” my_image # my_image 放最后 这里hahaha 加不加引号 无所谓 docker logs -f –tail 200 my_container ` Dockerfile中 最后一行 $PARAMS 会解析为一个变量获取其值，也就是 docker run传入的参数 “hahaha”， 在python程序中通过 argv[1] 就可以获取到我们传入的”hahaha” ! 需要注意的一点是Dockerfile 中CMD的用法，如果我们不传参那么写法有很多如： CMD [“python3”, “ttt.py”] CMD [python3, ttt.py] CMD “python3” “ttt.py” CMD python3 ttt.py 但是要传参的话： 我们的参数 \$PARAMS 是万万不能用 “ “ 的，不然Dockerfile会认为是普通字符串 CMD [“python”, “ttt.py”, \$PARAMS] (×) 原因可能是字符串和变量放到一个列表时，字符串优先级高，直接将 $PARAMS当作一个字符串处理 CMD [python3, ttt.py, $PARAMS] (×) CMD “python3” “ttt.py” $PARAMS (√，推荐) CMD python3 ttt.py $PARAMS (√)CMD [] 形式，中括号中 必须用逗号分割； 如果不用中括号，不能用逗号分割！传参机制 容器运行的最终入口由 ENTRYPOINT 和实际的 CMD 拼接而成。 ENTRYPOINT 和 CMD 合并前需转换为 exec 格式(用 docker inspect 查看)，合并后(相当于数组) 第一个元素是命令，其他都为参数 CMD 可在 Dockerfile 中配置，在启动容器时会被 docker run 后的参数覆盖 CMD 的 exec 格式中，第一个元素是 shell 的 $0, 其余元素是 shell 的 $@。当 ENTRYPOINT 中用 shell 格式或显式的 sh(bash等)就可以引用 $0, $@ 环境变量的解析是通过 sh(bash 等) 来解析的，所以 ENTRYPOINT [“echo”, “$name”] 中的 $name 是不被解析的最能说明问题的是 docker inspect 看个究竟，Path 和 Args 说明了一切 参考资料 CSDN博客 博客]]></content>
      <categories>
        <category>镜像</category>
        <category>云计算</category>
        <category>pipeline</category>
        <category>container</category>
      </categories>
      <tags>
        <tag>流程开发</tag>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[docker - 镜像管理]]></title>
    <url>%2F0002.%E5%BC%80%E5%8F%91-04.pipeline%2Fpipeline-container-docker-%E9%95%9C%E5%83%8F%E7%AE%A1%E7%90%86-%E4%B8%8B%E8%BD%BD%E4%B8%8A%E4%BC%A0%2F</url>
    <content type="text"><![CDATA[docker产生的本质是为了让开发环境更好的进行迁移和分享，因此我们需要一个远程仓库可以保存我们的镜像，以便进行镜像的分享，和不同环境的迁移。同时在此介绍一些常用的docker仓库和配置方式，供大家参考 下载镜像12345# 下载一个公共镜像$ docker pull centos:centos7.6.1810 # 从私有仓库下载一个镜像$ docker pull registry.cn-shenzhen.aliyuncs.com/ben*/ubuntu:[镜像版本号] 上传镜像针对没指定远程的镜像，需要先进行标记（docker tag），将本地镜像和远程仓库进行关联。粗暴一点理解就是在镜像前面加上自己的docker hub的Docker ID，如果是第三方仓库或私有仓库还需要标注注册的主机名/IP和端口 123456789101112131415161718#docker tag [OPTIONS] IMAGE[:TAG] [REGISTRYHOST/][USERNAME/]NAME[:TAG]#可以使用镜像ID进行标记docker tag 0e5574283393 ben*/ubuntu:version1.0# 可以使用本地的镜像名称进行标记docker tag ubuntu ben*/ubuntu:version1.0# 也可以使用镜像的本地名称和tag进行标记docker tag ubuntu:test ben*/ubuntu:version1.0.test# 如果推送到私有仓库，则需要在目标仓库中明文标记主机名（默认推送的docker hub）# 示例docker tag 0e5574283393 myregistryhost:5000/ben*/ubuntu:version1.0# 以阿里云ACR服务为例docker tag 0e5574283393 registry.cn-shenzhen.aliyuncs.com/ben*/ubuntu:version1.0# 如果推送到私有仓库docker tag 0e5574283393 localhost:5000/ubuntu:version1.0 完成仓库的关联后，直接通过docker push命令进行推送1234567891011121314# docker push [OPTIONS] NAME[:TAG]OPTIONS说明：--disable-content-trust :忽略镜像的校验,默认开启# 请确定已经参考前文进行了账户信息的配置$ docker push ben*/ubuntu:version1.0The push refers to repository [docker.io/ben*/pancancer]3c8d9fba298a: Pushedf9a878ee0ce7: Pushed58f7c9d6536a: Pushed89df9bf0321e: Pushedf925ccf73093: Pushedf8b471c2b6cb: Pushed89169d87dbe2: Mounted from library/centos# 会基于镜像的层数，逐层进行推送， 至此，你的本地镜像就推送到了远程，就可以非常方便的进行共享了。]]></content>
      <categories>
        <category>镜像</category>
        <category>云计算</category>
        <category>pipeline</category>
        <category>container</category>
      </categories>
      <tags>
        <tag>流程开发</tag>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[docker - 基于DockerFile的镜像制作]]></title>
    <url>%2F0002.%E5%BC%80%E5%8F%91-04.pipeline%2Fpipeline-container-docker-%E9%95%9C%E5%83%8F%E5%88%B6%E4%BD%9C-DockerFile%2F</url>
    <content type="text"><![CDATA[制作自己的Docker镜像主要有如下两种方式： 运行一个已有的镜像，启动镜像后，进入容器，进行相关的操作（安装、配置等）然后将容器保存为一个新的镜像 构建一个DockerFile（记录了镜像创建步骤），然后使用docker build创建一个新的镜像。 本文主要介绍通过DockerFile进行镜像的创建，commit的创建可以参考另外一篇文章。 Dockerfile是一个文本文档，其中包含用户可以在命令行上运行调用以生成镜像的所有命令。获得 Dockerfile 文件后，我们可以使用 docker build 来完成镜像的创建。所以我们首先需要了解，如何创建一个 Dockerfile 文件，以及创建Dockerfile 的过程中，可以使用的指令。 Dockerfile 基本的语法Dockerfile 中使用#来进行注释，同时Dockerfile支持多种不同的命令分别用以指定初始镜像（FROM）、工作目录（WORKDIR）、运行数据挂载和网络设置（RUN）、命令行执行（CMD）、设置环境变量（ENV）、添加文件到镜像文件系统中（ADD）、添加文件到容器文件系统中（COPY）、创建挂载点（VOLUME ）、设置用户（USER）、变量（ARG）、镜像添加元数据（LABEL）、监听网络（EXPOSE）、入口点（ENTRYPOINT）等等。现在为大家介绍一些镜像构建过程中会用到的基本命令，一些指令在现在的个人进行镜像构建的过程中并未用到，暂时不进行扩展，大家有需要的时候可以参考官方文档DockerFile reference From1FROM [--platform=&lt;platform&gt;] &lt;image&gt;[:&lt;tag&gt;] [AS &lt;name&gt;] FROM 初始化一个新的构建阶段并为后续指令设置基础镜像。因此，有效的 Dockerfile 必须以 FROM 开头。该镜像可以是任何有效的镜像（尽可能选择满足需求的最小镜像）。 ARG是唯一可以位于FROM之前的指令 FROM 可以在单个 Dockerfile 中出现多次，以创建多个镜像或使用一个构建阶段作为另一个构建阶段的依赖项。前面的FROM及对应的构建阶段都不会保存，只有最后一个阶段构建的镜像会被直接保存。 构建镜像的步骤 新建一个目录和一个 Dockerfile 123$ mkdir new_folder$ cd new_folder$ touch Dockerfile 编写Dockerfile，Dockerfile中每一条/行指令都创建镜像的一层，所以尽可能合并无意义的层，避免镜像过大，例如： 不建议的写法（命令拆分产生无意义的多层镜像）：12345678# 这里是注释# 设置继承自哪个镜像FROM ubuntu:14.04# 下面是一些创建者的基本信息MAINTAINER birdben (191654006@163.com)# 在终端需要执行的命令RUN apt-get install -y openssh-server # 创建第一层RUN mkdir -p /var/run/sshd # 创建第二层 建议的写法（只创建一层）123456# 设置继承自哪个镜像FROM ubuntu:14.04# 下面是一些创建者的基本信息MAINTAINER birdben (191654006@163.com)# 在终端需要执行的命令RUN apt-get install -y openssh-server &amp;&amp; mkdir -p /var/run/sshd # 创建第一层 编写完成 Dockerfile 后可以使用 docker build 来生成镜像。12345678910111213141516171819202122$ sudo docker build -t=&quot;birdben/ubuntu:v1&quot; .# 下面是一堆构建日志信息############我是日志############# 参数：# -t 标记来添加 tag，指定新的镜像的用户和镜像名称信息。 # “.” 是 Dockerfile 所在的路径（当前目录），也可以替换为一个具体的 Dockerfile 的路径。# 以交互方式运行docker$ docker run -it birdben/ubuntu:v1 /bin/bash# 运行docker时指定配置$ sudo docker run -d -p 10.211.55.4:9999:22 ubuntu:tools &apos;/usr/sbin/sshd&apos; -D# 参数：# -i：表示以“交互模式”运行容器，-i 则让容器的标准输入保持打开# -t：表示容器启动后会进入其命令行，-t 选项让Docker分配一个伪终端（pseudo-tty）并绑定到容器的标准输入上# -v：表示需要将本地哪个目录挂载到容器中，格式：-v &lt;宿主机目录&gt;:&lt;容器目录&gt;，-v 标记来创建一个数据卷并挂载到容器里。在一次 run 中多次使用可以挂载多个数据卷。# -p：指定对外80端口# 不一定要使用“镜像 ID”，也可以使用“仓库名:标签名” DockerFile支持的指令功能及示例 DockerFile指令 作用 示例 FROM 初始基础镜像，后续所有操作都是以基础镜像为初始环境 FROM ubuntu:14.04 RUN 用于执行后面跟着的命令行命令。有以下俩种格式。 RUN &lt;命令行命令&gt; COPY 复制指令，从上下文目录中复制文件或者目录到容器里指定路径。 COPY [--chown=&lt;user&gt;:&lt;group&gt;] &lt;源路径1&gt;... &lt;目标路径&gt; RUN示例 1234567#shell格式，&lt;命令行命令&gt; 等同于，在终端操作的 shell 命令。RUN &lt;命令行命令&gt;#exec 格式：RUN ["可执行文件", "参数1", "参数2"]# 例如：# RUN ["./test.php", "dev", "offline"] 等价于 RUN ./test.php dev offline COPY示例 12 在build阶段设置参数 Dockerfile 最后一行如下： 12345678910111213[root@fangjike temp]# cat Dockerfile FROM python:2.7-slimMAINTAINER yellowtailCOPY startup.sh /optRUN chmod +x /opt/startup.shARG envType=xxxENV envType $&#123;envType&#125;CMD /opt/startup.sh $&#123;envType&#125; build 1docker build -t yellow:4.0 --build-arg envType=dev . run 12[root@fangjike temp]# docker run -ti --rm=true yellow:4.0in startup, args: dev 参考资料官方指南知乎]]></content>
      <categories>
        <category>镜像</category>
        <category>云计算</category>
        <category>pipeline</category>
        <category>container</category>
      </categories>
      <tags>
        <tag>流程开发</tag>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[docker - 常用命令 - 运行任务是参数的传递]]></title>
    <url>%2F0002.%E5%BC%80%E5%8F%91-04.pipeline%2Fpipeline-container-docker-%E5%BC%82%E5%B8%B8%E6%B8%85%E5%8D%95%2F</url>
    <content type="text"><![CDATA[环境问题交互式启动容器（-it)，进入容器后可以执行安装的软件（以perl为例），但是通过 -i 运行容器找不到安装的软件。 启动容器时不会运行bash 文件，需要将软件链接到 Usr/bin 中。 通过 -it运行示例如下1234$ docker run -v /home/gedongcen/WDL/:/home/gedongcen/WDL/ -it aio_dev:1.0.0.0#进入环境root@caf48909e8b5[Wed Feb 08]$ perl -v# This is perl 5, version 22, subversion 0 (v5.22.0) built for x86_64-linux-thread-multi 通过 -i 运行示例如下12345$ docker run -v /home/gedongcen/WDL/:/home/gedongcen/WDL/ -i aio_dev:1.0.0.0 perl -v# docker: Error response from daemon: failed to create shim task: OCI runtime create failed: runc create failed: unable to start container process: exec: "perl": executable file not found in $PATH: unknown.$ docker run -v /home/gedongcen/WDL/:/home/gedongcen/WDL/ -i aio_dev:1.0.0.0 /bin/bash perl -v# /bin/bash: perl: No such file or directory]]></content>
      <categories>
        <category>镜像</category>
        <category>云计算</category>
        <category>pipeline</category>
        <category>container</category>
      </categories>
      <tags>
        <tag>流程开发</tag>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[docker - 常用命令]]></title>
    <url>%2F0002.%E5%BC%80%E5%8F%91-04.pipeline%2Fpipeline-container-docker-02.%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[常用命令docker相关常用命令123456789sudo systemctl start docker #启动dockersystemctl restart docker.service #重启dockerdocker version #查看 docker 版本docker info #显示 docker 系统信息（容器情况、镜像情况等 ）docker –help #显示所有命令docker login/logout # 登录、退出Docker账号docker ps -a # 查看所有容器；docker search keyword # 从Docker Hub中搜索相关镜像docker top # 查看容器中正在运行的进程 镜像相关常用命令123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119===== 查看镜像 # 查看本地镜像列表docker images # 查看镜像制作过程docker history ===== 获取镜像 # 从配置的仓库下载镜像 docker pull &lt;name:tag&gt; # 获取指定版本的镜像eg: docker pull centos:centos7 # 默认获得最新版本（latest）eg: docker pull centos ===== 创建镜像(DockerFile)docker build &lt;Path&gt; \ # Path为包含DokcerFile的目录-f python.dockerFile #若DockerFile的文件名不是默认名称则需要说明===== 创建镜像(快照)docker load # 通过tar包，导入镜像# 通过镜像tar（save创建）包导入镜像库eg: docker load -i python.dockerimage.tar # 从容器快照文件(export创建的tar) 导入到镜像库docker import ubuntu.tar ubuntu:v1 ===== 重命名镜像# 对镜像进行从命名docker tag &lt;镜像ID&gt; &lt;newname:newtag&gt; ===== 保存镜像docker save &lt;镜像ID&gt; # 将镜像保存到一个tar包eg: docker save python:latest -o python.tar===== 上传镜像docker push &lt;name:tag&gt; # 推送镜像到远程仓库eg: eg: docker push benair/pancancer:v0 ===== 删除镜像docker rmi &lt;镜像ID&gt;eg: docker rmi -f ae9660359c2ad``` ## 容器相关常用命令```shell===== 查看容器$ docker ps #查看运行中的容器CONTAINER-ID IMAGE COMMAND CREATED STATUS PORTS NAMES5917eac21c36 ubuntu:15.10 "/bin/bash" 52 minutes ago Up 52 minutes sweet_agnesi# 查看容器的内容变更docker diff &lt;CONTAINERID&gt; # 查看特定任务的日志文件$ docker logs &lt;CONTAINERID&gt; # 用于查看容器的配置信息，包含容器名、环境变量、运行命令、主机配置、网络配置和数据卷配置等。docker inspect &lt;CONTAINERID&gt; ===== 启动容器docker run &lt;镜像ID&gt;/&lt;name:tag&gt; # 通过特定镜像启动一个容器eg: docker run \-d \ # -d 后台启动一个容器 -it \ # -it 交互式启动容器 -v /share:/share \ # -v 将宿主机的目录挂载的容器内部（实现文件的交互）； -u root \ # -u 指定登陆容器后的用户名称 -p 80:80 \ # -p 对宿主机和容器的端口建立映射-H hostname \ # -H 制定容器的主机名称--name centos \ # --name 制定容器的名称centos:centos7 \ # 启动的容器镜像/bin/bash # 启动容器后，执行的命令eg: docker run -it===== 进入容器docker attach/exec # attach 进入容器后，如果退出容器会导致容器停止eg: docker attach &lt;镜像ID&gt;/&lt;name:tag&gt; # exec 进入容器后，如果退出，容器不会停止eg: docker exec -it &lt;镜像ID&gt;/&lt;name:tag&gt; /bin/bash ===== 退出容器# 交互状态中镜像中运行exit 或者使用 ctrk+d 退出容器$ exit===== 容器内文件交互# 将文件从容器拷贝到宿主机docker cp &lt;CONTAINERID&gt;:/FilePath /FilePath # 将文件从宿主机拷贝到容器docker cp /FilePath &lt;CONTAINERID&gt;:/FilePath===== 删除容器 docker rm# 删除指定容器,删除容器前需要确定容器是关闭的。docker rm -f 1e560fca3906# 批量删除某个镜像衍生的容器docker ps -a | grep &lt;镜像ID&gt; | awk -F ' ' '&#123;print "docker rm "$1&#125;' | sh # 清理掉所有处于终止状态的容器。docker container prune ===== 保存容器（快照）# docker export 基于当前版本建立快照（之前的层不保存）docker export &lt;CONTAINERID&gt; &gt; ubuntu.tardocker export &lt;CONTAINERID&gt; -o ubuntu.tar# docker commit 当前容器内容新建一层（在原有启动镜像基础上新建一层）docker commit &lt;CONTAINERID&gt; &lt;name:tag&gt; # 如果容器需要上传到远程仓库，在保存时还需要指定用户docker commit &lt;CONTAINERID&gt; usr/&lt;name:tag&gt; ===== 关启容器# 启动容器（不能是运行状态的容器）docker start &lt;镜像ID&gt;/&lt;name:tag&gt;# 停止容器 docker stop &lt;镜像ID&gt;/&lt;name:tag&gt;# 重启容器docker restart &lt;镜像ID&gt;/&lt;name:tag&gt;# 终止容器docker kill &lt;镜像ID&gt;/&lt;name:tag&gt;===== 其他命令docker rename # 重命名容器 参考资料官方文档Docker:Guides:Get started 生信常用镜像清单bwavepGATK 官方介绍]]></content>
      <categories>
        <category>镜像</category>
        <category>云计算</category>
        <category>pipeline</category>
        <category>container</category>
      </categories>
      <tags>
        <tag>流程开发</tag>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[云计算技术及性能优化]]></title>
    <url>%2F0003.%E5%BA%94%E7%94%A8-01.cloud_computing%2F%E4%BA%91%E8%AE%A1%E7%AE%97-%E4%B9%A6_%E4%BA%91%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E5%8F%8A%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%9601%2F</url>
    <content type="text"><![CDATA[写在前面，最近一些业务工作，涉及到整体的云计算业务平台开发。之前虽然都有一些零散的涉猎和了解，但是知识框架不成体系，零散的认知在紧迫的业务面前捉襟见肘。所以趁着双十一，买了一些教材构建一个针对云计算的系统认知。各种博客、课程、教材看的再多，可能都不如自己亲自体验经历一遍，因此也会借此机会在系列课程的学习过程中，利用一些现有条件进行实践。 - 纸上得来终觉浅，绝知此事要躬行。 云图 – 云计算图志 云计算的产生与发展1.1 云计算的产生20世纪60年代，John McCarthy提到“计算迟早有一天会变成一种公用基础设施”。 2007年10月，IBM和GOogle宣布在云计算领域的合作，云计算开始吸引了众多的关注迅速发展。 21世纪初，Web2.0 的流程让网络迎来了新的发展高峰。 技术上，分布式计算(Distributed Computing)技术的日益成熟和应用，特别是网络计算的发展通过Internet把分散在各处的硬件、软件、信息资源链接成为一个巨大的整体，是的人们能够利用地理上分散的资源，完成大规模、复杂的计算和数据处理任务。 数据存储的快速增长产生了以谷歌文件系统(Google File System, GFS)、存储域网络(Storage Area Network, SAN)为代表的的高性能存储技术。另外服务器整合需求推动了虚拟化技术的进步，这些技术的发展未构建更强大的计算能力和服务平台提供了可能。云计算应运而生。 1.2 云计算发展历程1.2.1 计算模式演进云计算是在并行计算(Parallel Computing)、分布式计算、网格计算(Grid Computing)和效用计算(Utility Computing)的基础上发展起来的，经过持续烟花和融合改进逐步形成目前留下的云计算模型，云计算的演化过程如下所述： 1. 并行计算包括空间并行、基于流水线技术的时间并行，以及基于优化算法的数据并行和任务并行等。是对穿行计算的单指令流单数据流做出优化，以及通过采用多指令流多数据流的并行计算的大幅度提升系统的处理能力。 2. 分布式计算分布式计算模式是在处理庞大的计算请求时，将需要解决的问题分解成细小的组成部分，然后将这些组成部分分散给众多的计算机进行处理，处理完成后将结果进行汇总，形成最终结果。 3. 网格计算网格计算是一种无缝、集成的计算和协作环境。安装网格提供的功能，网格可分为两类：计算网格和存储网格。计算网格可以提供虚拟的、无限制的计算和分布数据资源，存储网格则提供一个合作环境。 4. 效用计算效用计算的具体目标是结合分散各地的服务器、存储系统及应用程序来立即提供需求数据的技术。效用这个词是指为客户提供个性化的服务，并且可以满足不断变化的客户需求，可以基于实际占用的资源进行收费。 5. 云计算云计算强调所有资源均以服务的形态出现，包括基础设施即服务、平台即服务、软件即服务、数据即服务、知识即服务、存储即服务、安全即服务等。 1.2.2 云计算发展大事记 1959年6月，Christopher Strachey发表虚拟化论文，虚拟化是今天云计算基础架构的基石。 1961年，John McCarthy提出计算力和通过公用事业销售计算机应用的思想。 1962年，J.C.R. Licklider提出“星际计算机网络”设想。 1965年 美国电话公司Western Union一位高管提出建立信息公用事业的设想。 1984年，Sun公司的联合创始人John Gage说出了“网络就是计算机”的名言，用于描述分布式计算技术带来的新世界，今天的云计算正在将这一理念变成现实。 1996年，网格计算Globus开源网格平台起步。 1997年，南加州大学教授Ramnath K. Chellappa提出云计算的第一个学术定义”，认为计算的边界可以不是技术局限，而是经济合理性。 1998年，VMware(威睿公司)成立并首次引入X86的虚拟技术。 1999年，Marc Andreessen创建LoudCloud，是第一个商业化的IaaS平台。 1999年，salesforce.com公司成立，宣布“软件终结”革命开始。 2000年，SaaS兴起。 2004年，Web 2.0会议举行，Web 2.0成为技术流行词，互联网发展进入新阶段。 2004年，Google发布MapReduce论文。Hadoop就是Google集群系统的一个开源项目总称，主要由HDFS、MapReduce和Hbase组成，其中HDFS是Google File System(GFS)的开源实现;MapReduce是Google MapReduce的开源实现;HBase是Google BigTable的开源实现。 2004年，Doug Cutting 和 Mike Cafarella实现了Hadoop分布式文件系统(HDFS)和Map-Reduce，Hadoop并成为了非常优秀的分布式系统基础架构。 2005年，Amazon宣布Amazon Web Services云计算平台。 2006年，Amazon相继推出在线存储服务S3和弹性计算云EC2等云服务。 2006年，Sun推出基于云计算理论的“BlackBox”计划。 2007年，Google与IBM在大学开设云计算课程。 2007年3月，戴尔成立数据中心解决方案部门，先后为全球5大云计算平台中的三个(包括Windows Azure、Facebook和Ask.com)提供云基础架构。 2007年7月，亚马逊公司推出了简单队列服务(Simple Queue Service，SQS)，这项服务使托管主机可以存储计算机之间发送的消息。 2007年11月，IBM首次发布云计算商业解决方案，推出“蓝云”(Blue Cloud)计划。 2008年1月，Salesforce.com推出了随需应变平台DevForce,Force.com平台是世界上第一个平台即服务的应用。 2008年2月，EMC中国研发集团云架构和服务部正式成立，该部门结合云基础架构部、Mozy和Pi两家公司共同形成EMC云战略体系。 2008年2月，IBM宣布在中国无锡太湖新城科教产业园为中国的软件公司建立第一个云计算中心。 2008年4月，Google App Engine发布。 2008年中，Gartner发布报告，认为云计算代表了计算的方向。 2008年5月，Sun在2008JavaOne开发者大会上宣布推出“Hydrazine”计划。 2008年6月，EMC公司中国研发中心启动“道里”可信基础架构联合研究项目。 2008年6月，IBM宣布成立IBM大中华区云计算中心。 2008年7月，HP、Intel和Yahoo联合创建云计算试验台Open Cirrus。 2008年8月3日，美国专利商标局(以下简称“SPTO”)网站信息显示，戴尔正在申请“云计算”(Cloud Computing)商标，此举旨在加强对这一未来可能重塑技术架构的术语的控制权。戴尔在申请文件中称，云计算是“在数据中心和巨型规模的计算环境中，为他人提供计算机硬件定制制造”。 2008年9月 Google公司推出Google Chrome浏览器，将浏览器彻底融入云计算时代。 2008年9月，甲骨文和亚马逊AWS合作，用户可在云中部署甲骨文软件、在云中备份甲骨文数据库。 2008年9月，思杰公布云计算战略，并发布新的思杰云中心(Citrix Cloud Center，C3)产品系列。 2008年10月，微软发布其公共云计算平台——Windows Azure Platform，由此拉开了微软的云计算大幕。 2008年12月，Gartner披露十大数据中心突破性技术，虚拟化和云计算上榜。 2008年，亚马逊、Google和Flexiscale的云服务相继发生宕机故障，引发业界对云计算安全的讨论。 2009年，思科先后发布统一计算系统(UCS)、云计算服务平台，并与EMC、Vmware建立虚拟计算环境联盟。 2009年1月，阿里软件在江苏南京建立首个“电子商务云计算中心”。 2009年4月，VMware推出业界首款云操作系统VMware vSphere 4。 2009年7月 Google宣布将推出Chrome OS操作系统。 2009年7月，中国首个企业云计算平台诞生(中化企业云计算平台)。 2009年9月，VMware启动vCloud计划 构建全新云服务。 2009年11月，中国移动云计算平台“大云”计划启动。 2010年1月，HP和微软联合提供完整的云计算解决方案。 2010年1月，IBM与松下达成迄今为止全球最大的云计算交易。 2010年1月，Microsoft正式发布Microsoft Azure云平台服务。 2010年4月，英特尔在IDF上提出互联计算，图谋用X86架构统一嵌入式、物联网和云计算领域。 2010年，微软宣布其90%员工将从事云计算及相关工作。 2010年4月，戴尔推出源于DCS部门设计的PowerEdgeC系列云计算服务器及相关服务。 1.2.3 云计算时代企业的云化IT设施建设过程可以分为三个阶段，如下图 第一个阶段：大集中过程。这一过程将企业分散的数据资源、IT 资源进行了物理集中，形成了规模化的数据中心基础设施。在数据集中过程中，不断实施数据和业务的整合，大多数企业的数据中心基本完成了自身的标准化，使得既有业务的扩展和新业务的部署能够规划、可控，并以企业标准进行IT 业务的实施，解决了数据业务分散时期的混乱无序问题。在这一阶段中，很多企业在数据集中后期也开始了容灾建设，特别是在雪灾、大地震之后，企业的容灾中心建设普遍受到重视，以金融为热点行业几乎开展了全行业的容灾建设热潮，并且金融行业的大部分容灾建设的级别都非常高，面向应用级容灾（数据零丢失为目标）。总的来说，第一阶段过程解决了企业IT 分散管理和容灾的问题。 第二个阶段：实施虚拟化的过程。在数据集中与容灾实现之后，随着企业的快速发展，数据中心IT 基础设施扩张很快，但是系统建设成本高、周期长，即使是标准化的业务模块建设（哪怕是系统的复制性建设），软硬件采购成本、调试运行成本与业务实现周期并没有显著下降。标准化并没有给系统带来灵活性， 集中的大规模IT 基础设施出现了大量系统利用率不足的问题，不同的系统运行在独占的硬件资源中，效率低下而数据中心的能耗、空间问题逐步突显出来。因此，以降低成本、提升 IT 运行灵活性、提升资源利用率为目的的虚拟化开始在数据中心进行部署。虚拟化屏蔽了不同物理设备的异构性，将基于标准化接口的物理资源虚拟化成逻辑上也完全 标准化和一致化的逻辑计算资源（虚拟机）和逻辑存储空间。虚拟化可以将多台物理服务器整合成单台，每台服务器上运行多种应用的虚拟机，实现物理服务器资源 利用率的提升，由于虚拟化环境可以实现计算与存储资源的逻辑化变更，特别是虚拟机的克隆，使得数据中心IT 实施的灵活性大幅提升，业务部署周期可用数月缩小到一天以内。虚拟化后，应用以VM 为单元部署运行，数据中心服务器数量可大为减少且计算能效提升，使得数据中心的能耗与空间问题得到控制。 总的来说，第二阶段过程提升了企业IT 架构的灵活性，数据中心资源利用率有效提高，运行成本降低。 第三个阶段：云计算阶段。对企业而言，数据中心的各种系统（包括软硬件与基础设施）是一大笔资源投入。新系统（特别是硬件）在建成后一般经历3-5 年即面临逐步老化与更换，而软件技术则不断面临升级的压力。另一方面，IT 的投入难以匹配业务的需求，即使虚拟化后，也难以解决不断增加的业务对资源的变化需求，在一定时期内扩展性总是有所限制。于是企业IT 产生新的期望蓝图：IT 资源能够弹性扩展、按需服务，将服务作为IT 的核心，提升业务敏捷性，进一步大幅降低成本。因此，面向服务的IT 需求开始演化到云计算架构上。云计算架构可以由企业自己构建，也可采用第三方云设施，但基本趋势是企业将逐步采取租用IT 资源的方式来实现业务需要，如同水力、电力资源一样，计算、存储、网络将成为企业IT 运行的一种被使用的资源，无需自己建设，可按需获得。从企业角度，云计算解决了IT 资源的动态需求和最终成本问题，使得IT 部门可以专注于服务的提供和业务运营。 这三个阶段中，大集中与容灾是面向数据中心物理组件和业务模块，虚拟化是面向数据中心的计算与存储资源，云计算最终面向IT 服务。这样一个演进过程，表现出IT 运营模式的逐步改变，而云计算则最终根本改变了传统IT 的服务结构，它剥离了IT 系统中与企业核心业务无关的因素(如IT 基础设施)，将IT 与核心业务完全融合，使企业IT 服务能力与自身业务的变化相适应。在技术变革不断发生的过程中，网络逐步从基本互联网功能转换到WEB 服务时代(典型的WEB2.0 时代)，IT 也由企业网络互通性转换到提供信息架构全面支撑企业核心业务。技术驱动力也为云计算提供了实现的客观条件，如图2 所示，在关键领域云计算技术已经就绪： 标准化：公共技术的长期发展，使得基础组件的标准化非常完善，硬件层面的互通已经没有阻碍（即使是非常封闭的大型机目前也开始支持对外直接出IP 接口），大规模运营的云计算能够极大降低单位建设成本。 虚拟化与自动化：虚拟化技术不断纵深发展，IT 资源已经可以通过自动化的架构提供全局动态调度能力，自动化提升了IT 架构的伸缩性和扩展性。 并行/分布式架构：大规模的计算与数据处理系统已经在分布式、并行处理的架构上得到广泛应用，计算密集、数据密集、大型数据文件系统成为云计算的实现基础，从而要求整个基础架构具有更高的弹性与扩展性。 带宽：大规模的数据交换需要超高带宽的支撑，网络平台在40G/100G 能力下可具备更扁平化的结构，使得云计算的信息交互以最短快速路径执行。 因此，从传统WEB 服务向云计算服务发展已经具备技术基础，而企业的IT 从信息架构演进到弹性的IT 服务也成为必然。]]></content>
      <categories>
        <category>云计算</category>
      </categories>
      <tags>
        <tag>流程开发</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[云计算-基础概念]]></title>
    <url>%2F0003.%E5%BA%94%E7%94%A8-01.cloud_computing%2F%E4%BA%91%E8%AE%A1%E7%AE%97-%E6%A6%82%E5%BF%B5-%E5%B8%B8%E8%A7%81%E6%A6%82%E5%BF%B5%E6%80%BB%E8%A7%88%2F</url>
    <content type="text"><![CDATA[服务器相关ECS (Elastic Compute Service, ECS) 云服务器云服务器(Elastic Compute Service, ECS)是一种简单高效、安全可靠、处理能力可弹性伸缩的计算服务。其管理方式比物理服务器更简单高效。用户无需提前购买硬件，即可迅速创建或释放任意多台云服务器。 云服务器是云计算服务的重要组成部分，是面向各类互联网用户提供综合业务能力的服务平台。平台整合了传统意义上的互联网应用三大核心要素：计算、存储、网络，面向用户提供公用化的互联网基础设施服务。 VPS (Virtual Private Server) 虚拟专用服务器是将一台服务器分割成多个虚拟专享服务器的优质服务。实现VPS的技术分为容器技术，和虚拟化技术。在容器或虚拟机中，每个VPS都可选配独立公网IP地址、独立操作系统、实现不同VPS间磁盘空间、内存、CPU资源、进程和系统配置的隔离，为用户和应用程序模拟出“独占”使用计算资源的体验。VPS可以像独立服务器一样，重装操作系统，安装程序，单独重启服务器。 IDC (Internet Data Center) 互联网数据中心IDC（Internet Data Center）互联网数据中心，是集中计算、存放数据的地方。是一个集中式收集、存储、处理和发送数据的设备提供运行维护的设施以及相关的服务体系。其实就是大型机房。IDC提供的主要业务包括主机托管(机位、机架、VIP机房出租)、资源出租(如虚拟主机业务、数据存储服务)、系统维护(系统配置、数据备份、故障排除服务)、管理服务(如带宽管理、流量分析、负载均衡、入侵检测、系统漏洞诊断)，以及其他支撑、运行服务等。 对于IDC的概念，目前还没有一个统一的标准，但从概念上可以将其理解为公共的商业化的Internet“机房”，同时它也是一种IT专业服务，是IT工业的重要基础设施。IDC不仅是一个服务概念，而且是一个网络的概念，它构成了网络基础资源的一部分，就像骨干网、接入网一样，提供了一种高端的数据传输(DataDelivery)的服务和高速接入服务。 资源扩展 水平扩展 (scale out)，针对于实例数目的增减。 垂直扩展 (scal up)，即单个实例可以使用的资源的增减, 比如增加cpu和增大内存。 网络相关VPC (Virtual Private Cloud) 虚拟私有云从服务的角度来看，VPC指的是一种云（Cloud），这与它的字面意思相符。对于基础架构服务（IaaS），云就是指资源池。你或许听过公有云（Public Cloud），私有云（Private Cloud），混合云（Hybrid Cloud）。不过，VPC不属于这三种云中任一种。这是一种运行在公有云上，将一部分公有云资源为某个用户隔离出来，给这个用户私有使用的资源的集合。 OSI（Open System Interconnection）七层网络模型参考模型是国际标准化组织（ISO）制定的一个用于计算机或通信系统间互联的标准体系，一般称为OSI参考模型或七层模型。它是一个七层的、抽象的模型体，不仅包括一系列抽象的术语或概念，也包括具体的协议。 SLB（Server Load Balance） 负载均衡 应用型负载均衡ALB（Application Load Balancer）：专门面向七层，提供超强的业务处理性能，例如HTTPS卸载能力。单实例每秒查询数QPS（Query Per Second）可达100万次。同时ALB提供基于内容的高级路由特性，例如基于HTTP报头、Cookie和查询字符串进行转发、重定向和重写等，是阿里云官方云原生Ingress网关。更多信息，请参见什么是应用型负载均衡ALB。 网络型负载均衡NLB（Network Load Balancer）：面向万物互联时代推出的新一代四层负载均衡，支持超高性能和自动弹性能力，单实例可以达到1亿并发连接，帮您轻松应对高并发业务。NLB面向海量终端上连、高并发消息服务、音视频传输等业务场景针对性地推出了TCPSSL卸载、新建连接限速、多端口监听等高级特性，在物联网MQTTS加密卸载、抗洪峰上联等场景为用户提供多种辅助手段，是适合IoT业务的新一代负载均衡。更多信息，请参见什么是网络型负载均衡NLB。 传统型负载均衡CLB（Classic Load Balancer）：支持TCP、UDP、HTTP和HTTPS协议，具备良好的四层处理能力，以及基础的七层处理能力。更多信息，请参见什么是传统型负载均衡CLB。 相关概念说明 术语 全称 中文 说明 负载均衡 Server Load Balancer 负载均衡服务，简称 阿里云计算提供的一种网络负载均衡服务，可以结合阿里云提供的负载均衡服务.ECS服务为用户提供基于ECS实例的TCP、UDP与HTTP负载均衡服务。 Region Region 地域 代表资源所在并有效的地域，每个地域包含一组数据中心。 Zone Zone 可用区 代表负载均衡所在的Zone LoadBalancer Load Balancer 简称负载均衡实例 负载均衡实例可以理解为负载均衡服务的一个运行实例，用户要使用负载均衡服务，就必须先创建一个负载均衡实例，LoadBalancerld是识别用户负载均衡实例的唯一标识。 Listener Listener 负载均衡服务监听 负载均衡服务监听，包括监听端口、负载均衡策略和健康检查配置等。 BackendServer Backend Server 后端服务器 接受负载均衡分发请求的一组云服务器，负载均衡服务将外部的访问请求按照用户设定的规侧转发到这一组后端服务器上进行处理。 Address Address 服务地址 系统分配的服务地址，当前为P地址。用户可以选择该服务地址是否对外公开，来分别创建公网和私网类型的负载均衡服务。 ### OSSRDSCDN其他CAPCAP原则又称为CAP理论，主要思想是在任何一个分布式系统中都无法同时满足CAP。 C（Consistency）：表示一致性，所有的节点同一时间看到的是相同的数据。 A（Avaliablity）：表示可用性，不管是否成功，确保一个请求都能接收到响应。 P（Partion Tolerance）：分区容错性，系统任意分区后，在网络故障时，仍能操作。 Region]]></content>
      <categories>
        <category>云计算</category>
      </categories>
      <tags>
        <tag>基础概念</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[专有网络（VPC：Virtual Private Cloud）介绍]]></title>
    <url>%2F0003.%E5%BA%94%E7%94%A8-01.cloud_computing%2F%E4%BA%91%E8%AE%A1%E7%AE%97-%E6%A6%82%E5%BF%B5-%E5%AD%90%E7%BD%91%E6%8E%A9%E7%A0%81%2F</url>
    <content type="text"><![CDATA[当下网络技术的发展使得全球范围内的计算机能够相互连接和通信。然而，为了实现有效的数据传输和网络管理，需要将IP地址划分为网络和主机两个部分。子网掩码应运而生，它是一种用于确定IP地址的网络和主机部分的掩码。本文将介绍子网掩码的背景、概念、作用、规范和使用示例。 网络（Network）：网络是指由一组连接在一起的计算机和其他设备组成的通信系统。它可以是局域网（LAN）、广域网（WAN）或互联网（Internet）等。网络提供了连接和通信的基础设施，允许计算机和设备之间进行数据传输、资源共享和通信。 主机（Host）：主机是指连接到网络并具有自己唯一标识（如IP地址）的计算机或网络设备。主机可以是个人计算机、服务器、路由器、交换机、物联网设备等。它们在网络中扮演着发送和接收数据的角色，可以是数据的源或目的地。 在IP网络中，主机通常被视为具有唯一IP地址的终端设备，而网络则是这些主机之间的连接和通信基础设施。IP地址由网络部分和主机部分组成，通过子网掩码将其划分为网络地址和主机地址，以便确定数据在网络中的传输路径和目的地。 背景在网络通信中，IP地址是唯一标识网络上的设备的方式。然而，一个IP地址本身并不能提供足够的信息来确定网络和主机的边界。为了解决这个问题，需要引入子网掩码。 概念子网掩码是一个32位的二进制数，与IP地址进行逻辑与（AND）操作，用于划分IP地址的网络和主机部分。子网掩码中的1表示网络位，0表示主机位。通过确定哪些位是网络位和主机位，子网掩码定义了网络的规模和主机的容量。 作用子网掩码的主要作用是确定IP地址的网络地址和主机地址。通过与IP地址进行逻辑与操作，子网掩码将网络位设为1，主机位设为0，从而确定IP地址所属的网络地址。同时，子网掩码将网络位设为0，主机位保留不变，从而确定IP地址所属的主机地址。 此外，子网掩码还可以划分子网，将一个网络进一步划分为多个子网，以满足不同子网的需求。它还支持网络管理和路由的实施，提供了一种有效的方式来控制网络中不同子网的访问和通信。 规范子网掩码通常与IPv4地址一起使用。IPv4地址由32位二进制数组成，用四个十进制数表示，如192.168.0.1。子网掩码也是32位的二进制数，由四个八进制数（也可表示为十进制数或十六进制数）表示，如255.255.255.0。 使用示例假设有一个IP地址为192.168.0.1，子网掩码为255.255.255.0的网络。子网掩码中前24位（即前三个八进制数）均为1，表示前24位是网络位，后8位是主机位。因此，这个网络可以容纳256个主机（2^8-2，减去网络地址和广播地址）。 通过子网划分，可以将一个网络划分为多个子网，每个子网可以有不同的IP地址范围和主机容量。这样可以更好地管理网络资源和控制网络通信。 总结子网掩码是一种用于确定IP地址的网络和主机部分的掩码。它通过与IP地址进行逻辑与操作，确定网络地址和主机地址，实现网络划分和管理。子网掩码的作用包括确定网络地址、确定主机地址、划分子网和支持网络管理与路由。在IPv4中，子网掩码与IP地址一起使用，由32位二进制数表示。通过子网划分，可以将一个网络划分为多个子网，每个子网具有不同的IP地址范围和主机容量。了解和正确使用子网掩码对于构建有效的网络架构和实现网络通信至关重要。]]></content>
      <categories>
        <category>云计算</category>
      </categories>
      <tags>
        <tag>基础概念</tag>
        <tag>阿里云</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[云计算安全相关概念]]></title>
    <url>%2F0003.%E5%BA%94%E7%94%A8-01.cloud_computing%2F%E4%BA%91%E8%AE%A1%E7%AE%97-%E6%A6%82%E5%BF%B5-%E5%AE%89%E5%85%A8%E7%9B%B8%E5%85%B3%2F</url>
    <content type="text"><![CDATA[安全防护相关概念DDos防护DDos攻击的概念分布式拒绝服务（Distributed Denial of Service，简称DDoS）将多台计算机联合起来作为攻击平台，通过远程连接，利用恶意程序对一个或多个目标发起DDoS攻击，消耗目标服务器性能或网络带宽，从而造成服务器无法正常地提供服务。 攻击原理攻击者使用一个非法账号将DDoS主控程序安装在一台计算机上，并在网络上的多台计算机上安装代理程序。在所设定的时间内，主控程序与大量代理程序进行通讯，代理程序收到指令时对目标发动攻击，主控程序甚至能在几秒钟内激活成百上千次代理程序的运行。 常见的DDoS攻击类型 畸形报文 畸形报文攻击指通过向目标系统发送有缺陷的IP报文，使得目标系统在处理这样的报文时出现崩溃，从而达到拒绝服务的攻击目的。 传输层DDoS攻击 以Syn Flood攻击为例，它利用了TCP协议的三次握手机制，当服务端接收到一个Syn请求时，服务端必须使用一个监听队列将该连接保存一定时间。因此，通过向服务端不停发送Syn请求，但不响应Syn+Ack报文，从而消耗服务端的资源。当监听队列被占满时，服务端将无法响应正常用户的请求，达到拒绝服务攻击的目的。 DNS DDoS攻击 以DNS Query Flood攻击为例，其本质上执行的是真实的Query请求，属于正常业务行为。但如果多台傀儡机同时发起海量的域名查询请求，服务端无法响应正常的Query请求，从而导致拒绝服务。 连接型DDoS攻击 以Slowloris攻击为例，其攻击目标是Web服务器的并发上限。当Web服务器的连接并发数达到上限后，Web服务即无法接收新的请求。Web服务接收到新的HTTP请求时，建立新的连接来处理请求，并在处理完成后关闭这个连接。如果该连接一直处于连接状态，收到新的HTTP请求时则需要建立新的连接进行处理。而当所有连接都处于连接状态时，Web将无法处理任何新的请求。 Slowloris攻击利用HTTP协议的特性来达到攻击目的。HTTP请求以\r\n\r\n标识Headers的结束，如果Web服务端只收到\r\n，则认为HTTP Headers部分没有结束，将保留该连接并等待后续的请求内容。 Web应用层DDoS攻击 通常应用层攻击完全模拟用户请求，类似于各种搜索引擎和爬虫一样，这些攻击行为和正常的业务并没有严格的边界，难以辨别。 Web服务中一些资源消耗较大的事务和页面。例如，Web应用中的分页和分表，如果控制页面的参数过大，频繁的翻页将会占用较多的Web服务资源。尤其在高并发频繁调用的情况下，类似这样的事务就成了早期CC攻击的目标。 由于现在的攻击大都是混合型的，因此模拟用户行为的频繁操作都可以被认为是CC攻击。例如，各种刷票软件对网站的访问，从某种程度上来说就是CC攻击。 CC攻击瞄准的是Web应用的后端业务，除了导致拒绝服务外，还会直接影响Web应用的功能和性能，包括Web响应时间、数据库服务、磁盘读写等。 跨站脚本 (XSS:Cross Site Scripting)### DDos基础防护每日上限5G的基础DDos共计防护 事态感知态势感知具备异常登录检测、网站后门查杀、网站后门查杀、进程异常行为、敏感文件算改、异常网络连接、Linuⅸ软件漏洞、Windows系统漏洞、Web-CMS漏洞、应急漏洞、Web漏洞扫描、主机基线、云产品基线、资产指纹、AK和账号密码泄露、数据大屏、日志检索、全量日志分析。 维护Web应用防火墙安骑士云安全中心安全管家内容安全]]></content>
      <categories>
        <category>云计算</category>
      </categories>
      <tags>
        <tag>基础概念</tag>
        <tag>阿里云</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基因分析平台 - 超大数据分析技巧]]></title>
    <url>%2F0003.%E5%BA%94%E7%94%A8-01.cloud_computing%2F%E5%9F%BA%E5%9B%A0%E5%88%86%E6%9E%90%E5%B9%B3%E5%8F%B0%20-%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[之前也一直在使用阿里云的基因分析平台，但是更多是作为解决方案的技术储备，而且其实更多的是平台迁移，数据都比较常规。而最近接手到一些样本量非常大的非常规项目（单样本有接近2T的数据），这个数据量，不管是计算资源还是存储资源，其实本地IDC都不太能支持，所以将视角转向通过基因分析平台进行实现。由于数据规模超大，也因此遇到许多新的问题，所以趁此机会介绍一下基因分析平台的使用方法和一些隐藏的坑。 基因分析平台简介适用场景只要涉及数据，没有合规性限制，上云可能都整体是一个更稳定更省心的解决方案。首先自建机房，往往在资源上需要进行过饱和的投入，以应对业务峰值和未来的业务成长场景，而这也就带来了低谷期的业务浪费。简单说，要么一直有资源浪费，要么经常资源不足。这也是本地IDC搭建在底层逻辑上无法解决的问题。 而且自建机房，除了硬件资源外，还需要机房（静电地板、空调、备用发电机、安保、网络、电力）、各类运维管理人员投入、设备的阶段性维护投入等。对于小型企业来讲，自建集群在前期需要一笔不菲的投入。针对业务量稳定的企业，使用本地自建IDC集群，对集群有更高的可控性，资源可以稳定在饱和运行的水平，本地IDC的性价比会比较高。但是现实中这类情况非常少，甚至一些稳定业务也是周期性稳定，业务低谷无法避免会存在资源的浪费。所以其实现在针对大型企业（有资源使用量的基本盘，但是依然存在顶峰波动的情况）混合云是一个比较不错的选择，业务执行逻辑优先充分利用本地资源，资源不足时，进行云商资源的弹性扩展应对峰值需求。 使用方式基因分析平台就是阿里云针对基因检测领域开发的云计算平台，面向基因检测的垂直细分领域进行了一些适配，如果大家本地使用WDL和docker的话，可以实现分析流程的无缝迁移。基因分析平台工作模式如下：基因分析平台本身采用的模式是，存算分离，按需分配资源的模式。每个任务的计算资源、存储资源、执行环境都是作为独立的资源项，进行单独的配置。每个资源项支持的上限(64核心 512G内存，512T存储)还是蛮高的。然后再任务执行阶段，想一块块积木进行组装，然后进行相关的分析。 基因分析平台，是工作空间作为一个整体管理一个独立完整的项目，以下是工作空间主要包含的几个内容，如果大家第一次接触，主要关注文件、应用、运行即可，另外的表格、模板、投递三部分属于批量任务投递的部分，可以放到后面进行了解，| 类别 | 名称 | 用途 | 含义 ||-|-|-|-|| 数据管理 | 文件 | OSS存储 | 账号可以访问的OSS存储，可以直接查看OSS的文件 || 应用配置 | 应用 | 分析流程 | 记录了流程的wdl文件和对应的说明文档、版本信息等 || 分析任务 | 运行 | 流程执行 | 启动一个分析流程，执行分析任务，查看任务进度和状态 || 数据管理 | 表格 | 表格文件 | 用户按特定表格模板上传的表格文件，用于批量进行任务投递 || 应用配置 | 模板 | 投递模板 | 基于应用和表格文件，配置好的任务投递模板，可以直接用于任务投递 || 分析任务 | 投递 | 流程投递 | 基于应用和表格，启动一批分析任务，执行分析 | 因此如果我们想使用基因分析平台进行任务分析，就需要做一些准备工作： 需要将我们要分析的数据存储到OSS上，所有输入输出数据的长期保存都依赖于OSS，分析中挂载的存储上产生的中间数据均不可见。 搭建一个分析流程，分析流程目前仅支持 WDL ，调用的分析环境（docker镜像或阿里提供的software）、计算资源（最小 1c2Gb 到最大 64c512Gb ）、分析存储（HDD和SSD）都是在WDL中指定的。 性能监控云平台 注意事项 流程部署过程中，WDL语法中所有的File类型会作为文件关联到分析容器内（/cromwell_inputs/ 目录下），所以一定要明确数据的真实类型，错误的类型使用，会导致分析过程中出现文件缺失。 由于流程运行环境依赖于容器，因此需要注意相关的容器需要开放访问权限。 wdl本身提供了scatter语法，以便进行一些遍历/并行操作，但是在基因分析平台任务监控中，只能进行2层深度的scatter解析（多深度可以进行任务的分析，但是并没有对应颗粒度的任务状态监控），所以为了更好的查看所有任务的运行情况和资源使用情况，最好控制wdl的数据深度。比如最高维度可以借助投递逻辑来实现（比如下述示例中的Sample_info层)，流程只是针对一个样本,批次多样本的投递依赖于表格和模板结合进行投递的方式实现。 1234567scatter (Sample_info in Samples_info)&#123; scatter (sample_lib in Sample_info)&#123; scatter (Fastq_info in sample_lib.fastqlist)&#123; call xxxx ; &#125; &#125;&#125; 历史的一些隐藏小坑 基因分析平台对 wdl 文件大小有一个非明文限制（100kb以下），23年2月初和阿里反馈调整过一次，目前我们没有出现由于文件大小导致的问题，但是限制可能依然存在。不过常规流程可能不会遇到文件大小瓶颈问题。 系统盘（local-disk）的存储范围 40G~500G，超出后无法启动任务，可以不配置（默认40G），如果为了规避资源浪费设置了10G资源，会导致任务无法启动。 数据盘官网提示可以挂载 16*32T，但是自定义挂载目录（例如说明文档提供的 /mnt1 、/mnt2），那么生成结果无法正常导入OSS，数据盘挂载目录只有使用 /cromwell_root 时且output中文件名不指定根目录时，生成的数据才能正常导出OSS，并被作为正常输入被下游任务调用。 如果使用根目录，会导出到OSS，但是文件路径会有问题影响下游调用。为方便理解，提供一个示例供大家参考。 12345678910111213141516task test_run &#123; input &#123; String text &#125; command &#123; echo $&#123;text&#125; &gt; "out1.txt" &#125; output &#123; File out1 = "out1.txt" &#125; runtime &#123; cpu: 1 memory: "2G" disks: "/cromwell_root 20 cloud_ssd" &#125;&#125;]]></content>
      <categories>
        <category>基因分析平台</category>
      </categories>
      <tags>
        <tag>基础概念</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[弹性伸缩 (Auto Scaling)]]></title>
    <url>%2F0003.%E5%BA%94%E7%94%A8-01.cloud_computing%2F%E4%BA%91%E8%AE%A1%E7%AE%97-%E6%A6%82%E5%BF%B5-AS-%E5%BC%B9%E6%80%A7%E4%BC%B8%E7%BC%A9%2F</url>
    <content type="text"><![CDATA[概念跟进业务需求和策略设置伸缩规则，在业务新一区一增长时自动增加ECS实例，以保证计算能力；在业务需求下降时自动是减少ECS实例以节省成本。 适用场景 无规律的业务量波动某新闻网站播出了热点新闻，访问量突增，新闻的时效性降低后，访问量回落。由于该新闻网站的业务量波动无规律，访问量突增和回落的具体时间难以预测，所以手动调整实例很难做到及时性，而且调整数量也不确定。您可以利用弹性伸缩的报警任务，由阿里云自动根据CPU使用率等衡量指标进行弹性伸缩。 有规律的业务量波动某游戏公司每天18:00业务需求急速增长进入高峰期，到22:00业务需求降低，高峰期结束。该游戏公司的业务量波动有规律，但是每天手动调整计算能力浪费人力和时间成本。您可以利用弹性伸缩的定时任务，由阿里云定时自动进行弹性伸缩。您可以设置两个定时任务，报警任务执行的伸缩规则是简单规则类型。一个定时任务用于在每天17:55自动为您增加3台实例，另一个定时任务用于在每天22:05自动为您减少3台实例。该方式可以很好地应对每天18:00~22:00高峰期的业务量，且在高峰期结束后及时释放实例，不浪费多余的实例资源和成本。 无明显的业务量波动某通信公司的业务支撑系统需要全天运作，业务量一段时间内无明显波动。如果现有计算资源突然出现故障，会导致业务受到影响，很难及时进行故障修复或者替换。您可以利用弹性伸缩的高可用优势，开启健康检查模式。阿里云会自动检查实例的健康状态，当发现存在实例不健康时，自动增加实例替换不健康的实例，确保故障的计算资源及时得到修复。而且伸缩组必须设置最小实例数，确保无论在哪种情况下，伸缩组内的实例数量都至少等于下限，确保业务可以运作。 混合型的业务场景如果某公司的业务场景比较复杂，日常业务量波动不明显，且在某个时间段内，业务量是在一定基础上波动的，您已经订购了一部分包年包月的实例，只是想针对波动的业务量合理调整实例数量。您可以手动将已订购的包年包月实例加入伸缩组，再结合弹性伸缩的报警任务，由阿里云自动根据CPU使用率等衡量指标进行弹性伸缩，更经济、稳定地管理业务的计算能力。 除手动调整实例数量和报警任务，弹性伸缩还支持定时任务、健康检查等。您可以根据业务场景灵活组合以上功能，从而在使用弹性伸缩的时候获得更丰富灵活的使用体验。 工作原理 支持的伸缩模式固定数量模式如果您创建伸缩组时设置期望实例数，伸缩组会自动将ECS实例数量维持在期望实例数 健康模式如果您在伸缩组开启健康检查功能，伸缩组会定期检查ECS实例的运行状态，如果发现一台ECS实例未处于运行中状态，则判定该ECS实例为不健康实例并移出。 定时模式您可以创建定时任务，在指定时间执行指定伸缩规则。 动态模式您可以基于云监控性能指标（例如CPU使用率）创建报警任务，当伸缩组的指标数据满足您指定的报警条件（例如伸缩组内所有ECS实例的CPU平均值大于60%）时，触发报警并执行您指定的伸缩规则。 自定义模式您可以手动进行弹性伸缩，包括手动执行伸缩规则，或者手动添加、移出或者删除已有的ECS实例。 弹性伸缩组件伸缩组概述伸缩组是弹性伸缩的核心单元，用来管理一组具有相同应用场景和相同实例类型的实例。 伸缩配置定义了弹性伸缩的ECS示例的配置信息。必须配置：付费模式（按量付费、抢占式实例）、实例和镜像（实例规格、镜像）、存储（系统盘、数据盘）、网络（公网IP、安全组）；可选配置：登录凭证（秘钥）、高级设置（资源组、实例名称、主机名、RAM角色、负载均衡权重） 伸缩触发任务用于触发伸缩规则的任务，如定时任务、云监控的报警任务。 报警任务支持：CPU,内存，系统平均负载，内外网出和入流量，TCP总连接数和已建立连接数。系统盘读和写BPS,系统盘读和写IOPS,内网网卡收包数和发包数。磁盘IOPS不是具体的确定的属性，具体的要区分具体的系统盘读和写BPS,系统盘读和写IOPS。 伸缩规则伸缩规则的作用由伸缩规则的类型来决定，可用于触发伸缩活动或者智能设置伸缩组边界值。创建好伸缩组后，您需要设置伸缩规则来实现手动或自动伸缩ECS实例或者ECI实例资源。 通过设置伸缩规则，您可以触发伸缩组的伸缩活动或者智能设置伸缩组边界值。在计算和执行过程中，伸缩规则可以根据伸缩组的最小实例数、最大实例数或者期望实例数自动调整增加或减少的ECS实例数。例如，伸缩规则中指定将伸缩组的ECS实例数调整至50台，但伸缩组最大实例数只支持45台，则整个伸缩规则会按调整至45台来计算和执行。 执行伸缩规则主要有以下几种方式： 手动执行。 通过定时任务执行。更多信息，请参见配置定时任务。 通过报警任务执行。更多信息，请参见报警任务概述。 伸缩活动在执行伸缩规则、手动添加或移出已有ECS实例时，均会触发伸缩活动实现扩张或收缩操作，用来描述伸缩组内实例的变化情况。伸缩活动的生命周期为判断伸缩组健康状态和边界条件和启动cooldown步骤间的所有活动，步骤及顺序如下： 判断伸缩组的健康状态、边界条件和ECS实例的状态、类型。 分配Activityld和执行伸缩活动。 加入ECS实例。 修改Total Capacity. 添加RDS白名单。 挂载负载均衡，将权重设为当前伸缩组中已激活的伸缩配置上指定的“负载均衡权重”。此处使用了伸缩配置上指定的负载均衡权重”。 伸缩活动完成，启动cooldown。 冷却时间自动触发的伸缩活动有冷却时间，冷却时间指伸缩组成功执行伸缩活动后的一段锁定时间。在冷却时间内，伸缩组会拒绝由报警任务触发的伸缩活动请求。但非报警任务（手动执行任务、定时任务、健康检查等）触发的伸缩活动可以立即执行，绕过冷却时间。 冷却时间规则 冷却时间不能为空，如果不配置使用默认冷却时间。 如果在伸缩活动中，多台ECS实例加入或者移出伸缩组，则从最后一台ECS实例成功加入或者移出伸缩组后，弹性伸缩服务开始计算冷却时间。如果在伸缩活动中，没有ECS实例成功加入或者移出伸缩组，则弹性伸缩服务不会开始计算冷却时间。 如果您停用再启用伸缩组，伸缩组启用后的首次伸缩活动可以立即执行，不会受冷却时间影响。当伸缩组启用后首次成功执行伸缩活动，弹性伸缩服务才开始计算冷却时间。 生命周期挂钩生命周期挂钩是一个管理伸缩组内ECS实例或ECI实例生命周期的工具。弹性伸缩会自动触发扩缩容活动，并触发生命周期挂钩使伸缩活动中的ECS实例或ECI实例处于挂起中的状态（即等待的状态），为您保留一段自定义操作的时间，直至生命周期挂钩超时结束。 最佳实践生命周期挂钩和OOS模板通过生命周期挂钩，您能够在扩缩容流程中挂起ECS实例，执行自定义操作后再使用或者释放ECS实例。运维编排服务（OOS）是阿里云提供的云上自动化运维服务，能够自动化管理和执行任务。您可以通过模板定义执行任务、执行顺序、执行输入和输出，然后执行模板完成一组运维操作。 ACP考试相关外延知识点 弹性伸缩只支持ECS的伸缩，不支持其他服务的伸缩 用户手动添加到伸缩组中的ECS实例不会停止和释放，弹性伸缩只会停止和释放自动创建的ECS. 参考资料首页&gt;弹性伸缩&gt;用户指南]]></content>
      <categories>
        <category>云计算</category>
      </categories>
      <tags>
        <tag>基础概念</tag>
        <tag>阿里云</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[云服务器(Elastic Compute Service, ECS)]]></title>
    <url>%2F0003.%E5%BA%94%E7%94%A8-01.cloud_computing%2F%E4%BA%91%E8%AE%A1%E7%AE%97-%E5%9F%BA%E5%9B%A0%E5%88%86%E6%9E%90%E5%B9%B3%E5%8F%B0%2F</url>
    <content type="text"><![CDATA[基因分析平台部署过程可能的坑docker镜像基因分析平台默认使用 wdl版本基因分析平台默认支持 v1.0 的wdl语法， runtime的个性化配置基因分析平台上有一些独立的资源配置例如 ：disks 默认是40G，instanceType 可以直接指定节点类型（而不是内存和cpu数）software 可以调用一些软件模块 数据访问类型 oss路径中，除了开始的 oss:// 外，后续目录在进行拼接时，需要规避 // 的出现，基因分析平台的数据访问都是基于oss挂载到容器中进行访问，oss中不支持后续路径中 //的存在。 所有oss中的文件，以文件名结尾，但是所有的目录必须以 / 结尾。 以 / 结尾在分析中会将整个目录挂载到镜像中进行分析，]]></content>
      <categories>
        <category>云计算</category>
        <category>pipeline</category>
        <category>framework</category>
      </categories>
      <tags>
        <tag>流程开发</tag>
        <tag>阿里云</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[负载均衡(SLB:Server Load Balance) 介绍]]></title>
    <url>%2F0003.%E5%BA%94%E7%94%A8-01.cloud_computing%2F%E4%BA%91%E8%AE%A1%E7%AE%97-%E6%A6%82%E5%BF%B5-SLB-%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1%2F</url>
    <content type="text"><![CDATA[SLB（Server Load Balance）服务通过设置虚拟服务地址（IP），将位于同一地域（Region）的多台云服务器（Elastic Compute Service，简称ECS）资源虚拟成一个高性能、高可用的应用服务池；再根据应用指定的方式，将来自客户端的网络请求分发到云服务器池中。负载均衡（Server Load Balancer，下文简称 SLB）的引入，可以降低单台云服务器 ECS（下文简称 ECS）出现异常时对业务的冲击，提升业务的可用性。同时，结合弹性伸缩服务，通过动态调整后端服务器，可以快速对业务进行弹性调整（扩容或缩容），以快速应对业务的发展。 SLB 组成SLB由 LoadBalancer、Listener、Backend Server 三个部分组成。 负载均衡实例性能保障型实例的三个关键指标： 最大连接数-Max Connection最大连接数定义了一个负载均衡实例能够承载的最大连接数量。当实例上的连接超过规格定义的最大连接数时，新建连接请求将被丢弃。 每秒新建连接数-Connection Per Second（CPS）每秒新建连接数定义了新建连接的速率。当新建连接的速率超过规格定义的每秒新建连接数时，新建连接请求将被丢弃。 每秒查询数-Query Per Second（QPS）每秒请求数是七层监听特有的概念，指的是每秒可以完成的HTTP或HTTPS的查询（请求）的数量。当请求速率超过规格所定义的每秒查询数时，新建连接请求将被丢弃。 SLB架构整个SLB架构分成三种：四层负载均衡，七层负载均衡 和 控制系统（用于 配置和监控 负载均衡系统），如下图所示； 四层负载均衡（传输层四层负载均衡，采用开源软件LVS（linux virtual server），并根据云计算需求对其进行了定制化。适合无状态的 TCP建议的应用场景注重可靠性，对数据准确性要求高，速度可以相对较慢的场景；示例：文件传输、发送或接收邮件、远程登录、无特殊要求的 Web 应用 特性 面向连接的协议； 在正式收发数据前，必须和对方建立可靠的连接； 基于源地址会话保持，在网络层可直接看到来源地址； 监听支持 TCP 和 HTTP 两种方式进行健康检查； 转发路径短，所以性能比 7 层更好，数据传输速度更快 UDP建议的应用场景 关注实时性而相对不注重可靠性的场景； 示例：视频聊天、金融实时行情推送特性 面向非连接的协议； 在数据发送前不与对方进行三次握手，直接进行数据包发送，不提供差错恢复和数据重传； 可靠性相对低；数据传输快 通过UDP报文探测来获取状态信息。 实现 LVS（Linux Virtual Server）：即Linux虚拟服务器 基于NAT的LVS模式负载均衡 基于TUN的LVS负载均衡 基于DR的LVS负载均衡 七层负载均衡（应用层）七层负载均衡，采用开源软件Tengine HTTP建议的应用场景 需要对数据内容进行识别的应用，如 Web 应用、小的手机游戏等 特性 应用层协议，主要解决如何包装数据； 基于 Cookie 会话保持；使用 X-Forward-For 获取源地址； 监听只支持 HTTP 方式健康检查 HTTPs建议的应用场景 有更高的安全性需求，需要加密传输的应用特性 加密传输数据，可以阻止未经授权的访问； 统一的证书管理服务，用户可以将证书上传到负载均衡，解密操作直接在负载均衡上完成 实现 通过Tengine实现 会话保持四层TCP同一P地址的请求持续发往一台服务器 七层HTTP相同cookiel同的请求发往一台服务器 负载均衡的调度算法1.轮询调度轮询调度（Round Robin 简称’RR’）算法就是按依次循环的方式将请求调度到不同的服务器上，该算法最大的特点就是实现简单。轮询算法假设所有的服务器处理请求的能力都一样的，调度器会将所有的请求平均分配给每个真实服务器。 2.加权轮询调度加权轮询（Weight Round Robin 简称’WRR’）算法主要是对轮询算法的一种优化与补充，LVS会考虑每台服务器的性能，并给每台服务器添加一个权值，如果服务器A的权值为1，服务器B的权值为2，则调度器调度到服务器B的请求会是服务器A的两倍。权值越高的服务器，处理的请求越多。 3.最小连接调度最小连接调度（Least Connections 简称’LC’）算法是把新的连接请求分配到当前连接数最小的服务器。最小连接调度是一种动态的调度算法，它通过服务器当前活跃的连接数来估计服务器的情况。调度器需要记录各个服务器已建立连接的数目，当一个请求被调度到某台服务器，其连接数加1；当连接中断或者超时，其连接数减1。 （集群系统的真实服务器具有相近的系统性能，采用最小连接调度算法可以比较好地均衡负载。) 4.加权最小连接调度加权最少连接（Weight Least Connections 简称’WLC’）算法是最小连接调度的超集，各个服务器相应的权值表示其处理性能。服务器的缺省权值为1，系统管理员可以动态地设置服务器的权值。加权最小连接调度在调度新连接时尽可能使服务器的已建立连接数和其权值成比例。调度器可以自动问询真实服务器的负载情况，并动态地调整其权值。 5.基于局部的最少连接基于局部的最少连接调度（Locality-Based Least Connections 简称’LBLC’）算法是针对请求报文的目标IP地址的 负载均衡调度，目前主要用于Cache集群系统，因为在Cache集群客户请求报文的目标IP地址是变化的。这里假设任何后端服务器都可以处理任一请求，算法的设计目标是在服务器的负载基本平衡情况下，将相同目标IP地址的请求调度到同一台服务器，来提高各台服务器的访问局部性和Cache命中率，从而提升整个集群系统的处理能力。LBLC调度算法先根据请求的目标IP地址找出该目标IP地址最近使用的服务器，若该服务器是可用的且没有超载，将请求发送到该服务器；若服务器不存在，或者该服务器超载且有服务器处于一半的工作负载，则使用’最少连接’的原则选出一个可用的服务器，将请求发送到服务器。 6.带复制的基于局部性的最少连接带复制的基于局部性的最少连接（Locality-Based Least Connections with Replication 简称’LBLCR’）算法也是针对目标IP地址的负载均衡，目前主要用于Cache集群系统，它与LBLC算法不同之处是它要维护从一个目标IP地址到一组服务器的映射，而LBLC算法维护从一个目标IP地址到一台服务器的映射。按’最小连接’原则从该服务器组中选出一一台服务器，若服务器没有超载，将请求发送到该服务器；若服务器超载，则按’最小连接’原则从整个集群中选出一台服务器，将该服务器加入到这个服务器组中，将请求发送到该服务器。同时，当该服务器组有一段时间没有被修改，将最忙的服务器从服务器组中删除，以降低复制的程度。 7.目标地址散列调度目标地址散列调度（Destination Hashing 简称’DH’）算法先根据请求的目标IP地址，作为散列键（Hash Key）从静态分配的散列表找出对应的服务器，若该服务器是可用的且并未超载，将请求发送到该服务器，否则返回空。 8.源地址散列调度U源地址散列调度（Source Hashing 简称’SH’）算法先根据请求的源IP地址，作为散列键（Hash Key）从静态分配的散列表找出对应的服务器，若该服务器是可用的且并未超载，将请求发送到该服务器，否则返回空。它采用的散列函数与目标地址散列调度算法的相同，它的算法流程与目标地址散列调度算法的基本相似。 9.最短的期望的延迟最短的期望的延迟调度（Shortest Expected Delay 简称’SED’）算法基于WLC算法。举个例子吧，ABC三台服务器的权重分别为1、2、3 。那么如果使用WLC算法的话一个新请求进入时它可能会分给ABC中的任意一个。使用SED算法后会进行一个运算 A：（1+1）/1=2 B：（1+2）/2=3/2 C：（1+3）/3=4/3 就把请求交给得出运算结果最小的服务器。 10.最少队列调度最少队列调度（Never Queue 简称’NQ’）算法，无需队列。如果有realserver的连接数等于0就直接分配过去，不需要在进行SED运算。 ACP考试相关外延知识点 后端服务器的健康状况为normal,abnormal和unavailable三种。其中unavailable表示这个负载均衡实例没有配置健康检查，无法获取后端服务器的健康状况。 负载均衡目前仅支持阿里云云服务器（ECS）实例。 开启负载均衡后，请求分布不均匀的可能原因 总体的请求数较少。 后端服务器组中ECS实例的权重不一致。 后端服务器的健康状态异常。 负载均衡SLB实例开启了会话保持功能。 后端服务器组中仅部分ECS实例开启了TCP的Keepalive保持长连接特性。 四层网络是基于 源IP实现， 7层网络是基于Cookie实现的 健康检查过程中，4层服务健康检查基于端口，七层服务检查是基于返回的健康码。 共享实例带宽, 如果给某个监听设置带宽峰值，则会再总带宽中剥离出对应的贷款作为该监控的独享（其他监听任务无法使用）]]></content>
      <categories>
        <category>云计算</category>
      </categories>
      <tags>
        <tag>基础概念</tag>
        <tag>阿里云</tag>
        <tag>负载均衡</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[software-ossutil-阿里云数据传输]]></title>
    <url>%2F0003.%E5%BA%94%E7%94%A8-03.software%2Fsoftware-%E5%AF%B9%E8%B1%A1%E5%AD%98%E5%82%A8-ossutil-%E9%98%BF%E9%87%8Capi%2F</url>
    <content type="text"><![CDATA[数据上传阿里云对象存储文档 配置12345/jdfstj1/B2C_COM_P1/Research_and_Development/Software/ossutil64 -i LTAI5tCC7sn** # 对象存储账号 -k bEawRWhj9dY**-e oss-cn-shenzhen.aliyuncs.com cp -r /share/nastj8/B2C_RD_P1/Liubo/WGS-菁良 oss://canseq-3/Pancancer688_LDT/jingliang-A_GW-WES/ 上述明文会导致整个账号存在潜在的风险,同时每次的重复输入也会导致更高的工作成本，可以使用 ossutil64 生成配置文件 123456789101112131415161718$ /jdfstj1/B2C_COM_P1/Research_and_Development/Software/ossutil64 config后通过交互作业分别输入下列相关信息生成config# 输入配置文件存储目录Please enter the config file name,the file name can include path(default /home/liubo4/.ossutilconfig, carriage return will use the default file. If you specified this option to other file, you should specify --config-file option to the file when you use other commands): */path/liubo/.aliClodu.configPlease enter language(CH/EN, default is:EN, the configuration will go into effect after the command successfully executed):Please enter endpoint: oss-cn-shenzhen-internal.aliyuncs.comPlease enter accessKeyID:LTAI5*Please enter accessKeySecret:bEawRW*Please enter stsToken: # 可不填写# 非交互生成配置文件./ossutil64 config -e oss-cn-beijing.aliyuncs.com -i LTAIbZcdVCmQ**** -k D26oqKBudxDRBg8Wuh2EWDBrM0**** -L CH -c /myconfig# 使用配置文件/jdfstj1/B2C_COM_P1/Research_and_Development/Software/ossutil64 \-c ~/.aliCloud.config -u --maxupspeed=100000 \cp -r LocalPath oss://canseq-3/CloudPath cp参数必须参数 选项名称 描述 -c，–config-file ossutil工具的配置文件路径，ossutil启动时将从配置文件读取配置。当您需要管理多个账号下的Bucket时，可以生成多个配置文件，并将其中一个指定为默认配置文件。当您需要管理其他账户下的Bucket时，请通过-c指定正确的配置文件。 -e，–endpoint 指定Bucket对应的Endpoint，当您需要管理多个地域的Bucket时，可以通过此选项指定多个Endpoint。各地域Endpoint详情请参见访问域名和数据中心。 -i，–access-key-id 指定访问OSS使用的AccessKey ID，当您需要管理多个账号下的Bucket时，可通过此选项指定对应的AccessKey ID。 -k，–access-key-secret 指定访问OSS使用的AccessKey Secret，当您需要管理多个账号下的Bucket时，可通过此选项指定对应的AccessKey Secret。 -p， –password 指定访问OSS使用的AccessKey Secret，输入该选项时会提示用户从键盘输入AccessKey Secret，ossutil工具以从键盘读取的AccessKey Secret为准，并忽略通过其他方式配置的AccessKey Secret。 –loglevel 在当前工作目录下输出ossutil日志文件ossutil.log。该选项默认为空，表示不输出日志文件。 重要参数 选项名称 描述 –start-time 值为Linux或Unix系统下面的时间戳，如果输入这个选项，最后更新时间早于该时间的Object会被忽略。 –maxupspeed 最大上传速度，单位为KB/s，默认值为0（不受限制）。 –maxdownspeed 最大下载速度，单位为KB/s，默认值为0（不受限制）。 –bigfile-threshold 开启大文件断点续传的文件大小阈值，单位为Byte，默认值为100 MByte，取值范围为0~9223372036854775807。 –type 数据校验的方式。取值如下： crc64（默认值）：数据CRC64校验。 md5：数据MD5校验。 -u， –update 更新操作。 -j， –jobs 多文件操作时的并发任务数，默认值为3，取值范围为1~10000。 –output-dir 指定输出文件所在的目录，输出文件目前包含cp命令批量拷贝文件出错时所产生的report文件。默认值：当前目录下的ossutil_output目录。 –meta 设置Object的meta，格式为[header：value#header：value…]，如：Cache-Control：no-cache#Content-Encoding：gzip。 –end-time Linux或Unix系统下的时间戳。如果使用该选项，则最后更新时间晚于通过此选项指定时间的Object会被忽略。 –start-time 值为Linux或Unix系统下面的时间戳，如果输入这个选项，最后更新时间早于该时间的Object会被忽略。 –storage-class 设置Object的存储方式。取值如下：Standard（默认值）：适用于频繁的数据访问。IA：适用于较低访问频率（平均每月访问频率1到2次）的业务场景，有最低存储时间（30天）和最小计量单位（64 KB）要求。支持数据实时访问，访问数据时会产生数据取回费用。Archive：适用于数据长期保存的业务场景，有最低存储时间（60天）和最小计量单位（64 KB）要求。数据需解冻（约1分钟）后访问，解冻会产生数据取回费用。ColdArchive：适用于需要超长时间存放的极冷数据，有最低存储时间（180天）和最小计量单位（64 KB）要求。数据需解冻后访问，解冻时间根据数据大小和选择的解冻模式决定，解冻会产生数据取回费用。 –disable-all-symlink 上传时忽略所有的符号链接子文件以及符号链接子目录。 –tagging 上传或复制文件时设置文件的对象标签，格式为”abc=1&amp;bcd=2&amp;……”。 rsync12345678910111213141516171819202122232425./ossutil64 sync file_url cloud_url[-f --force][-u --update][--delete][--enable-symlink-dir][--disable-all-symlink][--disable-ignore-error][--only-current-dir][--output-dir &lt;value&gt;][--bigfile-threshold &lt;value&gt;][--part-size &lt;value&gt;][--checkpoint-dir &lt;value&gt;][--encoding-type &lt;value&gt;][--snapshot-path &lt;value&gt;][--include &lt;value&gt;][--exclude &lt;value&gt;][--meta &lt;value&gt;][--acl &lt;value&gt;][--maxupspeed &lt;value&gt;][--disable-crc64][--payer &lt;value&gt;][-j, --job &lt;value&gt;][--parallel &lt;value&gt;][--retry-times &lt;value&gt;][--tagging &lt;value&gt;] 重要参数 配置项 说明 file_url 待同步的本地文件夹路径。例如Linux系统文件路径/localfolder/，Windows系统文件路径D:\localfolder\。 cloud_url OSS文件夹路径。格式为oss://bucketname/path/。例如oss://examplebucket/exampledir/。如果输入的cloud_url没有以正斜线（/）结尾，ossutil会自动在结尾处添加一个正斜线（/）。 -u，–update 只有当目标文件不存在，或源文件的最后修改时间晚于目标文件时，ossutil才会执行同步操作。 -f –force 强制操作，不进行询问提示。 –only-current-dir 仅同步当前目录下的文件，忽略子目录及子目录下的文件。 Reference官方参数说明]]></content>
      <categories>
        <category>云计算</category>
      </categories>
      <tags>
        <tag>基础概念</tag>
        <tag>对象存储</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[云计算-基础概念-私有云公有云混合云]]></title>
    <url>%2F0003.%E5%BA%94%E7%94%A8-01.cloud_computing%2F%E4%BA%91%E8%AE%A1%E7%AE%97-%E6%A6%82%E5%BF%B5-%E7%A7%81%E6%9C%89%E4%BA%91%E5%85%AC%E6%9C%89%E4%BA%91%E6%B7%B7%E5%90%88%E4%BA%91%2F</url>
    <content type="text"><![CDATA[概念公有云概念公有云是部署云计算最常见的方式。公有云资源（如服务器和存储空间）由第三方云服务提供商拥有和运营，这些资源通过 Internet 提供。在公有云中，所有硬件、软件和其他支持性基础结构均为云提供商所拥有和管理。在公有云中，你与其他组织或云“租户”共享相同的硬件、存储和网络设备。你可以使用 Web 浏览器访问服务和管理帐户。公有云部署通常用于提供基于 Web 的电子邮件、网上办公应用、存储以及测试和开发环境。 优势 成本更低 — 无需购买硬件或软件，仅对使用的服务付费。 无需维护 — 维护由服务提供商提供。 近乎无限制的缩放性 — 提供按需资源，可满足业务需求。 高可靠性 — 具备众多服务器，确保免受故障影响。劣势 保密数据的安全性差、网络性能和匹配性问题。 但是同时，很多人担心公有云的安全性、私密性等问题。于是就有了私有云。 私有云概念私有云是云计算的另一种形式。它为一个企业或组织提供专用的云环境。私有云可以由企业或组织内部的IT团队在该组织的防火墙后面进行内部操作，因此组织可以更好地控制其计算资源。私有云主要由企业使用，因此它也被视为一种企业云。私有云可在物理上位于组织的现场数据中心，也可由第三方服务提供商托管。私有云中，服务和基础结构始终在私有网络上进行维护，硬件和软件专供组织使用。私有云可使组织更加方便地自定义资源，从而满足特定的 IT 需求。私有云的使用对象通常为政府机构、金融机构以及其他具备业务关键性运营且希望对环境拥有更大控制权的中型到大型组织。 优势 灵活性更高 — 组织可自定义云环境以满足特定业务需求。 安全性更高 — 资源不与其他组织共享，从而可实现更高控制性和安全性级别。 缩放性更高 — 私有云仍然具有公有云的缩放性和效率。劣势 比公共云的成本更昂贵。 但是私有云的费用相对较高， 并且维护成本也不低。于是有的厂商结合了公有云和私有云推出了混合云。 混合云概念混合云是一种云计算模型，它通过安全连接（如V**连接或租用线路）组合一个或多个公有云和私有云环境，从而允许在不同云环境之间共享数据和应用程序。当在私有云上运行的应用程序遇到使用高峰时，它们可以自动“突发”到公有云环境以获得额外的按需容量。这被称为“云爆发”。由于额外的需求将在公有云上，因此无需担心提前配置硬件以满足高峰需求。连接公有云和私有云有两种方法：V**和点对点专用连接。混合云通常被认为是“两全其美”，它将本地基础架构或私有云与公有云相结合，组织可利用这两者的优势。在混合云中，数据和应用程序可在私有云和公有云之间移动，从而可提供更大灵活性和更多部署选项。 优势 控制性 — 组织可针对敏感资产维持私有基础结构。 灵活性 — 需要时可利用公有云中的其他资源。 成本效益 — 具备扩展至公有云的能力，因此可仅在需要时支付额外的计算能力。 容易轻松 — 无需费时费力即可转换至云，因为可根据时间按工作负荷逐步迁移。 混合云整合了公有云和公有云的优势。它提供高可扩展性，几乎无限的存储空间，灵活的支付模式，并且与公有云一样具有成本效益。混合云也非常安全；它为您提供了更多的灵活性和对云资源的控制。 社群云社群云，也称社区云，是由几个组织共享的云端基础设施，它们支持特定的社群，有共同的关切事项，例如使命任务、安全需求、策略与法规遵循考量等。管理者可能是组织本身，也能是第三方；管理位置可能在组织内部，也可能在组织外部。 社群云在模式和机制、可靠性、安全、组织管理等方面面临挑战，有待进一步解决。社群云与私有云、公有云相比模式上复杂一些，由多个组织共同构建和共享云设施。 这四种云的主要区别就是使用的人群和使用的方式不一样： 公有云由公众开放使用 私有云由单一组织独占使用 混合云则是前述的两种以上模式的混合 社群云是由一个特定社区独占使用，该社区由具有共同关切 (如使命、安全要求、政策等) 的多个组织组成 本地化部署概念本地化部署(On-Premises)，是指运行在个人或组织所在的现实位置计算机内的高级数据处理硬件，目前大多数都是在自己的经营场地上部署并操作运行的一套系统。 优势一次购买，永久使用，且数据完全掌握在企业手里，数据信息更安全。由于是通过内联网，集成相对容易。数据在系统之间的传输会更快。 劣势企业承担硬件、软件费用和实施费用，以及后期维护费用，投入成本高，部署周期长，适合大中型企业使用。 不同方式的优缺点经济性公有云 &lt; 私有云 &lt; 本地化部署 公有云：购买服务的费用 私有云：机房、设备、运行维护费用 本地化部署：搭建、机房、设备、运行维护费用 从上面的比较我们可以明显看出，本地化部署成本是远远高于公有云和私有云服务的。 安全性公有云 &lt; 私有云 &lt; 本地化部署 公有云：通过运营商网络访问，可以通过算法对数据加密 私有云：数据由内部网络获取，第三方很难获取 本地化部署：数据储存在本地，第三方无法获取 管理性公有云：无需运营，但是灵活性有些受限 私有云：自定义程度高 本地化部署：可以本地开发 本地化部署、私有云、公有云如何选择网上比较多讨论是私有化部署和本地部署，但其实他们有没有绝对的好与坏之分，像有些企业为什么选择私有化部署？其实这不是一个技术决策，而是业务决策。具体在选择方面，可以从这几点考虑！（1）成本。私有化部署可以通过企业规模来选择，定制个性使用方案，成本较低。本地化部署则企业承担硬件、软件费用和实施费用，以及后期维护费用，成本较高。（2）功能使用不同。企业私有化部署可以根据自己的需求和情况，定制使用功能，拓展性强，在原有功能上二次开发进行自主升级，让产品更好的服务于企业。本地化部署功能定制需要专业人员进行开发，开发也需要二次付费。不过大部分企业都主要会考虑成本和使用方面，两者比较多会选择私有化部署。但企业往往也会有一些顾虑，比如担心自己的网络环境和基础设施不支持私有化部署，或者担心私有化部署无法满足自己企业的特殊需求。其实，这些担心都是多余的，私有化部署的特点就在于私有化、个性化，能够根据不同企业的需要定制不同的功能。 参考资料microsoftalibabacloud]]></content>
      <categories>
        <category>云计算</category>
      </categories>
      <tags>
        <tag>基础概念</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[边缘计算 - 概念]]></title>
    <url>%2F0003.%E5%BA%94%E7%94%A8-01.cloud_computing%2F%E8%BE%B9%E7%BC%98%E8%AE%A1%E7%AE%97-%E6%A6%82%E5%BF%B5%2F</url>
    <content type="text"><![CDATA[什么是边缘计算？边缘计算是在靠近物或数据源头的网络边缘侧，通过融合网络、计算、存储、应用核心能力的分布式开放平台，就近提供边缘智能服务。简单点讲，边缘计算是将从终端采集到的数据，直接在靠近数据产生的本地设备或网络中进行分析，无需再将数据传输至云端数据处理中心。 为什么需要边缘计算？物联网技术的快速发展，使得越来越多具备独立功能的普通物体实现互联互通，实现万物互联。得益于物联网的特征，各行各业均在利用物联网技术快速实现数字化转型，越来越多的行业终端设备通过网络联接起来。 然而，物联网作为庞大而复杂的系统，不同行业应用场景各异，据第三方分析机构统计，到2025年将有超过千亿的终端设备联网，终端数据量将达300ZB，如此大规模的数据量，按照传统数据处理方式，获取的所有数据均需上送云计算平台分析，云计算平台将面临着网络时延高、海量设备接入、海量数据处理难、带宽不够和功耗过高等高难度挑战。 为了解决传统数据处理方式下时延高、数据实时分析能力匮乏等弊端，边缘计算技术应运而生。边缘计算技术是在靠近物或数据源头的网络边缘侧，通过融合网络、计算、存储、应用核心能力的分布式开放平台，就近提供边缘智能服务。简单点讲，边缘计算是将从终端采集到的数据，直接在靠近数据产生的本地设备或网络中激进型分析，无需再将数据传输至云端数据处理中心。 边缘计算 VS 云计算边缘计算的概念是相对于云计算而言的，云计算的处理方式是将所有数据上传至计算资源集中的云端数据中心或服务器处理，任何需要访问该信息的请求都必须上送云端处理。 因此，云计算面对物联网数据量爆发的时代，弊端逐渐凸显： 云计算无法满足爆发式的海量数据处理诉求。 随着互联网与各个行业的融合，特别是在物联网技术普及后，计算需求出现爆发式增长，传统云计算架构将不能满足如此庞大的计算需求。 云计算不能满足数据实时处理的诉求。 传统云计算模式下，物联网数据被终端采集后要先传输至云计算中心，再通过集群计算后返回结果，这必然出现较长的响应时间，但一些新兴的应用场景如无人驾驶、智慧矿山等，对响应时间有极高要求，依赖云计算并不现实。 边缘计算的出现，可在一定程度上解决云计算遇到的这些问题。如下图所示，物联终端设备产生的数据不需要再传输至遥远的云数据中心处理，而是就近即在网络边缘侧完成数据分析和处理，相较于云计算更加高效和安全。 项目 边缘计算 云计算 计算方式 分布式计算，聚焦实时、短周期数据的分析 集中式计算，依赖云端数据中心 处理位置 靠近产生数据的终端设备或物联网关 云端数据中心 延时性 低延时 高延时 数据存储 只向远端传输有用的处理信息，无冗余信息 采集到的所有信息 部署成本 低 高 隐私安全 隐私性和安全性较高 隐私性和安全性相对低，需要高度关注 边缘计算是如何工作的？边缘计算架构如下图所示，尽可能靠近终端节点处处理数据，使数据、应用程序和计算能力远离集中式云计算中心。 边缘计算架构 终端节点：由各种物联网设备（如传感器、RFID标签、摄像头、智能手机等）组成，主要完成收集原始数据并上报的功能。在终端层中，只需提供各种物联网设备的感知能力，而不需要计算能力。 边缘计算节点：边缘计算节点通过合理部署和调配网络边缘侧节点的计算和存储能力，实现基础服务响应。 网络节点：负责将边缘计算节点处理后的有用数据上传至云计算节点进行分析处理。 云计算节点：边缘计算层的上报数据将在云计算节点进行永久性存储，同时边缘计算节点无法处理的分析任务和综合全局信息的处理任务仍旧需要在云计算节点完成。除此之外，云计算节点还可以根据网络资源分布动态调整边缘计算层的部署策略和算法。 边缘计算的典型应用正是基于这种更实时处理数据的能力、特性，更快的响应时间，边缘计算非常适合被应用于物联网领域，通过具有边缘计算能力的物联网关就近（网络边缘节点）提供设备管理控制等服务，解决物联网通信“最后一公里”的问题，最终实现物联网设备的智慧连接和高效管理。 边缘计算网联网架构如下图所示，它聚焦于工业物联网领域，不仅支持丰富的工业协议和物联接口，可以广泛适应不同行业设备的联接场景，而且通过开放的边缘计算能力和云管理架构，快速满足不同行业边缘智能数据处理诉求： 联接：实现海量终端设备接入物联网络，主要通过边缘计算网关支持的各种物联接口（IP化PLC/RF/RS485/RS232等）连接各种传感器和终端，实现终端设备接入。 云管理：通过物联网平台，应用云计算技术，实现边缘物联设备（如网络、设备、容器及应用）的统一云化管理，同时北向支持与其他行业应用系统灵活对接。 行业应用：物联网平台提供标准的开放接口与不同合作伙伴的行业应用系统开放对接，构建广泛的行业适应性，可开发更多契合行业场景，深度定制化物联网行业应用。]]></content>
      <categories>
        <category>边缘计算</category>
      </categories>
      <tags>
        <tag>基础概念</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[异构计算 - 概念]]></title>
    <url>%2F0003.%E5%BA%94%E7%94%A8-01.cloud_computing%2F%E5%BC%82%E6%9E%84%E8%AE%A1%E7%AE%97-%E6%A6%82%E5%BF%B5%2F</url>
    <content type="text"></content>
      <categories>
        <category>异构计算</category>
      </categories>
      <tags>
        <tag>基础概念</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[存储类型]]></title>
    <url>%2F0003.%E5%BA%94%E7%94%A8-01.cloud_computing%2F%E4%BA%91%E8%AE%A1%E7%AE%97-%E6%A6%82%E5%BF%B5-%E5%AD%98%E5%82%A8%E7%B1%BB%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[存储分类阿里云存储服务|存储类型|简介||-|-||块存储 |块存储是阿里云为云服务器ECS提供的块设备，高性能、低时延、随机读写。您可以像使用物理硬盘一样格式化并建立文件系统来使用块存储。||文件存储NAS |阿里云文件存储NAS（Network Attached Storage）是一款面向阿里云ECS实例、E-HPC和容器服务等计算节点的高可靠、高性能的分布式文件系统，可共享访问、弹性扩展。NAS基于POSIX文件接口，天然适配原生操作系统。||对象存储OSS |对象存储OSS（Object Storage Service）是一款海量、安全、低成本、高可靠的云存储服务，其容量和处理能力弹性扩展，提供多种存储类型供选择，覆盖从热到冷的各种数据存储场景，帮助您全面优化存储成本。||文件存储CPFS |文件存储CPFS（Cloud Paralleled File System）是一款并行文件系统，其数据存储在集群中的多个数据节点，多个客户端可以同时访问，满足大型高性能计算机集群的高IOPS、高吞吐、低时延的数据存储需求。||文件存储HDFS版 |文件存储HDFS版（Apsara File Storage for HDFS）是一款面向阿里云ECS实例及容器服务等计算资源的文件存储服务，满足以Hadoop为代表的分布式计算业务类型对分布式存储性能、容量和可靠性的多方面要求。||表格存储 |表格存储（Tablestore）是阿里云自研的结构化数据存储，提供海量结构化数据存储以及快速的查询和分析服务，具备PB级存储、千万TPS以及毫秒级延迟的服务能力。||云存储网关 |云存储网关（Cloud Storage Gateway）是一款可以部署在用户IDC和阿里云上的网关产品，以阿里云对象存储OSS为后端存储，为云上和云下应用提供业界标准的文件服务（NFS和SMB）和块存储服务（iSCSI）。| 对象存储OSSOSS使用基于纠删码、多副本的数据冗余存储机制，将每个对象的不同冗余存储在同一个区域内的多个设施的多个设备上。 基本概念单一账号存储空间不能超过30个，存储空间一旦成功，其名称、地域、存储类型不能修改，单个存储空间的容量无限制。存储空间Bucket权限包括：私有、公共读、公共读写 。 对象（Object） ：在IMG中，用户操作图片的基本数据单元是Object，也称为对象或文件。单个Object（每张图片）允许的大小最大不超过20 MB。频道（Channel） ：Channel是IMG上的命名空间，也是计费、权限控制、日志记录等高级功能的管理实体。IMG名称在整个图片处理服务中具有全局唯一性，且不能修改。每个用户最多可创建10个Channel，但每个Channel中存放的object的数量没有限制。样式（Style） : 图片处理服务支持您将图片的处理操作和参数保存为一个别名，即样式。您可以在一个样式中包含多个图片处理参数，快速实现复杂的图片处理操作。 存储类型 标准存储类型高持久、高可用、高性能的对象存储服务，支持频繁的数据访问。是各种社交、分享类的图片、音视频应用、大型网站、大数据分析的合适选择。 低频访问存储类型适合长期保存不经常访问的数据（平均每月访问频率1到2次）。存储单价低于标准类型，适合各类移动应用、智能设备、企业数据的长期备份，支持实时数据访问。（最少30天） 归档存储类型适合需要长期保存（建议半年以上）的归档数据，在存储周期内极少被访问，数据进入到可读取状态需要1分钟的解冻时间。适合需要长期保存的档案数据、医疗影像、科学资料、影视素材。（最少60天，使用前需要进行解冻；读取前需要一分钟的解冻时间，读取收费） 冷归档存储类型适合需要超长时间存放的极冷数据。例如因合规要求需要长期留存的数据、大数据及人工智能领域长期积累的原始数据、影视行业长期留存的媒体资源、在线教育行业的归档视频等。 日志记录服务器端加密当您在设置了服务器端加密的存储空间（Bucket）中上传文件（Object）时，OSS对收到的文件进行加密，再将得到的加密文件持久化保存。当您通过GetObject请求下载文件时，OSS自动将加密文件解密后返回给用户，并在响应头中返回x-oss-server-side-encryption，用于声明该文件进行了服务器端加密。 使用KMS托管密钥进行加解密（SSE-KMS）使用KMS托管的默认CMK（Customer Master Key）或指定CMK进行加解密操作。数据无需通过网络发送到KMS服务端进行加解密。 使用OSS完全托管密钥进行加解密（SSE-OSS）使用OSS完全托管的密钥加密每个Object。为了提升安全性，OSS还会使用定期轮转的主密钥对加密密钥本身进行加密。 延展概念存储单元对象是对象存储的基本单元，也被称为OSS文件。对象由元信息（Object Meta）、用户数据（Data）、和文件名（key）组成，对象由存储空间内部唯一的key标识，元信息是一组键值对，记录了对象的一些属性（如修改时间、大小等信息，也可以进行自定义）。 每个文件有一个Object Key（文件名），作为该文件在该存储空间中的唯一标识。OSS没有文件夹的概念，所有元素都是以文件来存储，但是您可以通过创建以正斜线（/）结尾的文件名（如folder1/folder2/file）来模拟文件夹。 图片处理功能OSS除了存储外，还提供部分图片处理功能包括： 图片缩放、添加水印 ； 图片格式转换(例如：由jpg转换成png) ； 图片360度范围内旋转。 存储挂载OSS存储可以通过使用ossfs挂载到本地系统 ACP考试相关外延知识点 除了通过PUT Object接口上传文件到OSS以外，OSS还提供了另外一种上传模式——Multipart Upload。以下情况（不仅限于此）建议使用 Multipart Upload 需要支持断点上传。 上传超过100MB大小的文件。 网络条件较差，和OSS的服务器之间的链接经常断开。 上传文件之前，无法确定上传文件的大小。 OSS收费包含三个部分，存储空间费用、流量费用和OSS API的费用， OSS API请求付费，目前仅支持按量付费。 存储费用支持包年包月 和 按量付费。 仅公网下行收费，公网上传/内网上下行不好收费。 OSS免费支持 DDos攻击、自动流量清洗和黑洞功能。 OSS 提供多种鉴权和授权机制及白名单、防盗链、主子账号功能。]]></content>
      <categories>
        <category>云计算</category>
      </categories>
      <tags>
        <tag>基础概念</tag>
        <tag>阿里云</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[专有网络（VPC：Virtual Private Cloud）介绍]]></title>
    <url>%2F0003.%E5%BA%94%E7%94%A8-01.cloud_computing%2F%E4%BA%91%E8%AE%A1%E7%AE%97-%E6%A6%82%E5%BF%B5-VPC%2F</url>
    <content type="text"><![CDATA[概念VPC不属于公有云（Public Cloud），私有云（Private Cloud），混合云（Hybrid Cloud）这三种云中任一种。这是一种运行在公有云上，将一部分公有云资源为某个用户隔离出来，给这个用户私有使用的资源的集合。 服务角度VPC是这么一种云，它由公有云管理，运行在公共资源上，但是保证每个用户之间的资源是隔离，用户在使用的时候不受其他用户的影响，感觉像是在使用自己的私有云一样 从这种意义上看，VPC不是网络，我们可以对比VPC和它一个字面上相近的概念：VPN（Virtual Private Network）。VPN在公共的网络资源上虚拟隔离出一个个用户网络，例如IPsec VPN可以是在互联网上构建连接用户私有网络的隧道，MPLS VPN更是直接在运营商的PE设备上划分隔离的VRF给不同的用户。从提供服务的角度来，说如果VPC指的只是网络的话，那它跟VPN的概念是重复的。所以，从公有云所提供的服务来说，VPC应该理解成，向用户提供的隔离资源的集合。 VPC最早是由AWS在2009年提出[1]，不过VPC的一些组成元素在其提出之前就已经存在。VPC只是将这些元素以私有云的视角重新包装了一下。在VPC之后，云主机只能使用VPC内部的对应的元素。从这个角度看，VPC更像是公有云服务商以打包的形式提供服务。 用户可以在公有云上创建一个或者多个VPC，每个部门一个VPC。对于需要连通的部门创建VPC连接。 技术角度VPC是用户专属的一个二层网络。 经典网络ACP考试相关外延知识点 VPC中使用的弹性公网IP是收费服务。 参考资料[https://zhuanlan.zhihu.com/p/33658624]]]></content>
      <categories>
        <category>云计算</category>
      </categories>
      <tags>
        <tag>基础概念</tag>
        <tag>阿里云</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[关系型数据库RDS（Relational Database Service）]]></title>
    <url>%2F0003.%E5%BA%94%E7%94%A8-01.cloud_computing%2F%E4%BA%91%E8%AE%A1%E7%AE%97-%E6%A6%82%E5%BF%B5-RDS-%E4%BA%91%E6%95%B0%E6%8D%AE%E5%BA%93%2F</url>
    <content type="text"><![CDATA[阿里云关系型数据库RDS（Relational Database Service） 是一种稳定可靠、可弹性伸缩的在线数据库服务。基于阿里云分布式文件系统和SSD盘高性能存储，RDS支持MySQL、SQL Server、PostgreSQL和MariaDB TX引擎，并且提供了容灾、备份、恢复、监控、迁移等方面的全套解决方案，帮助您解决数据库运维的烦恼。 相关概念 产品系列云数据库RDS实例包括四个系列：基础版、高可用版、集群版和三节点企业版（原金融版） 存储类型云数据库RDS提供三种存储类型：本地SSD盘、ESSD云盘和SSD云盘 规格族云数据库RDS根据CPU、内存、连接数和IOPS，提供多种实例规格族，一种实例规格族又包括多个实例规格。]]></content>
      <categories>
        <category>云计算</category>
      </categories>
      <tags>
        <tag>基础概念</tag>
        <tag>阿里云</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[网络七层模型 (OSI:Open System Interconnection）介绍]]></title>
    <url>%2F0003.%E5%BA%94%E7%94%A8-01.cloud_computing%2F%E4%BA%91%E8%AE%A1%E7%AE%97-%E6%A6%82%E5%BF%B5-OSI-%E7%BD%91%E7%BB%9C%E4%B8%83%E5%B1%82%E6%A8%A1%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[七层网络参考模型介绍各层网络概念 1. 物理层：解决两个硬件之间怎么通信的问题，常见的物理媒介有光纤、电缆、中继器等。它主要定义物理设备标准，如网线的接口类型、光纤的接口类型、各种传输介质的传输速率等。它的主要作用是传输比特流（就是由1、0转化为电流强弱来进行传输，到达目的地后在转化为1、0，也就是我们常说的数模转换与模数转换）。这一层的数据叫做比特。 2. 数据链路层：在计算机网络中由于各种干扰的存在，物理链路是不可靠的。该层的主要功能就是：通过各种控制协议，将有差错的物理信道变为无差错的、能可靠传输数据帧的数据链路。它的具体工作是接收来自物理层的位流形式的数据，并封装成帧，传送到上一层；同样，也将来自上层的数据帧，拆装为位流形式的数据转发到物理层。这一层的数据叫做帧。 3. 网络层：计算机网络中如果有多台计算机，怎么找到要发的那台？如果中间有多个节点，怎么选择路径？这就是路由要做的事。该层的主要任务就是：通过路由选择算法，为报文（该层的数据单位，由上一层数据打包而来）通过通信子网选择最适当的路径。这一层定义的是IP地址，通过IP地址寻址，所以产生了IP协议。 4. 传输层：定义传输数据的协议端口号，以及流控和差错校验，监控数据传输服务的质量，保证报文的正确传输。当发送大量数据时，很可能会出现丢包的情况，另一台电脑要告诉是否完整接收到全部的包。如果缺了，就告诉丢了哪些包，然后再发一次，直至全部接收为止。 5. 会话层：建立、管理和终止应用程序之间的通信 （在五层模型里面已经合并到了应用层）。虽然已经可以实现给正确的计算机，发送正确的封装过后的信息了。但我们总不可能每次都要调用传输层协议去打包，然后再调用IP协议去找路由，所以我们要建立一个自动收发包，自动寻址的功能(一个会话）。 6. 表示层：表示层负责数据格式的转换，将应用处理的信息转换为适合网络传输的格式，或者将来自下一层的数据转换为上层能处理的格式。（在五层模型里面已经合并到了应用层） 7. 应用层：应用层是计算机用户，以及各种应用程序和网络之间的接口，其功能是直接向用户提供服务，完成用户希望在网络上完成的各种工作。前端同学对应用层肯定是最熟悉的。 各层相关协议网络层：ICMP IGMP IP（IPV4 IPV6）传输层：TCP UDP，数据包一旦离开网卡即进入网络传输层应用层：HTTP FTP TFTP SMTP SNMP DNS TELNET HTTPS POP3 DHCP 路由器和交换机两者的区别 交换机：交换机 工作在第二层（链路层）（目前有更加高级的三层交换机，四层交换机，甚至还有七层交换机）交换机的主要功能是组织局域网，经过交换机内部处理解析信息之后，讲信息以点对点的形式发送给固定端 路由器路由器的主要功能：进行跨网段进行数据传输，路由选择最佳路径。每个路由器关联为唯一的路由表。 ex:如果你需要要多台电脑连接到一根网线，用交换机即可如果你只用一个外网IP，但是你有好多台电脑需要上网，用路由器即可 两者的原理路由器：寻址，转发（依靠 IP 地址）交换机：过滤，转发（依靠 MAC 地址） 我们可以看出这两者的主要工作就是转发数据，但是不同之处是，依靠的地址不同，这是一个根本区别！路由器内有一份路由表，里面有它的寻址信息（就像是一张地图），它收到网络层的数据报后，会根据路由表和选路算法将数据报转发到下一站（可能是路由器、交换机、目的主机） 路由器的主要功能：进行跨网段进行数据传输，路由选择最佳路径。寻址，转发（依靠 IP 地址） 交换机交换机内有一张MAC表，里面存放着和它相连的所有设备的MAC地址，它会根据收到的数据帧的首部信息内的目的MAC地址在自己的表中查找，如果有就转发，如果没有就放弃。交换机的主要功能是组织局域网，经过交换机内部处理解析信息之后，讲信息以点对点的形式发送给固定端作用主要是过滤，转发（依靠 MAC 地址） 简单理解7层模型 应用层 人做好信息，往下发 表示层 翻译一下 会话层 打包 传输层 把包发给下层 网络层 报文：给包贴个ip地址的标签 数据链路层 帧：查表ip转mac，然后转成电信号 物理层 定义好各种信号的意思，线路和插口的格式，发送吧 其他网络模型五层网络模型会话层和表示层，都整合到应用层中，从而通过剩余的5层网络进行通信。TCP/IP 层级模型结构，应用层之间的协议通过逐级调用传输层（Transport layer）、网络层（Network Layer）和物理数据链路层（Physical Data Link）而可以实现应用层的应用程序通信互联。 两种网络模型的关系 参考资料[https://www.jianshu.com/p/9b9438dff7a2][https://blog.csdn.net/yaopeng_2005/article/details/7064869][https://blog.csdn.net/superjunjin/article/details/7841099][https://blog.csdn.net/a369189453/article/details/81193661]]]></content>
      <categories>
        <category>云计算</category>
      </categories>
      <tags>
        <tag>基础概念</tag>
        <tag>阿里云</tag>
        <tag>网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[云服务器(Elastic Compute Service, ECS)]]></title>
    <url>%2F0003.%E5%BA%94%E7%94%A8-01.cloud_computing%2F%E4%BA%91%E8%AE%A1%E7%AE%97-%E6%A6%82%E5%BF%B5-ECS-%E4%BA%91%E6%9C%8D%E5%8A%A1%E5%99%A8%2F</url>
    <content type="text"><![CDATA[概念云服务器(Elastic Compute Service, ECS)是一种简单高效、安全可靠、处理能力可弹性伸缩的计算服务。其管理方式比物理服务器更简单高效。用户无需提前购买硬件，即可迅速创建或释放任意多台云服务器。 云服务器是云计算服务的重要组成部分，是面向各类互联网用户提供综合业务能力的服务平台。平台整合了传统意义上的互联网应用三大核心要素：计算、存储、网络，面向用户提供公用化的互联网基础设施服务。 实例类型BGP(Border Gateway Protocol)BGP全称是Border Gateway Protocol, 对应中文是边界网关协议。这个名字比较抽象，而维基中文的解释我觉得比较好（维基英文甚至都没有类似的解释）。BGP是互联网上一个核心的去中心化自治路由协议。从这个解释来看，首先这是一个用于互联网（Internet）上的路由协议。它的地位是核心的（目前是最重要的，互联网上唯一使用的路由协议），它的目的是去中心化，以达到各个网络自治。 参考资料 镜像和快照关系自定义镜像必须通过已创建成功的快照而进行创建，同时快照被用来创建自定义镜像后，在自定义镜像删除前，该快照不能被删除。自定义镜像适用范围：仅系统盘快照适用范围：数据盘、系统盘 不同 镜像可直接用来创建ECS实例，而快照不可以。 快照可以是ECS实例系统盘或数据盘的数据备份，而镜像一定包含ECS实例系统盘的数据。 快照只能用于当前ECS实例磁盘的数据恢复，而镜像可用于当前ECS实例及其他实例更换系统盘或创建新的ECS实例。 快照不可以跨地域使用。若您需要在其他地域恢复实例数据，可使用自定义镜像，详情请参见复制镜像。 应用场景不同。由于您只能使用自定义镜像备份数据，这里仅列举快照和自定义镜像的一些应用场景。 快照存放的位置与您自建的OSS Bucket相互独立，您无需为快照创建新的Bucket。 镜像导出时，可以指定导出到Bucket中。 镜像应用场景复制镜像可用于跨地域部署ECS实例， 例如跨地域或跨账号的ECS示例部署。因为镜像所在对象存储是不可以跨地域的，可以通过复制镜像（仅限自定义镜像，其他镜像需要先创建实例，导出为自定义镜像）到不同的地域/账户下，然后进行实例创建。 镜像类型 自定义镜像（自己上传的镜像也需要先制作成自定义镜像后才可以使用） 公共镜像 共享镜像 云市场镜像：收费镜像，弹性伸缩中不可用 块存储块存储是阿里云为云服务器ECS提供的块设备产品，具有高性能和低时延的特点，支持随机读写，满足大部分通用业务场景下的数据存储需求。您可以像使用物理硬盘一样格式化并建立文件系统来使用块存储。 计费 包年包月 按量付费 存储容量单位包SCU 按量付费+节省计划 硬盘挂载 AttachDisk一个ECS最多能挂载16块云盘作数据盘用，且云盘只能挂载到同一地域下统一可用区内的示例上。一块全新的Windows数据盘挂载到ECS实例后，还不能直接存储数据，通常您需要完成磁盘联机、新建分区、格式化等初始化操作后，才能供系统读写数据。 调用该接口时要求： 云盘的状态必须为待挂载（Available）。 挂载数据盘时： 目标ECS实例必须处于运行中（Running）或者已停止（Stopped）状态。 如果是您单独购买的云盘，计费方式必须是按量付费。 从ECS实例上卸载的系统盘作为数据盘挂载时，不限制计费方式。 挂载系统盘时： 目标ECS实例必须是卸载系统盘时的源实例。 目标ECS实例必须处于已停止（Stopped）状态。 您必须设置实例登录凭证。 查询ECS实例信息时，如果返回数据中包含{“OperationLocks”: {“LockReason” : “security”}}，则禁止一切操作。 开启多重挂载特性的云盘，只能挂载到支持NVMe协议的实例上。 挂载节点系统盘：/dev/xvda数据盘：/dev/xcdb,/dev/xcdc……/dev/xcdz 数据恢复在使用云服务器ECS的过程中，有时难免会出现数据被误删除的情况，在阿里云上恢复被误删除的数据有多种方式，比如： 通过ECS管理控制台回滚已创建的快照 通过ECS管理控制台恢复该磁盘对应的数据块副本 使用开源工具Extundelete恢复误删的数据 ACP考试相关外延知识点API相关 ECS服务的API服务地址：ecs.aliyuncs.com 可以在传入参数时，指定返回的数据格式，默认是XML格式 配置 每个ECS至少加入一个安全组，最多加入5个安全组。 Linux开启NetWorkManager（网络配置和管理的服务）服务后，会和系统内部网络服务出现冲突导致网络异常。 一个地域通常由多个可用区组成。只有同一地域内的不同可用区之间内网互通，且使用低时延链路相连。不同地域之间的可用区完全隔离。 ECS配置升级后，必须通过管理控制台重启ECS实例才能生效，ECS内重启无效；变更配置后，实例的公网和内网IP不会改变。 带宽的变配后，即时生效。 安全组配置 ECS安全组授权仅支持基于 IP、端口和安全组，不支持基于MAC地址。 应用场景多媒体、大流量的APP或网站；简单的Web应用；企业应用开发环境；访问量波动大的APP或网站；企业官网 是云服务的典型应用场景。]]></content>
      <categories>
        <category>云计算</category>
      </categories>
      <tags>
        <tag>基础概念</tag>
        <tag>阿里云</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[内容分发（Content Delivery Network，CDN）]]></title>
    <url>%2F0003.%E5%BA%94%E7%94%A8-01.cloud_computing%2F%E4%BA%91%E8%AE%A1%E7%AE%97-%E6%A6%82%E5%BF%B5-CDN-%E5%86%85%E5%AE%B9%E5%88%86%E5%8F%91%2F</url>
    <content type="text"><![CDATA[概念CDN=更智能的镜像+缓存+流量导流。 内容分发网络（英语：Content Delivery Network或Content Distribution Network，缩写：CDN）是指一种透过互联网互相连接的电脑网络系统，利用最靠近每位用户的服务器，更快、更可靠地将音乐、图片、视频、应用程序及其他文件发送给用户，来提供高性能、可扩展性及低成本的网络内容传递给用户。 (From wiki) 阿里云内容分发网络CDN（Content Delivery Network）是建立并覆盖在承载网之上，由遍布全球的边缘节点服务器群组成的分布式网络。阿里云CDN能分担源站压力，避免网络拥塞，确保在不同区域、不同场景下加速网站内容的分发，提高资源访问速度。(From aliyun) CDN分发，仅支持静态数据，不支持动态数据。 原理 当终端用户向www.aliyundoc.com下的指定资源发起请求时，首先向Local DNS（本地DNS）发起请求域名www.aliyundoc.com对应的IP。 Local DNS检查缓存中是否有www.aliyundoc.com的IP地址记录。如果有，则直接返回给终端用户；如果没有，则向网站授权DNS请求域名www.aliyundoc.com的解析记录。 当网站授权DNS解析www.aliyundoc.com后，返回域名的CNAME www.aliyundoc.com.example.com。 Local DNS向阿里云CDN的DNS调度系统请求域名www.aliyundoc.com.example.com的解析记录，阿里云CDN的DNS调度系统将为其分配最佳节点IP地址。 Local DNS获取阿里云CDN的DNS调度系统返回的最佳节点IP地址。 Local DNS将最佳节点IP地址返回给用户，用户获取到最佳节点IP地址。 用户向最佳节点IP地址发起对该资源的访问请求。 如果该最佳节点已缓存该资源，则会将请求的资源直接返回给用户（步骤8），此时请求结束。 如果该最佳节点未缓存该资源或者缓存的资源已经失效，则节点将会向源站发起对该资源的请求。获取源站资源后结合用户自定义配置的缓存策略，将资源缓存到CDN节点并返回给用户（步骤8），此时请求结束。 DNS服务要了解cdn就先要了解一下dns。当我们在浏览器中输入一个域名时，首先需要将域名转换为ip地址，再将ip地址转换为mac地址，这样才能在网络上找到该服务器。我们先不看ip转换mac地址的过程，先来看看是怎么将一个域名转换为ip的。 当我们向dns服务器发起解析域名的请求时，dns服务器首先会查询自己的缓存中有没有该域名，如果缓存中存在该域名，则可以直接返回ip地址。如果缓存中没有，服务器则会以递归的方式层层访问。例如，我们要访问www.baidu.com，首先我们会先向全球13个根服务器发起请求，询问com域名的地址，然后再向负责com域名的名称服务器发送请求，找到baidu.com，这样层层递归，最终找到我们需要的ip地址。 刷新和预热资源刷新把CDN所有节点上对应的缓存资源标记为失效，当用户再次请求时，CDN会直接回源站获取对应的资源并返回给用户，同时将资源重新缓存到CDN节点。刷新功能会降低缓存命中率。CDN刷新缓存的 方式有三种： URL刷新： 用户可以在刷新任务中提交含有正则表达式的URL，阿里云CDN会对匹配该正则表达式的所有URL进行批量刷新，这样可以更有针对性地刷新URL。 目录刷新 URL预热预热源站主动将对应的资源缓存到CDN节点，当您首次请求资源时，即可直接从CDN节点获取到最新的资源，无需再回源站获取。预热功能会提高缓存命中率。 缓存方式cdn中缓存了服务器上的部分资源。那么服务器怎么去更新cdn节点的缓存呢？这里有两种方式， 一种是服务器主动去更新缓存，cdn节点被动接受。 另一种方式是当用户请求的资源不存在时，cdn服务器向上游服务器发起请求，更新缓存，然后将数据返回给用户，这种方式是cdn服务器主动，源站服务器被动。 显然第一种方式存在很多问题，例如很容易产生404等，所以一般采用第二种缓存方式。 一些优势 体验方面，加速了网站的访问——用户与内容之间的物理距离缩短，用户的等待时间也得以缩短；分发至不同线路的缓存服务器，也让跨运营商之间的访问得以加速。 安全方面，内容进行分发后，源服务器的IP被隐藏，受到攻击的概率会大幅下降。而且，当某个服务器故障时，系统会调用临近的健康服务器，进行服务，避免对用户造成影响。 对运营商，能以存储换带宽——通过服务“下沉”，减轻上层骨干网络的流量压力，避免硬件扩容，降低网络建设成本。 内容分发网络节点会在多个地点，多个不同的网络上摆放。这些节点之间会动态的互相传输内容，对用户的下载行为优化，并借此减少内容供应者所需要的带宽成本，改善用户的下载速度，提高系统的稳定性。 内容分发网络所需要的节点数量随着需求而不同，依照所需要服务的对象大小，有可能有数万台服务器。 ACP考试相关外延知识点 CDN实际产生的流量比应用层统计到的流量高7%-15% ， 是由于会出现TCP重传，和TCP/IP包头的消耗。 背景补充上世纪80年代，互联网技术刚刚走入民用领域。人们主要通过拨号来访问网络，带宽很低，用户也很少，所以，没有对骨干网以及服务器带来压力。随着互联网的爆炸式发展，用户越来越多，加上宽带接入网的出现，内容源服务器和骨干网络的压力越来越大，无法及时响应用户的访问需求。 1995年，麻省理工学院教授、互联网的发明者之一，Tim Berners-Lee博士发现，网络拥塞越来越严重，将会成为互联网发展的最大障碍。于是，他提出一个学术难题，希望有人能发明一种全新的、从根本上解决问题的方法，来实现互联网内容的无拥塞分发。 当时Tim Berners-Lee博士的隔壁，是Tom Leighton教授的办公室。他是一位麻省理工学院应用数学教授。Tom Leighton 他被Berners-Lee的挑战激起了兴趣，于是他请研究生Danny C. Lewin和其他几位顶级研究人员一起破解这个技术难题。 最终，他们开发了利用数学运算法则来处理内容的动态路由算法技术，有效地解决了这个难题。这个技术，就是CDN。他们还为此专门成立了公司，发挥其商业价值。这个公司，就是后来鼎鼎大名的CDN服务鼻祖——Akamai公司。 参考资料阿里云官方文档内容分发网络到底什么是CDN？]]></content>
      <categories>
        <category>云计算</category>
      </categories>
      <tags>
        <tag>基础概念</tag>
        <tag>阿里云</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[WDL - 架构简介]]></title>
    <url>%2F0002.%E5%BC%80%E5%8F%91-04.pipeline%2Fpipeline-frameworks-WDL-1.%E6%9E%B6%E6%9E%84%E7%AE%80%E4%BB%8B%2F</url>
    <content type="text"><![CDATA[WDL 是 Workflow Description Language的缩写，有时也写作 Workflow Definition Language，是美国 Broad Institute 推出的工作流描述语言。从开发者及开发者赋予的名字中，我们就能看出WDL是一个面向生物信息/基因组学领域的专业的工具。 经过几年的发展，WDL 已经是生信行业广泛接受的一种工作流标准，具有下面的优势： Human-readable WDL 作为一种为工作流领域定制的语言，和 Shell、Python 等通用的脚本语言相比，没有过多复杂的概念，对使用者的计算机技能要求不高，对于生信用户容易上手。 Portable Workflow WDL 可以在多个平台执行，比如本地服务器、SGE 集群，云计算平台等，可以做到一次编写多处执行。 Standard 作为GA4GH支持的工作流描述语言之一，已经得到了众多大厂和行业协会的支持，形成了比较完善的生态。 在GATK4的best practice中，不再像以前那样给出每个步骤对应的代码，而是直接给出了官方使用的pipeline。这些pipeline采用WDL进行编写。 参考文档WDL参考文档 Broad WDL 官方论坛 WDL项目仓库 OpenWDL WDL-readthedoc 标准流程描述语言 WDL 阿里云最佳实践 在学习编写 WDL 的过程中，可以参考 Broad 官方的一些 GATK工作流 借鉴和学习 WDL 的用法。 WDL架构结构组件WDL是一种流程编写语言，没有太多复杂的逻辑和语法，入门简单。首先看一个hello world的例子 1234567891011workflow myWorkflow &#123; call myTask&#125;task myTask &#123; command &#123; echo &quot;hello world&quot; &#125; output &#123; String out = read_string(stdout()) &#125;&#125; 对于一个WDL脚本而言，有以下5个重要的核心结构 workflow：工作流定义 task：工作流包含的任务定义 call：调用或触发工作流里面的 task 执行 command：task在计算节点上要执行的命令行 runtime：task在计算节点上的运行时参数，包括 CPU、内存、docker 镜像等 output：task 或 workflow 的输出定义 1. workflow (call )每个脚本包含1个 workflow ，定义了一个可执行的流程，它由 call 调用的一系列 task 组成的。每个 task 在 workflow 代码块之外单独定义。 task可以是一个模块化的一系列命令（这个特性非常重要，特别是在容器化环境下，有机会详细说），它可以复用。由call 语句调用在workflow block里面，task本身定义在外围，它可以import,强烈建议这样书写增加可维护性，这里先不展开说了。task调用的顺序及书写顺序不决定流程执行task的顺序，但撰写过程应该尽可能保持整个流程的可读性。 2. task (command &amp; output )task是一个WDL中一个基本的任务模块，可以在不同的 workflow 中通过 call 进行调用。一个task模块至少包含两部分 (command &amp; output )。除了两个必须模块外，还有 runtime可以用于指定任务运行所需的资源、环境，还可以进行变量的定义。 task 代表任务，读取输入文件，执行相应命令，然后输出; command 中对应的就是执行的命令，比如一条具体的gatk的命令; output 指定task的输出值。 runtime task在计算节点上的运行时参数，包括 CPU、内存、docker 镜像等 可以将task理解为编程语言中的函数，每个函数读取输入的参数，执行代码，然后返回，command对应执行的具体代码，output对应返回值。在wdl中，DAG关系就是由output来确认的。 3. globglob 是指对 workflow 或 task 的输出，支持通配符匹配。123output &#123; Array[File] output_bams = glob(&quot;*.bam&quot;)&#125; 使用场景:输出文件有多个，且文件名不确定 使用方法:采用 glob 表达式，用 array 方式存储多个输出文件 价值:输出结果支持通配符匹配，简化 WDL 编写，采用数组方式，方便并发处理 4. Call cachingCall caching 是 Cromwell 的一个很有用的高级特性，通过 task 的复用，帮助客户节省时间，节省成本。 适用场景: 输入和运行环境不变的情况下，复用之前 task 的运行结果 命中条件: 输入 + 运行时参数相同 价值: 复用之前的执行结果，节省时间，节省成本 5.批量计算 runtime用于配置任务运行时的相关参数。使用批量计算作为后端时，主要的 runtime 参数有：1234567891011121314151617181920212223242526cluster: 计算集群环境 支持serverless 模式和固定集群模式mounts: 挂载设置 支持 OSS 和 NASdocker: 容器镜像地址 支持容器镜像服务simg: 容器镜像文件 支持singularity 镜像systemDisk: 系统盘设置 包括磁盘类型和磁盘大小dataDisk: 数据盘设置 包括磁盘类型、磁盘大小和挂载点memory: 所需的任务内存cpu: 所需的计算核心数目timeout: 作业超时时间maxRetries: 指令允许定义在发生故障时可以重新提交流程实例的最大次数。 具体的参数解释及填写方法，请参考 Cromwell 官方文档 除此之外，还有一些其他的概念 runtime parameter_meta meta从官方版本45开始，Cromwell 使用批量计算作为后端，支持 glob 和 Call caching 两个高级特性。 6. 变量的定义对应WDL的结构，变量也分为两层，task层和workflow层。最基本的变量有两个，一种是 File，对应文件，一种是String对应的是字符。task层的变量可以引用workflow层的变量，也可以直接传参。下面我举个例子来说明一下： 示例1234567891011121314151617181920212223242526272829workflow helloHaplotypeCaller &#123; File Ref String Sample call haplotypeCaller&#123; input: RefFasta=Ref, sampleName=Sample &#125;&#125;task haplotypeCaller &#123; File GATK File RefFasta File RefIndex File RefDict String sampleName File inputBAM File bamIndex command &#123; java -jar $&#123;GATK&#125; \ -T HaplotypeCaller \ -R $&#123;RefFasta&#125; \ -I $&#123;inputBAM&#125; \ -o $&#123;sampleName&#125;.raw.indels.snps.vcf &#125; output &#123; File rawVCF = &quot;$&#123;sampleName&#125;.raw.indels.snps.vcf&quot; &#125;&#125; 在workflow层，我们定义了两个变量，Ref和Sample，在call haplotypeCaller的过程中，我们用input语句将它们传给了task层的RefFasta和sampleName，这样的话，提升了传参的复用——只用给Ref和Sample两个变量传参，多个task都可以引用它们。而其它的变量则可以由输入文件一起传入。 变量的类型，主要有以下几种： String Int Float File Boolean Array[T] Map[K, V] Pair[X, Y] Object 关于每一种变量的使用，以及 WDL 的更多使用技巧，请参考官方规范文档。 以上就是关于WDL的基本架构与变量的内容，一个最基本的WDL文件就可以完成了，接来就要准备对应的输入与执行了，我们会继续介绍。 WDL架构执行逻辑task 如何组装成 workflow一个 workflow 里面包含多个 task，task 之前的串行或并行关系如何表达呢？主要有下面三种情况： Linear Chaining Multi-input / Multi-output第二种是多输入多输出的场景，一个 task 可以定义多个输入和输出，比如上面的例子，task B 有两个输出，作为 taskC 的输入。 Scatter-Gather Parallelism 第三种场景是用于 task 的并发执行。如果一个 task 有多个样本需要并发处理，可以使用数组的方式将样本传入，然后使用 scatter 并发的处理每个样本，每个执行的单元称为一个 shard。所有的 shard 执行完成，则当前 task 执行完成，所有 shard 的输出，又作为一个数组，可以传递到下一个 task 处理。 输入参数如何传入配置文件生成与填写workflow 的输入，比如基因样本的存储位置、计算软件的命令行参数、计算节点的资源配置等，可以通过 json 文件的形式来指定。使用 wdltools 工具可以根据 WDL 文件来生成输入模板：1java -jar wdltools.jar inputs myWorkflow.wdl &gt; myWorkflow_inputs.json 模板格式如下：123&#123; &quot;&lt;workflow name&gt;.&lt;task name&gt;.&lt;variable name&gt;&quot;:&quot;&lt;variable type&gt;&quot;&#125; 当然，如果工作流不是很复杂，也可以按照上面的格式手写 input 文件。下面是一个 GATK 工作流的 input 文件的片段： 示例 task定义：UnmappedBamToAlignedBam 工作流解析 整个 Workflow 由5个 task 组成 Task 之间通过 Linear Chaining 的方式组合 每个 Task 是子 Workflow，由多个 Task 组合而成。也就是说 WDL - 支持嵌套，workflow 里面的任务，既可以是一个 task，也可以是一个完整的 workflow，这个 workflow 被称为sub workflow。更多关于嵌套的用法请参考官方规范文档。 wikipedia]]></content>
      <categories>
        <category>任务调度</category>
        <category>云计算</category>
        <category>pipeline</category>
        <category>framework</category>
      </categories>
      <tags>
        <tag>流程开发</tag>
        <tag>WDL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[WDL - 开发应用环境配置]]></title>
    <url>%2F0002.%E5%BC%80%E5%8F%91-04.pipeline%2Fpipeline-frameworks-WDL-2.%E5%BC%80%E5%8F%91%E5%BA%94%E7%94%A8%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[工具插件Vim WDL 插件broadinstitute官方提供了在Vim环境下的WDL插件，提供基础的关键词、语法高亮等功能。其安装方式，可参考 https://github.com/broadinstitute/vim-wdl VScode WDL 插件VScode也提供了WDL语言的插件。用户可直接通过VScode插件管理进行安装。 WDL 语法校验工具WOMtool 可以对WDL脚本进行语法校验，其网站为 https://cromwell.readthedocs.io/en/stable/WOMtool/ 12# 通过conda安装conda install -c "bioconda/label/cf201901" womtool CWL 转化 WDL目前，除了WDL之外，也有相当一部分生物信息流程是以CWL语言编写的。用户可以采用dxCompiler将CWL脚本转化为WDL脚本。https://github.com/dnanexus/dxCompiler 调度引擎cromwell12# 通过conda安装conda install -c "bioconda/label/cf201901" cromwell widdlerwiddler GitHubWiddler is a command-line tool for executing WDL workflows on Cromwell servers.Features include: Workflow execution: Execute a workflow on a specified Cromwell server. Workflow restart: Restart a previously executed workflow. Workflow queries: Get the status, metadata, or logs for a specific workflow. Workflow result explanation: Get more detailed information on fails at the command line. Workflow monitoring: Monitor a specific workflow or set of user-specific workflows to completion. Workflow abortion: Abort a running workflow. JSON validation: Validate a JSON input file against the WDL file intended for use. 一个demo示例helloword.wdlhelloword.wdl123456789101112131415161718task echo &#123; String out command &#123; echo Hello World! &gt; $&#123;out&#125; &#125; output &#123; File outFile = "$&#123;out&#125;" &#125;&#125;workflow wf_echo &#123; call echo output &#123; echo.outFile &#125;&#125; 执行操作1234567891011# 生成配置文件（json格式）womtool inputs helloword.wdl &gt; helloword.wdl.input.json# 编辑配置文件vi helloword.wdl.input.json# &#123;# "wf_echo.echo.out": "Demo-Test"# &#125;# 运行任务cromwell run helloword.wdl -i helloword.wdl.input.json]]></content>
      <categories>
        <category>任务调度</category>
        <category>云计算</category>
        <category>pipeline</category>
        <category>framework</category>
      </categories>
      <tags>
        <tag>流程开发</tag>
        <tag>WDL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[WDL - 任务引擎]]></title>
    <url>%2F0002.%E5%BC%80%E5%8F%91-04.pipeline%2Fpipeline-frameworks-WDL-3.%E4%BB%BB%E5%8A%A1%E5%BC%95%E6%93%8E%2F</url>
    <content type="text"><![CDATA[WDL 怎么运行执行引擎 CromwellCromwell 是 Broad Institute 开发的工作流管理引擎。具有如下的优势： 支持 WDL 和 CWL 两种工作流描述语言 多平台支持，包括本地服务器、SGE集群、云计算平台等 阿里云批量计算是官方支持的云平台之一 丰富的元数据，展示工作流执行过程 支持多种高级特性，优化 workflow 的执行 语法检查工具WDL 编写完成后，在真正执行之前，我们可以使用官方工具进行语法检查：1java -jar wdltool.jar validate myWorkflow.wdl 使用 Cromwell 运行 WDL使用 Cromwell 运行 WDL 有两种模式 Run 模式用来执行单个 WDL，适用于调试初期，快速执行一个WDL。 1$ java -jar cromwell.jar run echo.wdl --inputs input.json Server 模式用下面的命令启动一个 HTTP server 1$ java -Dconfig.file=application.conf -jar cromwell.jar server 再使用 RESTful API 提交工作流到 server 执行：1$ java -jar cromwell.jar submit -t wdl -i input.json -o option.json -h http://localhost:8000 相比 Run 模式，Server 模式有以下优势： 可以并行处理多个 workflow，适用于生产环境 有 Call caching 等高级特性（下文会讲到），优化 workflow 的执行 提供丰富的 workflow metadata，来展示 workflow 的执行过程 注意：不管是使用Run 模式还是 Server模式，要使用批量计算作为后端运行 WDL，都需要对应的配置文件支持，配置文件详解请参考批量计算官方文档或Cromwell 官方文档。]]></content>
      <categories>
        <category>任务调度</category>
        <category>云计算</category>
        <category>pipeline</category>
        <category>framework</category>
      </categories>
      <tags>
        <tag>流程开发</tag>
        <tag>WDL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[WDL - 撰写语法]]></title>
    <url>%2F0002.%E5%BC%80%E5%8F%91-04.pipeline%2Fpipeline-frameworks-WDL-5.%E5%86%85%E7%BD%AE%E5%87%BD%E6%95%B0%2F</url>
    <content type="text"><![CDATA[版本和其他编程语法一样，WDL也存在不同版本间的语法差异，因此在使用WDL进行流程撰写时，需要明确参考的版本规范draft-3v1.0v1.1 截止202301，cromwell还不支持该版本。所以后文语法如无特殊标准，均基于 WDL v1.0 版本。 内置函数输入输出：stdout, stderr,read_tsv stdout()函数用于捕获command中命令生成的标准输出。 stderr()函数用于捕获command中命令生成的标准报错。 stderr比stdout更常用，更多用于捕获warning信息 信息获取类：defined, glob, basename, select_first 文件读入：read_tsv, read_json, read_lines 文件输出：write_tsv, write_lines 1234567891011121314151617# json file: person.json&#123; &quot;name&quot;:&quot;John&quot;, &quot;age&quot;:42&#125;# WDL readworkflow demo&#123; File json_file = &quot;person.json&quot; Object p = read_json(json_file) ... call record&#123; input: name = p.name, age = p.age &#125;&#125; glob：获取某一类型文件，返回文件数组 defined：判断变量是否被定义，返回布尔值True/False select_first：输入为数组，返回首个不为空的元素。很重要的函数！ size: 计算文件大小，资源动态适配中的重要函数，示例 size(Merge_Bam, &quot;GB&quot;) ceil: 向上取整数， 示例 ceil(1.2) 返回 2]]></content>
      <categories>
        <category>云计算</category>
        <category>pipeline</category>
        <category>framework</category>
      </categories>
      <tags>
        <tag>流程开发</tag>
        <tag>WDL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[WDL - 撰写语法]]></title>
    <url>%2F0002.%E5%BC%80%E5%8F%91-04.pipeline%2Fpipeline-frameworks-WDL-6.demo%E7%A4%BA%E4%BE%8B%2F</url>
    <content type="text"><![CDATA[和其他编程语法一样，WDL也存在不同版本间的语法差异，因此在使用WDL进行流程撰写时，需要明确参考的版本规范所以后文语法如无特殊标准，本文想过内容均基于 WDL v1.0 版本。 变量类型基础变量(primitive types )12345Int i = 0 # An integer valueFloat f = 27.3 # A floating point numberBoolean b = true # A boolean true/falseString s = &quot;hello, world&quot; # A string valueFile f = &quot;path/to/file&quot; # A file 复合变量(compound types)12345# In the examples below P represents any of the primitive types above, and X and Y represent any valid type (even nested compound types)Array[X] xs = [x1, x2, x3] # An array of XsMap[P,Y] p_to_y = &#123; p1: y1, p2: y2, p3: y3 &#125; # A map from Ps to YsPair[X,Y] x_and_y = (x, y) # A pair of one X and one YObject o = &#123; &quot;field1&quot;: f1, &quot;field2&quot;: f2 &#125; # Object keys are always `String`s 自定义结构（Struct Definition）struct是一种类似c的构造，它允许用户创建由先前存在的类型组成的新的复合类型。然后，可以在Task或Workflow定义中使用struct作为声明来代替任何其他常规类型。在许多情况下，结构体替代了Object类型，并允许对其成员进行适当的类型设置。 1struct SampleData&#123;&#125; 复合类型还可以在结构中使用，以便轻松地将它们封装在单个对象中。 Json Type 2 WDL Type JSON Type WDL Type object Map[String, ?] array Array[?] number Int or Float string String boolean Boolean null ??? object转换成Map存疑，测试过程中，使用object的方式可以正常，但是使用map的遍历方式失败。 示例Demo存档可以看到WDL本身提供了比较充足的变量类型，但是由于wdl本身对于文本/变量的处理函数非常匮乏，因此无法像snakemake一样，在读取数据后进行自定义的数据处理构造需要的数据结构。因此针对wdl应用到生信检测过程时，一些复杂的逻辑关系需要提前梳理形成特定的数据结构，本文记录一些学习开发阶段，接触的复杂数据结构和 WDL 解析方式，以备后用。 样本文库下机数据多层依赖关系最终生效的输入文件格式（json），通过wdl解析成object + 数组，实现多层结构实现。 单样本解析输入的文本文件.json12345678910111213&#123; "cancer":[&#123; "LibID":"1C-Lib-1", "LibData":[["FastqA1","FastqA2"],["FastqB1","FastqB2"],["FastqC1","FastqC2"],["FastqD1","FastqD2"]] &#125;,&#123; "LibID":"1C-Lib-2", "LibData":[["L2FastqA1","L2FastqA2"],["L2FastqB1","L2FastqB2"],["L2FastqC1","L2FastqC2"]] &#125;], "normal":[&#123; "LibID":"1N-Lib-1", "LibData":[["NFastqA1","NFastqA2"],["NFastqB1","NFastqB2"],["NFastqC1","NFastqC2"]] &#125;]&#125; 解析脚本.wdl123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566version 1.0workflow wf_echo &#123; input&#123; File data &#125; Object All_Sample = read_json(data) scatter ( singleLin in All_Sample.cancer)&#123; String lib = singleLin.LibID Array[Array[String]] FastqList = singleLin.LibData scatter (singleFastq in FastqList)&#123; Array[String] Fastq = singleFastq call echoa as cancer_fastq&#123; input: Fastq=Fastq &#125; &#125; call Singlelib as Cancer_lib&#123; input: lib = lib, Fastq = cancer_fastq.outputa &#125; &#125; call Singlelib as Cancer_sample&#123; input: lib = &quot;All_cancer&quot;, Fastq = Cancer_lib.Tag &#125;&#125;task Singlelib &#123; input&#123; String lib Array[String] Fastq &#125; String out = lib + &quot;.txt&quot; String sample = lib command &lt;&lt;&lt; echo ~&#123;sep=&quot;,&quot; Fastq&#125; &gt; ~&#123;sample&#125;.txt &gt;&gt;&gt; output &#123; String outFile = out String Tag = lib &#125;&#125;task echoa &#123; input&#123; Array[String] Fastq &#125; String Fastq1=Fastq[0] String Fastq2=Fastq[1] command &lt;&lt;&lt; echo ~&#123;Fastq1&#125; ~&#123;Fastq2&#125; &gt;&gt;&gt; output&#123; String outputa=Fastq1+&quot;:&quot;+Fastq2 &#125;&#125; 执行示例脚本12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970[2023-02-09 20:13:43,87] [info] BackgroundConfigAsyncJobExecutionActor [70365163wf_echo.cancer_fastq:0:1]: echo L2FastqA1 L2FastqA2[2023-02-09 20:13:43,87] [info] BackgroundConfigAsyncJobExecutionActor [3956881dwf_echo.cancer_fastq:3:1]: echo FastqD1 FastqD2[2023-02-09 20:13:43,87] [info] BackgroundConfigAsyncJobExecutionActor [70365163wf_echo.cancer_fastq:2:1]: echo L2FastqC1 L2FastqC2[2023-02-09 20:13:43,87] [info] BackgroundConfigAsyncJobExecutionActor [3956881dwf_echo.cancer_fastq:2:1]: echo FastqC1 FastqC2[2023-02-09 20:13:43,87] [info] BackgroundConfigAsyncJobExecutionActor [3956881dwf_echo.cancer_fastq:0:1]: echo FastqA1 FastqA2[2023-02-09 20:13:43,87] [info] BackgroundConfigAsyncJobExecutionActor [70365163wf_echo.cancer_fastq:1:1]: echo L2FastqB1 L2FastqB2[2023-02-09 20:13:43,87] [info] BackgroundConfigAsyncJobExecutionActor [3956881dwf_echo.cancer_fastq:1:1]: echo FastqB1 FastqB2[2023-02-09 20:13:50,36] [info] 70365163-898e-417f-b7a7-7ce6866b4dae-SubWorkflowActor-SubWorkflow-ScatterAt45_12:1:1 [70365163]: Workflow ScatterAt45_12 complete. Final Outputs:&#123; &quot;cancer_fastq.outputa&quot;: [&quot;L2FastqA1:L2FastqA2&quot;, &quot;L2FastqB1:L2FastqB2&quot;, &quot;L2FastqC1:L2FastqC2&quot;], &quot;Fastq&quot;: [[&quot;L2FastqA1&quot;, &quot;L2FastqA2&quot;], [&quot;L2FastqB1&quot;, &quot;L2FastqB2&quot;], [&quot;L2FastqC1&quot;, &quot;L2FastqC2&quot;]]&#125;[2023-02-09 20:13:50,36] [info] 3956881d-5958-469c-bd04-e4f41509fd51-SubWorkflowActor-SubWorkflow-ScatterAt45_12:0:1 [3956881d]: Workflow ScatterAt45_12 complete. Final Outputs:&#123; &quot;Fastq&quot;: [[&quot;FastqA1&quot;, &quot;FastqA2&quot;], [&quot;FastqB1&quot;, &quot;FastqB2&quot;], [&quot;FastqC1&quot;, &quot;FastqC2&quot;], [&quot;FastqD1&quot;, &quot;FastqD2&quot;]], &quot;cancer_fastq.outputa&quot;: [&quot;FastqA1:FastqA2&quot;, &quot;FastqB1:FastqB2&quot;, &quot;FastqC1:FastqC2&quot;, &quot;FastqD1:FastqD2&quot;]&#125;[2023-02-09 20:14:03,75] [info] BackgroundConfigAsyncJobExecutionActor [00e87cbbwf_echo.Cancer_lib:1:1]: echo L2FastqA1:L2FastqA2,L2FastqB1:L2FastqB2,L2FastqC1:L2FastqC2 &gt; 1C-Lib-2.txt[2023-02-09 20:14:03,75] [info] BackgroundConfigAsyncJobExecutionActor [00e87cbbwf_echo.Cancer_lib:0:1]: echo FastqA1:FastqA2,FastqB1:FastqB2,FastqC1:FastqC2,FastqD1:FastqD2 &gt; 1C-Lib-1.txt[2023-02-09 20:14:13,74] [info] BackgroundConfigAsyncJobExecutionActor [00e87cbbwf_echo.Cancer_sample:NA:1]: echo 1C-Lib-1,1C-Lib-2 &gt; All_cancer.txt[2023-02-09 20:14:18,89] [info] WorkflowExecutionActor-00e87cbb-2f53-4a0e-ae74-4ff2d8eaf88c [00e87cbb]: Workflow wf_echo complete. Final Outputs:&#123; &quot;wf_echo.cancer_fastq.outputa&quot;: [[&quot;FastqA1:FastqA2&quot;, &quot;FastqB1:FastqB2&quot;, &quot;FastqC1:FastqC2&quot;, &quot;FastqD1:FastqD2&quot;], [&quot;L2FastqA1:L2FastqA2&quot;, &quot;L2FastqB1:L2FastqB2&quot;, &quot;L2FastqC1:L2FastqC2&quot;]], &quot;wf_echo.lib&quot;: [&quot;1C-Lib-1&quot;, &quot;1C-Lib-2&quot;], &quot;wf_echo.Cancer_sample.Tag&quot;: &quot;All_cancer&quot;, &quot;wf_echo.Fastq&quot;: [[[&quot;FastqA1&quot;, &quot;FastqA2&quot;], [&quot;FastqB1&quot;, &quot;FastqB2&quot;], [&quot;FastqC1&quot;, &quot;FastqC2&quot;], [&quot;FastqD1&quot;, &quot;FastqD2&quot;]], [[&quot;L2FastqA1&quot;, &quot;L2FastqA2&quot;], [&quot;L2FastqB1&quot;, &quot;L2FastqB2&quot;], [&quot;L2FastqC1&quot;, &quot;L2FastqC2&quot;]]], &quot;wf_echo.Cancer_lib.Tag&quot;: [&quot;1C-Lib-1&quot;, &quot;1C-Lib-2&quot;], &quot;wf_echo.Cancer_lib.outFile&quot;: [&quot;1C-Lib-1.txt&quot;, &quot;1C-Lib-2.txt&quot;], &quot;wf_echo.Cancer_sample.outFile&quot;: &quot;All_cancer.txt&quot;, &quot;wf_echo.FastqList&quot;: [[[&quot;FastqA1&quot;, &quot;FastqA2&quot;], [&quot;FastqB1&quot;, &quot;FastqB2&quot;], [&quot;FastqC1&quot;, &quot;FastqC2&quot;], [&quot;FastqD1&quot;, &quot;FastqD2&quot;]], [[&quot;L2FastqA1&quot;, &quot;L2FastqA2&quot;], [&quot;L2FastqB1&quot;, &quot;L2FastqB2&quot;], [&quot;L2FastqC1&quot;, &quot;L2FastqC2&quot;]]]&#125;### 批次多样本解析wdl在解析json时，最外层总会强制解析成一个object（输入 list 也无法识别为Array进行scatter操作），所以顶层必须使用object。``` json &#123;&quot;All_Sample&quot;:[&#123; &quot;sampleID&quot;:&quot;SampleA&quot;, &quot;cancer&quot;:[&#123; &quot;LibID&quot;:&quot;1C-Lib-1&quot;, &quot;LibData&quot;:[[&quot;FastqA1&quot;,&quot;FastqA2&quot;],[&quot;FastqB1&quot;,&quot;FastqB2&quot;],[&quot;FastqC1&quot;,&quot;FastqC2&quot;],[&quot;FastqD1&quot;,&quot;FastqD2&quot;]] &#125;,&#123; &quot;LibID&quot;:&quot;1C-Lib-2&quot;, &quot;LibData&quot;:[[&quot;L2FastqA1&quot;,&quot;L2FastqA2&quot;],[&quot;L2FastqB1&quot;,&quot;L2FastqB2&quot;],[&quot;L2FastqC1&quot;,&quot;L2FastqC2&quot;]] &#125;], &quot;normal&quot;:[&#123; &quot;LibID&quot;:&quot;1N-Lib-1&quot;, &quot;LibData&quot;:[[&quot;NFastqA1&quot;,&quot;NFastqA2&quot;],[&quot;NFastqB1&quot;,&quot;NFastqB2&quot;],[&quot;NFastqC1&quot;,&quot;NFastqC2&quot;]] &#125;]&#125;,&#123; &quot;sampleID&quot;:&quot;SampleB&quot;, &quot;cancer&quot;:[&#123; &quot;LibID&quot;:&quot;1C-Lib-1&quot;, &quot;LibData&quot;:[[&quot;FastqA1&quot;,&quot;FastqA2&quot;],[&quot;FastqB1&quot;,&quot;FastqB2&quot;],[&quot;FastqC1&quot;,&quot;FastqC2&quot;],[&quot;FastqD1&quot;,&quot;FastqD2&quot;]] &#125;,&#123; &quot;LibID&quot;:&quot;1C-Lib-2&quot;, &quot;LibData&quot;:[[&quot;L2FastqA1&quot;,&quot;L2FastqA2&quot;],[&quot;L2FastqB1&quot;,&quot;L2FastqB2&quot;],[&quot;L2FastqC1&quot;,&quot;L2FastqC2&quot;]] &#125;], &quot;normal&quot;:[&#123; &quot;LibID&quot;:&quot;1N-Lib-1&quot;, &quot;LibData&quot;:[[&quot;NFastqA1&quot;,&quot;NFastqA2&quot;],[&quot;NFastqB1&quot;,&quot;NFastqB2&quot;],[&quot;NFastqC1&quot;,&quot;NFastqC2&quot;]] &#125;]&#125;]&#125; 解析脚本123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081version 1.0workflow wf_echo &#123; input&#123; File data &#125; Object All_Sample = read_json(data) scatter (singlesample in All_Sample.All_Sample)&#123; String sampleID = singlesample.sampleID scatter (singleLin in singlesample.cancer)&#123; String Type = &quot;cancer&quot; String lib = singleLin.LibID Array[Array[String]] FastqList = singleLin.LibData scatter (singleFastq in FastqList)&#123; Array[String] Fastq = singleFastq call echoa as cancer_fastq&#123; input: Fastq=Fastq &#125; &#125; call Singlelib as Cancer_lib&#123; input: sample = sampleID, Type = Type, lib = lib, Fastq = cancer_fastq.outputa &#125; &#125; call Singlelib as Cancer_sample&#123; input: sample = sampleID, Type = &quot;cancer&quot;, lib = &quot;All_lib&quot;, Fastq = Cancer_lib.Tag &#125; &#125; call Singlelib as sample_level&#123; input: sample = &quot;All_sample&quot;, Type = &quot;All_type&quot;, lib = &quot;All_case&quot;, Fastq = Cancer_sample.Tag &#125;&#125;task Singlelib &#123; input&#123; String sample String Type String lib Array[String] Fastq &#125; String out = lib + &quot;.txt&quot; command &lt;&lt;&lt; echo ~&#123;sep=&quot;,&quot; Fastq&#125; &gt; ~&#123;sample&#125;.txt &gt;&gt;&gt; output &#123; String outFile = out String Tag = lib &#125;&#125;task echoa &#123; input&#123; Array[String] Fastq &#125; String Fastq1=Fastq[0] String Fastq2=Fastq[1] command &lt;&lt;&lt; echo ~&#123;Fastq1&#125; ~&#123;Fastq2&#125; &gt;&gt;&gt; output&#123; String outputa=Fastq1+&quot;:&quot;+Fastq2 &#125;&#125;]]></content>
      <categories>
        <category>云计算</category>
        <category>pipeline</category>
        <category>framework</category>
      </categories>
      <tags>
        <tag>流程开发</tag>
        <tag>WDL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[WDL - 撰写语法]]></title>
    <url>%2F0002.%E5%BC%80%E5%8F%91-04.pipeline%2Fpipeline-frameworks-WDL-4.%E6%92%B0%E5%86%99%E8%AF%AD%E6%B3%95%2F</url>
    <content type="text"><![CDATA[版本和其他编程语法一样，WDL也存在不同版本间的语法差异，因此在使用WDL进行流程撰写时，需要明确参考的版本规范draft-3v1.0v1.1 截止202301，cromwell还不支持该版本。所以后文语法如无特殊标准，均基于 WDL v1.0 版本。 语法规则变量变量的类型，主要有以下几种：基础变量(primitive types )12345Int i = 0 # An integer valueFloat f = 27.3 # A floating point numberBoolean b = true # A boolean true/falseString s = &quot;hello, world&quot; # A string valueFile f = &quot;path/to/file&quot; # A file 复合变量(compound types)12345# In the examples below P represents any of the primitive types above, and X and Y represent any valid type (even nested compound types)Array[X] xs = [x1, x2, x3] # An array of XsMap[P,Y] p_to_y = &#123; p1: y1, p2: y2, p3: y3 &#125; # A map from Ps to YsPair[X,Y] x_and_y = (x, y) # A pair of one X and one YObject o = &#123; &quot;field1&quot;: f1, &quot;field2&quot;: f2 &#125; # Object keys are always `String`s Array LiteralsArrays values can be specified using Python-like syntax, as follows:12Array[String] a = [&quot;a&quot;, &quot;b&quot;, &quot;c&quot;]Array[Int] b = [0,1,2] Map LiteralsMaps values can be specified using a similar Python-like sytntax:12Map[Int, Int] = &#123;1: 10, 2: 11&#125;Map[String, Int] = &#123;&quot;a&quot;: 1, &quot;b&quot;: 2&#125; 自定义结构（Struct Definition）struct 是一种类似c的构造，它允许用户创建由先前存在的类型组成的新的复合类型。然后，可以在Task或Workflow定义中使用struct作为声明来代替任何其他常规类型。在许多情况下，结构体替代了Object类型，并允许对其成员进行适当的类型设置。 1struct SampleData&#123;&#125; 复合类型还可以在结构中使用，以便轻松地将它们封装在单个对象中。 变量的调用访问Objectwdl本身提供了多种读取文件信息生成Object的接口（read_tsv、read_json、read_map)等。部分示例如下：Config_File_software：123pipeline_root /home/liubo4/Project/aio.wdl/Gdc/aio.NewAnnotation/java bin/java_v1.8tabix bin/tabix 要在wdl中提取上述文件信息123456789101112workflow wf_echo &#123; input&#123; String Config_File_software Object software = read_map(Config_File_software)# 方式一：直接通过Config_File_software文件的第一列值作为属性直接提取； pipeline_root = software.pipeline_root# 方式二： 通过中间变量提取，这种方式可以高效的处理一些差异化配置需求； String tmp="pipeline_root" pipeline_root = software[tmp] &#125;&#125; 在一些复杂业务中，可以使用json提供丰富的数据结构。 遍历map的方式由于map本身的特殊性（键值对），通过scatter调用map时，会返回一个Pair类型的数据，实现key、value的获取。参考12345678910111213workflow foo &#123; Map[String, Int] map scatter (pair in map) &#123; String key = pair.left Int value = pair.right &#125; output &#123; # Automatically gathered from inside the scatter: Array[String] keys = key Array[Int] values = value &#125;&#125; 变量的可选参数Optional Parameters &amp; Type ConstraintsTypes can be optionally suffixed with a ? or + in certain cases.? means that the parameter is optional. A user does not need to specify a value for the parameter in order to satisfy all the inputs to the workflow.+ applies only to Array types and it represents a constraint that the Array value must containe one-or-more elements. 关于每一种变量的使用，以及 WDL 的更多使用技巧，请参考官方规范文档。 表达式(Expressions) LHS Type Operators RHS Type Result Semantics Boolean == Boolean Boolean Boolean != Boolean Boolean Boolean &gt; Boolean Boolean Boolean &gt;= Boolean Boolean Boolean &lt; Boolean Boolean Boolean &lt;= Boolean Boolean Boolean `\ \ ` Boolean Boolean Boolean &amp;&amp; Boolean Boolean File + File File Append file paths File == File Boolean File != File Boolean File + String File File == String Boolean File != String Boolean Float + Float Float Float - Float Float Float * Float Float Float / Float Float Float % Float Float Float == Float Boolean Float != Float Boolean Float &gt; Float Boolean Float &gt;= Float Boolean Float &lt; Float Boolean Float &lt;= Float Boolean Float + Int Float Float - Int Float Float * Int Float Float / Int Float Float % Int Float Float == Int Boolean Float != Int Boolean Float &gt; Int Boolean Float &gt;= Int Boolean Float &lt; Int Boolean Float &lt;= Int Boolean Float + String String Int + Float Float Int - Float Float Int * Float Float Int / Float Float Int % Float Float Int == Float Boolean Int != Float Boolean Int &gt; Float Boolean Int &gt;= Float Boolean Int &lt; Float Boolean Int &lt;= Float Boolean Int + Int Int Int - Int Int Int * Int Int Int / Int Int Integer division Int % Int Int Integer division, return remainder Int == Int Boolean Int != Int Boolean Int &gt; Int Boolean Int &gt;= Int Boolean Int &lt; Int Boolean Int &lt;= Int Boolean Int + String String String + Float String String + Int String String + String String String == String Boolean String != String Boolean String &gt; String Boolean String &gt;= String Boolean String &lt; String Boolean String &lt;= String Boolean - Float Float + Float Float - Int Int + Int Int ! Boolean Boolean 参考 If then else1234Int array_length = length(array)runtime &#123; memory: if array_length &gt; 100 then &quot;16GB&quot; else &quot;8GB&quot;&#125; 参考该指标，可以根据数据量动态的给每个任务分配内存。 Member Access语法 x.y 访问对象的属性. 其中 x 必须是一个对象或者是一个 workflow中的 task 。一个Task可以被视为一个对象，而Task的属性就是一个Task中的 output 。wdl123456789101112131415workflow wf &#123; input &#123; Object obj Object foo &#125; # This would cause a syntax error, # because foo is defined twice in the same namespace. call foo &#123; input: var=obj.attr # Object attribute &#125; call foo as foo2 &#123; input: var=foo.out # Task output &#125;&#125; Map and Array Indexingx[y] 用于建立索引的 maps 和arrays ； 针对maps，y必须是x中的一个key； 针对arrays, y必须是一个整数。 Pair Indexing如果 x 是一对，则其中左边和右边的元素可以使用 x.left and x.right 获取。 内置函数输入输出：stdout, stderr,read_tsv stdout()函数用于捕获command中命令生成的标准输出。 stderr()函数用于捕获command中命令生成的标准报错。 stderr比stdout更常用，更多用于捕获warning信息 信息获取类：defined, glob, basename, select_first 文件读入：read_tsv, read_json, read_lines 文件输出：write_tsv, write_lines 1234567891011121314151617# json file: person.json&#123; &quot;name&quot;:&quot;John&quot;, &quot;age&quot;:42&#125;# WDL readworkflow demo&#123; File json_file = &quot;person.json&quot; Object p = read_json(json_file) ... call record&#123; input: name = p.name, age = p.age &#125;&#125; glob：获取某一类型文件，返回文件数组 defined：判断变量是否被定义，返回布尔值True/False select_first：输入为数组，返回首个不为空的元素。很重要的函数！ 变量操作函数 prefix：为数组变量加上前缀。对于同类型的多输入文件非常重要！ sub：提供正则表达式功能（不建议在WDL中使用） 变量操作：prefix, subWorkflowTask批量计算 (runtime)用于配置任务运行时的相关参数。使用批量计算作为后端时，主要的 runtime 参数有：1234567891011121314151617181920212223242526cluster: 计算集群环境 支持serverless 模式和固定集群模式mounts: 挂载设置 支持 OSS 和 NASdocker: 容器镜像地址 支持容器镜像服务simg: 容器镜像文件 支持singularity 镜像systemDisk: 系统盘设置 包括磁盘类型和磁盘大小dataDisk: 数据盘设置 包括磁盘类型、磁盘大小和挂载点memory: 所需的任务内存cpu: 所需的计算核心数目timeout: 作业超时时间maxRetries: 指令允许定义在发生故障时可以重新提交流程实例的最大次数。 具体的参数解释及填写方法，请参考 Cromwell 官方文档 除此之外，还有一些其他的概念 runtime parameter_meta meta从官方版本45开始，Cromwell 使用批量计算作为后端，支持 glob 和 Call caching 两个高级特性。]]></content>
      <categories>
        <category>云计算</category>
        <category>pipeline</category>
        <category>framework</category>
      </categories>
      <tags>
        <tag>流程开发</tag>
        <tag>WDL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[注释软件的转录本选择]]></title>
    <url>%2F0001.%E7%9F%A5%E8%AF%86-03.standard%2FStandard-%E8%BD%AC%E5%BD%95%E6%9C%AC%E7%9A%84%E9%80%89%E6%8B%A9%2F</url>
    <content type="text"><![CDATA[简介临床检测过程中，面对的一个比较复杂的问题，就是转录本的选择。 转录本的选择转录本的选择需要结合相关预期临床用途。目前华大相关产品的转录本选择方案参考文档： 肿瘤产品转录本选择管理规程文档仍在准备受控阶段，后续确定终板需要进行更新。 转录本切换评估方案构建结构注释bed文件首先基于新的转录本，构建一个描述基因结构的bed文件。该文件应该包含所关注的转录本结构区域。 在NCBI上下载对应版本的基因结构注释结果文件*gff格式。 使用 /jdfstj1/B2C_COM_P1/Research_and_Development/Database/Transtript_choose/5.1.1NCBI/gff2bed.pl 脚本将将gff格式的结构文件转化为bed文件（NCBI.gff2bed.bed)。 使用bedtools对产品检测范围进行结构注释。需要基于捕获区间分别对新旧转录本获取该文件。 1234567891011# 对原始bed进行排序bedtools sort -i BedPrePare/Pancancer_v2.bed &gt; BedPrePare/Pancancer_v2.sort.bed# 对原始bed区域进行合并bedtools merge -i BedPrePare/Pancancer_v2.sort.bed &gt; BedPrePare/Pancancer_v2.merge.bed# 基于结构文件对原始bed区域进行注释bedtools intersect -a BedPrePare/Pancancer_v2.merge.bed -b NCBI.gff2bed.bed -wb | cut -f1-3,7-10 &gt; BedPrePare/Pancancer_v2.anno.bed# 评估相关基因的覆盖情况bedtools intersect -b BedPrePare/Pancancer_v2.merge.bed -a NCBI.gff2bed.bed -wao &gt; BedPrePare/Pancancer_v2.covercheck.bed 对现有产品的新旧转录本文件进行比较。确定差异检测范围 获得两套转录本区域（该区域指和检测范围区间交集为基础）之间的交集 1bedtools intersect -a Pancancer_v2.anno.Oldtrans.bed -b Pancancer_v2.anno.Newtrans.bed &gt; old_new.overlap.bed 获得切换转录本后减少的区域 1bedtools subtract -a Pancancer_v2.anno.Oldtrans.bed -b Pancancer_v2.anno.Newtrans.bed &gt; old.subtract.new.bed 切换转录本后增加的区域 1bedtools subtract -a Pancancer_v2.anno.Newtrans.bed -b Pancancer_v2.anno.Oldtrans.bed &gt; new.subtract.old.bed 接受替换的标准 针对就转录本进行替换时，需要额外补充如下评估内容 更换转录本后，由于转录本涵盖区间发生改变，需要将更换后，不会提报的范围反馈解读。基于临床历史检测结果的变异进行汇总统计。确认是否会影响致病变异位点的输出。 核查新旧转录本特异性区域，对应的公共数据库交集区域，确定更换转录本对公共数据库变异范围产生的影响。 针对肿瘤检测相关产品可参考数据库如下： ClinVar Cosmic 备注由于部分原始下载文件中，染色体编号常规命名和常规使用命名方式可能会存在出入； 因此在下游处理过程中需要对染色体编号进行转换，转换关系如下： NCBI获取染色体编号和NC编码之间的对应关系|Molecule name | GenBank sequence | |RefSeq sequence|Unlocalized sequences count||-|-|-|-|-||Chromosome 1|CM000663.1|=|NC_000001.10|2||Chromosome 2|CM000664.1|=|NC_000002.11|0||Chromosome 3|CM000665.1|=|NC_000003.11|0||Chromosome 4|CM000666.1|=|NC_000004.11|2||Chromosome 5|CM000667.1|=|NC_000005.9|0||Chromosome 6|CM000668.1|=|NC_000006.11|0||Chromosome 7|CM000669.1|=|NC_000007.13|1||Chromosome 8|CM000670.1|=|NC_000008.10|2||Chromosome 9|CM000671.1|=|NC_000009.11|4||Chromosome 10|CM000672.1|=|NC_000010.10|0||Chromosome 11|CM000673.1|=|NC_000011.9|1||Chromosome 12|CM000674.1|=|NC_000012.11|0||Chromosome 13|CM000675.1|=|NC_000013.10|0||Chromosome 14|CM000676.1|=|NC_000014.8|0||Chromosome 15|CM000677.1|=|NC_000015.9|0||Chromosome 16|CM000678.1|=|NC_000016.9|0||Chromosome 17|CM000679.1|=|NC_000017.10|4||Chromosome 18|CM000680.1|=|NC_000018.9|1||Chromosome 19|CM000681.1|=|NC_000019.9|2||Chromosome 20|CM000682.1|=|NC_000020.10|0||Chromosome 21|CM000683.1|=|NC_000021.8|1||Chromosome 22|CM000684.1|=|NC_000022.10|0||Chromosome X|CM000685.1|=|NC_000023.10|0||Chromosome Y|CM000686.1|=|NC_000024.9|0| 竞品公司的转录本信息世和燃石基因转录本-part1燃石基因转录本-part2]]></content>
      <categories>
        <category>NGS</category>
        <category>肿瘤检测</category>
        <category>数据</category>
      </categories>
      <tags>
        <tag>数据库</tag>
        <tag>Annotation</tag>
        <tag>BGI work</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基因注释软件-VEP]]></title>
    <url>%2F0003.%E5%BA%94%E7%94%A8-03.software%2FSoftware-%E5%8F%98%E5%BC%82%E6%B3%A8%E9%87%8A-VEP-%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[在使用VEP软件进行功能注释阶段，其实会发现存在部分注释结果和整体注释逻辑存在偏差。由于不清楚是认识的不足还是VEP本身代码存在Bug。因此对VEP的源码进行溯源核查。 相关疑似的问题 部分变异检测结果存在cHGVS，且cHGVS位于编码区，但是未输出pHGVS信息。 Chr Start End Ref Alt Gene Trans cHGVS pHGVS chr12 122064779 122064785 ACCGCCA C ORAI1 NM_032790.3 c.132_138delinsC - 从注释的氨基酸结果来看，不涉及终止密码子，但是注释得到的Function为stop_lost Chr Start End Ref Alt Gene Trans cHGVS pHGVS chr1 10342506 10342555 TCAGGTGGGCTTGACGTCTGTGACCAGTATTCAAGAGAGGATCATGTCTA CCAGGTGTAGACATGATCCTCTCTTGAATACTGGTCACAGACGTCAAGCC KIF1B NM_015074.3 c.1211_1260delinsCCAGGTGTAGACATGATCCTCTCTTGAATACTGGTCACAGACGTCAAGCC p.I404_L420delinsTRCRHDPLLNTGHRRQA 源码重点部分记录 VEP输出结果整体记录在 “ modules\Bio\EnsEMBL\VEP\OutputFactory.pm ” 中，如果是处于核查可以基于该模块进行反向溯源； 预测氨基酸变化代码在 “modules\Bio\EnsEMBL\Variation\TranscriptVariationAllele.pm” Line:684 12345678=head2 peptide Description: Return the amino acid sequence that this allele is predicted to result in Returntype : string or undef if this allele is not in the CDS or is a frameshift Exceptions : none Status : Stable=cut 对输入文件进行解析校验 modules\Bio\EnsEMBL\VEP\Parser.pm123456789101112131415161718=head2 validate_vf Arg 1 : Bio::EnsEMBL::Variation::BaseVariationFeature Example : $is_valid = $parser-&gt;validate_vf($vf); Description: Performs various (configurable) checks on a VariationFeature as produced by the parser: - creates a variation_name from the location+alleles if none - checks if chr is in user-specified list if provided - checks start and end look like numbers - checks start/end are valid (start &lt;= end + 1) and corresponds to ref allele - checks chr is in valid list - checks ref allele vs genome if requested Returntype : bool Exceptions : none Caller : next() Status : Stable=cut HGVS信息注释 获取变异最接近的转录本modules\Bio\EnsEMBL\VEP\AnnotationType\Transcript.pm123456789101112 =head2 get_nearest Arg 1 : Bio::EnsEMBL::Variation::BaseVariationFeature $vf Arg 2 : string $type (transcript, gene or symbol) Example : $nearest_gene = $as-&gt;get_nearest($vf, 'symbol'); Description: Gets ID (one of transcript, gene, symbol) of the nearest transcript to the given variant. Returntype : string Exceptions : none Caller : annotate_InputBuffer() Status : Stable=cut 对应代码： modules\Bio\EnsEMBL\Variation\TranscriptVariation.pm sub _hgvs_generic { my $self = shift; my $reference = pop; my $hgvs = shift; #The rna and mitochondrial modes have not yet been implemented, so return undef in case we get a call to these return undef if ($reference =~ m/rna|mitochondrial/); # The HGVS subroutine my $sub = qq{hgvs_$reference}; # Loop over the TranscriptVariationAllele objects associated with this TranscriptVariation foreach my $tv_allele (@{ $self-&gt;get_all_alternate_TranscriptVariationAlleles }) { #If an HGVS hash was supplied and the allele exists as key, set the HGVS notation for this allele if (defined($hgvs)) { my $notation = $hgvs-&gt;{$tv_allele-&gt;variation_feature_seq()}; $tv_allele-&gt;$sub($notation) if defined $notation; } # Else, add the HGVS notation for this allele to the HGVS hash else { $hgvs-&gt;{$tv_allele-&gt;variation_feature_seq()} = $tv_allele-&gt;$sub(); } } return $hgvs; } modules\Bio\EnsEMBL\VEP\VariantRecoder.pm]]></content>
      <categories>
        <category>NGS</category>
        <category>software</category>
      </categories>
      <tags>
        <tag>Annotation</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基因注释软件-综述信息]]></title>
    <url>%2F0003.%E5%BA%94%E7%94%A8-03.software%2Fsoftware-%E8%BD%AF%E4%BB%B6%E7%BB%BC%E8%BF%B0-%E5%8F%98%E5%BC%82%E6%B3%A8%E9%87%8A%2F</url>
    <content type="text"><![CDATA[参考信息The State of Variant Annotation: A Comparison of AnnoVar, snpEff and VEP #|注释工具|开发者|开发语言|HGVS 标准输出|支持转录本|可否商用|是否仍在维护|数据库-公司-机构||-|-|-|-|-|-|-|-||VEP|欧洲生物信息学中心|Perl|是|Refseq、UCSC、Ensembl |可以|是|TCGA/COSMIC/gnomAD/吉因加/求臻/泛生子||annovar|哥伦比亚大学王凯教授|Perl|是|Refseq、UCSC、Ensembl |否|是|HGMD/HGVD/领星医药/世和/燃石/诺禾致源/和瑞/安诺优达/荣之联/博奥/至本/臻和/先声医药/泛生子||snpEff|麦吉尔大学的Pabio Cingolan教授|Java|是||可以|是|HGMD/燃石/博奥/先声医药||Oncotator|Broad Institute的科学家|Python|||否||Broad Institute||Bcfanno|BGI|Perl|否||可以|否|||Bgicg|BGI|Perl|否|Refseq|可以|否|||VAI|||||||UCSC||BaseSpace|||是||||illumina|]]></content>
      <categories>
        <category>NGS</category>
        <category>software</category>
      </categories>
      <tags>
        <tag>Annotation</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基因注释软件-VEP]]></title>
    <url>%2F0003.%E5%BA%94%E7%94%A8-03.software%2FSoftware-%E5%8F%98%E5%BC%82%E6%B3%A8%E9%87%8A-VEP%2F</url>
    <content type="text"><![CDATA[简介在TCGA等大型项目中，也推荐使用了VEP作为注释软件，同时，TCGA项目提供了由VEP向MAF转换的工具 vcf2maf，该工具由MSK长期跟进及维护。VEP 是Ensemble旗下开发的软件，有众多的专业人员进行着更新维护，同时他也符合 CLIA Variant Effect Predictor（VEP）是功能强大的注释、分析工具。它可以对二代测试产生的不同类型变异进行注释，包含SNPs, insertions, deletions, copy number variants和structural variants。也可以依据各种数据库的内容，根据需要，对变异进行过滤和排序。 VEP 输入格式VEP 结果格式VEP Calculated variant consequences 软件问题VEP 本身的对齐，只可以进行全碱基的对齐； 原始变异 ：GAATCATCATCATCATG在vep中 ：GAATCATCATCATCATG而不会拆分碱基进行对齐GAATCATCATCATCATG 安装环境依赖123456conda install -c bioconda perl-DBIconda install -c bioconda perl-Archive-Zipconda install -c bioconda perl-Bio-DB-HTS-Tabix # 使用gff进行注释时，该包必须安装perl -e 'use DBI ; use Archive::Zip ; use Bio::DB:HTS::Tabix'conda install -c bioconda perl-DBD-mysql # 使用离线模式，可以跳过该包 VEP的安装可以直接参考官方说明 通过conda安装VEP本身可以直接通过conda进行安装，由于官方安装文档未提及，在此进行补充说明1conda install -c bioconda ensembl-vep==108 # 如果需要安装新版本需要指定，本文撰写阶段最新版本108，但是默认安装版本是v92. 库文件配置在线版进行数据库下载的速度非常慢，而conda也只会进行vep注释软件的安装，不包含相关库文件，因此需要单独准备库文件，速度会更快。 数据集下载 运行VEP软件自带的 INSTALL.pl 进行下载，非常之慢，具体方式可以参考官方安装说法说明。 从官方ftp进行下载cache文件可以访问官方ftp进行下载https://ftp.ensembl.org/pub/grch37/release-108/variation/vep/homo_sapiens_refseq_vep_108_GRCh37.tar.gz参考基因组的fasta序列，https://ftp.ensembl.org/pub/grch37/release-108/fasta/homo_sapiens/dna/Homo_sapiens.GRCh37.dna_sm.primary_assembly.fa.gz 扩展功能配置VEP本身提供了一个外部扩展功能模块 Plugins可以使用INSTALL.pl 进行安装，也可以直接通过Github仓库进行下载 相关问题及处理记录20221104 安装VEP108时会发发现存在版本兼容性错误因为bioconda中123# 报错信息# Compress::Raw::Zlib version 2.201 required--this is only version 2.105conda install -c conda-forge perl-compress-raw-zlib # 使用构建自己的注释库自建注释数据库的参考文档 VEP can use a variety of annotation sources to retrieve the transcript models used to predict consequence types. Cache - a downloadable file containing all transcript models, regulatory features and variant data for a species GFF or GTF - use transcript models defined in a tabix-indexed GFF or GTF file Database - connect to a MySQL database server hosting Ensembl databasesGFF/GTF filesVEP can use transcript annotations defined in GFF or GTF files. The files must be bgzipped and indexed with tabix and a FASTA file containing the genomic sequence is required in order to generate transcript models. 123grep -v "#" data.gff | sort -k1,1 -k4,4n -k5,5n -t$'\t' | bgzip -c &gt; data.gff.gztabix -p gff data.gff.gz./vep -i input.vcf --gff data.gff.gz --fasta genome.fa.gz GFF fileExample of command line with GFF, using of flag –gff :1./vep -i input.vcf --cache --gff data.gff.gz --fasta genome.fa.gz This functionality uses VEP’s custom annotation feature, and the –gff flag is a shortcut to:1--custom data.gff.gz,,gff NOTE: You should use the longer custom annotation form if you wish to customise the name of the GFF as it appears in the SOURCE field and VEP output header. GTF fileExample of command line with GTF, using of flag –gtf :1./vep -i input.vcf --cache --gtf data.gtf.gz --fasta genome.fa.gz This functionality uses VEP’s custom annotation feature, and the –gtf flag is a shortcut to:1--custom data.gtf.gz,,gtf NOTE: You should use the longer custom annotation form if you wish to customise the name of the GTF as it appears in the SOURCE field and VEP output header. VEP的功能Ensembl Variation - Calculated variant consequences同时由于一个突变可能会存在多个不同类型的变异描述方式（Stop lost; Inframe insertion），在某些情况下需要形成一个唯一的功能描述，因此Ensemble提供了一套用于评估不同变异描述的优先级顺序，可以参考官方文档。 VEP使用过程中发现的问题/风险汇总####1. 注释结果未见终止密码子，功能输出 stop_lost123变异结果： chr1_10342506_TCAGGTGGGCTTGACGTCTGTGACCAGTATTCAAGAGAGGATCATGTCTA/CCAGGTGTAGACATGATCCTCTCTTGAATACTGGTCACAGACGTCAAGCC注释结果： NM_015074.3：c.1211_1260delinsCCAGGTGTAGACATGATCCTCTCTTGAATACTGGTCACAGACGTCAAGCC 注释结果： NP_055889.2：p.I404_L420delinsTRCRHDPLLNTGHRRQA 在UCSC中核查发现读码框不一致。 123456789VEP读码框为 |a T C|A G G|T G G|G C T|T G A|C G T|C T G|T G A|C ......UCSC读码框为 a g T|C A G|G T G|G G C|T T G|A C G|T C T|G T G|A C ......在VEP使用的密码框中，存在多个读码框对应的氨基酸序列为终止密码子（TGA），同时namechecker和Varsome中的突变后氨基酸均基于VEP的读码框翻译产生。Ref / Alt aTCAGGTGGGCTTGACGTCTGTGACCAGTATTCAAGAGAGGATCATGTCTA/aCCAGGTGTAGACATGATCCTCTCTTGAATACTGGTCACAGACGTCAAGCCI R W A * R L * P V F K R G S C L /T R C R H D P L L N T G H R R Q A04 05 06 07 08 09 10 11 12 13 14 15 16 17 18 19 20 经过核查是用于使用了 –use_given_ref 参数，当不使用该参数是测试发现问题可以消除。 2. start-lost /init loss 的写法不规范，其实密码子缺失应该输出为 “p.0” 但是VEP输出结果格式为 “p.Leu1?” 3. VEP 注释结果中，存在部分变异结果没有提供外显子亚区测试阶段发现如下情况时，变异注释结果未输出外显子， 变异位于内含子和外显子交界处，示例如下：123 Exon-1 | ins | Intron-1Ref: TCAGGTGGGCTTGACGTCTGTGAC| |CAGTATTCAAGAGAGGATCATGTCTAAlt: TCAGGTGGGCTTGACGTCTGTGAC|TTGAA|CAGTATTCAAGAGAGGATCATGTCTA 在使用中，可以进行了人工数据库的构建，但是需要注意一点，bed文件格式为0base的左闭右开，但是VEP的位置信息1base的 4. 跨越多个区域的变异，是否需要有氨基酸变化的描述 方法 cHGVS pHGVS VEP c.111+1_117delinsCTGCGC p.A38_*39delinsLR Varsome c.111+1_117delGCTGCGGinsCTGCGC . 不同方法注释得到的氨基酸变化存在差异，现阶段无法确定正确答案。 Tag value CHR chr10 Pos 43609928 Ref . Alt GTCCACTGTGCGACGAGCTGTGCCGCACGGTGGATCTGTGGGTGGGGGTGGTGTGAGGCTTGGCACCGCCACCCACAG Trans NM_020975.6 方法 cHGVS pHGVS VEP c.1880-1_1880ins GTCCACTGTGCGACGAGCTGTGCCGCACGGTGGATCTGTGGGTGGGGGTGGTGTGAGGCTTGGCACCGCCACCCACAGA p.Q626_D627insGPLCDELCRTVDLWVGVV* Varsome c.1880-1_1880insGTCCACTGTGCGACGAGCTGTGCCGCACGGTGGATCTGTGGGTGGGGGTGGTGTGAGGCTTGGCACCGCCACCCACAG p.D627_S1114delinsRVHCATSCAARWICGWGWCEAWHRHPQIHCATSCAAR Namechecker c.1879_1880insGTCCACTGTGCGACGAGCTGTGCCGCACGGTGGATCTGTGGGTGGGGGTGGTGTGAGGCTTGGCACCGCCACCCACAG p.Asp627_Ser1114delinsGlyProLeuCysAspGluLeuCysArgThrValAspLeuTrpValGlyValVal HGVS未明确描述导致氨基酸截断时描述规范 Tag value CHR chr19 Pos 42795240 Ref CGCCCCCTGCTGTCCAGTT Alt GGCAATGAACTGGACAGCA Trans NM_015125.5 方法 cHGVS pHGVS VEP c.2321_2339delinsGGCAATGAACTGGACAGCA p.Ala774_Phe780delinsGlyGlnTer Varsome c.2321_2339delinsGGCAATGAACTGGACAGCA p.Ala774_Arg1608delinsGlyGln Namechecker c.2321_2339delinsGGCAATGAACTGGACAGCA p.Ala774_Arg1608delinsGlyGln 变异导致多个氨基酸均为同一突变]]></content>
      <categories>
        <category>NGS</category>
        <category>software</category>
      </categories>
      <tags>
        <tag>Annotation</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[免疫组化分析软件 IMonitor]]></title>
    <url>%2F0003.%E5%BA%94%E7%94%A8-03.software%2FSoftware-%E5%85%8D%E7%96%AB%E7%BB%84%E5%BA%93-IMonitor%2F</url>
    <content type="text"><![CDATA[Methods核心分析流程共分为4个步骤 basic data processingIn the first step, the readswere checked for inclusion of adaptorsequences. If any adaptor sequence was detected and locatedwithin 50 bp of the 39end of the read, it was deleted from theread. Reads bearing adaptor sequence at the 59 end or .5%“N” bases were discarded. The average base quality of eachreadwas calculated after removing the low-quality bases (basequality,10) at the 39 end. Further filtration left out reads withaverage quality ,15. For Illumina paired-end (PE) sequencing,the PE reads were merged at their overlapping region. ForPE readswith insertion length longer than the length of a singleread, the COPE (Liu et al. 2012) toolwas used; otherwise readswere assembled by an in-house program. Themain parametersfor both tools included themaximumoverlapping length (readlength), minimum overlapping length (10 bp), mismatch rate(10%) at the overlapping region, and ratio V(D)J assignmentThe V/D/J reference sequences were downloaded from the IMGT database, the international ImMunoGeneTics information system (http://www.imgt.org/). Processed sequences were aligned to the V, (D), J references, respectively, by BLAST (Altschul et al. 1990; Zhang et al. 2000; Ye et al. 2006) and specific parameters were applied to accommodate the differences in lengths of V, (D), J segments (BLAST parameters: V, -W 15 -K 3 -v 1 -b 3; D, -W 4 -K 3 -v 3 -b 5; and J, -W 10 -K 3 -v 1 -b 3). The high similarity among the genes and alleles of the germline sequences, along with the diversity of V/D/J gene rearrangement, gave rise to difficulties for accurate alignment. This might eventually lead to an incorrect structural analysis (CDR3 identification, deletion, or insertion). To improve the accuracy, a second alignment procedure was developed to identify exactly the V/D/J genes (Figure 2). First, a global alignment strategy, which attempted to align every base in every sequence, was used for the non-CDR3 region of the sequence. The mapped region generated from BLAST became a new seed and served as starting points for bootstrapping (base-by-base) extension to both directions, until the entire non-CDR3 region in the query was mapped to the target (reference) sequence. The mapping score was calculated according to these rules: reward for a nucleotide match was 5 and penalty for a nucleotide mismatch was 24. Second, the M-mismatch extension model of local alignment strategy was applied to locate the exact end positions of V and J genes during CDR3 region realignment. The procedure began at the CDR3 start position in the V gene or the CDR3 end position in the J gene and continuously extended in one direction until the preset mismatch limit was reached, generating the longest possible interval with the highest score. The mismatch numbers allowed for V/D/J genes were determined based on the analysis result of publicly available rearrangement sequences (http://www.imgt.org/ligmdb/) (Supporting Information, Figure S2A) and adjusted accordingly for different TCR and BCR chains (mismatches allowed: TRBV/J, TRAV/J, 0; IGHV/J, 2; IGKV/J, IGLV/J, 7). As shown in Figure S2A, these mismatch limits took mutations into consideration and covered .99.5% of all defined rearrangement sequences. Because the entire D gene was located within the CDR3 region, only the M-mismatch extension model was used for its realignment (mismatches allowed: TRBD, 0; IGHD, 4). Finally, all data including alignment score, identity, mismatch number, and alignment length were processed, and the alignment with highest score and identity larger than the threshold (.80%) was selected as the best hit. However, there might be several best hits with the same score due to the homology among the germline genes and alleles. In this case, the reference with the fewest deletions was selected, as shorter deletions are more likely to happen according to previous reported results (Warren et al. 2009) and our analysis from actual public rearrangement data (Figure S2B). structural analysisstatistics/visualization]]></content>
      <categories>
        <category>NGS</category>
        <category>software</category>
      </categories>
      <tags>
        <tag>免疫组库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[肿瘤NGS检测开发过程中的方法学]]></title>
    <url>%2F0001.%E7%9F%A5%E8%AF%86-01.NGS%2F2021-10-29.%E8%82%BF%E7%98%A4NGS%E6%A3%80%E6%B5%8B%E5%BC%80%E5%8F%91%E8%BF%87%E7%A8%8B%E4%B8%AD%E7%9A%84%E6%96%B9%E6%B3%95%E5%AD%A6%2F</url>
    <content type="text"><![CDATA[肿瘤NGS检测开发过程中的方法学 李金明，高通量测序技术[M]. 李金明，个体化医疗中的临床分子诊断[M]. Rehm H L, et al. ACMG clinical laboratory standards for next-generation sequencing[J] 2013. Aziz N, et al. College of American Pathologists’ laboratory standards for next-generation sequencing clinical tests[J] 2015. Matthijs G, et al. Guidelines for diagnostic next-generation sequencing[J] 2016. Jennings L J, et al. Guidelines for validation of next-generation sequencing–based oncology panels[J] 2017. Li M M, et al. Standards and guidelines for the interpretation and reporting of sequence variants in cancer[J] 2017. Roy S, et al. Standards and guidelines for validating next-generation sequencing bioinformatics pipelines[J] 2018. 产品开发过程中的深度确定测序深度或覆盖深度被定义为覆盖给定核苷酸位置的reads的数量，生物信息学工具极其依赖于足够的覆盖深度，以便灵敏和特异地检测变异。覆盖深度与稳定检测样本的变异之间的关系很简单，因为更高数量的高质量测序数据为特定位置的碱基检测供了信心，无论来自测序样本的碱基调用是否是与参考碱基相同（未识别出变异）或者是非参考碱基（识别出变异）。 然而，许多因素会影响所需的深度，包括测序平台，目标区域的序列复杂性（与基因组的多个区域具有同源性的区域、重复序列元件或假基因的存在以及GC富集区域）。此外，用于目标富集的文库制备和需要评估的变异类型也是重要的考虑因素。因此，必须在检测开发和验证过程中系统地评估每个 NGS 测试的覆盖模型。 这些性能参数可以并且应该在开发阶段进行评估，以帮助定义验证的接受标准。例如，对于给定比例的突变等位基因，可以使用二项分布方程来确定检测到最小数量等位基因的概率： 而在一个确定的检测体系下，我们可以知道一个体系的错误率（LOB），和预期的检测限。这时我们可以根据所需的检测下限、读取质量和假阳性或假阴性结果的容忍度来估计所需的覆盖深度(可以游有效区分真阳性和真阴性的最低深度)。123456789101112131415161718192021222324252627282930313233343536373839404142library("ggplot2")Totaldepth=5000 # 预期深度，控制评估的深度上限ErrorRate=0.004 # 基于产品首批高深度产品，评估获得LOBLoDRate = 0.01 # 产品预期的检测性能，后期LOD需要单独进行评估补充。CI = 0.9 # 对性能结果要求的置信区间。depthlist = c(1:Totaldepth)ErrorReadNumlist = depthlistSupportReadNumlist = depthlistfor (depth in 1:Totaldepth) &#123; ErrorReadNum = round(depth * ErrorRate) # 考虑错误率统计过程中本身已经是错误率的最高值，所以不再进行二项分布扩展。 #ErrorReadNum = qbinom(CI,depth,ErrorRate) # 错误率考虑上95置信区间。 SupportReadNum = qbinom(1-CI,depth,LoDRate) ErrorReadNumlist[depth] = ErrorReadNum SupportReadNumlist[depth] = SupportReadNum&#125;Difference= SupportReadNumlist - ErrorReadNumlisttype=c(rep(paste('Error:',ErrorRate),times=Totaldepth),rep(paste('Detect:',LoDRate),times=Totaldepth),rep('Difference',times=Totaldepth))depth=c(depthlist,depthlist,depthlist)read_num=c(ErrorReadNumlist,SupportReadNumlist, Difference)data=data.frame(type, depth,read_num)# ggplot(data,aes(x=depth,y=read_num,cluster=type,color=type))+geom_line()+geom_hline(yintercept = 2)## 确定目标深度Target_depth="Not Find "for(i in Totaldepth:1)&#123; if(Difference[i]&lt;2)&#123; Target_depth = i+1 break &#125;&#125;## 绘图ggplot(data,aes(x=depth,y=read_num,cluster=type,color=type)) + geom_line() + geom_hline(yintercept = 2) + annotate(geom = 'text', x= 2000, y = 30, label = paste("Totaldepth=5000\nErrorRate=",ErrorRate,"\nLOD-E=",LoDRate,"\nCI=",CI,"\nDepth=",Target_depth,"\ncutoff-E=",qbinom(1-CI,Target_depth,LoDRate) ))qbinom(1-CI,Target_depth,LoDRate) 内部共享PPT]]></content>
      <categories>
        <category>NGS</category>
        <category>肿瘤检测</category>
        <category>标准化</category>
      </categories>
      <tags>
        <tag>Standards and guidelines</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Singularity - 入门简介]]></title>
    <url>%2F0002.%E5%BC%80%E5%8F%91-04.pipeline%2Fpipeline-container-Singularity-%E5%85%A5%E9%97%A8%E7%AE%80%E4%BB%8B%2F</url>
    <content type="text"><![CDATA[基本概念Singularity 是基于Docker已经进行应用阶段开发的。针对和Docker的官方说明如下 You don’t need Docker installed You can shell into a Singularity-ized Docker image You can run a Docker image instantly as a Singularity image You can pull a Docker image (without sudo) You can build images with bases from assembled Docker layers that include environment, guts, and labels Singularity 安装可以直接使用conda进行安装1conda create -n singularity singularity]]></content>
      <categories>
        <category>镜像</category>
        <category>云计算</category>
        <category>pipeline</category>
        <category>container</category>
      </categories>
      <tags>
        <tag>流程开发</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[docker - 入门简介及环境配置]]></title>
    <url>%2F0002.%E5%BC%80%E5%8F%91-04.pipeline%2Fpipeline-container-docker-01.%E5%85%A5%E9%97%A8%E7%AE%80%E4%BB%8B%E5%8F%8A%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[#Docker容器与虚拟机类似，但二者在原理上不同。容器是将操作系统层虚拟化，虚拟机则是虚拟化硬件，因此容器更具有便携性、更能高效地利用服务器。 容器更多的用于表示软件的一个标准化单元。由于容器的标准化，因此它可以无视基础设施（Infrastructure）的差异，部署到任何一个地方。另外，Docker也为容器提供更强的业界的隔离兼容。 基本概念 镜像（Image）：Docker 镜像就相当于是一个静态的root文件系统。就类似一个模板，根据这个模板可以创建多个容器。 容器（Container）：镜像和容器的关系，就像是面向对象程序设计中的类和实例一样，镜像是静态的定义，容器是镜像运行时的实体。容器可以被创建、启动、停止、删除、暂停等。 仓库（Repository）：用来保存镜像的地方。有共有仓库和私有仓库之分。 相关资源官网文档Docker官方镜像仓库 账号：6143****@qq.comDocker官方文档 培训材料大IT一体机培训PPT Docker安装Docker是一种流行的容器化平台，它可以帮助开发人员和运维团队简化应用程序的构建、部署和管理过程。本文将介绍Docker的安装、配置以及一些常用的命令示例，帮助您快速上手并开始使用这个强大的工具。docker常用的Linux平台安装命令如下，123456# Install on Debian/Ubuntusudo apt-get updatesudo apt-get install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin# Install Docker Engine on CentOSsudo yum install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin 在Windows/Mac上安装，可以直接在官网下载 Docker Desktop 程序，并按照提示进行安装。 启动docker1sudo systemctl start docker 确定安装成功1sudo docker run hello-world 初始配置权限配置因为Docker引擎使用Linux中的cgroups和namespaces等技术来隔离容器，这些技术需要访问主机系统资源。Docker的守护进程是绑定到Unix socket 的，因此总是默认是root身份运行的。所以默认必须使用root用户或使用 sudo 才能运行docker引擎，其他用户需要将其添加到Docker组中才能运行Docker命令。因此在初次安装Docker后，我们需要进行如下的系统组创建，以便使非root用户运行docker1234567891011# 创建docker组sudo groupadd docker# 将用户添加到Docker组中, 也可以以root身份直接编辑/etc/group 文件，sudo usermod -aG docker &lt;your-user&gt;# 激活组的变更，重新登录也可以激活newgrp docker# 测试docker安装和配置docker run hello-world 仓库配置Docker官方仓库Docker Hub 是目前Docker官方维护的仓库，其中已经包括了数量超过15000个镜像。大部分需求都可以通过在Docker Hub中直接下载镜像来使用。配置也比较简单，直接使用docker login， 输入用户信息即可12345678910(base) root@manager[Fri Jul 07] ~/.docker$ docker loginLogin with your Docker ID to push and pull images from Docker Hub. If you don't have a Docker ID, head over to https://hub.docker.com to create one.Username: $usernamePassword:WARNING! Your password will be stored unencrypted in /home/root/.docker/config.json.Configure a credential helper to remove this warning. Seehttps://docs.docker.com/engine/reference/commandline/login/#credentials-storeLogin Succeeded 第三方仓库第三方仓库笔者只接触使用过阿里云的ACR，针对ACR服务，本身就提供了比较详细的代码示例。整体注册登录也比较简单。1$ docker login --username=bgioseq registry.cn-shenzhen.aliyuncs.com 私有仓库由于企业业务的特殊性，可能有时候，我们不方便讲我们的镜像放到第三方仓库，以防代码泄露等危险，这种时候，我们可以选择构建私有仓库。私有仓库构建，可以通过运行官方registry镜像来实现。默认情况下，仓库会被创建在容器的/var/lib/registry目录下。可以通过-v参数来将镜像文件存放在本地的指定路径。12# 可以使用Docker的 registry 镜像docker run -d -p 5000:5000 --name registry registry:2 然后你的私有仓库就创建好了，私有仓库的就是 localhost:5000当然也许你会使用你的同事创建的私有仓库，那你就需要更新下本地配置文件/etc/docker/daemon.json ，将IP改为部署服务的主机IP12345&#123; "insecure-registries":[ "192.168.1.1:5000" ]&#125; 仓库配置信息会保存在~/.docker/config.json文件中；12345678910&#123; &quot;auths&quot;: &#123; &quot;https://index.docker.io/v1/&quot;: &#123; &quot;auth&quot;: &quot;YVuYJlbkBkb2NrZXI=&quot; &#125;, &quot;registry.cn-shenzhen.aliyuncs.com&quot;: &#123; &quot;auth&quot;: &quot;bGlZXE6TGl1Ym8xMjM=&quot; &#125; &#125;&#125; 其中auth是使用 base64 编码的秘钥凭据，可以使用base64 -d 反编译获取原账号密码 镜像配置因为在访问中，我们可能会有一些需求访问官方镜像，和其他类似Conda、R的官方仓库使用一样，官方镜像也许会存在一些网络上的问题，所以我们可以调整使用国内的镜像，以便提高访问速度 1234#配置阿里云镜像sudo yum-config-manager \ --add-repo \ https://registry.docker-cn.com 镜像加速器 镜像加速器地址 Docker 中国官方镜像 https://registry.docker-cn.com DaoCloud 镜像站 http://f1361db2.m.daocloud.io Azure 中国镜像 https://dockerhub.azk8s.cn 科大镜像站 https://docker.mirrors.ustc.edu.cn 阿里云 https://&lt;your_code&gt;.mirror.aliyuncs.com 七牛云 https://reg-mirror.qiniu.com 网易云 https://hub-mirror.c.163.com 腾讯云 https://mirror.ccs.tencentyun.com 部署完以后，可以拉个镜像试一下 ​12345678(base) [root@VM_0_8_centos ~]\# docker pull centosUsing default tag: latestlatest: Pulling from library/centosa1d0c7532777: Pull completeDigest: sha256:a27fd8080b517143cbbbab9dfb7c8571c40d67d534bbdee55bd6c473f432b177Status: Downloaded newer image for centos:latestdocker.io/library/centos:latest(base) [root@VM_0_8_centos ~]\#]]></content>
      <categories>
        <category>镜像</category>
        <category>云计算</category>
        <category>pipeline</category>
        <category>container</category>
      </categories>
      <tags>
        <tag>流程开发</tag>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[文件格式说明 - MAF(Mutation Annotation Format )]]></title>
    <url>%2F0001.%E7%9F%A5%E8%AF%86-03.standard%2Ffileformat-maf%2F</url>
    <content type="text"><![CDATA[规范化参考来源TCGA 是其定义了一个相对标准的变异描述格式MAF 其中文件定义了，提升流程的规范性。同时，存在一个maftools，可以提供比较丰富的基于MAF文件的可视化处理。maftools : Summarize, Analyze and Visualize MAF Files]]></content>
      <categories>
        <category>知识沉淀</category>
        <category>NGS</category>
        <category>肿瘤检测</category>
        <category>文件格式</category>
      </categories>
      <tags>
        <tag>文件格式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MSI检测及相关共识]]></title>
    <url>%2F0001.%E7%9F%A5%E8%AF%86-03.standard%2F%E5%85%B1%E8%AF%86-MSI%E6%A3%80%E6%B5%8B%E5%8F%8A%E7%9B%B8%E5%85%B3%E5%85%B1%E8%AF%86%2F</url>
    <content type="text"><![CDATA[摘要微卫星不稳定性（microsatellite instability，MSI）由DNA错配修复（mismatch repair，MMR）蛋白功能缺陷导致，这一分子特征在结直肠癌和子宫内膜癌等相关实体瘤中具有重要的临床意义。目前检测MSI状态的手段包括免疫组织化学检测MMR蛋白、多重荧光聚合酶链反应（polymerase chain reaction，PCR）检测微卫星位点和基于二代测序（next generation sequencing，NGS）平台的MSI算法。本共识针对MSI的定义、临床意义及其3类检测手段各自的优势与不足展开阐述和推荐。希望专家共识的制订可大力推动恶性肿瘤MSI状态普筛工作，提高临床医师对各种检测方法的认识，从而更加准确地解读检测结果，为患者提供更优质的临床服务。 微卫星不稳定的定义微卫星(microsatellite, MS)是指细胞基因组中以少数几个核苷酸(多为1~6个)为单位串联重复的DNA序列，又称短串联重复(short tandem repeat, STR)。DNA错配修复(mismatch repair, MMR)功能出现异常时，微卫星出现的复制错误得不到纠正并不断累积，使得微卫星序列长度或碱基组成发生改变，称为微卫星不稳定性(microsatellite instability, MSI)，同时导致基因组呈现高突变表型。肿瘤中，MMR功能缺陷往往由于MMR基因(MLH1、MSH2、MSH6及PMS2)及其相关基因EPCAM的致病性突变导致，也可能由于MLH1启动子区高甲基化引起的MLH1表达缺失导致[1]。MSI现象于1993年在结直肠癌中被首次发现[2]。MSI根据程度可以被分成3类：微卫星高度不稳定性(MSI-high, MSI-H)、微卫星低度不稳定性(MSI-low, MSI-L)、微卫星稳定(microsatellite stability, MSS)[3]。MSI-H在不同癌种中的发生率存在较大差异。目前已知MSI-H发生率较高的实体瘤包括子宫内膜癌(20%~30%)、胃癌(15%~20%)和结直肠癌(12%~15%，其中Ⅳ期结直肠癌4%~5%)等[4]。 MSI状态检测方法及其对比IHC检测MMR蛋白MSI多由MMR蛋白表达缺失导致的MMR功能缺陷所致，故可通过检测MMR蛋白缺失来反映MSI状态。通过IHC检测MMR蛋白表达与基于DNA分析检测MSI状态是评估相同生物学效应的不同检测方法[5-6]。IHC方法采用分别针对MLH1、MSH2、MSH6及PMS2的特异性抗体，阳性表达定位于细胞核。如肿瘤样本中4个MMR蛋白均阳性表达，则为错配修复功能完整(proficient mismatch repair, pMMR)；任一MMR蛋白缺失即为dMMR。 多重荧光PCR毛细管电泳法检测MSI直接检测MSI状态的常用方法是多重荧光PCR毛细管电泳法，这也是当前公认的MSI检测“金标准”。当前市面上存在多个基于PCR平台的MSI检测商用试剂盒，但国内此类试剂盒尚未获得国家药品监督管理局(National Medical Products Administration, NMPA)批准。此类试剂盒设计原理均基于美国国立癌症研究所(National Cancer Institute, NCI)建议的MS位点[并进行微调和(或)扩展]，将肿瘤细胞与正常细胞的PCR法检测结果进行比较，以确定肿瘤细胞的MSI状态(有关PCR法检测MS位点及其判读的介绍详见附录1)《CSCO结直肠癌诊疗指南2018版》建议采用NCI推荐的5个MS位点(BAT-25、BAT-26、D2S123、D5S346和D17S250)进行MSI检测[7]。《遗传性结直肠癌临床诊治和家系管理中国专家共识》(2018)指出，基于肿瘤组织样本的MSI检测作为可选推荐，建议在有条件的医疗单位开展[。 二代测序(next generation sequencing, NGS)检测MSI近年来，随着高通量测序平台的广泛应用，NGS平台目标区域测序(即NGS panel)或全外显子组测序(whole exon sequencing, WES)/全基因组测序(whole genome sequencing, WGS)开始应用于MSI检测，使用计算工具同时研究基因组上的大量微卫星序列成为可能(当前主流NGS-MSI算法原理详见附录2)。2018年ESMO年会上，ESMO精准医学工作组(ESMO Precision Medicine Working Group)推荐将NGS作为MSI的二线检测方法(second line testing)。NCCN结直肠癌临床实践指南亦指出，MSI检测可通过经验证的NGS panel进行，尤其是对于那些需要同时检测RAS/BRAF突变状态的转移性CRC患者。 此外，基于外周血循环肿瘤DNA(circulating tumor DNA, ctDNA)的MSI(MSI from blood ctDNA, b-MSI)-NGS算法亦已崭露头角，为肿瘤组织取样困难或不足的晚期实体瘤患者MSI检测提供新选择。多个血检NGS panel及其各自bMSI-NGS算法数据已经开始在国际学术会议上以摘要形式陆续披露(表 2)[39-41]，但尚未在学术期刊上以全文形式发表。因此，b-MSI-NGS检测在当前不应常规推荐，仅作为缺乏组织的患者为明确MSI状态的一种替代手段。 MSI的临床意义 MSI检测作为林奇综合征初筛手段 MSI是Ⅱ期结直肠癌预后因子 MSI是Ⅱ期结直肠癌辅助化疗疗效预测因子 MSI是晚期实体瘤免疫治疗疗效预测因子 MSI检测专家共识 推荐意见1：所有结直肠癌患者均应进行MSI状态筛查 推荐意见2：晚期实体瘤患者(如胃癌、小肠癌、子宫内膜癌、尿路上皮癌、胰腺癌和胆管癌等)如考虑免疫治疗应行MSI状态检测 推荐意见3：MSI检测方法包括经认证的IHC、PCR和NGS方法(3种方法各有优势) IHC法可以直接鉴定出导致MSI-H发生的MMR缺陷基因。IHC法检测MMR蛋白表达可在多数医院的病理科完成，普及性强，且价格低廉。但该方法受判读人员的主观影响较大，存在一定的假阳性与假阴性。因此IHC的前处理务必遵循病理科相关规范，结果判断应由有经验的病理医师完成或进行双人复核，以尽可能避免个人主观造成偏倚。 基于样本微切割的多重荧光PCR毛细管电泳法，简便且便宜，敏感度和特异度均较好(特别是在经广泛验证的CRC肿瘤样本中)，但全国能开展该项检测的病理科相对较少，且存在一定的假阴性。 对于需要同时检测肿瘤驱动基因和(或)治疗相关基因变异的患者，目标区域NGS是个不错的选择。NGS法可同时检测panel覆盖的驱动基因变异，包括MMR基因胚系和(或)体细胞突变，甚至TMB等分子标签。基于ctDNA样本的b-MSI检测目前正处于验证阶段，有望为肿瘤组织取样困难或不足的晚期肿瘤患者提供新的选择。但NGS单独用于MSI检测则不推荐，理由是增加经济负担和浪费资源。所有基于NGS的MSI检测应用于临床前必须进行充分的可靠性认证及临床验证。如果遇到患者同时进行IHC-MMR蛋白表达和DNA分析(PCR或NGS法)MSI状态检测且检测结果不一致的情况，可考虑采用第3种方法(NGS或PCR法)进行验证(部分专家推荐)。 PCR方法的微卫星位点选择基于PCR方法的微卫星位点选择曾经历数次重大变迁。 1998年NCI推荐用于检测微卫星状态的5个微卫星位点，包括2个单核苷酸重复位点(BAT-25和BAT-26)以及3个双核苷酸重复位点(D2S123、D5S346和D17S250)；判读标准为：≥2个微卫星位点不稳定为MSI-H，1个位点不稳定为MSI-L，所有位点均稳定即为MSS。 NCI的2B3D位点是通过1997年两个重要的多中心研究筛选了30多个微卫星位点最终确认的，所以几乎国外所有的指南都是基于当年的这两个研究推荐MSI的检测位点。 2002年修订为Pentaplex panel，包含5个单核苷酸重复位点BAT-25、BAT-26、NR21、NR24和NR27，以提高检测敏感度及特异度。 2006年，MSI分析系统Promega以Mono-27取代NR27，并增加Penta C和D用于样本识别，进一步提高MSI检测敏感度。 从国际上多个 MSI 的研究来看，中国人群的 MSI-H 发生率是略低于欧美人群的，约为 4.5~13.3%，提示 MSI 的发生可能有种族差异性，位点的选择需要有中国人群的临床数据支持。「2B3D NCI Panel」在国内外都是最权威的检测标准。关于基于中国人群的多中心研究显示[1,2]，「2B3D NCI Panel」敏感度高于单核苷酸 Promega Panel，提示「2B3D NCI Panel」更适合中国肿瘤患者。比较「2B3D」 Panel 和 Promega Panel 两种 Panel 的检测结果与 MMR IHC 结果的一致性显示： 两个 Panel 的特异性相当，均为 99.1%（215/217），2B3D Panel 灵敏度为 89.3%(25/28) 高于 Promega 的 71.4%(20/28)[2] 中国人群中，Zhang等纳入了近6000例MSI检测结果的Meta分析数据最终显示，2B3D位点与Promega Panel的检出阳性率在散发性结直肠癌一致（13.5% vs 12.9%），但是，如果采用其他单核苷酸Panel（6个位点的MSI Panel），检出阳性率仅7.7%！至少漏诊了30%以上的MSI-H肿瘤患者！ 当前主流NGS-MSI算法原理基于NGS的MSI检测原理分为以微卫星位点重复序列长度变化为基础以及以突变负荷和突变类型为基础。其中，基于NGS目标区域测序(targeted gene sequencing, TGS)的检测技术通常基于前者进行MSI检测。此类MSI检测的技术核心包括选取最有效的微卫星标志位点组合以及构建合理的分类模型用以最大程度区分MSI-H和MSS状态下微卫星位点重复单元长度变化水平的差异，使得MSI检测敏感度和特异度达到最优，并保证在低肿瘤占比样本中检测的稳健性。 标志位点通常表现为在MSS状态下重复单元长度高度稳定，而MSI-H状态下高频不稳定，以保证MSI检测的最优敏感度和特异度。基于WGS和WES的研究证明，不同标志位点可能表现为癌种特异性，也可具有跨癌种普遍性2种特征，而其余大量微卫星位点高度稳定，无法为MSI-H提供有效信息，故而针对Panel的不同用途(特异性癌种/泛癌种)应选择不同的位点组合[36]。前期PCR-MSI研究表明，单核苷酸重复序列在PCR-MSI检测中敏感度更高[33]。而多核苷酸重复序列本身的多态性导致检测方法对配对正常样本具有依赖性。基于PCR法中的Pentaplex panel的5~7个标志位点组合BAT25、BAT26、NR21、NR22、NR24、NR27和MONO27被认定为PCR-MSI检测的金标准。由此该位点组合也通常在NGS panel中专门被设计和应用于MSI检测。 NGS panel的MSI算法通常通过刻画选取的标志位点不同重复序列长度对应的reads个数在MSI-H和MSS中的差异来判断位点的不稳定状态，以不稳定位点比例是否超过既定阈值来确定样本的MSI状态。位点在2种状态差异的评估方法包括：基于癌组织样本和配对正常样本不同重复序列长度对应的reads个数进行χ2检验(MSIsensor)或者距离评估(MANTIS)，以及独立的基于癌组织样本评估重复序列长度的类型个数与一组基线样本(非MSI-H样本)的差异(mSINGS)，或者MSS状态下主要的重复序列长度类型(peak)对应的覆盖占位点覆盖的比例在癌组织样本与一组基线样本(非MSI-H样本)的差异(ColonCore-MSI)，从而评估位点的稳定状态。选取的标志位点限制为1~5 bp重复单元(如MSIsensor和MANTIS)或仅单碱基重复序列(如ColonCore-MSI)的MS位点。判断样本MSI状态的不稳定位点比例的阈值包括3.5%(MSIsensor)、20%(mSINGS)和40%(ColonCore-MSI)不等。 各指南情况2021 年更新的 NCCN 指南将推荐 MSI 检测的适应症癌种从结肠癌、直肠癌、胃癌、子宫内膜癌扩大到了前列腺癌、胰腺癌等多个实体瘤。结直肠癌、胃癌、子宫内膜癌、小肠腺癌、胰腺癌这 5 个临床常见癌种 MSI 的发生率非常高. 参考指南&amp;共识 结直肠癌及其他相关实体瘤微卫星不稳定性检测中国专家共识 结直肠癌及其他相关实体瘤微卫星不稳定性检测中国专家共识 结直肠癌分子生物标志物检测专家共识 Chinese Society of Clinical Oncology (CSCO) diagnosis and treatment guidelines for colorectal cancer 2018 (English version) （2020.V1）NCCN临床实践指南：肝胆肿瘤.bak.pdf （2020.V1）NCCN临床实践指南：宫颈癌.pdf （2020.V1）NCCN临床实践指南：卵巢癌包括输卵管癌和原发性腹膜癌.pdf （2020.V1）NCCN临床实践指南：前列腺癌.pdf （2020.V1）NCCN临床实践指南：食道癌和胃食管交界处癌.pdf （2020.V1）NCCN临床实践指南：胃癌.pdf （2020.V1）NCCN临床实践指南：子宫肿瘤.pdf （2020.V2）NCCN临床实践指南：结肠癌.pdf （2020.V2）NCCN临床实践指南：小肠腺癌.pdf （2020.V3）NCCN临床实践指南：乳腺癌.pdf 丁香园]]></content>
      <categories>
        <category>NGS</category>
        <category>肿瘤检测</category>
        <category>共识</category>
      </categories>
      <tags>
        <tag>MSI</tag>
        <tag>实用肿瘤杂志</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[共识-用药推荐的驱动突变汇总]]></title>
    <url>%2F0001.%E7%9F%A5%E8%AF%86-03.standard%2F%E5%85%B1%E8%AF%86-%E7%94%A8%E8%8D%AF%E6%8E%A8%E8%8D%90%E7%9A%84%E9%A9%B1%E5%8A%A8%E7%AA%81%E5%8F%98%E6%B1%87%E6%80%BB%2F</url>
    <content type="text"><![CDATA[癌种 驱动突变 治疗方案 一线治疗 肺癌 ALK 基因融合 肺癌 RET 基因融合 普拉替尼作为二线治疗的II级推荐，LOXO-292（国内未上市）为III级推荐]]></content>
      <categories>
        <category>NGS</category>
        <category>肿瘤检测</category>
        <category>共识</category>
      </categories>
      <tags>
        <tag>驱动突变</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[流程管理规范]]></title>
    <url>%2F0002.%E5%BC%80%E5%8F%91-04.pipeline%2F%E6%B5%81%E7%A8%8B%E7%AE%A1%E7%90%86%E8%A7%84%E8%8C%83%2F</url>
    <content type="text"><![CDATA[历史材料流程管理优化.pptx 流程版本号规范结合目前肿瘤事业部落地推广的版本管理规范，进行介绍。流程版本号，共计4位。（ v主版本号 . 子版本号 . 阶段版本号. 修正版本号 )上级版本号升级时，所有下级版本号均需归为初始状态（0） 主版本号表示在软件或项目发生重大变更或功能修改时递增。当出现不兼容的 API 或用户界面变更时，应递增主版本号。例如，从版本1.0.0升级到2.0.0表示有重大功能变更或不兼容的修改。 子版本号 （某些模块/方法的重构性升级，需要设计独立的方法学评估）在原有的基础上增加了部分功能，或某些检测模块发生了重要的技术升级/方法迭代（一般需要进行转产答辩），主版本号不变，子版本号加 1，阶段版本号和修正版本号复位为 0，； 阶段版本号 (涉及流程输入/输出格式变动的更改/需求，或修订累计需求较多，不涉及流程整体框架和模块思路的重构)在完成一个阶段性的需求优化或bug修复后，需要对该阶段版本下的所有修订版本对应的内容进行统一的测试评估。完成生信内部的代码复核及测试后，提交需求方进行验收。需求方验收后，将所有的修正版本下的更新内容合并至主分支。同时阶段版本号加1，修正版本号归。所有涉及输入、输出格式变动的更改，均属于阶段版本号起步 修正版本号 (不涉及流程输入/输出格式变动的更改/需求)在接收到前端、交付等需求方提出的流程优化需求后，完成一个相对独立的需求点、或原有bug修复后，主版本号、子版本号和阶段版本号都不变，修正版本号加 1。每个修正版本号对应一个独立的需求： 例如数据库更新，代码bug的修复，不影响分析结果的资源配置调整等]]></content>
      <categories>
        <category>知识沉淀</category>
      </categories>
      <tags>
        <tag>pipeline</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[非小细胞肺癌分子残留病灶专家共识]]></title>
    <url>%2F0001.%E7%9F%A5%E8%AF%86-03.standard%2F%E5%85%B1%E8%AF%86-%E9%9D%9E%E5%B0%8F%E7%BB%86%E8%83%9E%E8%82%BA%E7%99%8C%E5%88%86%E5%AD%90%E6%AE%8B%E7%95%99%E7%97%85%E7%81%B6%E4%B8%93%E5%AE%B6%E5%85%B1%E8%AF%86%2F</url>
    <content type="text"><![CDATA[原文链接非小细胞肺癌分子残留病灶专家共识非小细胞肺癌分子残留病灶专家共识.pdf 背景由中国抗癌协会肺癌专业委员会和广东省临床试验协会/中国胸部肿瘤研究协作组主办的“第18届中国肺癌高峰论坛”于2021年3月6日在广州顺利召开。此次论坛以非小细胞肺癌（non⁃small cell lung cancer，NSCLC）分子残留病灶（molecular residual disease，MRD）为主题，从肺癌MRD 定义、检测以及临床应用三个角度开展相关学术探讨，并最终达成了专家共识。 MRD 在某些情况下也被称为微小残留病灶（minimal residual disease）或可测量残留病灶（measurable residual disease），其概念自血液肿瘤逐步延伸至实体肿瘤，检测对象主要包括有循环肿瘤DNA（circulating tumor DNA，ctDNA）或循环肿瘤细胞（circulating tumor cell，CTC）等。目前在肺癌这一癌种中，MRD 研究仍处于前期研究证据的积累当中，不同研究之间关于MRD的定义及研究方法差别较大，研究结论之间难以完全互通，整体上处于“摸着石头过河”的前期探索阶段。因此，本次肺癌高峰论坛主题与以往有所不同，重在“展望”，即在肺癌MRD研究热潮即将到来之时，综合近年来国内外重要研究结果及争议点，与会专家们进行了详细的讨论和各抒己见的争辩，旨在为今后的肺癌MRD研究制定规范及主体方向，并最后达成了以下五点共识。 本共识的共识级别为： 1A级：基于高水平证据（严谨的meta分析或随机对照试验结果），专家组有统一认识； 1B级：基于高水平证据（严谨的meta分析或随机对照试验结果），专家组有小争议； 2A级：基于低水平证据，专家组有统一认识； 2B级：基于低水平证据，专家组无统一认识，但争议不大； 3级：专家组存在较大争议 共识内容共识一：非小细胞肺癌MRD概念（共识级别：2A） 肺癌分子残留病变，指经过治疗后，传统影像学（包括PET/CT）或实验室方法不能发现，但通过液体活检发现的癌来源分子异常，代表着肺癌的持续存在和临床进展可能； 肺癌分子异常，指在外周血可稳定检测出丰度≥0.02%的ctDNA，包括肺癌驱动基因或其他的Ⅰ/Ⅱ类基因变异 共识二：非小细胞肺癌MRD检测的基本技术要求（共识级别：2B） MRD 检测的基本技术，包括肿瘤先验分析（tumor⁃informed assays，个体化定制或NGS panel）和肿瘤未知分析（tumor⁃agnostic assays，NGS panel和多组学技术），目前均处在探索阶段，需要前瞻性研究确定其敏感性、特异性和预测价值； 基于NGS 的突变检测技术，所选用的多基因panel中必须覆盖患者Ⅰ/Ⅱ类基因变异，基本技术标准是可稳定检测出丰度≥0.02%的ctDNA； 驱动基因阳性的非小细胞肺癌，MRD的分子检测panel应包括其驱动基因； MRD 评估报告中必须包括cfDNA 浓度、ctDNA浓度及所检测基因VAF值； 需要建立针对免疫治疗的MRD标准 共识三：可手术早期非小细胞肺癌MRD的应用（共识级别：2A） 早期非小细胞肺癌患者根治性切除术后，MRD阳性提示复发风险高，需进行密切随访管理，建议每3~6个月进行一次MRD检测； 建议基于MRD开展可手术非小细胞肺癌的围术期临床试验，尽可能提供围术期精准治疗方案； 建议分别探索MRD在驱动基因阳性和驱动基因阴性两种类型患者中的作用 共识四：局部晚期非小细胞肺癌MRD的应用（共识级别：2A） 局部晚期非小细胞肺癌根治性化放疗后完全缓解患者，建议检测MRD，有助于判断预后和制定进一步的治疗策略； 建议开展基于MRD的化放疗后巩固治疗的临床试验，尽可能提供精准的巩固治疗方案。 共识五：晚期非小细胞肺癌MRD的应用（共识级别：2A） 晚期非小细胞肺癌目前缺乏针对MRD的相关研究； 晚期非小细胞肺癌系统治疗后完全缓解患者，建议检测MRD，有助于判断预后和制定进一步的治疗策略； 建议在完全缓解患者中开展基于MRD的治疗策略研究，尽可能延长完全缓解持续时间，使患者能最大获益]]></content>
      <categories>
        <category>NGS</category>
        <category>肿瘤检测</category>
        <category>共识</category>
      </categories>
      <tags>
        <tag>MRD</tag>
        <tag>循证医学</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于靶标指导乳腺癌精准治疗标志物临床应用专家共识(2022版)]]></title>
    <url>%2F0001.%E7%9F%A5%E8%AF%86-03.standard%2F%E5%85%B1%E8%AF%86-%E5%9F%BA%E4%BA%8E%E9%9D%B6%E6%A0%87%E6%8C%87%E5%AF%BC%E4%B9%B3%E8%85%BA%E7%99%8C%E7%B2%BE%E5%87%86%E6%B2%BB%E7%96%97%E6%A0%87%E5%BF%97%E7%89%A9%E4%B8%B4%E5%BA%8A%E5%BA%94%E7%94%A8%E4%B8%93%E5%AE%B6%E5%85%B1%E8%AF%862022%E7%89%88%2F</url>
    <content type="text"><![CDATA[原文链接 专家共识：建议采用标准流程检测乳腺原发灶和转移灶ER、PR、HER2及Ki67，根据临床分型进行预后判断和指导分类治疗，并将生物学标志物纳入TNM分期系统中，以进一步提高分期系统的精确性。新的“复旦四分型”用于指导精准分类治疗有待更大样本的基于我国人群的临床试验验证。 专家共识：建议将CTC纳入cM0（i+）分期，早期乳腺癌患者外周血CTC≥1个提示预后不良。ctDNA可用于监测早期乳腺癌MRD以评估复发风险，根据ctDNA特定突变可指导无法获取肿瘤组织的晚期乳腺癌患者匹配相应的治疗。 专家共识：为了评估遗传风险，建议对相关高风险人群进行遗传咨询及胚系BRCA1 / 2 基因检测。HER2阴性乳腺癌患者建议行BRCA1/2 基因胚系突变检测，以确定PARP抑制剂治疗的优势人群。 专家共识：乳腺癌中PD‐1/PD‐L1抑制剂作为最重要的免疫检查点抑制剂已经改变了三阴性乳腺癌的新辅助和晚期治疗实践。PD‐L1阳性晚期三阴性乳腺癌患者一线可以考虑PD‐1 抑制剂联合化疗方案，早期有高危因素的三阴性乳腺癌也可考虑采用PD‐1抑制剂联合化疗方案进行新辅助治疗。 专家共识：临床可借助pCR 、MP系统、RCB系统、CPS+EG评分等评估早期乳腺癌患者的新辅助化疗疗效，PEPI评分可用于评估新辅助内分泌治疗疗效，以上指标均可用于乳腺癌患者预后评估和指导后续辅助强化治疗方案。 专家共识：STEPP评分可用于预测HR+的绝经前乳腺癌患者的复发风险，CTS5对绝经前和绝经后乳腺癌患者的远处复发转移有良好的预测作用。BCI、PAM50 ROR在HR+/HER2-患者中均具有预后价值，可助力制定精准的术后辅助内分泌强化/延长治疗方案。然而，影响治疗决策的因素复杂且多元，评分系统仅是一种辅助工具，临床治疗决策应从个体实际出发，综合权衡治疗的获益与风险。 专家共识：Oncotype Dx、Mamaprint、RecurIndex和EPclin多基因检测评分在HR+/HER2-早期乳腺癌患者中具有预后判断价值。Oncotype Dx检测的N0期患者，当RS&lt;11分时可考虑豁免化疗；当RS为11~25分时需根据月经情况进行判断；&gt;50岁的患者可考虑豁免化疗，但当RS≥26 分时建议化疗。Oncotype Dx 检测的N1期患者，当RS&lt;26分时需根据月经情况进行判断，绝经前患者在内分泌治疗基础上加用化疗可以降低远处复发率，但无法排除是否受化疗产生的卵巢抑制作用影响；绝经后患者可考虑豁免化疗，但当RS≥26分时建议辅助化疗联合内分泌治疗。MamaPrint检测可将早期乳腺癌区分为高复发风险和低复发风险人群，同时可以避免中等风险的不确定性，其中HR+/HER2-、淋巴结1~3枚阳性、临床判定为高复发风险的患者可应用MamaPrint再次检测，评估为低风险的患者可考虑豁免化疗。RecurIndex可指导乳腺癌患者辅助放疗方案的选择，低复发风险患者建议减免放疗，高复发风险患者建议放疗。EPclin评分可用于区分高复发乳腺癌患者，且高危患者需行术后辅助化疗联合内分泌治疗。 专家共识：HRD 作为泛癌种生物标志物评估PARP抑制剂在临床应用中还处于起始阶段，目前临床上主要通过检测基因组瘢痕或HRR基因突变间接评估HRD状态，但用于评估HRD状态的确切生物标志物尚无统一标准。乳腺癌HRD检测和临床应用的标准化仍任重而道远，但其应用前景值得期待。 专家共识：内分泌治疗是ER+复发/转移性乳腺癌患者优先推荐的治疗手段。分析ESR1 基因突变状态对指导乳腺癌临床精准治疗具有重要意义。综合现有证据，ESR1 突变是ER+乳腺癌继发性耐药的重要机制之一，也是预后不良的指标，而携带ESR1 突变的患者仍可能从氟维司群或联合CDK4/6抑制剂治疗方案中获益。 专家共识：PIK3CA 基因突变可作为预测ER+乳腺癌PI3Kα特异性抑制剂阿培利司疗效的敏感性标志物。现行PCR 检测方法可覆盖大部分PIK3CA 突变形式，故建议采用经认证的试剂和平台常规对复发/转移性乳腺癌患者开展检测；NGS 检测可能不会改变临床实践，但会提供更多的患者基因信息，可以为后续治疗方案的选择提供参考，因此应积极探索NGS在包含PIK3CA、ESR1、HER2等多基因联合检测中的规范应用。 专家共识：FGFR1过表达与乳腺癌不良预后和内分泌治疗耐药密切相关，FGFR1抑制剂联合内分泌治疗的临床研究目前正在进行，有望为HR+合并FGFR1过表达患者提供更有效的治疗手段。 专家共识：CDK4/6抑制剂改善了HR+乳腺癌患者的预后，但是并非所有患者均对CDK4/6抑制剂有反应，且部分对CDK4/6抑制剂敏感的患者也可能产生获得性耐药。CCNE1高表达、Rb缺失、TK活性等在预测CDK4/6抑制剂疗效中具有潜在价值，但仍需进一步验证。 专家共识：HER2是肿瘤增殖、侵袭的重要驱动，除HER2 基因扩增外，HER2 异质性、HER2 低表达、ERBB2 点突变均可影响乳腺癌患者预后及抗HER2治疗反应。以T‐DXd为代表的新一代抗HER2 ADC药物有望对抗HER2低表达和异质性难题。此外，采用泛HER酪氨酸激酶抑制剂，如来那替尼、吡咯替尼可能部分逆转HER2 点突变介导的耐药。 专家共识：PIK3CA/AKT1/PTEN通路不仅在HR+乳腺癌患者中易变异，在三阴性乳腺癌中也常见PTEN缺失等，因此AKT抑制剂等靶向治疗联合化疗在三阴性乳腺癌中仍需进一步确认其疗效和最佳获益人群。 专家共识：PD‐L1、TILs、TMB及MSI可作为乳腺癌患者预后评估及免疫检查点抑制剂疗效预测标志物，但仍需大样本研究验证这些标志物作为免疫治疗伴随诊断标志物及其指导新型免疫治疗策略的应用价值。]]></content>
      <categories>
        <category>NGS</category>
        <category>肿瘤检测</category>
        <category>共识</category>
        <category>乳腺癌</category>
      </categories>
      <tags>
        <tag>中国癌症防治杂志</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[肿瘤突变负荷检测及临床应用中国专家共识（2020年版）]]></title>
    <url>%2F0001.%E7%9F%A5%E8%AF%86-03.standard%2F%E5%85%B1%E8%AF%86-%E8%82%BF%E7%98%A4%E7%AA%81%E5%8F%98%E8%B4%9F%E8%8D%B7%E6%A3%80%E6%B5%8B%E5%8F%8A%E4%B8%B4%E5%BA%8A%E5%BA%94%E7%94%A8%E4%B8%AD%E5%9B%BD%E4%B8%93%E5%AE%B6%E5%85%B1%E8%AF%86-2020%2F</url>
    <content type="text"><![CDATA[共识原文链接2020.10 肿瘤突变负荷检测及临床应用中国专家共识（2020年版）肿瘤突变负荷（tumor mutational burden，TMB）作为一个新兴的生物标志物，在预测肿瘤免疫治疗疗效中的作用越来越受到重视。目前 TMB 的检测方法主要是基于高通量测序平台的全外显子测序和靶向 Panel 测序的拟合算法，但检测方法、阈值和报告格式缺乏统一标准。此外，TMB 值在不同癌种中存在显著差异，也为该标志物在临床中规范应用带来困难。本共识围绕 TMB 的定义、临床意义、检测标准化及与其他免疫标志物如 PD鄄L1、dMMR/MSI鄄H 的关系等要点，结合中国实践，为临床提供 8 条 TMB 检测及应用共识推荐。希望本专家共识可以提高临床医师及检测人员对 TMB 临床意义及检测规范的认识，从而更加准确地解读检测结果，为患者提供更优质的临床服务。 2021.11 肿瘤突变负荷应用于肺癌免疫治疗的专家共识肺癌是全球范围内发病率和死亡率最高的恶性肿瘤之一。免疫检查点抑制剂（ immune checkpoint inhibitors, ICIs），包括程序性死亡受体1（ programmed cell death 1, PD-1）抗体、程序性死亡受体1配体（ programmed cell death ligand 1, PD-L1）抗体和细胞毒性T淋巴细胞相关蛋白4（ cytotoxic T lymphocyte antigen 4, CTLA-4）抗体，给部分晚期肺癌患者带来了显著的生存获益，改变了晚期肺癌的治疗格局。既往研究表明， PD-1/PD-L1抗体在晚期非小细胞肺癌（ non-small cell lung cancer, NSCLC）中的客观缓解率只有20%左右。所以临床亟需可靠的生物标志物协助筛选ICIs潜在获益人群，提高治疗响应率。肿瘤突变负荷（ tumor mutational burden, TMB）是除PD-L1表达以外新兴的免疫治疗标志物。肺癌中PD-L1表达与TMB的相关性不大，评估TMB可扩大免疫治疗的获益人群。然而在临床实践中， TMB的检测、阈值的确定和临床指导策略仍然没有形成规范。本共识将对TMB检测和应用场景给出指导性建议，以促进TMB在肺癌免疫治疗中应用的规范化。 核心内容：肿瘤突变负荷检测及临床应用中国专家共识（2020年版） 专家共识：TMB 一般是指特定区域内体细胞非同义突变的个数，通常用每兆碱基有多少个突变表示（XX 个突变/Mb）。TMB 评估受样本质量和数量、检测基因组大小、生信分析方法等多种因素影响，临床应用前应了解 TMB 的适用范围。不同检测方法获得的 TMB 应进行系统评估,判断是否具有可比性。TMB 数值可反映肿瘤内产生肿瘤新抗原的潜力，与 DNA 修复缺陷密切相关，在多种肿瘤中dMMR和 MSI鄄H 患者具有较高的 TMB。 专家共识：tTMB 是一个新兴的独立 ICIs 治疗疗效预测标志物，与多种肿瘤类型 ICIs 单药或两种ICIs 联合治疗的疗效相关，已证实可作为泛癌种免疫治疗疗效的预测标志物。推荐既往标准治疗后疾病进展且没有更好替代疗法的实体瘤患者，尤其是高 TMB 的患者进行 TMB 检测，有助于扩大免疫治疗获益人群。中国人群 TMB 的独立预测价值仍需更多前瞻性研究验证。 专家共识：目前研究证据显示在 NSCLC 中 bTMB与 tTMB 具有显著相关性，但 bTMB 检测无统一标准。多项回顾性研究发现高 bTMB 与 NSCLC 患者接受单药 ICIs 治疗获益显著相关，但尚未获得高级别前瞻性临床研究证实。 专家共识：推荐使用近期石蜡包埋肿瘤组织样本进行 tTMB 检测，待检测组织应首先完成病理质控并确保恶性肿瘤细胞数能够满足检测要求。为过滤胚系突变对后续 tTMB 评估的影响，应采集患者外周血、唾液或正常组织作为对照样本。建议优先采用NMPA 批 准上 市的 核 酸 提 取 试剂 盒 进 行 基 因 组DNA 提取，tTMB 检测实验室应根据实际需求建立合适的 DNA 样品质控标准和操作流程，对待测DNA样品纯度、浓度及片段化程度进行严格质控。肿瘤原发灶与远处转移灶组织均可用于 tTMB 评估。 专家共识：采用靶向测序 Panel 进行 TMB 评估时，建议与 WES 评估的 TMB 进行一致性评价。靶向测序 Panel 覆盖范围原则上不应约1.0 Mb，最低有效测序深度应逸500伊。建议进行 TMB 检测的靶向测序 Panel 尽可能涵盖患者更多的其他分子遗传信息，包括可指导靶向治疗的驱动基因突变、与基因变异产生相关的免疫治疗正向预测因子以及可能的免疫治疗负向预测因子。目前已有多款 NGS 测序仪获国家 NMPA 批准用于临床基因检测，不同实验室可依据样本量、时效要求选择不同测序平台。 专家共识：基于靶向测序 Panel 的 TMB 检测应以 WES 检测为金标准，纳入影响蛋白质编码的体细胞突变，应保证检出突变频率逸5%的体细胞突变，以保证 TMB 检测值的准确性和稳定性；应依托 ICIs疗效随访数据库对基因组比对和突变检出算法开展标准化研究；Panel 检测区域可影响 TMB 值，应通过至少 1 000 例 WES 数据予以校正。同时建议使用对照样本过滤胚系变异。 专家共识：TMB 值在不同癌种中存在显著差异，应依据 ICIs 临床疗效确定阈值，才能最大可能筛选出 ICIs 治疗的潜在获益人群。值得注意的是，不同靶向测序 Panel 的 TMB 检测体系之间 TMB 阈值不能通用。 专家共识：TMB 报告内容除重点描述 TMB 计算原则和数值外，还应针对癌种的免疫治疗意义进行解读；同时还需系统评估 Panel 检测的各种驱动基因突变情况，以全面解患者的肿瘤生物学特征，建议应用分子肿瘤诊治专家组模式（MTB）进行临床辅助决策。 肿瘤突变负荷应用于肺癌免疫治疗的专家共识 共识一： W ES是TMB检测的金标准，但目前测序成本和分析难度较高。 NGS panel与WES的TMB检测结果具有高度一致性，经过验证的NGS panel可作为临床检测TMB的替代方式。 共识二：通过病理质控的石蜡包埋肿瘤组织方可用于TMB检测。在肿瘤组织获取困难或石蜡包埋样本中肿瘤细胞含量不足的情况下，晚期NSCLC患者可选择外周血进行基于ctDNA的bTMB检测。 共识三：既往未接受过免疫治疗的晚期NSCLC患者，在接受免疫单药治疗前推荐进行TMB检测。 TMB预测SCLC一线免疫治疗疗效的临床证据尚不充分，暂不作为提示免疫一线治疗疗效标志物，但可考虑作为二线及后线免疫治疗的疗效标志物。 共识四：不推荐PD-1/PD-L1抗体联合化疗的晚期NSCLC或SCLC患者接受TMB检测。 共识五：暂不推荐TMB用于预测免疫新辅助治疗疗效。 共识六：目前尚没有公认的区分高、低TMB的cut-off值，推荐各免疫治疗药物根据各自临床研究数据确定cut-off值，并进行前瞻性验证。 共识七： TMB检测报告应呈现纳入TMB计算的基因变异类型、 TMB排序以及支持TMB排序的参考数据库，并提供辅助临床决策所需要的医学证据 共识八：建议联合PD-L1表达、免疫治疗相关基因、组织和循环中的免疫细胞等信息，对TMB指导肺癌免疫治疗进行综合解读。]]></content>
      <categories>
        <category>NGS</category>
        <category>肿瘤检测</category>
        <category>共识</category>
      </categories>
      <tags>
        <tag>注释</tag>
        <tag>中国癌症防治杂志</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[共识-肿瘤二代测序临床报告解读共识]]></title>
    <url>%2F0001.%E7%9F%A5%E8%AF%86-03.standard%2F%E5%85%B1%E8%AF%86-%E8%82%BF%E7%98%A4%E4%BA%8C%E4%BB%A3%E6%B5%8B%E5%BA%8F%E4%B8%B4%E5%BA%8A%E6%8A%A5%E5%91%8A%E8%A7%A3%E8%AF%BB%E5%85%B1%E8%AF%86%2F</url>
    <content type="text"><![CDATA[参考原文循证医学官网：肿瘤二代测序临床报告解读共识肿瘤二代测序临床报告解读共识.pdf 随着新型治疗药物的研发以及多学科综合治疗模式的优化，传统的病理分型及检测方法已经不足以满足临床需求，二代测序（next generation sequencing，NGS）已成为中国肿瘤医生常用的检测手段。为进一步协助临床医生理解临床靶点或驱动基因相关变异注释及解读、梳理 NGS 报告的逻辑、提升抓取关键信息，二代测序临床报告解读肿瘤学专家组对国内外 NGS 检测最新进展进行认真分析、讨论和总结，在《二代测序临床报告解读指引》基础上增加部分 NGS 报告解读示例，同源重组缺陷（homologous recombination deficiency，HRD）及微小/分子/可测量残留病灶（minimal/molecular/measurable residual disease，MRD）相关内容解读，制定了《肿瘤二代测序临床报告解读共识》，最终帮助临床医生做出正确的临床决策。 目前有多个循证分级系统可用于指导基因体细胞变异的临床解读，包括 2017 年美国分子病理学协会（Association for Molecular Pathology，AMP）/美 国 临 床 肿 瘤 学 会（American Society of ClinicalOncology，ASCO）/美 国 病 理 学 家 协 会（College ofAmerican Pathologists，CAP）联合制定的体细胞变异解读指南；2018 年欧洲肿瘤内科学会（EuropeanSociety for Medical Oncology，ESMO）发 布 的 分 子靶 点临床可操作性量表（ESMO Scale for Clinical Actionability of molecular Targets，ESCAT）；以及纪念斯隆⁃凯特琳癌症中心（Memorial Sloan Kettering Cancer Center，MSKCC）的 精 准 医 疗 肿 瘤 数 据 库（Precision Oncology Knowledge Base，OncoKB）证据等级规则。总体而言，无论哪个分级系统都遵循一些共性原则，包括循证、跨癌种处理等，故其中并无优先推荐者。临床医生在阅读 1 份 NGS 报告时应先了解其变异解读依据的证据分级原则及其采用知识库的局限性，以帮助自己更好的理解报告内容。 根据 2017 年 AMP/ASCO/CAP 联合制定的体细胞变异解读指南［6］，体细胞变异在不同癌种中对应的药物敏感性证据分为 4 个等级： A 级，美国食品药品监督管理局（Food and Drug Administration，FDA）批准或专业临床指南推荐； B 级，经具有足够统计学效能的临床研究证实、获得该领域专家共识； C 级，其他癌种中的 A 级证据（跨适应证用药）、或已作为临床试验的入组标准； D 级，临床病例报道或临床前证据支持。 体细胞变异对特定肿瘤的诊断及预后价值，亦给出相应分级： A 级，专业指南中定义的特定肿瘤的诊断预后因子 ； B 级，经具有足够统计学效能的临床研究证实其诊断/预后价值； C 级，多项小型研究支持其诊断/预后价值； D 级，小型研究或个案报道提示其辅助诊断/预后价值（独立或联合其他标志物） 基因变异按照其临床意义的重要性分为 4 类变异： Ⅰ类变异，有重要的临床意义，具有 A 级或B 级证据； Ⅱ类变异，有潜在的临床意义，具有C级或 D 级证据； Ⅲ类变异，临床意义不明； Ⅳ类变异，无害或可能无害， 该体细胞变异解读指南在国内影响范围最广，许多第三方 NGS 检测公司的临检报告即遵循该指南的分级原则对基因变异进行解读，详见表1 一份结构化循证报告的背后，需要基于特定逻辑建立临床解读知识库框架，并通过对开源知识库及海量科学文献的信息甄别、分级、编辑，不断完善机构内部临床解读知识库。通过生物信息分析流程将基因变异识别、注释、过滤后，可报告变异进入临床解读知识库进行变异、癌种、证据匹配，形成可读的结构化 NGS 临床报告［9］，最终对其进行解读及实施临床决策，NGS 报告解读决策树详见图2 同时文中还针对下列问题进行了说明： 针对“全阴报告”的解读逻辑 针对 NGS 报告中基因融合、外显子跳读突变检测的说明与突变丰度及拷贝数的解读逻辑 基于组织样本的NGS检测，突变频率的解读方式 基于外周血cfDNA的NGS检测 基于外周血ctDNA MRD的检测 针对单个变异对比多基因多变异的解读逻辑 胚系突变分级注释 NGS可报告的基因组标签 肿瘤突变负荷 微卫星不稳定性 同源重组修复缺陷 可报告范围及质量控制]]></content>
      <categories>
        <category>NGS</category>
        <category>肿瘤检测</category>
        <category>共识</category>
      </categories>
      <tags>
        <tag>循证医学</tag>
        <tag>解读</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Standards and guidelines for the interpretation of sequence variants]]></title>
    <url>%2F0001.%E7%9F%A5%E8%AF%86-03.standard%2FStandard-Guidelines-interpretation.of.sequence.variants%2F</url>
    <content type="text"><![CDATA[参考文献Standards and guidelines for the interpretation of sequence variants: a joint consensus recommendation of the American College of Medical Genetics and Genomics and the Association for Molecular Pathology 遗传变异分类标准与指南 遗传变异分类标准与指南整理PPT材料 工作组 ACMG、 分子病理协会(the Association for Molecular Pathology, AMP) 美国病理学家协会(the College of American Pathologists, CAP)应用范围基因分型、单基因、基因包、外显子组和基因组. 使用特定标准属于来描述孟德尔疾病相关的基因变异: ACMG Standards and Guidelines 遗传变异分类标准与指南 pathogenic 致病的 likely pathogenic 可能致病的 uncertain significance 意义不明确的 likely benign 可能良性的 benign 良性的 建议临床分子基因检测应在符合临床实验室改进修正案(CLIA)认证的实验室中进行, 其检测结果应由通过职业认证的临床分子遗传学家或分子遗传病理学家或相同职能的专业人员解读. 术语突变： 是指核苷酸序列的永久性改变 多态性：指频率超过1%的变异。 命名标准的基因变异命名由人类基因组变异协会(the Human Genome Variation Society,HGVS)维护和版本化(https://www.hgvs.org/mutnomen), 除非另有说明, 一般推荐该命名法作为确定变异命名的首要准。 当描述变异时，可利用这些工具提供正确的HGVS命名(http://mutalyzer.nl)。 编码命名应该使用翻译起始密码子ATG中的A作为位置编号1来描述。 协会支持的参考转录本通常可以通过LRG数据库(http://www.lrg-sequence.org)、CDS 共识数据库(https://www.ncbi.nlm.nih.gov/CCDS/CcdsBrowse.cgi) 、人类基因突变数据库(http://www.hgmd.cf.ac.uk) 、ClinVar(http://www.ncbi.nlm.nih.gov/clinvar)或特异基因座数据库来确定 文献及数据库使用在使用人群数据库时,须明确数据库收录的是健康群体的信息还是患病群体的信息; (如能确认)数据库是否收录了同一家庭多名成员的信息以及数据库收录的受试者的年龄范围. 当使用数据库时, 临床实验室应做到: 确定数据库的更新频率, 确定数据库收录相关数据时是否进行了校勘, 以及采用什么方法进行数据校勘; 确认采用HGVS 命名体系, 并确定􁧿述变异的基因组版本和转录本参考序列; 确定数据分析准确度的验证程度(如变异是源自于低覆盖的新一代测序,还是通过了Sanger 测序验证), 并分析用于评估数据准确度的各种指标, 要获得这些信息可能需要阅读相关的文献; 确定收录对象的来源及其唯一性. 生物信息学计算预测程序各种公共和商业化计算机工具可以辅助解读序列变异. 每种工具使用的算法可能有差异, 但都会包含序列变异在核苷酸及氨基酸水平上作用影响的判断, 包括变异对主要转录本, 可变转录本, 其他基因组元件影响作用的确认. 主要分为两类: 一类可以预测错义变异是否会破坏蛋白质的功能或结构; 另一种可以预测是否影响剪接 多数算法预测已知致病的错义突变的准确率能达65%~80%[12]. 但是大多数工具的特异性较低, 导致有些错义改变被过度预测为有害突变, 而且对于影响较小的错义变异的预测也不可靠[14]. 目前临床实验室常用的错义变异解读工具有PolyPhen 2[15],SIFT[16]和MutationTaster[17]. 序列变异解读的拟定标准以下评估变异证据的方法是用了解释在临床诊断实验室中具有疑似遗传(主要指孟德尔遗传)疾病患者的变异. 并不适用于解读体细胞变异、药物基因组(PGx)变异、或者是多基因非孟德尔复杂疾病相关的基因变异. 在外显子组或基因组研究中, 对候选基因(意义不明确的基因(GUS))应用这些准则时应当谨慎(见下面注意事项), 因为本指南目的不是满足鉴定新致病基因的研究需求.本指南提供了两套标准: 一是用于对致病或可能致病的变异进行分类(表3), 另一是用于对良性或可能良性的变异进行分类(表4). 致病变异标准可分为非常强(very strong, PVS1), 强(strong, PS1~4); 中等(moderate, PM1~6), 或辅助证据(supporting,PP1~5). 良性变异证据可分为独立(stand-alone, BA1), PVS1 极强致病性变异从业人员需谨慎考虑以下原则: 当将该类变异归类为致病性时, 需确认无功能变异(null variants)是已知的致病机理, 且与该疾病的遗传模式相一致. 例如, 有些基因(如许多肥厚性心肌病基因)只有杂合错义突变时才致病, 而杂合无功能变异却是良性的. 仅基于这一项证据来看, 对显性肥厚性心肌病来说, MYH7 基因上出现一个新的杂合无义突变不一定是致病的, 而CFTR 基因上出现一个新的杂合无义突变则有可能是一个隐性致病变异. 当文献中将3′远端下游截短变异注释成致病突变时, 要特别小心. 特别是当所预测的终止密码子出现在最后一个外显子, 或者出现在倒数第二个外显子的最后50 个碱基对时, 这种无义突变介导的转录降解[22]可能不会发生, 这个蛋白很可能会表达.据此所预测的截短蛋白的长度也是致病性评估的因素, 但这些变异未经功能分析是无法进行判定的. 就剪接位点变异而言, 因外显子剪切位点的供体/受体位点改变或产生了新的剪切位点, 从而可能导致外显子丢失、缩短, 也可能会使内含子序列变成外显子部分. 虽然剪切位点变异可能被预测为无功能变异, 然而该变异类型造成的影响需要通过RNA 或蛋白质功能分析确认. 还必须考虑可读框内缺失/插入的可能性, 其长度变化较小(PM4), 可以保留蛋白质的关键结构域, 因此导致轻微或中性效应,或功能获得效应. 基因会有不同的转录本, 哪一种转录本与生物学功能相关, 在哪些组织会表达哪些转录本, 这些都是需要进行重点考虑的. 如果一个截短变异只限于一个或并非所有转录本, 则必须谨慎考虑到可能存在其他同功型蛋白质, 防止过度解释. 如果发现一个无功能变异位于某个外显子上, 而该外显子先前无致病变异报道, 那么该外显子可能被选择性剪切了, 此时需要谨慎考虑该变异的致病性. 当预测的截短变异是偶然发现时(与检测指征无关)则应特别小心, 在这种情况下该位点致病的可能性非常低. PS1 极强致病性变异多数情况下, 尤其是当致病机制是蛋白质功能发生改变时, 如已确定某一错义变异是致病变异, 应考虑到与其位于同一变异位点的不同形式的碱基改变也可能产生相同的错义突变结果——氨基酸改变相同(如c.34G&gt;C(p.Val12Leu)和c.34G&gt;T(p. Val12Leu)), 那么, 这些变异也应是致病突变. 需要重点注意:变异可能不是通过改变氨基酸的水平, 而是通过改变DNA 的序列来发挥作用, 例如, 破坏剪接位点(可通过软件分析确定), PS2 PM6 新发变异当将一个新发变异(父母样本检测结果阴性)归类为强的致病证据时, 需要满足以下条件: (i) 身份检验表明患者的父母是其生物学父母. 注意如果父母的身份是假定的而没有被证实, 则判定为PM6; (ii)患者的家族史符合新发变异特征. 例如, 显性遗传病患者的父母均未患病. 在存在生殖细胞嵌合现象时也可能有1 个以上同胞患病; (iii) 患者的表型与变异基因异常引起的表型相吻合. 例如患者具有特殊面容、多毛和上肢缺陷(即Cornelia de Lange 综合征), 检测到NIPBL 基因的新生突变即为强致病证据, 而患者仅表现为非特异性的发育迟缓, 通过外显子组测序发现的该基因的新发变异, 则判断此变异致病性的证据较弱. PS3 BS3 功能研究功能实验研究是一种研究变异致病性的非常强大的工具, 然而并非所有的功能研究都能有效地预测基因或蛋白的功能. 重点注意功能实验的有效性、重复性和稳定性应重点考虑, 这些参数用来评估功能实验的分析性能以及判定样本诊断信息的完整性, 该完整性容易受标本采集的方法及时间、存储及运输的影响. 评估变异在剪接位点、编码序列、非翻译区以及更深的内含子区域的影响时, 对变异在信使RNA 水平(如信使RNA 的稳定性、加工或翻译)进行评估, 可以提供丰富的信息. 相关的技术方法包括对RNA 和/或互补DNA 衍生物进行直接分析, 以及体外微小基因剪接分析 PS4 PM2 BA1 BS1 BS2 变异频率及对照人群的使用通过搜索公共人群数据库(如千人基因组数据库, NHLBI 外显子测序数据库, EXAC 数据库; 表1),并利用已发表文献中相同种族的对照数据进行基因变异频率分析(译者注: 此条款在指南更新时会有修改), 通过分析变异基因在对照人群或普通人群中的携带频率, 有助于评估该变异的潜在致病性. NHLBI外显子测序数据库来源于白种人和非裔美国人群,根据其数据覆盖量能够识别是否存在基因变异. 尽管千人基因组数据库缺乏评估基因变异能力, 但它囊括了更多的种族人群, 因此其数据具有更广泛代表性的. EXAC 数据库近期发布了一组来源于不同人群的6 万多个外显子组的等位基因频率数据, 包括了大约三分之二的NHLBI 外显子测序数据. 一般情况下, 某一等位基因在对照人群的频率大于疾病预期人群(表7)时, 可认为是罕见孟德尔疾病良性变异的强证据(BS1), 如果频率超过5%时, 则可认为是良性变异的独立证据(BA1). 此外, 如果疾病发生在早期,且变异在健康成人中以隐性(纯合子)、显性(杂合子)或X-连锁(半合子)的状态存在, 那么这就是良性变异的强证据(BS2). 如果数据库中未能检出变异的存在,应该确认建立该数据库采用的测序读长深度是否足以检测出该位点上的变异. 如果在一个大样本的普通人群或队列数据的对照人群(&gt;1000 人)中变异不存在(或隐性遗传的突变频率是低频), 并且携带此变异的患者与对照人群为同一种族, 那么可以认为该变异是致病性的中等证据(PM2). 许多良性变异是“个体化的”(即个人或家系独有的), 因此即使在相同种族的人群中缺乏也不能作为致病性的充足甚至强的证据 PM1 热点突变和/或关键的、得到确认的功能域某些蛋白结构域对蛋白质的功能起到了关键作用, 如果在这些结构域上发现的所有错义突变均已被证实为致病突变, 且这些结构域中一定没有已知的良性突变, 那么这就能作为致病的中等证据. 此外, 基因中某些功能尚未确定的区域已被证实存在许多突变热点, 若突变发生在基因突变热点上, 且一个或多个邻近残基中存在较高频率的已知致病突变,那么这也能作为致病的中等证据. PM3 BP2 顺式/反式检测检测双亲样本以确定变异在基因上以顺式(incis)(位于基因的同一拷贝)或是反式(in trans)(位于基因的不同拷贝)方式排列, 这对评估变异的致病性非常重要. 例如, 当两个杂合变异发生在隐性遗传病的致病基因上时, 如果已知其中一个变异为致病变异,那么当另一个待分类变异与其呈反式排列时, 这可以作为待分类变异的中等致病证据(PM3). 另外, 若待分类变异与多个已知致病变异均呈反式排列, 则该证据可升级为强致病证据. 但是, 若待分类变异在普通人群中存在, 则需要用统计学方法判断该现象是否为随机共发生事件. 相反, 当已知致病变异与另一个待分类变异呈顺式排列时, 这可以作为待分类变异的良性支持证据(BP2). 如果发生在隐性遗传病致病基因上的两个杂合变异的致病性均未知, 那么确定它们以顺式或是反式排列, 并不能为判断其中任一变异的致病性􁨀供更多信息. 但是, 如果两者以顺式排列, 则该基因两个拷贝均受影响的可能性将会降低.对于显性遗传病而言, 若待分类变异与致病变异呈反式排列, 则可作为该变异的良性支持证据(BP2); 对于特定研究成熟的疾病模型, 甚至可以考虑将其作为独立良性证据(如CFTR 相关变异的评估) PM4 BP3 由于框内缺失/插入和终止密码子丧失导致的蛋白长度改变相较于单一的错义突变所导致的蛋白质长度变化, 一个或多个氨基酸的缺失或插入、以及由终止密码子变为翻译氨基酸的密码子(如终止密码子丢失)而导致的蛋白质延长更可能破坏蛋白质功能. 因此,框内缺失/插入以及终止密码子丢失可作为中等致病证据. 缺失、插入或延伸范围越大, 缺失区域的氨基酸越保守, 则支持致病的证据越强. 相反, 在重复区域或在进化中不是很保守的区域中小的框内缺失/插入致病的可能性较小 PM5 同一位置新的错义变异如果一个新发错义突变发生在一已知致病突变导致相同氨基酸改变的位置上( 如Trp38Ser 和Trp38Leu), 那么可作为中等致病证据(但不能假定一定是致病的), 尤其当新的突变比已知致病错义突变更保守时. 此外, 不同的氨基酸变化可能导致不同的表型. 例如, FGFR3 基因编码的Lys650 残基的不同变化与不同的临床表型相关: p.Lys650Gln 或p.Lys650Asn 会导致轻度软骨发育不良; p.Lys650Met会导致严重的软骨发育不全伴发育迟缓和黑棘皮病;p.Lys650Glu 会导致2 型发育异常及致命的骨骼发育不良. PP1 BS4 共分离分析在使用家系中变异的共分离现象作为致病性证据时需谨慎. 事实上, 一个与某种表型相关的特定变异在某一家系中的共分离现象是位点与疾病连锁的证据, 而不是变异本身致病性的证据. 一个已经发表的统计方法显示, 在某个家系中鉴定的变异可能与真正的致病变异是连锁不平衡的. 统计模型考虑到了年龄相关的外显率和拟表型率, 一些新的方法也将生物信息分析预测以及与已知致病突变共存作为致病性的单独定量指标. 将远亲纳入统计之中是很重要的, 因为与核心家系成员相比, 他们不太可能同时有该疾病和变异. 对整个基因进行测序(包括整个内含子和5′和3′非编码区)可排查其他致病变异或另一个可能致病的变异的存在. 除非仔细评估基因位点, 否则非致病变异可能被错误地认为是致病变异.当目标基因的特定变异在多个患病的家系成员中以及不同种族背景的多个家系中与表型或疾病共分离时, 则其作为致病的证据不太会受到连锁不平衡和确认偏倚的影响. 在这种情况下, 该标准可以作为中等或强致病证据而不是支持性证据, 其强度取决于共分离的程å度.另一方面, 一个变异与表型并不共分离时, 为其非致病的强证据. 需要进行仔细的临床评估来排除正常个体的轻度症状和可能的拟表型(患者表型由非遗传或不同的遗传原因引起). 此外, 需确认生物学家庭关系来排除收养、非生父、精子和卵子捐献以及其他非生物学关系. 同时, 外显率下降和年龄依赖性的外显率也必须考虑, 以确保无症状家系成员是真正的无症状.在临床实验室进行共分离的统计评估可能并不容易, 当鉴定了合适的家系时, 为了确保建模合适,并避免得出变异与疾病相关性的错误结论, 鼓励临床实验室与统计或群体遗传学专家合作. PP2 BP1 变异谱许多基因具有明确的致病变异和良性变异谱.在某些基因中, 错义突变是导致疾病的常见原因, 且该基因上的良性突变非常少, 那么这种基因上的新发错义突变可作为致病变异的支持证据(PP2). 相反,有些基因致病的唯一已知变异是截短突变, 该基因上的新发错义突变可作为良性的支持证据(BP1). 例如, ASPM 基因的截短变异是该基因引起常染色体隐性遗传小头畸形的主要致病变异类型, 且该基因发生错义多态性突变的频率高, 因此ASPM 基因上的错义变异可认为是良性影响的支持证据. PP3 BP4 生物信息分析数据不能过分相信生物信息分析所得到的结果, 特别是不同的生物信息算法依赖于相同或相近的数据进行预测, 并且大多数生物信息算法未被已知致病变异验证过. 此外, 相同算法对不同的基因的预测结果可能完全不同. 如果不同种类算法的分析预测结果一致, 那么生物信息分析结果可以作为支持的证据. 如果绝大多数算法的预测结果不一致, 则这些预测的结果不能用于对变异进行分类. 若某一变异引起的氨基酸改变, 在多个非人哺乳动物物种不太保守的区域中出现, 说明该变异可能不会损害功能, 可以作为良性解读的强的证据. 然而, 如果某基因已在人类中发生进化(如参与免疫功能的基因), 那么在判定该基因在非保守区域中发生的变异为良性时必须小心. PP4 表型支持考虑到几乎所有接受疾病针对性测试的患者都有某种表型, 通常, 不将患者表型与某个基因临床特征谱匹配作为判断致病的证据. 但是, 如果满足以下条件, 患者的表型可作为支持证据: (i) 临床检测的灵敏度高, 大多数带有该基因致病突变的患者都被检测为阳性; (ii) 患者有某种明确的综合征的症状,与其他临床表现几乎无重叠(如戈尔林综合征包括基底细胞癌、掌跖坑和牙源性角化); (iii) 该基因通常不存在太多的良性变异(可通过外显子组等人群测序确定的良性变异); (iv) 家族史与疾病遗传方式一致. PP5 BP6 可靠的来源现在有越来越多可靠来源(如长期专注于某一疾病领域的临床实验室)的致病性分类信息被分享在数据库中, 但分类判断所依据的证据往往并未􁨀供或者很难获取. 在这种情况下, 如果分类信息是近期􁨀交的, 那它就可以作为一个单独的支持证据. 然而,还是鼓励实验室共享分类的判断依据, 并与􁨀交者进行沟通以评估和创建分类证据. 如果能获得证据,则不应使用这一条款, 而是应该使用相关的证据. BP5 对共发变异的观察一般情况下, 当某一变异是在一个有明确的遗传病因的疾病患者中被观察到时, 可作为将该变异解读为良性的证据. 不过, 也有例外. 某一个体可以是某一不相关隐性遗传疾病致病变异的携带者, 因此本证据与隐性遗传性疾病相比, 更支持显性遗传性疾病基因良性变异的分类. 此外, 有些疾病当具有多个变异可以导致更严重的疾病. 例如, 在一个具有严重表型的显性遗传患者中鉴定了两个变异, 一个是致病的, 一个是新的变异, 父母中的一个也有轻微的疾病, 这种情况下, 必须考虑新的变异致病的可能性, 且新的变异使先证者表型加重. 在这种临床情况下, 观察到的第二个新的变异不应分类为良性变异,(尽管在无进一步证据的前􁨀下也不认为该变异是致病的). 最后, 有些疾病已知为多基因遗传模式, 如Bardet-Beidel 综合征, 在第二个基因座位上的额外变异也有可能是致病的, 但应谨慎进行报告. BP7 同义变异人们逐渐认识到经典的剪接序列以外的剪接错误是一类重要的致病机制, 特别是对那些功能丧失为其常见致病机制的基因. 因此, 在假设同义核苷酸改变没有影响时应持谨慎态度. 然而如果核苷酸位置进化不保守, 且剪接评估算法预测其对剪接一致序列没有影响, 也不会产生新的经典剪接序列, 那么剪接影响的可能性就比较小. 因此, 如果生物信息分析证据支持(BP4), 可将新发同义变异分类为可能良性. 然而, 如果生物信息分析证据表明剪接可能有影响或怀疑有影响(例如, 发生在隐性遗传病致病基因上, 且与已知致病突变呈反式排列的变异), 那么在有功能评估可以􁨀供更确切的对影响的评估, 或者得到其他可排除该变异致病作用的证据之前, 该类变异应该归类为意义不明确. 序列变异报告报告应该使用清晰的语言书写, 避免使用医学遗传学术语, 当必须要使用时需指明所用术语的定义。 报告应包含所有的检测基本要素:, 结构化的结果 解释 参考文献 检测方法 适当的免责声明. 结果应根据HGVS 命名规则(见命名部分)列出变异.基本内容包括： 核苷酸(基因组和cDNA)和蛋白质水平的命名 基因名称 疾病 遗传模式 外显子 合子性 变异的分类. 若亲本来源明确, 也可包括在内. 当报告外显子组或全基因组测序结果, 或偶尔报告包含基因数目较多的疾病基因包检测结果时, 将变异按“与表型明确相关的疾病基因的变异”、“与表型可能相关的疾病基因的变异”及(在适当情况下)“附带(次要)发现”进行分类可能有益。 解读解读应包含对变异检测结果进行分类的证据,包括编码蛋白的功能影响预测, 以及检测所发现的变异是否可能全部或部分地解释患者的临床表型.报告也应包括对临床医生的建议, 这些建议包括一些需补充的临床检测, 如对患者进行细胞酶学/功能的检测, 以及对患者家系其他成员进行的变异检测,以便为进一步解读变异检测结果􁨀供支持. 解读应当包括检测结果部分􁧿述的全部变异, 以及其他附加信息. 对于各个变异需要注明是否已经在先前的文献、疾病病例或对照数据库中有过报道. 在报告结尾处需要列出对变异检测结果分类时所引用的全部参考文献和信息. 方法学报告中应说明使用的实验方法、检测所涉及的变异类型、检测过程的难点, 以及检测变异所使用的方法的局限性. 需要说明的实验方法应包括核酸的获取方法(如聚合酶链式反应、捕获、全基因组扩增等)以及核酸的检测方法(如双向Sanger 测序、下一代测序、染色体基因芯片、基因分型技术等), 这些信息可以为医务工作者􁨀供必要的信息, 以帮助其决定是否需要追加实验来跟进这些检测结果. 方法部分还应包括人类基因组组织基因命名委员会批准的正式基因名称、转录本的RefSeq 登录号和所参考的基因组版本. 对于大的基因包, 基因水平的信息可以通过引用URL 来加以说明. 实验室还可以选择增加对检测过程中常见问题(如样本质量问题、样品混合污染等)的免责声明. 患者维权团体、临床实验和研究的获取尽管不提倡在实验室报告中对患者􁨀供具体临床指导, 但是在报告中􁨀供对于检测结果分类的总体信息(如全部阳性检测结果)是恰当且有益的. 大量病人群体和临床试验现在可用于多种疾病的支持和治疗. 实验室可以选择将此信息添加到报告的正文或附加信息, 并且与报告一起发送给医务工作者. 在遵守医疗保险便携性和责任法案(HIPAA)保护患者隐私的前􁨀下, 当某一变异检测结果被归为意义不明确时, 实验室可尝试帮助医务工作者和特定的疾病研究小组建立联系. 变异再分析随着新的变异证据增加, 现有的分类标准需要修订. 例如, 当大样本的有效的人群变异频率被报道后, 许多原本意义不明确的变异, 可以因为明确意义而进行重新分类, 而检测家系中其他成员的结果也可以导致重新分类.随着检测变异数量的增加及检测范围的扩大,无论是全外显子检测还是全基因组测序, 都可以得到数以百万的变异信息量. 如果实验室缺乏有效的分析方法和足够的文献数据库支撑, 将无法进行变异再分析. 为了满足医务人员和患者的实际需求, 实验室应该开展基因检测数据再分析, 并明确再分析是否产生额外费用. 应该鼓励实验室为帮助医务人员和患者而不断开发更新信息的新途径[31,32].当报告中有针对主要指征的基因中存在临床意义不明的变异, 在实验室又无法及时􁨀供更新的数据时, 建议医务人员定期查询其不明意义的变异结果是否被更改. 另一方面, 鼓励实验室在对变异的分类有重要变化时(如致病性或良性的变异被修改)必须主动及时地更新报告. 关于医生对病人报告更新方面的责任, 可详见ACMG 有关指南. 验证对于孟德尔疾病的致病或可能致病变异需进行正交法验证. 具体方法包括但不限于以下几种: 重新取样和检测、检测父母的变异情况、限制性内切酶消化、对于目标区域重新测序或使用另一种基因分型技术。 特殊考虑]]></content>
      <categories>
        <category>NGS</category>
        <category>肿瘤检测</category>
        <category>标准化</category>
      </categories>
      <tags>
        <tag>Standards and guidelines</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Standards and Guidelines for the Interpretation and Reporting of Sequence Variants in Cancer]]></title>
    <url>%2F0001.%E7%9F%A5%E8%AF%86-03.standard%2FStandard-Guidelines-Interpretation.and.Reporting.of.Sequence.Variants.in.Cancer%2F</url>
    <content type="text"><![CDATA[参考文献Standards and Guidelines for the Interpretation and Reporting of Sequence Variants in Cancer 整理汇总PPT材料 摘要下一代基于测序的癌症测试的临床实验室的广泛实施突出了标准化实验室间分子结果的解释和报告的重要性和潜在的好处。分子病理学协会召集了一个多学科工作组，负责评估基于下一代测序技术的癌症检测的现状，并建立体细胞序列变异的标准化共识分类，注释，解释和报告惯例，并由美国医学遗传学和基因组学学院联络代表，美国临床肿瘤学会和美国病理学家学院。根据专业调查，文献综述和工作组主题专家共识的结果，提出了一种基于体细胞序列变异的临床意义进行分类的四级系统：一级，具有很强临床意义的变异； II级，具有潜在临床意义的变体； III级，临床意义未知的变体；和第四级，变体被视为良性或可能良性。癌症基因组学是一个快速发展的领域。因此，应不断评估治疗，诊断或预后中任何变异的临床意义。报告基因组变异应遵循标准命名法，并明确描述测试方法和限制。临床建议应简明扼要，并与组织学和临床发现相关联。 分类 tier I, variants with strong clinical significance; tier II, variants with potential clinical significance; tier III, variants of unknown clinical significance; tier IV, variants deemed benign or likely benign. 导读基于二代测序技术的肿瘤检测已越来越多的应用于临床实验室中，但目前在不同实验室间存在检测方法、报告内容等方面的差异，这对遗传检测的解读以及普及应用造成了一定的影响。因此，在不同实验室间建立统一的分子检测结果的解读和报告标准，及建立行业标准，显得尤为迫切。 在2015年春天，在美国专门成立了一个以临床实验室为核心的工作组，其组成包含了分子病理协会（AMP）、美国医学遗传学与基因组学协会（ACMGG）、美国临床肿瘤学会(ASCO）与美国病理学家协会（CAP)的一线专家，该工作组的主要工作为对肿瘤及疑似肿瘤相关的序列变异检测建立检测标准并在行业达成共识。 该工作组首先对北美地区超过40家的临床检测实验室进行了问卷调查，结果显示不同实验室在检测组织类型、检测基因数量、是否检测肿瘤组织全外显子组或全基因组、以及其他细节方面都存在较大差别，此外在不同单位的检测报告的报告内容方面也存在较多差异。该工作组认为，为医疗机构提供分类的遗传变异报告对病人及整个医疗行业都极为重要，包括：提供精确的肿瘤对靶向治疗反应性信息；建立国家级别的医疗指南；以及与临床试验合作，对建立不同实验室间的通用标准提供支持。基于以上这些考量，工作组专家们根据已有数据、文献报道和专业知识，给出以下指南建议. 对多个公司的报告情况调研结果如下 图1 AMP对NGS技术及NGS结果解读的调研 A: MAF阈值. B: 变异分类数目 C:报告中是否包含治疗性建议 . D: 报告中是否包含潜在的生殖细胞突变. E: 报告是否包含变异等位频率Variant allele frequency (VAF) F: 报告是否包含基因组坐标 G: 报告是否包含转录本ID(Transcript accession) H: 报告是否包含不符合质控的基因/区间 数据库 Genomic Databases随着越来越多的针对各种肿瘤类型的大规模基因组测序项目的发布，全球正在产生大量的基因组信息并将其整合到许多公共数据库中（表1）。例如，美国国家癌症研究所（National Cancer Institute）的基因组数据共享（Genome Data Commons）包含美国国家癌症研究所（National Institutes Institutes）从一些最大和最全面的癌症基因组数据集中生成的数据.包括 ： The Cancer Genome Atlas, Therapeutically Applicable Research to Generate Effective Therapies, and the Cancer Genome Characterization Initiative (https://gdc.cancer.gov, last accessed September 25, 2016). Another public somatic variant database is the Catalog of Somatic Mutations in Cancer (http://cancer.sanger.ac.uk/cosmic, last accessed September 30, 2016). 其中包含数百万种跨多种肿瘤类型的体细胞变异。体细胞变异分析中经常使用的其他几个数据库，例如参考序列信息，种群数据库和种系变异数据库也在不断增加和改进。基因组数据库提供了准确注释和确定体变异优先级所必需的信息。 在使用这些数据库是，整体上应该遵循乳癌规则： 了解数据库的内容以及如何汇总数据。临床实验室应查看与给定数据库有关的文档或公开文献，以确定数据库的来源，类型和意图 特别注意每个数据库的限制，以避免对注释结果的过度解释。 确认人类基因组装配的版本以及mRNA转录本参考，以确保适当的人类基因组变异学会（HGVS）注释。 尽可能使用基因组坐标而不是HGVS命名法来明确查询基因组数据库。 根据出版物或其他数据库的来源，单个或多个特定条目的数量，研究的深度，使用适当的对照，确认变异的体细胞来源，评估提供的基因组数据的质量以及功能和潜在药物反应研究。 验证所提供病理诊断的数据质量（例如，地点，诊断和子类型 Reference Sequence Databases参考序列数据库提供有关人类基因组装配版本的信息以及相关信息，例如基因组坐标，以明确表示序列变异。附加信息，例如mRNA转录本的登录和版本（例如，BRAF NM_004333.4）和外显子边界定义，对于产生变异的正确HGVS命名法至关重要。可以从这些数据库中计算基因的变异位置图谱（编码，非编码，非翻译区和剪接位点）和链表述（阳性与阴性）。这也允许在没有基因组坐标信息的情况下明确表示变异。一些常用资源包括： RefSeq（国家生物技术信息中心参考序列数据库，https：//www.ncbi.nlm.nih.gov/refseq，上次访问时间为2016年1月2日） Ensembl（http://www.Ensembl.org）。 ensembl.org/index.html，最后访问时间为2016年1月2日） Locus Reference Genomic （https://www.lrg-sequence.org，最近访问时间为2016年2月2日）。 Population Databases这些数据库提供了有关大量特定人群中给定基因座上替代（次要）等位基因频率的全面信息。这些数据库通常用于根据次要等位基因频率（MAF）的任意临界值筛选出被认为是多态/良性的变异。目前尚无用于去除多态或良性变异的MAF的标准临界值。在没有正常组织配对的情况下，工作组建议使用1％（0.01）作为主要阈值，这在许多临床实验室中也很普遍。尽管总体MAF最常用，但临床实验室可能会考虑使用种族-根据患者的种族背景确定特定的MAF。在解释体细胞变异时，必须谨慎使用这些数据库，因为在参与研究时，假定参与这些测序研究的个体是健康的或没有亚临床疾病。确实，一些众所周知的经典癌症相关的和可靶向的体细胞改变已作为种群数据库的种系变异包括在内。例如，变异NM_004972.3（JAK2）：c.1849G&gt; T（c.V617F）通常被看作是叶绿体增生的体细胞变异体肿瘤，可以用FDA批准的Janus激酶（JAK）抑制剂靶向。它也包含在多个人群数据库中，例如 The Database of Short Genetic Variation (the National Center for Biotechnology Information database of genetic variation), Exome Variant Server, and Exome Aggregation Consortium（表格1）。在评估可能的血液系统恶性肿瘤时应格外小心，因为白血病和骨髓增生异常综合症中的许多常见突变基因也可能在其他健康个体的血液中发生体细胞突变，因此可能被错误地注释为多态性。 Cancer-Specific Databases这些数据库提供了有关不同癌症和亚型谱中序列变异的发生率和普遍性的信息，对其他基因组数据库的交叉引用以及对已发表或未进行系统综述的文献的引用，细胞途径，靶向疗法，临床试验 以及结果数据。 从这些数据库中提取的不同癌症中的序列变异体的普遍性和分布，应谨慎解释，因为病理诊断标准的代表性较差，缺乏临床级别的文献管理以及提交变异体的来源控制不严（例如，探索性或 发现研究）。 例如，这些数据库中包括一些常见的种系良性变异，例如 the Catalog of Somatic Mutations in Cancer database 中的NM_000222.2（KIT）：c.1621A&gt; C（p.M541L）。 表1列出了常用的体细胞变异数据库。 Constitutional Variant Databases常见的是，在有或没有匹配的正常组织的情况下进行肿瘤测序可能会揭示出种系起源的变异，例如与癌症易感综合症相关的基因中的致病变异。种系突变数据库，例如人类基因突变数据库和其他疾病或基因座特异性突变数据库，是评估这些变异的有用资源。这些数据库也可用于评估在这些数据库中报道了经过充分研究的种系对应物的体细胞变异（例如，TP53和PTEN基因中的某些变异）。另一个常用的数据库是ClinVar（http://www.ncbi.nlm.nih.gov/clinvar）。 ClinVar处理所有种类的稀有种系变异，例如病原体和良性，并在可用时提供相关的临床和实验证据。专家小组对ClinVar中的某些变异进行了有关其致病性的审查。目前，该数据库仅宿主种系变异，并有望在不久的将来纳入体细胞变异。 Internal (Laboratory-Generated) Databases需要强调的是，临床实验室应该建立一个标注良好的内部数据库，以跟踪实验室中识别出的变异并提供一致的变异注释。这样的数据库可用于识别可能由测序比对伪像引起的潜在假阳性呼叫，以及确定实验室通常遇到的癌症类型的突变频率。我们强烈鼓励体细胞变异数据共享，并敦促临床实验室将精心挑选的变异体贡献到公共变异数据库中，以促进对体细胞变异体的准确解释。但是，此类提交过程应标准化并符合联邦隐私法规，即《健康保险可移植性和责任法案》以及《经济和临床健康信息技术法案》（原文 the Health Insurance Portability and Accountability Act andthe Health Information Technology for Economic and ClinicalHealth Act.）。 正在努力建立临床级基因组数据库 In Silico (Computational) Prediction Algorithms 在计算机模拟中，预测算法是预测基因中核苷酸变化是否会改变蛋白质结构和功能的常用工具（表2）。起初，开发了早期常用算法并验证了用于胚系变异的算法。随后，外推它们的用途以解释体细胞变异。尽管各个算法的核心风险预测方法可能有所不同，但是它们可以分为两类：错义变异对蛋白质功能的影响的预测和序列变异对剪接的影响。考虑到氨基酸或核苷酸残基的进化保守程度，特定理化特性的氨基酸取代的生物化学影响以及变异在翻译蛋白质中的位置，是不同算法用来预测功能的一些主要标准错义变异的影响。拼接位点预测算法使用各种统计方法，例如马尔可夫模型，机器学习（神经网络）和最大熵原理，来预测变异是否会对拼接产生任何影响。通常，错义和剪接位点预测工具具有中等的特异性（大约60％至80％），并且倾向于过度预测有害影响。在癌症基因功能的背景下，对这些预测的解释通常并不直接了当，特别是对于激活突变。例如，当通过多种计算机模拟算法进行分析时，经典的BRAF V600E致癌突变结果会具有冲突甚至良性的作用。这是临床实验室在进行体细胞变异解释时应该意识到的几种情况之一，同时在解释计算机分析的评分结果时应谨慎行事。建议不要将这些预测算法的结果用作变异分类或临床决策的唯一依据。 Variant Identification and Annotation变异检测是变异解释的关键起点。 有许多变异检测软件工具(Supplemental Table S2).可以满足一种特定的检测，例如SNV，插入缺失，结构变异和CNV (Supplemental Table S1): Variant caller Location (URL) MuTect v1.1.555 https://www.broadinstitute.org/cancer/cga/mutect Genome Analysis Toolkit (GATK) – MuTect v2s https://www.broadinstitute.org/gatk/guide/tooldocs/org_broadinstitute_gatk_tools_walkers_cancer_m2_MuTect2.php VarScan 256 http://dkoboldt.github.io/varscan/ VarDict57 https://github.com/AstraZeneca-NGS/VarDict Sterlka58 https://sites.google.com/site/strelkasomaticvariantcaller/ FreeBayes59 https://github.com/ekg/freebayes Scalpel60 http://scalpel.sourceforge.net/ Pindel61 http://gmt.genome.wustl.edu/packages/pindel/ SAMtools62 http://samtools.sourceforge.net/ Torrent Suite Variant Caller https://github.com/iontorrent/TS SomaticSniper63 http://gmt.genome.wustl.edu/packages/somatic-sniper/ 对于临床实验室来说，了解这些变异检测工具的局限性很重要。 对生物信息学流程（包括商业购买的生物信息学软件包）进行适当的实验室验证对于确保结果的质量至关重要。 某些称为变异的指标对于变异解释至关重要，例如 supporting reads (depth of coverage) and variant allele frequency (VAF)，应纳入变异体评估中； 后者对于在没有配对正常的情况下的体细胞变异解释和评估肿瘤克隆多样性特别重要。 变异检测结果一般使用一些标准格式进行输出展示，例如clinical variant call format(VCF), genomic VCF 和 general feature format (alias gene-finding format or generic feature format). The VCF is the most widely used schema in the clinical laboratories as of 2016 to represent detected variants (Clinical Variant Call Format, http://vcfclin.org, last accessed September 28, 2016). Required VCF fields include genomic coordinates, reference nucleotide(s), and variant nucleotide( s). However, complex, multinucleotide, and large structural variants are difficult to represent in the current specification of VCF file format version 4.2, despite the ongoing efforts for standardizing variant representation. For further clinical interpretation, additional metadata that add meaningful and readily identifiable information to variants should be included (eg, gene symbol, variant location, variant type, HGVS nomenclature for cDNA sequence changes, and predicted protein sequence alterations). Additional resources, such as cross-references to external databases (cancerspecific and general genomic databases) (Table 1) and precomputed in silico algorithm-based predictions (Table 2), can also be beneficial. This process is formally referred to as variant annotation, and may be automated by software tools(Supplemental Table S2). (Supplemental Table S2): Software Location (URL) Annovar64 http://annovar.openbioinformatics.org/en/latest/ snpEff65 http://snpeff.sourceforge.net/ SeattleSeq http://snp.gs.washington.edu/SeattleSeqAnnotation138/ AnnTools66 http://anntools.sourceforge.net/ NGS-SNP67 https://www.ualberta.ca/~stothard/downloads/NGS-SNP/ VEP (Variant Effect Predictor)15 http://useast.ensembl.org/info/docs/tools/vep/index.html 变异注释对于准确解释体细胞序列变异至关重要。这些对变异注释得到的多元数据构成了变异评估和解析的初始内容。变异注释面对的一个重要挑战就是将基因组坐标（即染色体和位置）转换为相应的cDNA /氨基酸坐标系统（分别为c.和p.syntax）以进行解读。 这个问题针对indel变异由于对齐排列导致变异表述不一致的问题上，表现尤其突出。尽管HGVS系统建议使用右对齐表示法（将变异的开始位置向右移动，直到不再可能这样做），但VCF规范要求使用左对齐表示法。当前可用的注释解决方案仅部分解决了该问题。缺乏左/右对齐的标准化可能会严重影响变异定位，从而导致变异命名错误。根据HGVS命名法，当存在多个替代转录本时，必须使用正确的mRNA转录本编号和版本信息，以确保变异描述的准确和一致。临床实验室在内部数据库中使用变异的基因组位置来存储变异信息也非常重要，以确保数据存储的明确和可回查。 Proposed Guideline for Evidence-Based Categorization of Somatic Variants体细胞变异包括SNV，插入缺失，基因组重排产生的融合基因和CNV。与胚系变异的解释（侧重于特定疾病的变异性或疾病因果关系）不同，体细胞变异的解释应着重于其对临床的影响。如果变异预测对特定疗法的敏感性，耐药性或毒性，改变基因的功能（可以被批准的或研究用的药物靶向），则该变异可以视为影响临床护理的生物标志物，可以作为临床试验的纳入标准，影响疾病的预后，帮助确定癌症的诊断或保证实施监视措施以早期发现癌症。因此，临床影响应包括治疗，预后，诊断和预防措施。给定变异的临床影响应根据当前可获得的证据确定。基于变异分类的证据在临床决策中的重要性，可以对其进行不同的权衡。在文献综述和工作组共识的基础上，我们建议将临床和实验证据分为四个级别 Level A, biomarkers that predict response or resistance toUS FDA-approved therapies for a specific type of tumoror have been included in professional guidelines astherapeutic, diagnostic, and/or prognostic biomarkers forspecific types of tumors; Level B, biomarkers that predict response or resistance toa therapy based on well-powered studies with consensusfrom experts in the field, or have diagnostic and/orprognostic significance of certain diseases based on wellpoweredstudies with expert consensus; Level C, biomarkers that predict response or resistance totherapies approved by FDA or professional societies for adifferent tumor type (ie, off-label use of a drug), serve asinclusion criteria for clinical trials, or have diagnosticand/or prognostic significance based on the results ofmultiple small studies; Level D, biomarkers that show plausible therapeuticsignificance based on preclinical studies, or may assistdisease diagnosis and/or prognosis themselves or alongwith other biomarkers based on small studies or multiplecase reports with no consensus. 可以将这些证据水平分配给基因组变异体，以确定其临床影响的重要性。 我们建议根据体细胞疾病的临床影响将其序列变体分为四类： tier I，具有强烈临床意义的变体（A和B级证据）； tier II，具有潜在临床意义的变体（C或D级证据）； tier III，临床意义未知的变体； tier IV，良性或可能良性的变体 各类别判断依据Tier I Variants: Variants with Strong Clinical Significance (Level A and B Evidence) Tier II Variants: Variants with Potential Clinical Significance (Level C and D Evidence) Tier III Variants: Variants of Unknown Significance Tier IV Variants: Benign or Likely Benign Germline Variants Identified during Cancer Testing在体细胞致癌突变的临床实验室研究中，重要的是将获得的体细胞变异体与遗传的胚系变异体区分开。大多数胚系变异体都是遗传变异，这些变异可以代代相传，并且通常在100％的细胞中具有变异，导致等位基因分数为0.5或1.0。体细胞变异是在出生后获得的，通常是由于DNA复制或修复错误或环境侵害引起的。由于存在污染正常组织的情况，因此即使在表面上纯净的肿瘤样本中，体细胞变异的等位基因分数通常&lt;0.5。实验室必须正确识别可能与诊断，预后，治疗干预和/或临床试验选择有关的体细胞突变，并且不得将高频体细胞突变错误识别为胚系变异，因为这可能具有重大的临床意义。同样，实验室必须认识到可能导致癌症易感综合症的胚系变异，这将对患者和其他家庭成员产生医疗保健影响。 为了帮助对变异进行分类，一些实验室在对正常的，匹配的对照DNA样品进行测序的同时，对肿瘤DNA进行了平行测试。当正常，匹配的对照组织与肿瘤一起测序时，胚系变异通常很明显。 In this case, the laboratory must have policies that address detection, disclosure/nondisclosure, and interpretation/reporting of germline variants (see section below). 当分析中未包括匹配的对照时，实验室应具有可用于推断变体是体细胞或胚系的标准。指定胚系的主要标准是VAF，对于杂合变体，应为约50％，对于纯合变体，应为100％。某些种系变体（例如大插入缺失）可能会导致正常等位基因的优先扩增（在基于扩增子的测试中）或捕获（在基于捕获的测试中），因为这些等位基因的序列同源性丧失，导致&lt;50％用于种系变体的VAF。当在已知的癌症易感综合征基因（例如TP53或BRCA1）中检测到明显的胚系变异时，有关疾病发作年龄的临床信息（年轻人与致癌基因中遗传的种系突变的较高风险相关） ，肿瘤的侧面性（更可能遗传双侧肿瘤）以及癌症的家族或个人病史可以帮助确定癌症易感性的可能性。文献综述和数据库查询还可以帮助确定该变异体先前是否已被报告为易感综合征患者的复发种系变异体。构成突变的有用数据库包括： Online Mendelian Inheritance in Man (National Center for Biotechnology Information, http://www.ncbi.nlm.nih.gov/ omim, last accessed March 6, 2016), Human Gene Mutation Database (http://www.hgmd.cf.ac.uk/ac/index.php, last accessed March 6, 2016), ClinVar locus-specific databases 实验室应制定一项政策，对恶性肿瘤中发现的遗传变异进行测试，以在收到患者的适当同意或根据临床医生的要求后，使用经过临床验证的生殖测试来确认变异的生殖或体细胞来源。 ACMG建议在胚系测试中显示阳性结果，以显示53个基因的阳性结果，其中大约一半与可能在体细胞检测面板上的癌症易感性基因有关。即使在仅将该胚系作为一部分进行评估的情况下，也建议进行公开 肿瘤/正常研究。 通过推断，在仅肿瘤的体细胞突变研究中也考虑种系致病变异的可能性似乎是谨慎的。 在单样本无法判断变异是体系还是胚系变异是，添加局限性说明When paired germline samples are not used, NGS analysisdoes not distinguish germline and somatic variants, andsequencing results may contain both findings. In this case,findings can be reported with a disclaimer that the NGS testused does not allow definitive differentiation betweengermline and somatic variants. In certain settings, a germlinevariant may be suspected (eg, MAF 40% to 60%). However,this interpretation should be made with caution and correlatedwith tumor cellularity. If a germline variant is suspected,testing of a patient germline sample (eg, blood in patientswith solid tumors) can be suggested if clinically indicatedafter an appropriate patient consent. The reports shouldinclude a statement addressing the manner in which thedistinction between somatic and germline alterations is made,and indications of remaining uncertainty, where appropriate. “ Interpretation and ReportingNomenclature所有检测到的遗传变异均应按照HUGO基因命名委员会的指定进行注释和报告: 基因名参考 ：HGNC :http://www.genenames.org 变异写法参考：HGVS，http：//www.hgvs.org SNV和插入缺失应使用p.和c. 符号进行报告（例如，BRAFp.V600E，c.1799T&gt; A）。 SV，列出两个融合的基因伴侣之间用斜杠分隔（例如，EML4 / ALK fusion）。 CNV应以表格形式报告为拷贝数GAIN或LOSS。 如果适用，可以包括基因/基因组基因座的基因组坐标。可以在适当的时候报告数字拷贝数的变化[例如，EGFR拷贝数GAIN（拷贝数比25）； CDKN2A拷贝数LOSS]。 使用标准术语不会超过与临床团队进行清晰明确沟通的需要。 根据需要，除了标准术语外，还应包括口语命名法，以便医生阅读报告并使用它们来确定治疗时向医生清晰地传达含义。例如，TERT启动子变体的报告可以为1-124C&gt; T（ HGVS命名法，然后是括号中的口语命名法（TERTC228T）。使用HGVS命名法报告基因组变体以明确将变体重新映射到参考基因组，而口语命名法则向临床团队传达了明确的信息。 Other Reporting Elements除了检测到的变体之外，报告还应包含其他一些元素，这些元素可能与更深入的结果分析或与随时间推移从该患者获得的其他结果进行比较有关，例如基因组座标，基因组构建和转录参考序列（ 例如NM_004333.4），前提是此信息不会损害患者和临床提供者解释该报告中与之直接相关的基本要素的能力。 本节或对结果进行扩展说明的另一节中，远离顶部结果。应评估等位基因分数（VAF）和覆盖率，并在适当时包括在报告中。该报告应包括所用NGS测定的测序覆盖率临界值。在报告中应声明所有未满足最低要求的测序覆盖率标准的基因和/或热点均已失败。 报告不应仅限于肯定的发现。 I类药物/癌症组合应包括相关的阴性结果（例如，肺癌患者中明确缺乏EGFR突变或黑色素瘤患者中明确缺乏BRAF突变）。如果存在不确定性， 必须在报告中进行沟通；这包括序列质量，样品充分性，肿瘤含量和生物医学知识的问题。 Reporting of Germline Variants希望对配对的胚系样品进行并发分析，因为这可以澄清解释。但是，这并不总是实用的，因此也不是必需的。当有成对的胚系样品可用时，测序管线可允许从体细胞获得性变体中分离出胚系发现。通常，仅解释和报告体细胞变异。如果患者或临床医生要求对胚系发现进行进一步分析，则可以在以后重新查看胚系NGS数据，并在获得患者适当同意后进行报告。成对的胚系测试可能需要获得同意并提供相关文件，以符合当地法律和政策。如果NGS小组的某些基因中未报告胚系变体，则初始报告应特别说明这一事实。当不使用成对的胚系样品时，NGS分析不能区分胚系和体细胞变体，测序结果可能包含这两个发现。在这种情况下，可以用免责声明报告使用的NGS测试不能明确区分胚系和体细胞变体。在某些情况下，可能会怀疑胚系变异（例如MAF 40％至60％）。然而，这种解释应谨慎进行，并与肿瘤细胞数量有关。如果怀疑胚系变异，如果在适当的患者同意后临床上有指示，可以建议对患者胚系样本（例如，实体瘤患者的血液）进行测试。报告中应包含一份声明，说明在体细胞和胚系变化之间进行区分的方式，并在适当情况下指示仍存在不确定性。首先测试癌症样品和稍后再配对的胚系样品的实验室可以选择进行以下操作： 将包含胚系样品发现的结果发布到癌症报告的附录中， 在胚系报告中单独发表一份关于胚系变体的报告，并附上唯一的解释性声明，并在初始癌症报告中添加一份附录。 作为单独的胚系报告和另外的单独报告发布，该报告整合了癌症和胚系样品的结果。 生殖细胞变体应按照ACMG / AMP指南进行报告。如果订购了针对癌症易感基因的生殖细胞测试，则生殖细胞变异的报告应遵循ACMG / AMP指南。应提供遗传咨询和转诊给临床医学遗传学家。实验室应制定政策，以报告重要性不明的变体并披露次要发现，包括在何种情况下将报告或不报告此类发现 Reporting the Clinical Significance of DetectedVariants对检测到的遗传变异提供解释性注释是很有用的，该变异将这种变异置于临床病理背景下可为管理决策提供依据。这对于I级和II级突变至关重要。必须对III级变体的详细分析与目标，使报告中最关键的信息保持简洁，清晰并突出显示的目的相平衡。这些评论可能包括该变体对于特定肿瘤类型，对生化途径的影响以及相关癌症的患病率的功能，预后或预测意义。但是，建议应该简短明了，并应谨慎措辞，并应理解治疗或其他患者管理决定是基于除遗传改变以外的许多医学信息，其中许多信息对于分子专业报告是不可用的。重要的是要认识到，治疗的适用性是基于许多因素，而不是根据测试申请书上写的诊断和通过测试发现的基因型。这些因素通常是分子专业报告结果所未知的（即，存在混杂的医学状况，例如葡萄糖耐量不足，自身免疫性疾病或心力衰竭），并且在推荐特定疗法时未考虑这些其他因素会导致混淆，患者与肿瘤科团队之间的冲突以及焦虑。 NGS实验室报告中的治疗建议应以证据为依据，应与患者的癌症诊断相关，并应使用某种语言来明确指出该报告包含结合实验室可用数据点的广义治疗建议（例如，diag-疾病和基因型），但需要将其他因素纳入为每个个体制定治疗计划的过程中。尽管可以接受有关相关试验的一般性陈述或已发表试验的引用结果，但不应针对具体临床试验提出建议。]]></content>
      <categories>
        <category>NGS</category>
        <category>肿瘤检测</category>
        <category>标准化</category>
      </categories>
      <tags>
        <tag>Standards and guidelines</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python-包-python-pptx生成PPT]]></title>
    <url>%2F0002.%E5%BC%80%E5%8F%91-02.python%2FPython-%E5%8C%85-python-pptx%E7%94%9F%E6%88%90PPT%2F</url>
    <content type="text"><![CDATA[简介处于某些上下游对接需求，所以需要频繁的将生信的分析结果整理成PPT文件，以便进行结果的展示。所以基于该模块可以更方便的在集群上自动化生成相关的文档示例，用于进行后续的处理。 使用python操作PPT，需要使用的模块就是python-pptx，下面来对该模块做一个简单的介绍。这里提前做一个说明：python操作PPT，最好是我们提前设计好自己的一套样式，然后利用进行python进行内容的获取和填充（最主要的功能！），最好是不用使用python代码操作PPT的格式，格式的修改肯定不如我们直接在PPT中修改方便。可以创建、修改PPT（.pptx）文件。 环境准备模块的安装需要单独安装，不包含在Python标准模块里1234# &quot;Windows用户命令行下输入&quot;pip install python-pptx# &quot;Mac用户命令行下输入&quot;pip3 install python-pptx 模块的导入1import pptx 模块的使用python读取PPT文档中的内容在使用python操作PPT之前，首先应该清楚PPT的结构，这个对于之后代码的编写很有帮助。 获取Slide12345from pptx import Presentationprs = Presentation(&quot;统计学习方法PPT.pptx&quot;)for slide in prs.slides:print(slide) 获取Shape形状12345678910import pptxfrom pptx import Presentationprs = Presentation(&quot;统计学习方法PPT.pptx&quot;)for slide in prs.slides:for shape in slide.shapes:print(shape)&quot;&quot;&quot;注意：这里得到的Shape对象，并不能看出什么，接着往下看。&quot;&quot;&quot; 判断每个Shape中是否存在文字 shape.has_text_frame ：是否有文字 shape.text_frame ：获取文字框123456789import pptxfrom pptx import Presentationprs = Presentation(&quot;统计学习方法PPT.pptx&quot;)for slide in prs.slides:for shape in slide.shapes:if shape.has_text_frame:text_frame = shape.text_frameprint(text_frame.text) 获取某一页Slide中的内容1234567891011import pptxfrom pptx import Presentationprs = Presentation(&quot;统计学习方法PPT.pptx&quot;)for i,slide in enumerate(prs.slides):if i == 5:for shape in slide.shapes:if shape.has_text_frame:text_frame = shape.text_frameprint(text_frame.text) 获取Shape中的某个Paragraph1234567891011121314151617import pptxfrom pptx import Presentationprs = Presentation(&quot;统计学习方法PPT.pptx&quot;)for i,slide in enumerate(prs.slides):if i == 5:for shape in slide.shapes:if shape.has_text_frame:text_frame = shape.text_framefor paragraph in text_frame.paragraphs:print(paragraph.text)&quot;&quot;&quot;注意：该方法和上述4)中的方法一摸一样。上述方法是直接获取Shpae中的文字内容；下面这个更灵活，先获取每个Shape，然后在获取每个Shape中的paragraph；下面方式更好：因为我们可以针对paragraph，写一个判断条件，只获取第几个paragraph；&quot;&quot;&quot; 参考readthedocs知乎CSDN]]></content>
      <categories>
        <category>编程拾慧</category>
        <category>python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Office处理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Characterization and mitigation of fragmentation enzyme-induced dual stranded artifacts]]></title>
    <url>%2F0001.%E7%9F%A5%E8%AF%86-01.NGS%2F2020-10-02.%E9%85%B6%E5%88%87%E6%89%93%E6%96%AD%E5%AF%BC%E8%87%B4%E7%9A%84%E5%81%87%E9%98%B3%E4%BF%A1%E5%8F%B7%E7%9A%84%E7%89%B9%E5%BE%81%E5%92%8C%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88%2F</url>
    <content type="text"><![CDATA[DNA片段化是基于杂交捕获的短读长测序文库制备过程中的基本步骤。文库制备包括DNA片段化过程中引入的错误，会显著影响下游的分析，并直接导致假阳性和假阴性的增加。目前主流的DNA片段化方法有两种： 基于超声波进行的物理打断，（超声打断本身比较昂贵（仪器上万）和耗时（每个样本几分钟），但是会带来一些样本损失; 另外一种就是依赖于DNA核算内切酶进行片段化的酶切打断，可以便捷的一次应用于许多样品，由于其易用性、可扩展性和低进入门槛，酶促片段化在高通量测序操作中越来越受欢迎。尽管这两种方法已经 集中研究中提供的建库打断方法KAPA HyperPlus kit 酶切打断SureSelect QXT（安捷伦科技公司） 酶切打断Fragmentase（新英格兰生物实验室）酶切打断Nextera Tagmentation（Illumina） 酶切打断 KAPA HyperPrep kit 超声打断.SureSelect kit 机械打断 研究一有研究[1]表明KAPA Biosystems 在其 HyperPlus 文库制备试剂盒中有专门的片段化酶，能提供比超声处理更简单的碎片化解决方案，但是在进行大型队列的变异分析时，发现了大量意想不到的单核苷酸、插入和缺失变异。经过仔细的检查，发现了这些假阳性信号是酶促裂解过程的副产品。同时也评估了IDT和NEB的其它基于酶切的试剂盒，发现都存在类似的问题。通过深入观察发现这写artifact还有一下特点： 这些检出具有比较低的等位基因频率 这些支持artifact信号的reads都存在soft-clipped，同时这部分soft-clipped的序列如果进行反向互补，通常可以在附近的参考序列中找到； 这些soft-clipped区域具有高碱基质量值，表明他们是真实的分子衍生物，而不是测序错误产生的。 这些soft-clipped的序列高度保守，但是和接头引物序列并不一致。并且偶尔会被比对到互补链。并且经过调查，这部分经过反向互补，经常可以在附近的参考基因组中发现。 使用酶切打断和超声打断制备的样品进行FADE分析，发现两种方法artifact的reads数目有明显差异。酶切中artifact的比例（所有reads中被定义为包含artifact的reads比例）是2%，而超声处理的样本比例是0.01%。 同时经过调查，发现超声处理的这0.01%的artifact像是来自基因组重复区域或者是打分超过阈值的一些比对错误。但是也有报道表明使用超声打断也容易发生DNA的氧化损伤，产生假阳信号（Discovery and characterization of artifactual mutations in deep coverage targeted capture sequencing data due to oxidative DNA damage during sample preparation）。同时文中提供了FADE软件，可以进行相关的过滤。PS 经过内部实测，实际效果优先，因为我们遇到的类似情况变异的序列都比较短，不能有效进行过滤。 研究二使用超声和酶片段化方法制备的相同肿瘤DNA样品的体细胞变异的成对比较。分析发现，与通过超声处理的文库相比，核酸内切酶处理的文库中复发的artifact信号导致的SNV和插入缺失数量要多得多。这些具有如下一些特征：基因组环境中的回文结构、reads上的位置偏好性和多核苷酸取代为标志。尽管这些试剂盒最大限度地减少了DNA损失，但酶片段化过程引起的测序错误程度仍然很大。在使用HyperPlus试剂盒构建的DNA片段化文库中发现了许多人工SNV / indels。这些测序错误有如下特征：在于位于回文结构中心和读段5’或3’末端附近的变异，具有多核苷酸取代被认为是由核酸内切酶处理步骤和随后的末端修复填充过程引入的，而不是测序过程本身的结果。同时研究中同时使用SureSelect（机械打断）和HyperPlus（酶切打断）处理的6个相同DNA文库。 最终结果发现，虽然源自相同的DNA样本，但是HyperPlust文库的SNV/InDel是SureSelect构建文库的2.3~9.9倍。SureSelect处理的大多数SNV/插入缺失都嵌套在HyperPlus库中 从数据看，HyperPlust检出的突变在多个样本中重复出现（a），而SureSelect特异检出的变异则没有这样的现象（b） 如C 图左侧面板所示。在[a]（红色）中检测到的体细胞SNV/插入缺失的位置。右面板。在 [b]（蓝色）中检测到的体细胞 SNV/插入缺失的位置。映射到与检测到的SNV/插入缺失相同的基因组坐标的野生型核苷酸读数的数量以灰色表示 D图展示的是softclip的reads比例。HyperPlus特异检出的变异含有更高的soft-clipped比例。 对数据的仔细检查发现，许多这些体细胞SNV恰好位于回文序列的中心，此处指定为“SNV-centered palindromes”（SCP）。HyperPlus文库也更频繁地生成更长的SCP，而在SureSelect文库中没有检测到长度超过15个碱基的SCP（图2B）。同时发现a 和b 还有三个区别： 类别a中的大多数SNV和InDel被检出过不止一次。 类别[a]中的SNV/插入缺失通常位于距离读数的5’或3’边缘10至15个碱基 来自类别a的变异具有更多的soft-clipped（平均为50.8%和5.0%) 提供的建议方案， 排除了被反复检测到的SNV/InDel, 除非在Cosmic中注册； 使用KS检验对比变异和野生型数据的位置偏好性； 同时计算每个变异所有reads中，soft-clipped的reads比例。然后使用逻辑回归进行噪声或信号的分类。效果如下图：类别 [a] SNV/插入缺失（主要是测序伪影）的特征是较低的 KS p 值和/或较高的软削波读取比率，但 [b] 类中的 SNV/插入缺失（主要是真正的 SNV/插入缺失）具有较高的 KS p 值和较低的软削波读取比率。然后估计阈值，以使用具有 logit 链接函数的广义线性模型来区分两个类别之间的 SNV/插入缺失。通过对六样本训练数据的受试者工作特征（ROC）曲线分析，建立了最终模型，并证明能够区分两个类别之间的SNV/插入缺失，特异性为0.914，灵敏度为0.979。同时评估测试发现HyperPlus、Hyper和SureSelect数据集的剩余SNV/插入缺失的中位数（范围）比例分别为10.8%（0.01%–46.9%）、85.2%（47.6%–98.8%）和94.3%（86.5%–98.6%），酶切建库的假阳性过滤明显，但是机械打断的过滤影响明显小很多。同时还针对HH 组合（正常–HyperPlus 与肿瘤–HyperPlus）和 SS 组合（正常–SureSelect 与肿瘤–SureSelect）进行了测试，发现即使配对的正常和肿瘤样本中使用相同的片段化方法后，也很难完全消除HyperPlus治疗产生的测序噪声，有必要使用信息学来过滤噪声。 参考文献[1] Characterization and mitigation of fragmentation enzyme-induced dual stranded artifacts[2] Sequencing artifacts derived from a library preparation method using enzymatic fragmentation[3] Optimization of enzymatic fragmentation is crucial to maximize genome coverage: a comparison of library preparation methods for Illumina sequencing]]></content>
      <categories>
        <category>NGS</category>
        <category>肿瘤检测</category>
        <category>文献</category>
      </categories>
      <tags>
        <tag>NGS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[snakemake介绍]]></title>
    <url>%2F0002.%E5%BC%80%E5%8F%91-04.pipeline%2Fpipeline-frameworks-snakemake%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[snakemake分享pptSnakemake介绍官方文档 其他材料：blog: snakemake-vs-nextflowarticle: Sustainable data analysis with Snakemake 任务投递示例：1234567snakemake -s LowFreqConsencus.Pipeline.split.smk -p \# 向流程传递参数-C S=/ifstj2/B2C_RD_H1/Project/Halos_VersionUpdate_test/v1.4.4.2/InterTest/4Pipe.path O=/ifstj2/B2C_RD_H1/Project/Halos_VersionUpdate_test/v1.4.4.2/InterTest \# 设置任务投递方式（使用集群sge示例）--cluster "qsub -clear -V -cwd -P P18Z11900N0299 -q bc_b2c.q,b2c_rd.q -l num_proc=&#123;threads&#125; -l vf=&#123;resources.mem_mb&#125;M -binding linear:&#123;threads&#125;" # 设置snakemake调度任务的指标--rerun-incomplete --jobs 1000 --restart-times 5 --keep-going --rerun-incomplete --latency-wait 60 --stats runtime.json 常用参数： -C向流程传递参数例如 -C S=”string” 则在smk中可以通过使用 config[‘S’] 调用相关参数完成参数传递。 -C 传递的值统一保存在字典 “config” 中。 –jobs设置并行的最大任务数目。 –config向snakemake传递参数（字典形式） –configfile指定配置文件路径（可以支持多个） –cores设置任务最多使用的核数 –resources设置任务最多使用的内存 –forceall强制执行某条Rule及它的依赖。 –list展示smk脚本中所能获得的所有Rule –dag 123456生成流程逻辑框架图：snakemake xxxx(流程本身的参数) --dag | dot -Tsvg &gt; dag.svg #如果snakemake流程本身会打印输出内容，则需要单独处理如下：snakemake xxxx(流程本身的参数) --dag &gt;tmp.txt # 删除tmp.txt文档中流程自身打印的内容，然后生成图片dot -Tsvg tmp.txt &gt; dag.svg –touch更新文件的时间戳（不会重新跑） –force, -f 重新运行第一条或指定的某条Rule –forcerun, -R 强制执行snakefile，更新rule时，使用此命令。会同步更新后续的所有结果 –dry-run, -n 生成相关分析路径的shell脚本，但是不进行实际的执行。 –keep-going, 在某个任务失败后，继续运行其他的独立任务； –cluster 针对集群投递到计算节点的参数设置； 12345 --cluster &quot;qsub -clear -cwd -P P18Z11900N0299 -l num_proc=&#123;threads&#125; -l vf=&#123;params.resources&#125; -binding linear:&#123;threads&#125;&quot; ``` - --restart-times 任务失败后，重投的次数 –restart-times 5 12345678910111213141516171819202122232425- --rerun-incomplete, 针对结果不完整的数据，重跑所有的rules；- --unlock 解锁被异常锁定的目录 - --stats 记录任务的执行状态，输出到指定文件 - --nocolor 不输出彩色的结果 - --drmaa-log-dir 仅限使用 -drmaa 进行任务调度时有效。 指定shell的log（.e 和 .o）输出目录. #### 资源配置##### SGE以SGE为例，可以通过设置配置文件，调整每个任务运行时所需要的资源``` qsub -clear -cwd -l vf=$&#123;mem&#125;g,num_proc=$&#123;cpu&#125; -P $&#123;Project_ID&#125; -binding linear:$&#123;cpu&#125; -q $&#123;Queue_ID&#125; work.sh -o work.sh.o -e work.sh.e ``` 创建资源配置文件 cluster.yaml 编辑各个任务资源的相关资源需求。 localrules: alldefault: # 默认标准 queue: “Queue_ID” project: “Project_ID” workdir: “./“ mem: “1G” cores: 1 trim: … rmhost: mem: “4G” cores: 4 output: “cluster_logs/{rule}.{wildcards}.o” error: “cluster_logs/{rule}.{wildcards}.e”123456789#### 为任务单独设置运行环境##### 通过conda为计算规则设置单独的计算环境不同的计算规则中依赖的软件之间可能有冲突，无法在同一个环境中配置好。使用conda指令，smk可以为每个计算规则新建conda环境，并在其中运行计算。为rmhost规则新建一个环境： mkdir envscat &gt; envs/rmhost.yamlchannels: biocondadependencies: bowtie2=2.3.5.1 samtools=1.10 修改Snakefile文件： rule rmhost: input: … output: … conda: “envs/rmhost.yaml” shell: …1运行smk时，加上--use-conda参数即可为每个规则建立conda的独立环境。 snakemake –use-conda123### rule内的函数- protected 在文件生成后，将文件权限改为 只读文件。 ``` priority设置任务优先级，从而一定调整各个rule的运行的顺序。]]></content>
      <categories>
        <category>编程拾慧</category>
        <category>任务调度</category>
        <category>pipeline</category>
        <category>python</category>
        <category>framework</category>
      </categories>
      <tags>
        <tag>流程开发</tag>
        <tag>培训</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[snakemake介绍]]></title>
    <url>%2F0002.%E5%BC%80%E5%8F%91-04.pipeline%2Fpipeline-frameworks-snakemake%E6%97%A5%E5%BF%97%E6%9F%A5%E7%9C%8B%2F</url>
    <content type="text"><![CDATA[snakemake分享pptSnakemake介绍官方文档 bench通过 benchmark 指定输出文件，可以记录每个任务运行过程中所使用的计算资源。1234567rule Demo: input: ... output: ... conda: &quot;envs/rmhost.yaml&quot; shell: ... benchmark: log.txt 运行rule后，会在 log.txt 文件中输出任务运行的相关资源信息。 s h: m: s max_rss max_vms max_uss max_pss io_in io_out mean_load 1653.7956 0:27:33 11794.34 39378.64 11782.93 11787.65 5094.00 2639.98 118.52 其中每个字段含义如下（官网链接）: s: 运行时间，秒为单位 h: m: s : 运行时间，时分秒制 RSS: Resident Set Size 实际使用物理内存（包含共享库占用的内存） uss： 最大虚拟内存 USS: Unique Set Size 进程独自占用的物理内存（不包含共享库占用的内存） PSS: Proportional Set Size 实际使用的物理内存（比例分配共享库占用的内存） io_in: I/O read in bytes io_out: I/O written in bytes mean_load VSS- Virtual Set Size 虚拟耗用内存（包含共享库占用的内存） 一般来说内存占用大小有如下规律：VSS &gt;= RSS &gt;= PSS &gt;= USS 统计任务CPU时随着计算的精细话管理，越来越多的工作需要精确统计每个步骤/每个流程分析所需时间， 使用的snakemake版本大于5.30时，框架自带的benchmark 记录会保存每个任务的cpu_time。12345678# &lt;= v5.29s h:m:s max_rss max_vms max_uss max_pss io_in io_out mean_load3.5506 0:00:03 34.39 239.52 32.57 32.66 80.04 0.01 41.13# &gt;= v5.30$ cat benchmarks/test1.rmhost.benchmark.txt s h:m:s max_rss max_vms max_uss max_pss io_in io_out mean_load cpu_time62.8049 0:01:02 3528.02 4267.68 3524.98 3525.02 12.81 48.00 267.62 168.59]]></content>
      <categories>
        <category>编程拾慧</category>
        <category>任务调度</category>
        <category>pipeline</category>
        <category>python</category>
        <category>framework</category>
      </categories>
      <tags>
        <tag>流程开发</tag>
        <tag>培训</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[snakemake进阶]]></title>
    <url>%2F0002.%E5%BC%80%E5%8F%91-04.pipeline%2Fpipeline-frameworks-snakemake%E8%BF%9B%E9%98%B6%2F</url>
    <content type="text"><![CDATA[分支的实现在进行snakemake分析时，所有的参数都是通过 -C 进行传递的，例如 -C S=”string” 则在smk中可以通过使用 config[‘S’] 调用相关参数完成参数传递。 -C 传递的值统一保存在字典 “config” 中。1234567-C \# 指定一些分支的选择（例如688中是否进行某些数据库的分析）,Tequila=T \ # 指定输入文件S=/ifstj2/B2C_RD_H1/Project/Halos_VersionUpdate_test/v1.4.4.6/4Pipe.path \ # 指定输出文件O=/ifstj2/B2C_RD_H1/Project/Halos_VersionUpdate_test/v1.4.4.6 向流程传递参数 数据的二次加工snakemake本身是基于python语法解析的，因此整个snakemake中是支持python语法进行数据处理的。借助这个特点，我们可以在snakemake中进行一些高级的操作来帮助我们更便捷的进行数据的处理。比如针对输入文件，生成一些相对复杂的数据结构，实现一些复杂的功能或者业务逻辑12345678910111213141516aln_bam = "&#123;0&#125;__&#123;1&#125;/&#123;2&#125;/3.BWA_Aln/&#123;3&#125;__&#123;4&#125;__&#123;5&#125;__&#123;6&#125;_sort.bam".format(product, sample, type, chip, lane, barcode, umi) if not aln_bam in library_bams[(product, sample, type, library)]: library_bams[(product, sample, type, library)].append(aln_bam) if library_bamnum[(product, sample, type, library)] == "S" or library_bamnum[(product, sample, type, library)] == "M" : library_bamnum[(product, sample, type, library)] = "M"; else: library_bamnum[(product, sample, type, library)]="S";######## Save Sample Bam ############ Markdup_bam = "&#123;0&#125;__&#123;1&#125;/&#123;2&#125;/4.MarkDup.lib/&#123;3&#125;.markdup.bam".format(product, sample, type, library) if not Markdup_bam in sample_bams[(product, sample, type)]: sample_bams[(product, sample, type)].append(Markdup_bam) if sample_bamnum[(product, sample, type)] == "S" or sample_bamnum[(product, sample, type)] == "M" : sample_bamnum[(product, sample, type)] = "M"; else: sample_bamnum[(product, sample, type)] = "S"; 补充日志补充一个相对完善的流程说明帮助文档1234567891011121314151617181920212223242526try: SampleFile = &#123;config["S"]&#125;.pop()except: Errlog = """\033[0;32m\nMiss The Config File for Sample Infomation\nPlease Add Sample Information By ' -C S=Sample_Infomation O=OutDir ') Usage:snakemake -s PanCancer.Pipeline.smk -p -C S=Sample_Infomation O=OutDir \\Analysis="MSI,QC21,filt_variation,filt_sv,Drug_ct,final.CNV,CNV.detail,final.hot,CisMut,final_hereditary_tumor" ##: Analysis Some part of this pipeline \\UnAnalysis="MSI,QC21,filt_variation,filt_sv,Drug_ct,final.CNV,CNV.detail,final.hot,CisMut,final_hereditary_tumor" ## : skip some part of this pipeline ; \\Tequila=T (T|F Use Tequila Online Database or Not ) \\--cluster "qsub -clear -cwd -P P19Z11900N0186 -q b2c_rd.q -l num_proc=&#123;threads&#125; -l vf=&#123;resources.mem_mb&#125;M -binding linear:&#123;threads&#125;" \\--jobs 500 \\--rerun-incomplete \\--restart-times 5 Version: v1.3.1(20200903@ Liubo4)=====================Sample_Infomation format==================#Product Sample Type chipInfo Lane Barcode UMI_ID Fq1 Fq2PanCancer Test Tissue V300014810 L04 14 14 XXXXX.14_1.fq.gz XXXXX.14_2.fq.gzPanCancer Test Normal V300014810 L04 15 15 XXXXX.15_1.fq.gz XXXXX.15_2.fq.gz================================End============================ \033[0m\n""" sys.exit(Errlog) 生成一系列的记录文档，记录分析的相关信息。123456789# 获取分析日期Analysisdate =datetime.datetime.now().strftime('%Y-%m-%d')# 基于git管理的流程，获取流程的版本PipeVersion = subprocess.getoutput("cut -d ' ' -f2 "+bin_path+"/.git/logs/HEAD| tail -n1 ")with open("log","w+") as LOG: LOG.write("Start Date :"+Analysisdate+"\n\n") LOG.write("Analysis Version :" + PipeVersion + "\n\n") LOG.write("Analysis Dir :"+config['O']+"\n\n")]]></content>
      <categories>
        <category>编程拾慧</category>
        <category>任务调度</category>
        <category>pipeline</category>
        <category>python</category>
        <category>framework</category>
      </categories>
      <tags>
        <tag>流程开发</tag>
        <tag>培训</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Git-2.相关命令常用参数]]></title>
    <url>%2F0003.%E5%BA%94%E7%94%A8-99.other%2FGit-2.%E7%9B%B8%E5%85%B3%E5%91%BD%E4%BB%A4%E5%B8%B8%E7%94%A8%E5%8F%82%E6%95%B0%2F</url>
    <content type="text"><![CDATA[相关材料官方文档官方文档-中文 merge123456789101112131415161718git merge --edit和-e # 用于在成功合并、提交前调用编辑器来进一步编辑自动生成的合并信息。因此使用者能够进一步解释和判断合并的结果。--no-edit # 参数能够用于接受自动合并的信息（通常情况下并不鼓励这样做）。--ff # 是指fast-forward命令。当使用fast-forward模式进行合并时，将不会创造一个新的commit节点。默认情况下，git-merge采用fast-forward模式。--ff-only # 除非当前HEAD节点已经up-to-date（更新指向到最新节点）或者能够使用fast-forward模式进行合并，否则的话将拒绝合并，并返回一个失败状态。--no-ff命令 # 即使可以使用fast-forward模式，也要创建一个新的合并节点。这是当git merge在合并一个tag时的默认行为。--log[=&lt;n&gt;]# 将在合并提交时，除了含有分支名以外，还将含有最多n个被合并commit节点的日志信息。--no-log # 并不会列出该信息。--stat # 参数将会在合并结果的末端显示文件差异的状态。文件差异的状态也可以在git配置文件中的merge.stat配置。-n/--no-stat # 参数将不会显示该信息。--squash # 当一个合并发生时，从当前分支和对方分支的共同祖先节点之后的对方分支节点，一直到对方分支的顶部节点将会压缩在一起，使用者可以经过审视后进行提交，产生一个新的节点。开发者可能在本地提交了大量且无意义的节点，当需要合并到develop分支时，可能仅仅需要用一个新的节点来表示这一长串节点的修改内容，这时--squash命令将会发挥作用。如果功能分支的多次提交并不是琐碎而都是有意义的，使用--no-ff命令更为合适。-q和 --quiet # 静默操作，不显示合并进度信息。git merge --abort # 在合并出现冲突时使用，放弃合并过程，重建合并前的状态]]></content>
      <categories>
        <category>编程拾慧</category>
        <category>Git</category>
      </categories>
      <tags>
        <tag>培训</tag>
        <tag>Git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RPKM, FPKM and TPM]]></title>
    <url>%2F0001.%E7%9F%A5%E8%AF%86-01.NGS%2F2019-03-02.RNA%E8%A1%A8%E8%BE%BE%E9%87%8F%E7%9A%84%E8%AE%A1%E7%AE%97%2F</url>
    <content type="text"><![CDATA[过去，进行 RNA-seq 时，会以 RPKM（每千碱基百万读取数）或 FPKM（每千碱基百万片段数）的形式报告结果。然而， TPM（每千基百万转录本）现在变得非常流行。由于这些术语似乎有很多混淆，我想我会使用 StatQuest 来清除所有内容。 这三个指标试图标准化测序深度和基因长度。 RPKM 的计算过程参考如下操作： 计算样本中的总读数并将该数字除以 1,000,000——这是我们的“每百万”比例因子。 将读取计数除以“每百万”比例因子。这使测序深度标准化，为您提供每百万读数 (RPM) 将 RPM 值除以基因的长度，以千碱基为单位。这为您提供 RPKM。 FPKM 与 RPKM 非常相似。 RPKM 是为单端 RNA-seq 制作的，其中每个读取对应一个已测序的片段。 FPKM 是为PE测序的 RNA-seq 制作的。使用配对末端 RNA-seq，两个读数可以对应一个片段，或者，如果对中的一个读数没有映射，一个读数可以对应一个片段。 RPKM 和 FPKM 之间的唯一区别是 FPKM 考虑到两次读取可以映射到一个片段（因此它不会将该片段计算两次）。 TPM 与 RPKM 和 FPKM 非常相似。唯一的区别是操作顺序。以下是计算 TPM 的方法： 将读取计数除以每个基因的长度（以千碱基为单位）。这为您提供了每千碱基 (RPK) 的读数。 计算一个样本中的所有 RPK 值并将这个数字除以 1,000,000。这是您的“每百万”比例因子。 将 RPK 值除以“每百万”比例因子。这为您提供了 TPM。 所以你看，在计算 TPM 时，唯一的区别是你首先对基因长度进行归一化，然后对测序深度进行归一化。然而，这种差异的影响是相当深远的。 当您使用 TPM 时，每个样本中所有 TPM 的总和是相同的。这使得比较每个样本中映射到基因的读数比例变得更加容易。相比之下，使用 RPKM 和 FPKM，每个样本中归一化读数的总和可能不同，这使得直接比较样本变得更加困难。 这是一个例子。如果样本 1 中基因 A 的 TPM 为 3.33，样本 B 中的 TPM 为 3.33，那么我知道在两个样本中映射到基因 A 的总读数的比例完全相同。这是因为两个样本中的 TPM 总和总是相同的数字（因此计算比例所需的分母是相同的，无论您正在查看哪个样本。） 使用 RPKM 或 FPKM，每个样本中归一化读数的总和可能不同。因此，如果样本 1 中基因 A 的 RPKM 为 3.33，样本 2 中 RPKM 为 3.33，我不知道样本 1 中相同比例的读数是否与样本 2 中的基因 A 对应。这是因为需要分母计算两个样本的比例可能不同。 参考来源rna-seqblog]]></content>
      <categories>
        <category>RNA</category>
      </categories>
      <tags>
        <tag>NGS</tag>
        <tag>RNA</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SGE任务调度系统相关命令记录]]></title>
    <url>%2F0003.%E5%BA%94%E7%94%A8-02.Linux%2FLinux-SGE%E4%BB%BB%E5%8A%A1%E8%B0%83%E5%BA%A6%E7%B3%BB%E7%BB%9F%E7%9B%B8%E5%85%B3%E5%91%BD%E4%BB%A4%E8%AE%B0%E5%BD%95%2F</url>
    <content type="text"><![CDATA[http://gridscheduler.sourceforge.net/htmlman/ http://gridscheduler.sourceforge.net/htmlman/htmlman1/qstat.html?pathrev=V62u5_TAG sge系统管理命令队列信息查询 #### 命令 功能 qconf -sq 查询特定队列的信息 qconf -sprjl 显示项目列表 qsub123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172-V: 将当前的环境变量传递到执行命令的节点中。可以使用-v PWD 来代替-cwdcwd: 在当前目录下执行任务, sge的日志会输出到当前路径。 不增加该指令，所有投递的任务都会在家目录下执行-l resource=value: 请求资源数, 例如 -l vf=25G -l h=node1 就是任务的预估内存要25G(内存估计的值应稍微大于真实的内存，内存预估偏小可能会导致节点跑挂), 申请在node1上运行eg: qsub -cwd -l h=compute-12-13 run.sh -S： /bin/bash: 表示在bash环境下执行命令。默认tcsh.-sync y|n: 是否等待任务结束，返回退出码-o path: 指定标准输出的文件夹-j y|n ：是否将标准输入和标准输入合并成一个文件-a date_time 作业开始运行时间-b y[es]|n[o]判断作业指定是二进制文件或scripts。y ：是 n：scripts-display 使用X-windows-dl date_time 定义作业到期时间，在作业到期时间之前，作业的优先级会逐步提高，直到管理员指定的最高级别。-e 指定输出error文件的路径及文件名-hard 定义作业被调度的硬性要求-h 作业hold类型。u：表示用户hold,s:表示系统hold，o：表示被操作员hold，n：取消hold，U：取消用户hold，S：取消系统hold，O：取消操作员hold。-i 定义输入文件-j y[es]|n[o] 定义作业的标准错误输出是否写入的输出文件中-l resource=value, 表明作业运行所需要的资源。-m b|e|a|s|n 。定义邮件发送规则。b：作业开始时发送。e：作业结束时发送。a：作业失败时发送 s：作业挂起时发送。n：不发送-M user[@host] 定义邮件地址-notify ：定义发送SIGSTOP or SIGKILL信号的延迟时间-now y[es]|n[o]：立即执行作业-N 作业名-o [[hostname]:]path ：定义输出文件路径、文件名-P project_name：定义项目名称-p priority ：定义优先级-pe parallel_environment：定义并行环境-q wc_queue_list：定义作业运行队列-R y[es]|n[o]：定义是否为作业保留资源。-r y[es]|n[o]：定义作业失败后是否重新运行-soft 定义作业被调度的软性要求-u username,只有qlter命令可以使用该参数。修改作业的用户名-v variable：定义环境变量-verbose 使qrsh命令输出信息-verify 验证作业参数时使用-V 传递当前命令的所有环境变量-clear 清除预设的环境配资，恢复为默认值 qconf 队列 1234567# 显示所有队列qconf -sql #显示一个队列的详细配置信息（包含的节点和负载等信息）qconf -sq queue_name #显示当前配置为执行主机的所有主机的名称列表#添加一个队列qconf -aq queue_name 用户组 12345678# 查看所有用户组qconf -sul# 查看某个用户组信息qconf -su 用户组名# 编辑用户组配置qconf -mu 用户组名# 创建用户组并把用户添加到用户组qconf -au 用户名 用户组名 其他 1234567891011121314&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEADqconf -sel #显示当前配置为执行主机的所有主机的名称列表qconf -se hostname #显示指定的执行主机的详细信息qconf -sh #显示具有管理权限的主机列表qconf -ss #显示提交主机列表qconf -sc #显示所配置的资源属性列表qconf -su #显示指定项目下的用户 &#123;project&#125;_userqconf -sconf #显示当前配置qconf -aprj #添加一个新项目qconf -sprjl #显示项目列表qconf -aq queue_name #添加一个队列qconf -ahgrp @host_group_name #添加主机组，hostlist中主机列表间隔用空格qconf -mconf #编辑默认Shell【login_shells bash,sh,ksh,csh,tcsh】qconf -aq #显示默认队列模板 qstat 显示队列和作业的状态qstat 命令—用于查询作业状态信息123456789101112131415命令格式：qatat [-f][-a][-i] [-n][-s] [-R] [-Q][-q][-B][-u]参数说明：-f jobid 列出指定作业的信息-a 列出系统所有作业-i 列出不在运行的作业-n 列出分配给此作业的结点-s 列出队列管理员与scheduler 所提供的建议-R 列出磁盘预留信息-Q 操作符是destination id，指明请求的是队列状态-q 列出队列状态，并以alternative 形式显示-au userid 列出指定用户的所有作业-B 列出PBS Server 信息-r 列出所有正在运行的作业-Qf queue 列出指定队列的信息-u 若操作符为作业号，则列出其状态。 任务状态123456qw: 表示等待状态hqw: 任务挂起等待中，待依赖的任务完成后执行Eqw: 投递任务出错r: 表示任务正在运行s: 暂时挂起dr: 节点挂了之后，删除任务就会出现这个状态，只有节点重启之后，任务才会消失 qmod 修改队列和作业的状态123456789# 任务的挂起与恢复qmod -sj jid (已在run的job) 或 qhold jid(qw的job)挂起qmod -usj jid 或 qrls jid恢复之前挂起的任务## 登陆节点任务挂起kill -STOP pid ## 登陆节点任务恢复kill -CONT pid]]></content>
      <categories>
        <category>任务调度</category>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>SGE</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Git-1.安装配置及基本操作]]></title>
    <url>%2F0003.%E5%BA%94%E7%94%A8-99.other%2FGit-1.%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AE%E5%8F%8A%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[培训材料组内培训PPT 官方文档官方文档-中文 Git安装主流系统安装参考如下： linux：Debian或Ubuntu：sudo apt-get install gitCentos: sudo yum install git Windows/ macOS ：下载地址：https://git-scm.com/downloads 其他机器或系统版本可参考官网 Git安装 账号配置配置全局统一的账号配置全局的name和email，参照你创建的工程Git global setup 12345git config --global user.name &quot;liubo4&quot; #存储账号git config --global user.email &quot;liubo4@genomics.cn&quot; #存储邮箱# --global：表示该机器上所有的Git仓库都会使用这个配置ssh-keygen -t rsa -C &quot;liubo4@genomics.cn&quot; #生成秘钥, 将秘钥生成的文件： id_rsa.pub 中的内容复制到gitlab中(Edit profile==&gt;SSH key==&gt; Add an SSH key )；后期可以通过密钥进行身份验证。 配置多个账号有时候，由于各种原因（需要同时使用多个仓库平台，例如：github和本地的gitlab），我们会需要配置多个账号。 基本知识Git的4个结构区域git的整体工作结构区域分为如下4个区域类型 工作区：项目的工作目录 暂存区：修改的文件会暂时存储在该区域，也叫index区 本地仓库：即隐藏目录 .git，是 Git 的版本库 远程仓库：即常说的github，gitlab，可在线获取 Git文件的4种基本状态 未追踪（untracked）文件已经在文件夹中，但没有加入git库，也不参与版本控制 已修改（modified）已修改表示修改了文件，但还没保存到本地git仓库中 已暂存（staged）已暂存表示对一个已修改的文件的当前版本做了标记，使之包含在下次提交的快照中。 已提交（committed）已提交表示数据已经安全的保存在本地git仓库中。 Git的常见配置文件忽略文件（.gitignore）当不想把某些文件纳入版本控制中，比如数据库文件，临时文件，设计文件时，可以使用“.gitignore”文件进行标识“.gitignore”文件规则： 忽略文件中的空行或以井号（#）开始的行。 可以使用Linux通配符。例如：星号（*）代表任意多个字符，问号（？）代表一个字符，方括号（[abc]）代表可选字符范围，大括号（{string1,string2,…}）代表可选的字符串等。 如果名称的最前面有一个感叹号（!），表示例外规则，将不被忽略。 如果名称的最前面是一个路径分隔符（/），表示要忽略的文件在此目录下，而子目录中的文件不忽略。 如果名称的最后面是一个路径分隔符（/），表示要忽略的是此目录下该名称的子目录，而非文件（默认文件或目录都忽略）。 eg：12345#为注释*.txt #忽略所有.txt结尾的文件!lib.txt #lib.txt除外build/ #忽略build/目录下的所有文件doc/*.txt #会忽略 doc目录下“.txt”为后缀的文件 常用命令仓库的初始化 本地创建新的仓库 12git init# 新建一个git仓库，运行完后会在当前目录下生成一个.git 目录 从远程仓库中拉取一个仓库到本地 完成秘钥配置后，可以将gitlab上的项目下载到本地进行部署 1git clone remote-repertor（https://github.com/libgit2/libgit2） local—repertory(可以省了默认保持原仓库名称) 完成项目到本地的传输后，可以进行相应的安装或使用，git clone仅能用于将远程项目初始化到本地（从无到有）不支持后续的更新同步 下载更新作为多人版本协作，会有不同的人同时对项目进行更新，所以有时候会有需求将他人的更新同步到本地流程中，使用 git pull命令 正规流程 123456789git status（查看本地分支文件信息，确保更新时不产生冲突）git checkout – [file name] （若文件有修改，可以还原到最初状态; 若文件需要更新到服务器上，应该先merge到服务器，再更新到本地）git branch（查看当前分支情况）git checkout remote branch (若分支为本地分支，则需切换到服务器的远程分支)git pull 快速流程 12#上面是比较安全的做法，如果你可以确定什么都没有改过只是更新本地代码git pull (一句命令搞定) 提交更新提交更新主要分三个部分，1，更新提交到本地暂存区；2.更新到本地版本库；3.推送更新。 1234567git add file.name #将更新的文件提交到本地的缓存区；git add . # 提交目录下所有更新git rm file.name # 把目录的文件从跟踪中移除git status # 查看当前提交到缓存区的所有更新git commit -m "version description" #将缓存区的内容更新到本地的版本库；git commit --amend -m "新的提交信息" # 更新已经提交过的commit内容git push #将本地最新的版本推送到远程仓库。如果是clone下来的仓库，会推送到原下载链接所在的仓库。 添加tag像其他版本控制系统（VCS）一样，Git 可以给仓库历史中的某一个提交打上标签，以示重要。 比较有代表性的是人们会使用这个功能来标记发布结点（ v1.0 、 v2.0 等等）。1234567891011121314151617#列出标签git tag # 例如符合条件的标签$ git tag -l "v1.8.5*"v1.8.5v1.8.5-rc0v1.8.5-rc1v1.8.5-rc2v1.8.5-rc3#创建标签，其中 -m可选，版本校验值缺省时默认是当前版本git tag -a v1.4 -m "my version 1.4" ca82a6dff817ec66f44342007202690a93763949# 共享标签， 标签在推送时，默认是不会进行推送的，需要进行显式的提交git push origin v1.5 # 推送指定的tag版本（v1.5)git push origin --tags # 推送全部tags 版本管理查看本地的版本记录12345# 查看版本更新日志$ git log # 只显示每个版本改动的文件名称$ git log --name-only 比较两个版本之间的区别12345678# 查看任意两个版本之间的改动：$ git diff 版本号码1 版本号码2# 查看两个提交版本id修改了那些文件，可以使用$ git diff commit-id1 commit-id2 --stat#比较两个版本号码的src 文件夹的差异$ git diff 版本号码1 版本号码2 src 对本地版本进行更改1234git reset gitversion filename# 撤销提交缓存区的偶作git reset --hard gitversion # 还原本地流程到之前的gitversion版本，放弃本地的后续版本git reset --hard HEAD^ #回退到上一个版本（"^" 代表回退的版本数，两个版本用"^^"git reset --hard HEAD~50#还原git到指定版本 –hard 不保存所有变更–soft 保留表更且变更内容处于staged–mixed 保留biang且变更内容处于modified（默认） 对特定文件进行管理 - 恢复修改的文件 只修改了文件，没进行任何git操作 12# 只是修改了文件，没有任何 git 操作，直接一个命令就可回退：$ git checkout -- aaa.txt # aaa.txt为文件名 修改了文件，并提交到暂存区（即编辑之后，gitadd但没有 git commit -m ….） 123$ git log --oneline # 可以省略$ git reset HEAD # 回退到当前版本$ git checkout -- aaa.txt # aaa.txt为文件名 修改了文件，并提交到仓库区（即编辑之后，gitadd和 git commit -m ….） 123$ git log --oneline # 可以省略$ git reset HEAD^ # 回退到上一个版本$ git checkout -- aaa.txt # aaa.txt为文件名 分支管理1234567891011121314151617181920212223#列出分支命令：git branch#创建分支命令：git branch (branchname)#切换分支命令:git checkout (branchname)#创建并切换该分支git checkout -b &lt;name&gt;#合并分支命令:git merge newtest # 将newtest分支合并到当前分支#重命名分支git branch -m/-M &lt;oldname&gt; &lt;newname&gt; #删除分支命令：git branch -d (branchname)#恢复的删除分支git branch &lt;name&gt; &lt;删除分支的commitID&gt; 远程仓库操作12345678910111213#克隆一个已有的远程git仓库git clone https://github.com/lh3/wgsim.git#把一个已有的本地仓库与远程仓库关联起来。git remote add origin https://github.com/lh3/wgsim.git#可以把本地库的所有内容推送到远程库master/dev分支git push origin master/ git push origin dev#将远程 origin 的 master 分支拉取过来，与本地的test分支合并。git pull origin master:testgit pull = git fetch + git merge Patching有记录的进行版本回退。 1234# Given one or more existing commits, revert the changes that the related patches introduce, and record some new commits that record them.git revert 当发生一次错误的提交以后，可以进行版本的还原（同时保留错误的提交记录） 大文件管理 通过使用 git lfs 进行流程中大文件的管理，需要提前安装 1.8.5以上版本的git 。 参考文档 git lfs下载安装下载 git-lfs-linux-386-v2.10.0.tar.gz 拷贝至集群。 1234567891011121314tar -zxvf git-lfs-linux-386-v2.10.0.tar.gz解压后会在当前目录产生下列4个文件-rw-r--r-- 1 OseqPub b2c1 77881 May 9 00:17 CHANGELOG.md-rwxr-xr-x 1 OseqPub b2c1 10248192 May 9 00:20 git-lfs-rwxr-xr-x 1 OseqPub b2c1 389 May 9 00:17 install.sh-rw-r--r-- 1 OseqPub b2c1 7522 May 9 00:17 README.md#将git-lfs所在目录加到环境变量中$PATH #执行 安装git lfsgit lfs install for i in `find . -type f -size +100M ` ;do git lfs track $i ;done # 跟踪大文件git lfs ls-files 可以显示当前跟踪的文件列表git lfs clone 下载lfs文件]]></content>
      <categories>
        <category>编程拾慧</category>
        <category>Git</category>
      </categories>
      <tags>
        <tag>培训</tag>
        <tag>Git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[天河二号作业调度系统简单使用]]></title>
    <url>%2F0003.%E5%BA%94%E7%94%A8-99.other%2F2018-12-07.task-manage-in-Tianhe2%2F</url>
    <content type="text"><![CDATA[天河用户手册天河用户手册 现将天河2号的作业调度系统的简单使用方法记录于此: MPI天河2号默认使用mpich-3.2.1（使用icc-14.0.2 编译），是天河2号自主实现的mpi版本，具有较高的效率。 module 模块管理1234567module avail # 查看可用的模块的列表。module load [modulesfile] # 加载需要使用的modulefiles。module load OpenFOAM/2.2.2 # 示例 module其它用法，可在help中查询 作业调度基础12345678yhi # yhinfo命令的简写，用于查看节点状态 其中PARTITION表示分区；NODES表示节点数；NODELIST为节点列表；STATE表示节点运行状态， 其中，idle表示节点处于空闲状态，allocated表示节点已经分配了一个或多个作业。yhq # yhqueue命令的简写，用于查看作业运行情况 推荐使用“yhq -a”查看作业状态信息 其中JOBID 表示任务ID，Name表示任务名称，USER为用户，TIME为已运行时间，NODES表示占用节点数，NODELIST为任务运行的节点列表。 交互式提交作业在shell窗口中执行yhrun命令，主要命令格式如下： 123456789101112131415161718192021222324252627282930yhrun [options] program-n, --ntasks=number # 指定要运行的任务数-c, --cpus-per-task=ncpus # 每个任务需要ncpus 个处理器核-N, --nodes=minnodes[-maxnodes] # 请求为作业至少分配minnodes（最大maxnodes）个节点。 （例如“-N 2-4”或“--nodes=2-4”） 如果没有指定-N，缺省行为是分配足够多的节点以满足-n和-c参数的需求-p, --partition=partition name # 在指定分区中分配资源。如未指定，则由控制进程在系统默认分区中分配资源。-w, --nodelist=node name list # 请求指定的节点名字列表。作业分配资源中将至少包含这些节点。 列表可以用逗号分隔的节点名或节点范围（如cn[1-5,7,...]）指定，或者用文件名指定；如果参数中包含“/”字符，则会被当作文件名。-x, --exclude=node name list # 不要将指定的节点分配给作业-D, --workdir=directory # set working directory for batch script-I, --immediate[=secs] # exit if resources not available in &quot;secs&quot;-o, --output=out # location of stdout redirection-J, --job-name=jobname # name of job-l, --label # prepend task number to lines of stdout/err-h, --help # 若需使用yhrun更多选项，可通过“yhrun –h”或“yhrun --help”查看。# eg:yhrun -n 4 -p bigdata hostname yhrun -n 4 -w cn[7303-7306] -p bigdata hostname yhrun -n 4 -N 4 -w cn[7303-7304] -p bigdata hostname yhrun -n 4 -N 4 -x cn[7303-7304] -p bigdata hostname 节点资源抢占命令 yhalloc该命令支持用户在提交作业前，抢占所需计算资源 1yhalloc -N 1 -p bigdata 通过yhq查看相应的jobID 为1051，节点为cn7314，然后ssh到对应节点进行操作 ####取消自己的作业 使用yhcancel命令 1yhcancel jobid 批处理作业命令 yhbatch在资源满足要求时，分配完计算节点之后，系统将在所分配的第一个计算节点（而不是登录节点）上加载执行用户的作业脚本。 123456789cat &gt; mybash.sh #!/bin/bash yhrun -n 4 -N 4 -p bigdata hostname chmou mybash.sh yhbatch -N 4 -p bigdata ./mybash.sh 计算开始后，工作目录中会生成以slurm开头的.out 文件为输出文件。更多选项，用户可以通过yhbatch --help命令查看。如果不需要使用MPI的话，也可以不使用yhrun 单个节点上提交多个作业因为天河2是独享作业，当一个节点上已经被分配出去之后，即便没有使用全部的核心，也无法继续提交作业。所以，若想在一个节点上运行多个作业，必须同时提交上去，如下：某用户有4个 a.out 需要运行，每个a.out最多只能高效运用6 个CPU 核，那么可以构建下面的任务脚本，在一个计算节点上同时运行多个作业: 12345678cat &gt; job.sh #!/bin/bashyhrun –n 4 a.out arg.1 &amp; yhrun –n 4 a.out arg.2 &amp;yhrun –n 4 a.out arg.3 &amp;yhrun –n 4 a.out arg.4 &amp;wait # important 然后通过yhbatch –N 1 job.sh来一次提交计算任务，使所有小的计算任务都可以在一个节点同时进行计算。如果不需要使用MPI的话，也可以不使用yhrun。]]></content>
      <categories>
        <category>任务调度</category>
      </categories>
      <tags>
        <tag>任务调度</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[爬虫学习记录-相关软件&包记录]]></title>
    <url>%2F0002.%E5%BC%80%E5%8F%91-02.python%2FPython-%E7%88%AC%E8%99%AB%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95-%E7%9B%B8%E5%85%B3%E8%BD%AF%E4%BB%B6-%E5%8C%85%E8%AE%B0%E5%BD%95%2F</url>
    <content type="text"><![CDATA[爬虫学习过程涉及的软件： 软件1. Python3.5.5整体爬虫代码的框架语言，所有Python程序及包均基于本版本。 2. chromeDriver模拟登陆chrome浏览器，驱动浏览器完成相关操作。 3.PhantomJS - 官方文档一个无界面的，可脚本编程的WebKit浏览器引擎，原生支持多种Web标准：DOM操作、CSS选择器、JSON、Canvas和SVG。Selenium包支持该引擎，使用该引擎，就可以避免运行过程中不断弹出浏览器。 4.MYSQL一个关系型数据库，用于存储相关的数据。 5.Docker一种容器技术，可以将环境和应用打包，形成一个独立的应用，这个应用可以直接被分发到任意一个支持Docker的环境中，通过简单的命令启动运行。 Python包请求库1. requests - 中文文档Python中唯一的一个http原生库。 2.Selenium - 中文文档一个自动化测试工具，利用他可以驱动浏览器执行特定的动作，如点击、下拉等操作。针对一些JavaScript渲染的页面来说，这种抓去方式非常有效。 3.aiohttp可以在爬取过程中，提供异步Web服务的库。 解析库1.lxmlPython的一个解析库，可以支持HTML和XML的解析，支持XPath的解析方式，解析效率较高。 2.beautifulsoup4HTML和XML的解析库 3.pyquery网页解析工具，提供了和jQuery类似的语法来解析html文档，支持css选择器，。 相关模块1.tesserocrPython的一个ORC（Optical Character Recognition光学字符识别）识别库。 2.pymysql用于连接MYSQL数据库，在Python中访问Mysql数据库。 3.Web库-FlaskFlask一个轻量级的Web服务程序，用来做一些Web的API服务。 4.TornodeTornode一个支持异步的Web框架。 APP爬取相关库的安装1.Charles一个网络抓包工具。 2.mitmproxy一个支持http和https的抓包工具，类似Charles和Fiddler的功能，通过控制台操作。 爬虫框架1.Pyspider2.Scrapy]]></content>
      <categories>
        <category>编程拾慧</category>
        <category>python</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[下机数据(fastq)质控软件汇总]]></title>
    <url>%2F0003.%E5%BA%94%E7%94%A8-03.software%2F2018-11-19.%E4%B8%8B%E6%9C%BA%E6%95%B0%E6%8D%AE%EF%BC%88fastq%EF%BC%89%E8%B4%A8%E6%8E%A7%E8%BD%AF%E4%BB%B6%E6%B1%87%E6%80%BB%2F</url>
    <content type="text"><![CDATA[常用的生物信息学ruan jian SoapNuke Fastp trimmomatic AfterQC seqtk]]></content>
      <categories>
        <category>NGS</category>
        <category>肿瘤检测</category>
        <category>software</category>
      </categories>
      <tags>
        <tag>NGS</tag>
        <tag>质控</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[肿瘤组Linux培训资料]]></title>
    <url>%2F0003.%E5%BA%94%E7%94%A8-02.Linux%2FLinux-%E8%82%BF%E7%98%A4%E7%BB%84Linux%E5%9F%B9%E8%AE%AD%E8%B5%84%E6%96%99%2F</url>
    <content type="text"><![CDATA[肿瘤组内部Linux系统操作普及培训材料1.简单了解Linux系统的使用 Windows安装软件XshellXftp Mac 安装软件FileZilla]]></content>
      <categories>
        <category>Linux</category>
        <category>内部培训材料</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[参考基因组版本问题汇总]]></title>
    <url>%2F0001.%E7%9F%A5%E8%AF%86-01.NGS%2F2018-07-11.%E5%8F%82%E8%80%83%E5%9F%BA%E5%9B%A0%E7%BB%84%E7%89%88%E6%9C%AC%E9%97%AE%E9%A2%98%E6%B1%87%E6%80%BB%2F</url>
    <content type="text"><![CDATA[不同版本参考基因组的位置坐标转换在使用参考基因组时，经常会遇到一些版本的问题，比如使用注释文件和bed文件时，不同版本的位置坐标不能直接使用，这时候，我们就需要对坐标进行转换，这里记录下一些常用的坐标转换工具： 类型 支持格式 地址 推荐指数 Liftover 在线 bed http://genome.ucsc.edu/cgi-bin/hgLiftOver 一般 Liftover 本地 bed和gff http://hgdownload.cse.ucsc.edu/admin/exe/linux.x86_64/liftOver 推荐 Remap 在线 hgvs，bed，gvf，gff，gtf，Text ASN.1，Binary ASN.1，UCSC Region和VCF https://www.ncbi.nlm.nih.gov/genome/tools/remap 推荐 CrossMap 本地 SAM/BAM,，Wiggle/BigWig， bed， gff/gtf，VCF http://crossmap.sourceforge.net/ 推荐 picard 本地 interval和VCF http://broadinstitute.github.io/picard/ 推荐VCF转换]]></content>
      <categories>
        <category>知识沉淀</category>
        <category>NGS</category>
        <category>数据库</category>
        <category>肿瘤检测</category>
      </categories>
      <tags>
        <tag>参考基因</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python-包-包管理器Conda]]></title>
    <url>%2F0003.%E5%BA%94%E7%94%A8-99.other%2F%E7%8E%AF%E5%A2%83%E7%AE%A1%E7%90%86-%E5%8C%85%E7%AE%A1%E7%90%86%E5%99%A8Conda%2F</url>
    <content type="text"><![CDATA[参考文档官方文档Github下载地址 Conda安装和卸载Conda安装下载相应的安装sh 示例如下:更多发行版本获取1234wget https://repo.continuum.io/archive/Anaconda3-5.0.0-Linux-x86_64.sh #(下载Anaconda的Linux版本)wget https://repo.anaconda.com/miniconda/Miniconda3-py37_4.11.0-Linux-x86_64.sh #(miniConda)bash Anaconda3-5.0.0-Linux-x86_64.sh #安装source ~/.bashrc #更新环境变量 mamba安装conda的一个优化插件，可以大幅度提高安装速度1conda install -c conda-forge -c bioconda mamba Conda配置镜像管理 备选镜像清单 12345678910111213141516171819202122232425262728293031323334353637383940# 添加清华的Conda镜像conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/conda config --add channels defaultsconda config --add channels conda-forgeconda config --add channels bioconda# 阿里云conda config --add channels https://mirrors.aliyun.com/anaconda/pkgs/main/conda config --add channels https://mirrors.aliyun.com/anaconda/cloud/conda-forge/conda config --add channels https://mirrors.aliyun.com/anaconda/cloud/bioconda/# 北京外国语conda config --add channels https://mirrors.bfsu.edu.cn/anaconda/pkgs/main/conda config --add channels https://mirrors.bfsu.edu.cn/anaconda/cloud/conda-forge/conda config --add channels https://mirrors.bfsu.edu.cn/anaconda/cloud/bioconda/# 北大镜像conda config --add channels https://mirrors.pku.edu.cn/anaconda/pkgs/main/conda config --add channels https://mirrors.pku.edu.cn/anaconda/cloud/conda-forge/conda config --add channels https://mirrors.pku.edu.cn/anaconda/cloud/bioconda/# 哈工大conda config --add channels https://mirrors.hit.edu.cn/anaconda/pkgs/main/conda config --add channels https://mirrors.hit.edu.cn/anaconda/cloud/conda-forge/conda config --add channels https://mirrors.hit.edu.cn/anaconda/cloud/bioconda/# 南京大学conda config --add channels https://mirror.nju.edu.cn/anaconda/pkgs/main/conda config --add channels https://mirror.nju.edu.cn/anaconda/cloud/conda-forge/conda config --add channels https://mirror.nju.edu.cn/anaconda/cloud/bioconda/# 北京交通大学conda config --add channels https://mirror.bjtu.edu.cn/anaconda/pkgs/main/conda config --add channels https://mirror.bjtu.edu.cn/anaconda/cloud/conda-forge/conda config --add channels https://mirror.bjtu.edu.cn/anaconda/cloud/bioconda/# 西安交通大学conda config --add channels https://mirrors.xjtu.edu.cn/anaconda/pkgs/main/conda config --add channels https://mirrors.xjtu.edu.cn/anaconda/cloud/conda-forge/conda config --add channels https://mirrors.xjtu.edu.cn/anaconda/cloud/bioconda/ 删除某个镜像 1conda config --remove channels &apos;https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/&apos; 删除所有镜像 1conda config --remove-key channels Conda卸载修改~/.bash_profile中的环境变量，去除家目录中隐藏的.condarc文件.conda文件和.continuum目录12rm -rf ~/minicondarm -rf ~/.condarc ~/.conda ~/.continuum Conda使用环境管理（conda env） 命令 功能 conda env list 列出所有Conda的环境 conda info -e 列出所有的conda环境 conda env create 创建环境 conda env create -f *.yaml 基于配置文件创建环境 conda create -n $env_name [package] 创建conda环境，同时安装相关的package（可选） conda remove -n env_name –all 删除环境 source activate \$env 切换环境 source deactivate \$env 退出环境 包管理 命令 功能 conda list 查看已经安装的包 conda list -n \$env 查看环境\$env中安装的包 conda search 查看可用的软件包 conda install &lt; package&gt;=x.x 安装x.x版本的package 更新conda环境conda update conda 创建环境conda create -n ENV_Demo package1 package2 package3；创建一个名为ENV_Demo的环境，并在环境中安装 package1 package2 package3 三个软件包 激活环境source activate ENV_Demo 虚拟环境的GCC升级123456conda install -c moussi gcc_impl_linux-64ln -s /share2/home/anconda3/envs/my_env/libexec/gcc/x86_64-conda_cos6-linux-gnu/7.3.0/gcc /share2/home/anaconda3/my_env/bin/gccconda install gcc_linux-64conda deactivateconda activate my_engcc -v 常见安装1234# 安装 perlconda install -c conda-forge perl=5.22 # 安装 R conda install -c conda-forge r-base]]></content>
      <categories>
        <category>编程拾慧</category>
        <category>python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[常用shell命令 - awk]]></title>
    <url>%2F0003.%E5%BA%94%E7%94%A8-02.Linux%2FLinux-%E5%B8%B8%E7%94%A8shell%E5%91%BD%E4%BB%A4-awk%2F</url>
    <content type="text"><![CDATA[awk是行处理器: 相比较屏幕处理的优点，在处理庞大文件时不会出现内存溢出或是处理缓慢的问题，通常用来格式化文本信息awk处理过程: 依次对每一行进行处理，然后输出awk命令形式:12345678awk [-F|-f|-v] ‘BEGIN&#123;&#125; //&#123;command1; command2&#125; END&#123;&#125;’ file [-F|-f|-v] 大参数，-F指定分隔符，-f调用脚本，-v定义变量 var=value&apos; &apos; 引用代码块BEGIN 初始化代码块，在对每一行进行处理之前，初始化代码，主要是引用全局变量，设置FS分隔符// 匹配代码块，可以是字符串或正则表达式&#123;&#125; 命令代码块，包含一条或多条命令； 多条命令使用分号分隔END 结尾代码块，在对每一行进行处理之后再执行的代码块，主要是进行最终计算或输出结尾摘要信息 特殊要点:|变量 | 含义||-|-||$0 |表示整个当前行 ||$1 | 每行第一个字段 ||NF | 字段数量变量，通过$(NF-1) 可以实现末尾字段进行取值 ||NR | 每行的记录号，多文件记录递增 ||FNR | 与NR类似，不过多文件记录不递增，每个文件都从1开始 ||\t | 制表符 ||\n | 换行符 ||FS | BEGIN时定义分隔符 ||RS | 输入的记录分隔符， 默认为换行符(即文本是按一行一行输入) ||~ | 匹配，与==相比不是精确比较 ||!~ | 不匹配，不精确比较 ||== | 等于，必须全部相等，精确比较 ||!= | 不等于，精确比较 ||&amp;&amp; | 逻辑与|||| | 逻辑或||+ | 匹配时表示1个或1个以上 ||/[0-9][0-9]+/ | 两个或两个以上数字 ||/[0-9][0-9]*/ | 一个或一个以上数字 ||FILENAME| 文件名||OFS | 输出字段分隔符， 默认也是空格，可以改为制表符等||ORS | 输出的记录分隔符，默认为换行符,即处理结果也是一行一行输出到屏幕||-F’[:#/]’ | 定义三个分隔符| 1234567891011121314151617181920print &amp; $0print 是awk打印指定内容的主要命令awk &apos;&#123;print&#125;&apos; /etc/passwd == awk &apos;&#123;print $0&#125;&apos; /etc/passwd awk &apos;&#123;print &quot; &quot;&#125;&apos; /etc/passwd //不输出passwd的内容，而是输出相同个数的空行，进一步解释了awk是一行一行处理文本awk &apos;&#123;print &quot;a&quot;&#125;&apos; /etc/passwd //输出相同个数的a行，一行只有一个a字母awk -F&quot;:&quot; &apos;&#123;print $1&#125;&apos; /etc/passwd awk -F: &apos;&#123;print $1; print $2&#125;&apos; /etc/passwd //将每一行的前二个字段，分行输出，进一步理解一行一行处理文本awk -F: &apos;&#123;print $1,$3,$6&#125;&apos; OFS=&quot;\t&quot; /etc/passwd //输出字段1,3,6，以制表符作为分隔符-f指定脚本文件awk -f script.awk fileBEGIN&#123;FS=&quot;:&quot;&#125;&#123;print $1&#125; //效果与awk -F&quot;:&quot; &apos;&#123;print $1&#125;&apos;相同,只是分隔符使用FS在代码自身中指定 awk &apos;BEGIN&#123;X=0&#125; /^$/&#123; X+=1 &#125; END&#123;print &quot;I find&quot;,X,&quot;blank lines.&quot;&#125;&apos; test I find 4 blank lines. ls -l|awk &apos;BEGIN&#123;sum=0&#125; !/^d/&#123;sum+=$5&#125; END&#123;print &quot;total size is&quot;,sum&#125;&apos; //计算文件大小total size is 17487 -F指定分隔符$1 指指定分隔符后，第一个字段，$3第三个字段， \t是制表符一个或多个连续的空格或制表符看做一个定界符，即多个空格看做一个空格1234567891011121314awk -F&quot;:&quot; &apos;&#123;print $1&#125;&apos; /etc/passwdawk -F&quot;:&quot; &apos;&#123;print $1 $3&#125;&apos; /etc/passwd //$1与$3相连输出，不分隔awk -F&quot;:&quot; &apos;&#123;print $1,$3&#125;&apos; /etc/passwd //多了一个逗号，$1与$3使用空格分隔awk -F&quot;:&quot; &apos;&#123;print $1 &quot; &quot; $3&#125;&apos; /etc/passwd //$1与$3之间手动添加空格分隔awk -F&quot;:&quot; &apos;&#123;print &quot;Username:&quot; $1 &quot;\t\t Uid:&quot; $3 &#125;&apos; /etc/passwd //自定义输出 awk -F: &apos;&#123;print NF&#125;&apos; /etc/passwd //显示每行有多少字段awk -F: &apos;&#123;print $NF&#125;&apos; /etc/passwd //将每行第NF个字段的值打印出来 awk -F: &apos;NF==4 &#123;print &#125;&apos; /etc/passwd //显示只有4个字段的行awk -F: &apos;NF&gt;2&#123;print $0&#125;&apos; /etc/passwd //显示每行字段数量大于2的行awk &apos;&#123;print NR,$0&#125;&apos; /etc/passwd //输出每行的行号awk -F: &apos;&#123;print NR,NF,$NF,&quot;\t&quot;,$0&#125;&apos; /etc/passwd //依次打印行号，字段数，最后字段值，制表符，每行内容awk -F: &apos;NR==5&#123;print&#125;&apos; /etc/passwd //显示第5行awk -F: &apos;NR==5 || NR==6&#123;print&#125;&apos; /etc/passwd //显示第5行和第6行route -n|awk &apos;NR!=1&#123;print&#125;&apos; //不显示第一行 //匹配代码块//纯字符匹配 !//纯字符不匹配 ~//字段值匹配 !~//字段值不匹配 ~/a1|a2/字段值匹配a1或a2123456789101112awk &apos;/mysql/&apos; /etc/passwdawk &apos;/mysql/&#123;print &#125;&apos; /etc/passwdawk &apos;/mysql/&#123;print $0&#125;&apos; /etc/passwd //三条指令结果一样awk &apos;!/mysql/&#123;print $0&#125;&apos; /etc/passwd //输出不匹配mysql的行awk &apos;/mysql|mail/&#123;print&#125;&apos; /etc/passwdawk &apos;!/mysql|mail/&#123;print&#125;&apos; /etc/passwdawk -F: &apos;/mail/,/mysql/&#123;print&#125;&apos; /etc/passwd //区间匹配awk &apos;/[2][7][7]*/&#123;print $0&#125;&apos; /etc/passwd //匹配包含27为数字开头的行，如27，277，2777...awk -F: &apos;$1~/mail/&#123;print $1&#125;&apos; /etc/passwd //$1匹配指定内容才显示awk -F: &apos;&#123;if($1~/mail/) print $1&#125;&apos; /etc/passwd //与上面相同awk -F: &apos;$1!~/mail/&#123;print $1&#125;&apos; /etc/passwd //不匹配awk -F: &apos;$1!~/mail|mysql/&#123;print $1&#125;&apos; /etc/passwd IF语句必须用在{}中，且比较内容用()扩起来123awk -F: &apos;&#123;if($1~/mail/) print $1&#125;&apos; /etc/passwd //简写awk -F: &apos;&#123;if($1~/mail/) &#123;print $1&#125;&#125;&apos; /etc/passwd //全写awk -F: &apos;&#123;if($1~/mail/) &#123;print $1&#125; else &#123;print $2&#125;&#125;&apos; /etc/passwd //if...else... 条件表达式== != &gt; &gt;=1234567awk -F&quot;:&quot; &apos;$1==&quot;mysql&quot;&#123;print $3&#125;&apos; /etc/passwd awk -F&quot;:&quot; &apos;&#123;if($1==&quot;mysql&quot;) print $3&#125;&apos; /etc/passwd //与上面相同 awk -F&quot;:&quot; &apos;$1!=&quot;mysql&quot;&#123;print $3&#125;&apos; /etc/passwd //不等于awk -F&quot;:&quot; &apos;$3&gt;1000&#123;print $3&#125;&apos; /etc/passwd //大于awk -F&quot;:&quot; &apos;$3&gt;=100&#123;print $3&#125;&apos; /etc/passwd //大于等于awk -F&quot;:&quot; &apos;$3&lt;1&#123;print $3&#125;&apos; /etc/passwd //小于awk -F&quot;:&quot; &apos;$3&lt;=1&#123;print $3&#125;&apos; /etc/passwd //小于等于 逻辑运算符&amp;&amp; ||1234awk -F: &apos;$1~/mail/ &amp;&amp; $3&gt;8 &#123;print &#125;&apos; /etc/passwd //逻辑与，$1匹配mail，并且$3&gt;8awk -F: &apos;&#123;if($1~/mail/ &amp;&amp; $3&gt;8) print &#125;&apos; /etc/passwdawk -F: &apos;$1~/mail/ || $3&gt;1000 &#123;print &#125;&apos; /etc/passwd //逻辑或awk -F: &apos;&#123;if($1~/mail/ || $3&gt;1000) print &#125;&apos; /etc/passwd 数值运算12345678awk -F: &apos;$3 &gt; 100&apos; /etc/passwd awk -F: &apos;$3 &gt; 100 || $3 &lt; 5&apos; /etc/passwd awk -F: &apos;$3+$4 &gt; 200&apos; /etc/passwdawk -F: &apos;/mysql|mail/&#123;print $3+10&#125;&apos; /etc/passwd //第三个字段加10打印 awk -F: &apos;/mysql/&#123;print $3-$4&#125;&apos; /etc/passwd //减法awk -F: &apos;/mysql/&#123;print $3*$4&#125;&apos; /etc/passwd //求乘积awk &apos;/MemFree/&#123;print $2/1024&#125;&apos; /proc/meminfo //除法awk &apos;/MemFree/&#123;print int($2/1024)&#125;&apos; /proc/meminfo //取整 输出分隔符OFS123awk &apos;$6 ~ /FIN/ || NR==1 &#123;print NR,$4,$5,$6&#125;&apos; OFS=&quot;\t&quot; netstat.txtawk &apos;$6 ~ /WAIT/ || NR==1 &#123;print NR,$4,$5,$6&#125;&apos; OFS=&quot;\t&quot; netstat.txt //输出字段6匹配WAIT的行，其中输出每行行号，字段4，5,6，并使用制表符分割字段 输出处理结果到文件①在命令代码块中直接输出 route -n|awk ‘NR!=1{print &gt; “./fs”}’②使用重定向进行输出 route -n|awk ‘NR!=1{print}’ &gt; ./fs 格式化输出1netstat -anp|awk &apos;&#123;printf &quot;%-8s %-8s %-10s\n&quot;,$1,$2,$3&#125;&apos; printf表示格式输出%格式化输出分隔符-8长度为8个字符s表示字符串类型打印每行前三个字段，指定第一个字段输出字符串类型(长度为8)，第二个字段输出字符串类型(长度为8),第三个字段输出字符串类型(长度为10) 12netstat -anp|awk &apos;$6==&quot;LISTEN&quot; || NR==1 &#123;printf &quot;%-10s %-10s %-10s \n&quot;,$1,$2,$3&#125;&apos;netstat -anp|awk &apos;$6==&quot;LISTEN&quot; || NR==1 &#123;printf &quot;%-3s %-10s %-10s %-10s \n&quot;,NR,$1,$2,$3&#125;&apos; IF语句1234567891011121314awk -F: &apos;&#123;if($3&gt;100) print &quot;large&quot;; else print &quot;small&quot;&#125;&apos; /etc/passwdsmallsmallsmalllargesmallsmallawk -F: &apos;BEGIN&#123;A=0;B=0&#125; &#123;if($3&gt;100) &#123;A++; print &quot;large&quot;&#125; else &#123;B++; print &quot;small&quot;&#125;&#125; END&#123;print A,&quot;\t&quot;,B&#125;&apos; /etc/passwd //ID大于100,A加1，否则B加1awk -F: &apos;&#123;if($3&lt;100) next; else print&#125;&apos; /etc/passwd //小于100跳过，否则显示awk -F: &apos;BEGIN&#123;i=1&#125; &#123;if(i&lt;NF) print NR,NF,i++ &#125;&apos; /etc/passwd awk -F: &apos;BEGIN&#123;i=1&#125; &#123;if(i&lt;NF) &#123;print NR,NF&#125; i++ &#125;&apos; /etc/passwd另一种形式awk -F: &apos;&#123;print ($3&gt;100 ? &quot;yes&quot;:&quot;no&quot;)&#125;&apos; /etc/passwd awk -F: &apos;&#123;print ($3&gt;100 ? $3&quot;:\tyes&quot;:$3&quot;:\tno&quot;)&#125;&apos; /etc/passwd while语句1234567awk -F: &apos;BEGIN&#123;i=1&#125; &#123;while(i&lt;NF) print NF,$i,i++&#125;&apos; /etc/passwd 7 root 17 x 27 0 37 0 47 root 57 /root 6 数组12345678910netstat -anp|awk &apos;NR!=1&#123;a[$6]++&#125; END&#123;for (i in a) print i,&quot;\t&quot;,a[i]&#125;&apos;netstat -anp|awk &apos;NR!=1&#123;a[$6]++&#125; END&#123;for (i in a) printf &quot;%-20s %-10s %-5s \n&quot;, i,&quot;\t&quot;,a[i]&#125;&apos;9523 1 9929 1 LISTEN 6 7903 1 3038/cupsd 1 7913 1 10837 1 9833 1 应用11234awk -F: &apos;&#123;print NF&#125;&apos; helloworld.sh #输出文件每行有多少字段awk -F: &apos;&#123;print $1,$2,$3,$4,$5&#125;&apos; helloworld.sh #输出前5个字段awk -F: &apos;&#123;print $1,$2,$3,$4,$5&#125;&apos; OFS=&apos;\t&apos; helloworld.sh #输出前5个字段并使用制表符分隔输出awk -F: &apos;&#123;print NR,$1,$2,$3,$4,$5&#125;&apos; OFS=&apos;\t&apos; helloworld.sh #制表符分隔输出前5个字段，并打印行号 应用212awk -F&apos;[:#]&apos; &apos;&#123;print NF&#125;&apos; helloworld.sh //指定多个分隔符: #，输出每行多少字段awk -F&apos;[:#]&apos; &apos;&#123;print $1,$2,$3,$4,$5,$6,$7&#125;&apos; OFS=&apos;\t&apos; helloworld.sh //制表符分隔输出多字段 应用312awk -F&apos;[:#/]&apos; &apos;&#123;print NF&#125;&apos; helloworld.sh //指定三个分隔符，并输出每行字段数awk -F&apos;[:#/]&apos; &apos;&#123;print $1,$2,$3,$4,$5,$6,$7,$8,$9,$10,$11,$12&#125;&apos; helloworld.sh //制表符分隔输出多字段 应用4计算/home目录下，普通文件的大小，使用KB作为单位123456ls -l|awk &apos;BEGIN&#123;sum=0&#125; !/^d/&#123;sum+=$5&#125; END&#123;print &quot;total size is:&quot;,sum/1024,&quot;KB&quot;&#125;&apos;ls -l|awk &apos;BEGIN&#123;sum=0&#125; !/^d/&#123;sum+=$5&#125; END&#123;print &quot;total size is:&quot;,int(sum/1024),&quot;KB&quot;&#125;&apos; //int是取整的意思``` 应用5统计netstat -anp 状态为LISTEN和CONNECT的连接数量分别是多少 netstat -anp|awk ‘$6~/LISTEN|CONNECTED/{sum[$6]++} END{for (i in sum) printf “%-10s %-6s %-3s \n”, i,” “,sum[i]}’123应用6统计/home目录下不同用户的普通文件的总数是多少？ ls -l|awk ‘NR!=1 &amp;&amp; !/^d/{sum[$3]++} END{for (i in sum) printf “%-6s %-5s %-3s \n”,i,” “,sum[i]}’mysql 199root 374统计/home目录下不同用户的普通文件的大小总size是多少？ls -l|awk ‘NR!=1 &amp;&amp; !/^d/{sum[$3]+=$5} END{for (i in sum) printf “%-6s %-5s %-3s %-2s \n”,i,” “,sum[i]/1024/1024,”MB”}’123应用7输出成绩表 awk ‘BEGIN{math=0;eng=0;com=0;printf “Lineno. Name No. Math English Computer Total\n”;printf “————————————————————\n”}{math+=$3; eng+=$4; com+=$5;printf “%-8s %-7s %-7s %-7s %-9s %-10s %-7s \n”,NR,$1,$2,$3,$4,$5,$3+$4+$5} END{printf “————————————————————\n”;printf “%-24s %-7s %-9s %-20s \n”,”Total:”,math,eng,com;printf “%-24s %-7s %-9s %-20s \n”,”Avg:”,math/NR,eng/NR,com/NR}’ test0 [root@localhost home]# cat test0Marry 2143 78 84 77Jack 2321 66 78 45Tom 2122 48 77 71Mike 2537 87 97 95Bob 2415 40 57 62` awk手册http://www.chinaunix.net/old_jh/7/16985.html]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Shell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[常用shell命令 - cp]]></title>
    <url>%2F0003.%E5%BA%94%E7%94%A8-02.Linux%2FLinux-%E5%B8%B8%E7%94%A8shell%E5%91%BD%E4%BB%A4-cp%2F</url>
    <content type="text"><![CDATA[Linux cp（英文全拼：copy file）命令主要用于复制文件或目录。 语法1cp [options] source dest 或1cp [options] source... directory 参数说明：-a 或 --archive 此选项通常在复制目录时使用，它保留链接、文件属性，并复制目录下的所有内容。其作用等于dpR参数组合 -b 或--backup 删除，覆盖目标文件之前的备份，备份文件会在字尾加上一个备份字符串。 -d 或--no-dereference 当复制符号连接时，把目标文件或目录也建立为符号连接，并指向与源文件或目录连接的原始文件或目录。 相当于 Windows 系统中的快捷方式。 -f 或 --force 强行复制文件或目录， 不论目的文件或目录是否已经存在，不给出提示。 -i 或 --interactive 与 -f 选项相反，覆盖文件之前先询问用户，回答 y 时目标文件将被覆盖。 -l 或 --link 对源文件建立硬链接，而非复制文件 -p 或 --preserve 保留源文件或目录的属性，包括所有者、所属组、权限与时间 -P 或 --parents 保留源文件或目录的路径，此路径可以是绝对路径或相对路径，且目的目录必须已经存在 -r 递归处理，将指定目录下的文件与子目录一并处理。若源文件或目录的形态，不属于目录或符号链接，则一律视为普通文件处理 -R 或 --recursive 递归处理，将指定目录下的文件及子目录一并处理 -s 或 --symbolic-link 对源文件建立符号链接，而非复制文件 -S &lt;备份字尾字符串&gt; 或 --suffix=&lt;备份字尾字符串&gt; 用&quot;-b&quot;参数备份目的文件后，备份文件的字尾会被加上一个备份字符串。默认的备份字尾符串是符号&quot;~&quot; -u 或 --update 使用这项参数之后，只会在源文件的修改时间(Modification Time)较目的文件更新时，或是名称相互对应的目的文件并不存在，才复制文件 -v 或 --verbose 显示执行过程 -V &lt;备份方式&gt; 或 --version-control=&lt;备份方式&gt; 指定当备份文件时，备份文件名的命名方式，有以下3种: 1.numbered或t, 将使用备份编号，会在字尾加上~1~字符串，其数字编号依次递增 2.simple或never 将使用简单备份，默认的备份字尾字符串是~, 也可通过-S来指定 3.existing或nil将使用当前方式，程序会先检查是否存在着备份编号，若有则采用备份编号，若无则采用简单备份 -x 或 --one-file-system 复制的文件或目录存放的文件系统，必须与cp指令执行时所处的文件系统相同，否则不复制，亦不处理位于其他分区的文件 --help 显示在线帮助 --sparse=&lt;使用时机&gt; 设置保存希疏文件的时机 --version 显示版本 示例 将文件file1复制成文件file2cp file1 file2 复制文件，只有源文件比目标文件的修改时间新时，才复制文件cp -u -v file1 file2 将文件file1复制成file2，因为目的文件已经存在，所以指定使用强制复制的模式cp -f file1 file2 同时将文件file1、file2、file3与目录dir1复制到dir2cp -R file1 file2 file3 dir1 dir2 复制时保留文件属性（权限、时间戳）cp -p a.txt tmp/ 复制时保留文件的目录结构cp -P /var/tmp/a.txt ./temp/ 复制时产生备份文件cp -b a.txt tmp/ 复制时产生备份文件，尾标 ~1~格式cp -b -V t a.txt /tmp 指定备份文件尾标cp -b -S _bak a.txt /tmp]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Shell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[常用shell命令 - crontabs]]></title>
    <url>%2F0003.%E5%BA%94%E7%94%A8-02.Linux%2FLinux-%E5%B8%B8%E7%94%A8shell%E5%91%BD%E4%BB%A4-crontabs%2F</url>
    <content type="text"><![CDATA[crontabscrontab 是用来让使用者在固定时间或固定间隔执行程序之用，换句话说，也就是类似使用者的时程表。-u user 是指设定指定 user 的时程表，这个前提是你必须要有其权限(比如说是 root)才能够指定他人的时程表。如果不使用 -u user 的话，就是表示设定自己的时程表。常用参数:12345678910crontab -l //查看当前用户下的cron任务crontab -e //编辑当前用户的定时任务crontab -u linuxso -e //编辑用户linuxso的定时任务crontab file [-u user]-用指定的文件替代目前的crontab。crontab-[-u user]-用标准输入替代目前的crontab.crontab-1[user]-列出用户目前的crontab.crontab-e[user]-编辑用户目前的crontab.crontab-d[user]-删除用户目前的crontab.crontab-c dir- 指定crontab的目录。crontab文件的格式：M H D m d cmd. 基本格式 :1* * * * * command 分 时 日 月 周 命令 第1列表示分钟1～59 每分钟用或者 /1表示 第2列表示小时1～23（0表示0点） 第3列表示日期1～31 第4列表示月份1～12 第5列标识号星期0～6（0表示星期天） 第6列要运行的命令 f1 f2 f3 f4 f5 program其中 f1 是表示分钟，f2 表示小时，f3 表示一个月份中的第几日，f4 表示月份，f5 表示一个星期中的第几天。program 表示要执行的程序。 当 f1 为 时表示每分钟都要执行 program，f2 为 时表示每小时都要执行程序，其馀类推 当 f1 为 a-b 时表示从第 a 分钟到第 b 分钟这段时间内要执行，f2 为 a-b 时表示从第 a 到第 b 小时都要执行，其馀类推 当 f1 为 /n 时表示每 n 分钟个时间间隔执行一次，f2 为 /n 表示每 n 小时个时间间隔执行一次，其馀类推 当 f1 为 a, b, c,… 时表示第 a, b, c,… 分钟要执行，f2 为 a, b, c,… 时表示第 a, b, c…个小时要执行，其馀类推使用者也可以将所有的设定先存放在档案 file 中，用 crontab file 的方式来设定时程表。 环境变量的重新引入 cmd要运行的程序，程序被送入sh执行，这个shell只有USER,HOME,SHELL这三个环境变量；如果执行的shell、Perl脚本中引用其他的变量或者是相应的模块包等，则需要重新导入相应的环境变量；如下： 12345#/bin/shexport PERL5LIB=&quot;/share/nas2/genome/biosoft/perl/current/lib/::/share/nas2/genome/biosoft/perl/current/lib/:/share/nas2/genome/biosoft/perl/current/lib/5.20.0/x86_64-linux-thread-multi:/share/nas2/genome/biosoft/perl/current/lib/:/share/nas2/genome/biosoft/perl/5.20.0/lib/site_perl/5.20.0/x86_64-linux-thread-multi:/share/nas2/genome/biosoft/perl/5.20.0/lib/site_perl/5.20.0:/share/nas2/genome/biosoft/perl/5.20.0/lib/5.20.0/x86_64-linux-thread-multi&quot; ;source /root/.bashrc;source /home/liubo/.bshrc ;/share/nas2/genome/bin/perl /share/nas2/database/genome/test.pl &gt;/home/liubo/err ;]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Shell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[常用shell命令 - sz/rz]]></title>
    <url>%2F0003.%E5%BA%94%E7%94%A8-02.Linux%2FLinux-%E5%B8%B8%E7%94%A8shell%E5%91%BD%E4%BB%A4-sz%E5%92%8Crz%2F</url>
    <content type="text"><![CDATA[简介rz、sz是用来在windows和Linux上互转文件的一个命令。lrzsz 官网入口lrzsz是一个unix通信套件提供的X，Y，和ZModem文件传输协议. 安装 1 yum安装 1yum -y install lrzsz 2 源码安装1234567wget http://www.ohse.de/uwe/releases/lrzsz-0.12.20.tar.gztar zxvf lrzsz-0.12.20.tar.gz &amp;&amp; cd lrzsz-0.12.20./configure &amp;&amp; make &amp;&amp; make install#上面安装过程默认把lsz和lrz安装到了/usr/local/bin/目录下，现在我们并不能直接使用，下面创建软链接，并命名为rz/sz：cd /usr/binln -s /usr/local/bin/lrz rzln -s /usr/local/bin/lsz sz 使用12345# 使用上传文件，执行命令rz，会跳出文件选择窗口，选择好文件，点击确认即可。rz# 下载文件，执行命令szsz 运行命令后，会弹出文件窗口，可以选择本地（Windows）目录保存下载的文件/选择要上传的文件。]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Shell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[常用shell命令 - 常用的组合命令]]></title>
    <url>%2F0003.%E5%BA%94%E7%94%A8-02.Linux%2FLinux-%E5%B8%B8%E7%94%A8shell%E5%91%BD%E4%BB%A4-%E5%B8%B8%E7%94%A8%E7%BB%84%E5%90%88%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[Linux 打印时间1234567891011# 输出当前年月日 echo $(date +%F)# 输出当前时间（时分秒） echo $(date +%T)# 输出星期 echo $(date +%A)# 输出年月日时分秒 echo $(date +%F%n%T) 符号 含义 %n 空格 %F 年月日 %T 时分秒 %A 星期]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Shell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[常用shell命令 - mail]]></title>
    <url>%2F0003.%E5%BA%94%E7%94%A8-02.Linux%2FLinux-%E5%B8%B8%E7%94%A8shell%E5%91%BD%E4%BB%A4-mail%2F</url>
    <content type="text"><![CDATA[12[root@localhost ~]# mail -s &quot;test mail&quot; root &lt;/root/ anaconda-ks.cfg#把/root/anaconda-ks.cfg文件的内容发送给root用户 1234567891011121314151617181920212223-f 表示发送者的邮箱-t 表示接收者的邮箱-cc 表示抄送发给谁-bcc 表示暗抄送给谁-o message-content-type=html 邮件内容的格式,html表示它是html格式-o message-charset=utf8 邮件内容编码-s 表示SMTP服务器的域名或者ip-u 表示邮件的主题-xu 表示SMTP验证的用户名-xp 表示SMTP验证的密码(注意,这个密码貌似有限制,例如我用d!5neyland就不能被正确识别)-m 邮件的内容-a 要发送的附件]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Shell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[常用数据库目录]]></title>
    <url>%2F0001.%E7%9F%A5%E8%AF%86-04.Database%2FDatabase-Cosmic%2F</url>
    <content type="text"><![CDATA[1. COSMIC癌症相关的体细胞位点，是整个网站的核心，收录了来自不同研究机构和数据库的体细胞突变数据，并提供了方便的浏览，检索，下载功能。 2. Cell Lines Project对癌症研究中常用的细胞系样本进行深入研究，分析其突变信息。相比COSMIC, 整个项目中涵盖的变异数据会少一点。该项目网址如下： `https://cancer.sanger.ac.uk/cell_lines` 3. COSMIC-3D通过交互式的网页，展现了基因突变导致的蛋白结构域的变化。该项目网址如下 `https://cancer.sanger.ac.uk/cosmic3d/` 在搜索框中输入一个具体的基因名称或者蛋白名称，可以查看具体的记录。 4. Cancer Gene Census在癌症研究中，找到相关的突变基因是最核心的目的之一。通过对各种癌症进行调研，整理了一份癌症相关的突变基因列表，这份列表就是Cancer Gene Census,简称CGC。该项目网址如下 `https://cancer.sanger.ac.uk/census` 在CGC种，将所有的癌症相关基因分成两类 - Tier1 : 对于这部分基因，有充分的证据表明，正是由于这些基因的突变，导致癌症的进一步发生。 - Tier2 : 对于这部分基因，只能说在癌症中检测到了大量该基因的突变，但是并没有充分证据表明该基因突变对癌症发生的影响。]]></content>
      <categories>
        <category>知识沉淀</category>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>数据库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[常用数据库 - Cancer Hotspots]]></title>
    <url>%2F0001.%E7%9F%A5%E8%AF%86-04.Database%2FDatabase-cancerhotspots%2F</url>
    <content type="text"><![CDATA[官网Cancer Hotspots]]></content>
      <categories>
        <category>知识沉淀</category>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>数据库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C语言学习-基础知识学习]]></title>
    <url>%2F0002.%E5%BC%80%E5%8F%91-07.c%2F2018-06-16.C%E8%AF%AD%E8%A8%80%E5%AD%A6%E4%B9%A0-%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[工作需要，前同事很多程序是用C编写的，而自己之前只是会有Perl、Python、R，没有接触过C语言的编程，都说C是底层编程语言，虽然在编写上会麻烦，但是执行效率会甩高级语言好几条街，所以想借着这个机会，学习一下C语言的编程，虽然没时间学精学熟，但是至少以后如果有些对效率需求比较的数据处理，可以通过C语言进行简单的实现。 编译过程简介和之前学习的Perl、Python之间最大的不同主要就是C语言在编写后执行前需要对代码编译，这也是C语言的一个被诟病的地方，尤其是在进行小规模的数据处理时，代码编写调试过程中，会在编译过程中浪费大量的时间，所以一般简单的小程序，可能用Perl、Python在实现上更简单一些。 看介绍，c语言在执行前，需要先编译然后进行连接，但是我在学习测试的时候，发现gcc会默认同步完成的，当然也可以通过-c参数分开进行执行。对于我们一些小程序来讲可能没有多少影响，但是在进行大型项目的时候，可能可以有效的帮助进行错误的判断。因为我是一步编译的，所以目前还不是很确定差异。编辑的C语言程序必须是 .c 为后缀， 在测试中，发现如果缺少后缀的话，是不能直接通过gcc进行编译的。这个很奇怪，因为一直感觉在Linux系统中，文件后缀其实是个摆设，这个还需要后期又时间详细了解一下原因。 1234gcc -o helloWorld helloWrold.c #多数情况下我们可以一步到位的编译+链接程序；gcc -c helloWrold.c # 进行编译会生成helloWorld.h 文件gcc -o helloWorld helloWorld.h #链接器将源代码文件中由编译器产生的各种对象模块组合起来，再从 C语言提供的程序库中添加必要的代码模块，将它们组合成一个可执行文件。 链接成功后就会生成一个可执行程序 C语言的结构简介第一个c语言程序代码如下： 123456#include&lt;stdio.h&gt;int main() # 函数首&#123; # &#123;&#125;括号里面的内容是对应的函数体，即函数功能。 printf(&quot;Hello World&quot;); # 定义函数的内容，以分号“；”结尾 。 return 0; # 函数结束的返回值。&#125; # 其中第一行 #include&lt;stdio.h&gt; #号不代表注释，而是一个预处理的标志，include相当于Python的import，perl的use，相当于通过include会导入一个头文件，头文件中定义了一些基本函数的说明。 后面main（）中的内容则是每个程序的一个主程序，每个c程序，都有且只有一个main（）函数， C语言常用函数1. 格式输出函数 printf一般形式：printf(格式控制，输出表列)。例如：printf(&quot;%d,%d&quot;,a,b); 括号内包含两个部分： “格式控制”是用双撇号括起来的一个字符串，称“转换控制字符串”，简称“格式字符串”，它包括两个信息： 格式声明：格式声明由 % 和格式字符组成，如 %d （%d 代表输出整数，%f 代表输出实数），它的作用是将输出的数据转换为指定的格式然后输出。格式声明总是由 % 字符开始。 普通字符：普通字符即在需要输出时原样输出的字符。例如上例中的 printf(“Please enter a value：”);中的 Please enter a value: 即为原样输出。 （2）“输出表列”是程序需要输出的数据。看下面例子： 1printf(&quot;I love %d and %d&quot;,x,s); 第一个 %d 对应的是x 的值，第二个 %d 对应的是 s 的值。 I love 和 and （注意这里包括空格）都是 普通字符会原样输出。 假如 x 的值是 3，s 的值是 4，这条语句将会输出“ I love 3 and 4 ”。常用格式字符 12345678910111213％d整型输出，%md：以m指定的字段宽度输出，右对齐，％ld长整型输出，%mld：输出指定宽度的长整型数据，％o以八进制数形式输出整数，％x以十六进制数形式输出整数，或输出字符串的地址。％u以十进制数输出unsigned型数据(无符号数)。注意：%d与%u有无符号的数值范围，也就是极限的值，不然数值打印出来会有误。％c用来输出一个字符，％s用来输出一个字符串，％f用来输出实数，以小数形式输出，默认情况下保留小数点6位。%.100f用来输出实数，保留小数点100位。％e以指数形式输出实数，％g根据大小自动选f格式或e格式，且不输出无意义的零。 2. 格式输入函数 scanf()一般形式：scanf(格式控制，地址表列)。“格式控制”的含义同 printf 函数。“地址表列”是由若干地址组成的表列，可以是变量的地址。 看下面的例子： 1scanf(&quot;a=%d,b=%d&quot;,&amp;a,&amp;b); 在格式字符串中除了有格式声明的 %d 以外，其它普通字符原样输出（如“ a= ”，“ b= ”和“，”），假如给 a 和 b 分别赋值 5 和 6 ，将显示“ a=5，b=6 ”。 注意：scanf()函数中的表列是地址表列。 scanf(“a=%d,b=%d”,&amp;a,&amp;b); 中 a 和 b 前面的 &amp; 不能省掉，这一点要和 printf 作区分。** printf() 函数和 scanf() 函数我们会在以后的“数据的输入与输出”版块继续讲述。 3. 输出函数putchar（）,输出一个字符 一般形式：putchar(c); 功能：输出变量 c 所代表的一个字符； 说明：c 为字符型变量或整型变量。 4.输入一个字符 一般形式：getchar(); 功能：要求用户从终端（键盘）输入单个ss注意：运行程序时，系统等待用户输入，注意回车也是一个合法字符。字符； 说明：返回值为从输入设备上得到的字符。 5. 注释位于“ /…….. / ”中的和“ // ”后面的内容为注释，用来对程序进行说明；注释在编译时会被自动忽略。 这是一个简单的计算程序，通过定义变量让用户可以自由设定 a 和 b 的值，之后通过 c=a+b; 这条语句实现把 a 和 b 的和计算出来并赋值给 c 。究竟什么是变量，什么是常量呢？接下来我们来一一讲述。 常量1.整型常量，如：整数2.实型常量，如：实数。小数还可以用指数形式表现，如32.23e3（表示 32.2310^3）, -323.34e-6（表示 323.3410^-6）, 由于计算机无法表示上角和下角，所以规定以字母 e 或者 E 代表以 10 为底的指数。 注意：e 或者 E 之前必须有数字，且 e 或者 E 后面必须为整数，不能是 12e4.1 或者 e3 这种形式。 字符常量 普通字符 用单撇号括起来的一个字符，如 ‘a’ 、’E’ 、’%’ 、’3’ 。不能写成 ‘ab’ 、’12’ 。字符常量只能是一个字符，不包括单撇号。 转义字符 除了以上形式的字符常量外，C 语言还允许用一种特殊形式的字符常量，就是以字符 \ 开头的字符序列，比如我们本节课的 3-1.c 中，\n 代表的就是换行符，显示跳转到下一行。这是一种在屏幕上无法显示的“控制字符”。常用转义字符 字符 含义 \n 换行 \r 回车（不换行） \t 制表符 \f 换页 \b 退格 变量变量代表一个有名字的、具有特殊属性的存储单元。它可以用来保存数据。变量的值是可以改变的。变量在程序中定义的一般形式就是： &lt;类型名称&gt; &lt;变量名称&gt;。例如：int a； int b; int a,b; int price;等。C 语言规定变量名只能由字母、数字和下划线构成，且第一个字符必须为字母或下划线。 数据类型C语言包含的数据类型 数组在C语言中不存在字符串类型的概念，字符串都是存储在一个字符数组中，同时数组中的每个字符以ASCII的形式存储在存储单元中。 int student[10]; # 定义一个包含10个元素的数组，从student[0]开始到student[9] ; 在数组定义时要指定元素个数。元素个数的定义不能包含变量。char string[10] ； #定义一个字符串，可以省略数组维数（10）； 数组常用函数 函数 说明 puts（字符数组） 其作用是讲一个字符串输出到终端，因此该函数用的不是很多，我们可以编写小程序来体验。 gets（字符数组） 其作用是从终端输入一个字符串到字符数组，并且得到一个函数值。 strcat（字符数组1，字符数组2） 把两个字符数组中的字符串连接起来，把字符串2接到1后面，结果放到字符串1中。注意字符串1必须足够长，不然长度会溢出。 strlen(字符数组) 测量字符串长度的函数。函数的值为字符串中的实际长度，比占用长度小（没有计算字符串结尾的 “\0”） strcpy（字符串 1，字符串 2） 作用是将字符串 2 复制到字符串 1 中。 strcmp（字符串1，字符串2） 比较字符串1和字符串2 (从左向右逐个比较每个字符的ASCII码) strlwr 函数 转换为小写的函数 strupr 函数 转换为大写的函数 整数类型 基本类型（int）：编译系统分配给 int 类型数据 2 个字节或者 4 个字节（由具体的编译系统自行决定）。我们使用的 gcc 编译器为每个整数类型分配四个字节（32 个二进位）。在存储单元中的存储方式是：用整数的补码形式存放。所以当 4 个字节的整数类型取值范围是 -2^31~（2^31-1）。无符号的基本整型表示为 unsigned int，和 int 类型占有的字节数相同，取值范围是 0~（2^32-1）。 短类型（short 类型）：短整型的类型名为 short，gcc 编译系统分配给 short 类型 2 个字节，存储方式和 int 类型一样，也是补码的形式存储，取值范围是 -2^15~（2^15-1），无符号短整型 unsigned short 的取值范围是 0~（2^16-1）。 长整型（long 类型）：gcc 编译系统分配给 long 类型 8 个字节，存储方式和 int 类型一样，也是补码的形式存储，取值范围是 -2^63~（2^63-1），无符号长整型 unsigned long 的取值范围是 0~（2^64-1）。 同类型占用的空间：在这里大家可以通过 sizeof() 运算符查看各类型的常量占据多少字节。 浮点型数据 单精度浮点数（float）gcc编译器为每个浮点数分配4个字节（32个位），每个浮点数的存储结构分为3个部分，+or- | 小数部分 | 指数部分 但是小数部分和指数部分所分配的位数根据编译器进行确定。 双精度浮点数（double）为了扩大数据的范围，用8个字节存储一个double类型的数据，可以编程查看 double 极限值，符号的下限为：DBL_MIN,上限为 DBL_MAX。 指针指针变量的类型说明1.对指针变量的类型说明包括三个内容： 指针类型说明，即定义变量为一个指针变量； 指针变量名； 变量值(指针)所指向的变量的数据类型。 1234567int *p1;表示p1是一个指针变量，它的值是某个整型变量的地址。staic int *p2; /*p2是指向静态整型变量的指针变量*/float *p3; /*p3是指向浮点变量的指针变量*/char *p4; /*p4是指向字符变量的指针变量*/ 应该注意的是，一个指针变量只能指向同类型的变量，如P3 只能指向浮点变量，不能时而指向一个浮点变量， 时而又指向一个字符变量。 2.指针变量的赋值(1)指针变量初始化的方法 12int a;int *p=&amp;a; (2)赋值语句的方法 123int a;int *p;p=&amp;a; 不允许把一个数赋予指针变量，故下面的赋值是错误的： int p;p=1000; 被赋值的指针变量前不能再加“”说明符，如写为*p=&amp;a 也是错误的 3.指针变量的运算指针变量只能进行赋值运算和部分算术运算及关系运算。 取地址运算符&amp; 取内容运算符 用来表示指针变量所指的变量，在运算符之后跟的变量必须是指针变量。需要注意的是指针运算符和指针变量说明中的指针说明符 不是一回事。 指针变量的运算 赋值运算 指针变量的赋值运算有以下几种形式： 指针变量初始化赋值，前面已作介绍。 把一个变量的地址赋予指向相同数据类型的指针变量。 把一个指针变量的值赋予指向相同类型变量的另一个指针变量。 把数组的首地址赋予指向数组的指针变量。 把字符串的首地址赋予指向字符类型的指针变量。 把函数的入口地址赋予指向函数的指针变量。 加减算数运算 指针变量的加减运算只能对数组指针变量进行， 对指向其它类型变量的指针变量作加减运算是毫无意义的。指针变量的加减运算，相当于以指针为原点，通过相对位置寻找数组中的其他元素。 两个指针变量之间的运算只有指向同一数组的两个指针变量之间才能进行运算， 否则运算毫无意义。 数组指针变量的说明和使用 12int a[5],*pa;pa=a; pa,a,&amp;a[0]均指向同一单元，它们是数组a的首地址，也是0 号元素a[0]的首地址。pa+1,a+1,&amp;a[1]均指向1号元素a[1]。类推可知a+i,a+i,&amp;a[i]指向i号 元素a[i]。应该说明的是pa是变量，而a,&amp;a[i]都是常量。引入指针变量后，就可以用两种方法来访问数组元素了。 第一种方法为下标法，即用a[i]形式访问数组元素。 第二种方法为指针法，即采用*(pa+i)形式，用间接访问的方法来访问数组元素。 字符型数据 字符常量C中字符型的基本类型是char,这是一个字符型常量，字符型常量是指用单引号扩起来的一个字符，每个字符在所有编译系统中均占用1个字节（8个位），利用ASCII码的形式进行存储，因此每个字符型常量都可以进行数学运算，运算时就是调用的每个字符对应的ASCII码值。同样在进行输出时，系统也会根据输出格式，确定输出字符还是输出其对应的ASCII码值。 字符串常量字符串常量是用一对双引号括起来的零个或多个字符组成的序列，如 “hello”，”China”，”b” 都是字符串常量。 字符串常量的存储与字符常量的存储是不同的。字符串中的每个字符占用一个字节，在存储字符串常量时还要自动在其末尾加上 ‘\0’ 作为字符串结束的标志。 “How do you do.” 存储示意如下。因此字符常量和字符串常量’b’ 和 “b” 是完全不同的。前者是字符常量，在内存中占用的字节数为 1；而后者是字符串常量，在内存中占用的字节数为 2，包含字符 ‘b’ 和 ‘\0’。 注意：在 C 语言中没有专门的字符串变量，如果你想要将一个字符串存放在变量中，必须使用字符数组，数组中每一个元素存放一个字符，数组的内容我们会在以后的课程中和大家详细讲述。 基础运算符四则运算12345x + y：将x与y相加x - y：将x与y相减x * y：将x与y相乘x / y：x除以yx % y：求x除以y的余数 x/y 中，两个实数（亲！注意说的是实数）相除的结果是双精度实数，两个整数相除的结果为整数。如 5/3 的结果为 1，舍去小数部分。 % 运算符要求参加运算的对象为整数，结果也是整数。如 7%3，结果为 1，除了%以外的运算符的操作数都可以是任何算数类型。 自增自减作用是使变量的值加 1 或减 1 ，例如：++i ，–i（在使用 i 之前，先使 i 加（减）1 ）；i++，i–（在使用 i 之后，使 i 的值加（减）1 ）。 不同数据类型混合运算在程序中经常会遇到不同类型的数据进行运算，比如 7*3.5。如果一个运算符的两侧数据类型不同，则先进行类型的转换，使两者具有同一种类型，然后进行运算。如下表，先转换成优先级较高的数据类型然后进行运算。 类型 优先级 long double 高 double float long unsigned int int short char 低 如果 int 类型的数据和 float 或 double 型数据进行运算时，先把 int 型和 float 型数据转换为 double 型数据，然后进行运算，结果为 double 型。其他的大家可以按照上图来做。字符 (char) 型数据和整形数据进行运算，就是把字符的 ASCII 代码与整形运算。如 4+’B’，由于字符 ‘B’ 的 ASCII 代码是 66，相当于 66+4=70。字符型数据可以直接和整形数据进行运算。如果字符型数据和浮点型数据运算，则将字符的 ASCII 码先转化为 double 型，然后在进行运算。 强制转换类型在运算前，可以根据需要对数据格式进行强制转换，一般形式就是（数据类型名）（表达式）如下: 123(double)a // (将a转换成为double型)(int)(x+y) //（将x+y的值转换成为int类型)(int)x+y //（将x的值转换成为int类型，然后进行加法运算) 基础结构switchif是一个但分支的逻辑结构，当我们有多个筛选条件时，如果用if会不断使用elsif， switch（表达式） { case 常量1：语句1 case 常量2：语句2 . . . case 常量n ：语句n default ： 语句n+1 } while循环1234while(expression) statement1;statament2; do…while123do statement;while(expression); for 循环12345for（表达式 1；表达式 2；表达式 3） 语句- 表达式 1：设置初始条件，只执行一次。可以为零个、一个或者多个表达式赋初值。 表达式可以省略但分号不可省略；- 表达式 2：是循环条件表达式，用来判定是否继续循环。在每次执行循环体前限制性此表达式，决定是否继续执行循环。 可以省略，省略后默认为真。- 表达式 3：作为循环的调整，例如循环变量的增值，是在执行完循环体后才进行的。可以省略，放入执行语句中。 break退出循环，执行后续命令。 continue退出本轮循环进行下一轮循环，（跳过循环中的后续命令）。 函数函数声明指定函数的类型为 void，表示函数无类型，即无函数值，也就是说，执行这函数不会返回任何值如果函数的定义之前被引用了的话，则在引用前进行声明。函数示意 123456789101112131415161718192021#include&lt;stdio.h&gt;int main()&#123; int fun(int m ) ; #函数使用在函数定义之前，需要先引用在使用 int x=fun(100); printf(&quot;x:%d&quot;,x) ;&#125;int fun(int m)&#123; int a[100]; int n=0; for(int i=1;i&lt;m;i++)&#123; if(i%7==0)&#123; a[n]=i ; n++; &#125; &#125; for(int i=0;i&lt;n;i++)&#123; printf(&quot;num is %d\n&quot;,a[i]); &#125; return(n) ;&#125; 函数的分类]]></content>
      <categories>
        <category>编程拾慧</category>
        <category>C++</category>
      </categories>
      <tags>
        <tag>编程语言</tag>
        <tag>C</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Book:机器学习实战-2.KNN(K-nearest-neighbor)]]></title>
    <url>%2F0002.%E5%BC%80%E5%8F%91-05.MachineLearn%2F2018-06-12.Book-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98-2-KNN-K-nearest-neighbor%2F</url>
    <content type="text"><![CDATA[算法概述1、指导思想kNN算法的指导思想是“近朱者赤，近墨者黑”，由你的邻居来推断出你的类别。计算步骤如下： 1）算距离：给定测试对象，计算它与训练集中的每个对象的距离 2）找邻居：圈定距离最近的k个训练对象，作为测试对象的近邻 3）做分类：根据这k个近邻归属的主要类别，来对测试对象分类 2、距离或相似度的衡量 1）在计算距离时，如果具有最大差值的属性值对距离的影响非常大时，需要对数据进行归一化（例如：newValue=oldValue/（MaxValue-MinValue））。 2）什么是合适的距离衡量？距离越近应该意味着这两个点属于一个分类的可能性越大。觉的距离衡量包括欧式距离、夹角余弦等。对于文本分类来说，使用余弦(cosine)来计算相似度就比欧式(Euclidean)距离更合适。 3、类别的判定 1）投票决定：少数服从多数，近邻中哪个类别的点最多就分为该类。 2）加权投票法：根据距离的远近，对近邻的投票进行加权，距离越近则权重越大（权重为距离平方的倒数） 优缺点1、优点简单，易于理解，易于实现，无需估计参数，无需训练适合对稀有事件进行分类（例如当流失率很低时，比如低于0.5%，构造流失预测模型）特别适合于多分类问题(multi-modal,对象具有多个类别标签)，例如根据基因特征来判断其功能分类，kNN比SVM的表现要好 2、缺点懒惰算法，对测试样本分类时的计算量大，内存开销大，评分慢可解释性较差，无法给出决策树那样的规则。 常见问题1、k值设定为多大？k太小，分类结果易受噪声点影响；k太大，近邻中又可能包含太多的其它类别的点。（对距离加权，可以降低k值设定的影响）k值通常是采用交叉检验来确定（以k=1为基准）经验规则：k一般低于训练样本数的平方根 2、类别如何判定最合适？投票法没有考虑近邻的距离的远近，距离更近的近邻也许更应该决定最终的分类，所以加权投票法更恰当一些。 3、如何选择合适的距离衡量？高维度对距离衡量的影响：众所周知当变量数越多，欧式距离的区分能力就越差。变量值域对距离的影响：值域越大的变量常常会在距离计算中占据主导作用，因此应先对变量进行标准化。 4、训练样本是否要一视同仁？在训练集中，有些样本可能是更值得依赖的。可以给不同的样本施加不同的权重，加强依赖样本的权重，降低不可信赖样本的影响。 5、性能问题？kNN是一种懒惰算法，平时不好好学习，考试（对测试样本分类）时才临阵磨枪（临时去找k个近邻）。懒惰的后果：构造模型很简单，但在对测试样本分类地的系统开销大，因为要扫描全部训练样本并计算距离。已经有一些方法提高计算的效率，例如压缩训练样本量等。 6、能否大幅减少训练样本量，同时又保持分类精度？浓缩技术(condensing)编辑技术(editing) 使用示例影片分类 电影分类简介： 电影有很多种，这里仅考虑两种，爱情片和动作片，爱情片里也会有打斗场景，动作片里也会有接吻镜头，因此不能单纯的依靠有无对影片进行分类，那这种时候，我们应该怎么去划分一个影片是爱情片还是动作片呢，这时候，我们可以利用该算法。方法： 首先一部影片中，接吻的镜头和打斗的镜头是可以进行量化的，比如我们可以数一下，有多少个打斗镜头，有多少个接吻镜头等等，因此，针对每个影片我们可以得到一个长度为2的数组c（接吻镜头数，打斗镜头数） ，同时我们需要寻找一些以知分类的影片同时我们知道这些电影的镜头信息。这些每个电影都可以在二维坐标系里面对应一个坐标点。 当我们需要对一部影片进行分类的时候，我们统计该影片的镜头信息，去计算这个影片和所有以知分类的影片间的距离，然后选取和这个影片距离最近的K个电影，然后看看这K个电影所属的分类，选取权重最高的1个电影分类（注意这里提到了是权重，而不是数目，因为有些情况下，可能需要考虑实际应用的关系，根据数据的可信程度，距离关系等对分类信息进行加权，从而提高分类的准确性），作为该电影的类别或者也可以得到该影片属于不同类别的概率。 代码1print &apos;a&apos; 手写识别 手写识别系统]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习相关素材框架资料]]></title>
    <url>%2F0002.%E5%BC%80%E5%8F%91-05.MachineLearn%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E7%B4%A0%E6%9D%90%E6%A1%86%E6%9E%B6%E8%B5%84%E6%96%99%2F</url>
    <content type="text"><![CDATA[]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python包-jupyter]]></title>
    <url>%2F0002.%E5%BC%80%E5%8F%91-02.python%2FPython-%E5%8C%85-jupyter%2F</url>
    <content type="text"><![CDATA[安装12pip3 install --upgrade pippip3 install jupyter 运行Jupyter Notebook12# 运行帮助jupyter notebook --help 接下来打开浏览器输入服务器的IP地址：端口号,例如 10.12.325.321:8888 即可看到 jupyter notebook。这里如果有问题的话首先可以检查下阿里云服务器（你自己买的服务器）端口是否开放，不行的话，再检查linux 系统防火墙是否开放，可以试着关系系统防火墙。 sudo ufw disable 如果登陆失败，则有可能是服务器防火墙设置的问题，此时最简单的方法是在本地建立一个ssh通道： 在本地终端中输入ssh username@address_of_remote -L127.0.0.1:1234:127.0.0.1:8888 便可以在localhost:1234直接访问远程的jupyter了。 ssh liubo4@120.24.188.250 -L127.0.0.1:8889:127.0.0.1:8889]]></content>
      <categories>
        <category>编程拾慧</category>
        <category>python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python包-Pyyaml]]></title>
    <url>%2F0002.%E5%BC%80%E5%8F%91-02.python%2FPython-%E5%8C%85-pyyaml%2F</url>
    <content type="text"><![CDATA[安装1pip install pyyaml 格式介绍123456raincoat: 1coins: 5books: 23spectacles: 2chairs: 12pens: 6 数据读取1234567#!/usr/bin/env python3import yamlwith open('items.yaml') as f: data = yaml.load(f, Loader=yaml.FullLoader) print(data) 我们打开items.yaml文件，并使用yaml.load()方法加载内容。 数据被打印到控制台。PyYAML 模块将标量值转换为 Python 字典。 12$ python read_yaml.py&#123;'raincoat': 1, 'coins': 5, 'books': 23, 'spectacles': 2, 'chairs': 12, 'pens': 6&#125;]]></content>
      <categories>
        <category>编程拾慧</category>
        <category>python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python包-WordCloud 绘制词云图]]></title>
    <url>%2F0002.%E5%BC%80%E5%8F%91-02.python%2FPython-%E5%8C%85-WordCloud-%E8%AF%8D%E4%BA%91%E5%9B%BE%2F</url>
    <content type="text"><![CDATA[安装1pip install wordcloud 使用12345w= wordcloud.WordCloud()w.generate() # 向WordCloud对象中加载文本txt&gt;&gt;&gt;w.generate("Python and WordCloud")w.to_file(filename) # 将词云输出为图像文件，.png或.jpg格式&gt;&gt;&gt;w.to_file("outfile.png") 数据获取wordcloud如何将文本转化为词云 1.分隔：以空格分隔单词 2.统计：单词出现次数并过滤 3.字体：根据统计配置字号 4.布局：颜色环境尺寸 参数介绍 参数 描述 示例 width 指定词云对象生成图片的宽度,默认400像素 w=wordcloud.WordCloud(width=600) height 指定词云对象生成图片的高度,默认200像素 w=wordcloud.WordCloud(height=400) min_font_size 指定词云中字体的最小字号，默认4号 w=wordcloud.WordCloud(min_font_size=10) max_font_size 指定词云中字体的最大字号，根据高度自动调节 w=wordcloud.WordCloud(max_font_size=20) font_step 指定词云中字体字号的步进间隔，默认为1 w=wordcloud.WordCloud(font_step=2) font_path 指定文体文件的路径，默认None w=wordcloud.WordCloud(font_path=”msyh.ttc”) max_words 指定词云显示的最大单词数量,默认200 w=wordcloud.WordCloud(max_words=20) stop_words 指定词云的排除词列表，即不显示的单词列表 w=wordcloud.WordCloud(stop_words=”Python”) mask 指定词云形状，默认为长方形，需要引用imread()函数 from scipy.msc import imread ;mk=imread(“pic.png”)；w=wordcloud.WordCloud(mask=mk) background_color 指定词云图片的背景颜色，默认为黑色 w=wordcloud.WordCloud(background_color=”white”)]]></content>
      <categories>
        <category>编程拾慧</category>
        <category>python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>可视化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python整体介绍]]></title>
    <url>%2F0002.%E5%BC%80%E5%8F%91-02.python%2FPython-0-index%2F</url>
    <content type="text"><![CDATA[参考资料Python 教程:极客教程Python 3 教程 RunOOB]]></content>
      <categories>
        <category>编程拾慧</category>
        <category>python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python包-OpenCV_图像处理]]></title>
    <url>%2F0002.%E5%BC%80%E5%8F%91-02.python%2FPython-%E5%8C%85-OpenCV_%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86%2F</url>
    <content type="text"><![CDATA[官方文档pypi:opencv-python OpenCV-Python Tutorials 参考网址 http://wiki.opencv.org.cn/index.php/%E9%A6%96%E9%A1%B5 http://www.opencv.org.cn/opencvdoc/2.3.2/html/index.html http://opencv.org/ 官网下载经常会有断，python 2.7 可以在github上的备份中获取;下载文件后，解压，并将文件放入python的库中即可；同时使用open CV需要另一个python的库文件。Numpy； 该model可以直接通过python安装命令进行安装； 安装123pip install opencv-python # 通过pip安装openCVpython -m pip install numpy # 通过pip安装openCV 基本的使用包的导入12import numpy as npimport cv2 文件的读取123456789101112131415161718192021cv2.namedWindow(&apos;image&apos;, cv2.WINDOW_NORMAL) #cv2.WINDOW_AUTOSIZE 窗口不可调节;cv2.namedWindow(&apos;image&apos;, cv2.WINDOW_NORMAL) #cv2.WINDOW_NORMAL 窗口大小可以调节；# Load an color image in grayscaleimg = cv2.imread(&apos;D:/github/Personal-TestDemo/openCV/testInputData/1.png&apos;)cv2.imshow(&apos;image&apos;,img) #展示图片,必须紧跟waitKey，否则可能出现图片不显示灰色背景cv2.waitKey(10) #等待键盘输入cv2.destroyAllWindows() #关闭窗口print img``` ## 图像操作### 绘制#### 绘制直线``` pythoncv2.line(img,(0,0),(150,150),(2,255,255),5)# cv2.line()接受以下参数：图片，开始坐标，结束坐标，颜色（bgr），线条粗细。cv2.imshow(&apos;image&apos;,img)cv2.waitKey(0)cv2.destroyAllWindows() 绘制矩形12cv2.rectangle(img,(15,25),(200,150),(0,0,255),15)# 图像，左上角坐标，右下角坐标，颜色和线条粗细。 绘制圆12cv2.circle(img,(100,63), 55, (0,255,0), -1)# 这里的参数是图像/帧，圆心，半径，颜色和粗细。 粗细-1 表示填充 添加文字12cv2.putText(img, 'OpenCV Tuts!',(10,400), font, 2, (200,255,155), 5, cv2.LINE_AA)#参数依次为 图片，文字，文字起始位置，font,文字大小，文字颜色，文字粗细，cv2.LINE_AA 图像操作更改像素12345px = img[55,55] # 获取某个像素点的颜色img[55,55] = [255,255,255] # 修改某个位点的像素 px = img[100:150,100:150] # 获取某个区域的颜色img[100:150,100:150] = [255,255,255] # 修改某个区域的颜色 图像算术和逻辑运算1234567# 读取两个图像img1 = cv2.imread('3D-Matplotlib.png')img2 = cv2.imread('mainsvmimage.png')add = img1+img2 # 对图像求和，对每个像素的三个颜色通道分别求和，由于颜色值的范围是0~255，所以超过255的会求和结果为255cv2.imshow('add',add)cv2.waitKey(0)cv2.destroyAllWindows() 阈值阈值的思想是进一步简化视觉数据的分析。首先，你可以转换为灰度，但是你必须考虑灰度仍然有至少 255 个值。阈值可以做的事情，在最基本的层面上，是基于阈值将所有东西都转换成白色或黑色。比方说，我们希望阈值为 125（最大为 255），那么 125 以下的所有内容都将被转换为 0 或黑色，而高于 125 的所有内容都将被转换为 255 或白色。如果你像平常一样转换成灰度，你会变成白色和黑色。如果你不转换灰度，你会得到二值化的图片，但会有颜色。 12345678910111213141516# 硬阈值过滤retval, threshold = cv2.threshold(img, 150, 255, cv2.THRESH_BINARY)# 参数分别为(图像，阈值，最大值，阈值类型)，通常情况下，125-150 左右的东西可能效果最好。如果图片过暗需要调低，过亮需要调高。cv2.imshow('origin',img)cv2.imshow('threshold',threshold)cv2.waitKey(0)cv2.destroyAllWindows()# 自适应阈值grayscaled = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY) #将图片变为灰度图片kernel=np.ones((2,2),np.uint8) #进行腐蚀膨胀操作th = cv2.adaptiveThreshold(grayscaled, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 115, 1)cv2.imshow('original',img)cv2.imshow('Adaptive threshold',th)cv2.waitKey(0) 颜色过滤创建一个过滤器，回顾按位操作，其中我们将过滤特定的颜色，试图显示它。或者，你也可以专门筛选出特定的颜色，然后将其替换为场景，就像我们用其他方法替换ROI（图像区域）一样，就像绿屏的工作方式。 为了像这样过滤，你有几个选项。通常，你可能会将你的颜色转换为 HSV，即“色调饱和度纯度”。例如，这可以帮助你根据色调和饱和度范围，使用变化的值确定一个更具体的颜色。如果你希望的话，你可以实际生成基于 BGR 值的过滤器，但是这会有点困难。如果你很难可视化 HSV，不要感到失落，查看维基百科页面上的 HSV，那里有一个非常有用的图形让你可视化它。 参考资料CSDN: SGchi : openCV超详细入门教程（python版）]]></content>
      <categories>
        <category>编程拾慧</category>
        <category>python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python包-json]]></title>
    <url>%2F0002.%E5%BC%80%E5%8F%91-02.python%2FPython-%E5%8C%85-json%2F</url>
    <content type="text"><![CDATA[安装1pip install json 格式介绍123&#123;"accesskeyID":"LTAI5tSRTFb****","accesskeysecret":"jAJGMov82****"&#125; 数据读取123456789#!/usr/bin/env python3import jsonwith open('.aliyun.json') as f: data = json.load(f)# 获取值data['accesskeyID'] # LTAI5tSRTFb****data['accesskeysecret'] # jAJGMov82****]]></content>
      <categories>
        <category>编程拾慧</category>
        <category>python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python包-argparse]]></title>
    <url>%2F0002.%E5%BC%80%E5%8F%91-02.python%2FPython-%E5%8C%85-argparse-%E5%8F%82%E6%95%B0%E7%AE%A1%E7%90%86%2F</url>
    <content type="text"><![CDATA[参考资料argparseg官方文档:python3 add_argument() 方法ArgumentParser.add_argument(name or flags…[, action][, nargs][, const][, default][, type][, choices][, required][, help][, metavar][, dest]) 定义单个的命令行参数应当如何解析。每个形参都在下面有它自己更多的描述，长话短说有： 123456789101112131415161718192021name or flags - 一个命名或者一个选项字符串的列表，例如 foo 或 -f, --foo。action - 当参数在命令行中出现时使用的动作基本类型。nargs - 命令行参数应当消耗的数目。const - 被一些 action 和 nargs 选择所需求的常数。default - 当参数未在命令行中出现并且也不存在于命名空间对象时所产生的值。type - 命令行参数应当被转换成的类型。choices - 可用的参数的容器。required - 此命令行选项是否可省略 （仅选项可用）。help - 一个此选项作用的简单描述。metavar - 在使用方法消息中使用的参数值示例。dest - 被添加到 parse_args() 所返回对象上的属性名。 ArgumentParser 对象12345678910111213141516171819202122232425prog - The name of the program (default: os.path.basename(sys.argv[0]))usage - 描述程序用途的字符串（默认值：从添加到解析器的参数生成）description - 在参数帮助文档之前显示的文本（默认值：无）epilog - 在参数帮助文档之后显示的文本（默认值：无）parents - 一个 ArgumentParser 对象的列表，它们的参数也应包含在内formatter_class - 用于自定义帮助文档输出格式的类prefix_chars - 可选参数的前缀字符集合（默认值： &apos;-&apos;）fromfile_prefix_chars - 当需要从文件中读取其他参数时，用于标识文件名的前缀字符集合（默认值： None）argument_default - 参数的全局默认值（默认值： None）conflict_handler - 解决冲突选项的策略（通常是不必要的）add_help - 为解析器添加一个 -h/--help 选项（默认值： True）allow_abbrev - 如果缩写是无歧义的，则允许缩写长选项 （默认值：True）exit_on_error - 决定当错误发生时是否让 ArgumentParser 附带错误信息退出。 (默认值: True)]]></content>
      <categories>
        <category>编程拾慧</category>
        <category>python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python包-Scipy_科学计算库]]></title>
    <url>%2F0002.%E5%BC%80%E5%8F%91-02.python%2FPython-%E5%8C%85-Scipy_%E7%A7%91%E5%AD%A6%E8%AE%A1%E7%AE%97%E5%BA%93%2F</url>
    <content type="text"><![CDATA[Scipy库的简介 Scipy高级科学计算库：和Numpy联系很密切，Scipy一般都是操控Numpy数组来进行科学计算、统计分析，所以可以说是基于Numpy之上了。Scipy有很多子模块可以应对不同的应用，例如插值运算，优化算法等等。SciPy则是在NumPy的基础上构建的更为强大，应用领域也更为广泛的科学计算包。正是出于这个原因，SciPy需要依赖NumPy的支持进行安装和运行。 Scipy是世界上著名的Python开源科学计算库，建立在Numpy之上。它增加的功能包括数值积分、最优化、统计和一些专用函数。 SciPy函数库在NumPy库的基础上增加了众多的数学、科学以及工程计算中常用的库函数。例如线性代数、常微分方程数值求解、信号处理、图像处理、稀疏矩阵等等。Scipy是基于Numpy构建的一个集成了多种数学算法和方便的函数的Python模块。通过给用户提供一些高层的命令和类，SciPy在python交互式会话中，大大增加了操作和可视化数据的能力。通过SciPy，Python的交互式会话变成了一个数据处理和一个system-prototyping环境，足以和MATLAB，IDL，Octave，R-Lab，以及SciLab抗衡。 更重要的是，在Python中使用SciPy，还可以同时用一门强大的语言————Python来开发复杂和专业的程序。用SciPy写科学应用，还能获得世界各地的开发者开发的模块的帮助。从并行程序到web到数据库子例程到各种类，都已经有可用的给Python程序员了。这些强大的功能，SciPy都有，特别是它的数学库。Scipy是在Python的NumPy扩展上构建的数学算法和方便函数的集合。它通过为用户提供高级命令和类来操作和可视化数据，为交互式Python会话添加了强大的功能。有了SciPy，交互式Python会话就变成了一个数据处理和系统原型环境，可以与MATLAB、IDL、Octave、R-Lab和SciLab等系统相匹敌。以Python为基础的SciPy的另一个好处是，它还提供了一种强大的编程语言，可用于开发复杂的程序和专门的应用程序。使用SciPy的科学应用程序受益于世界各地的开发人员在软件领域的许多小众领域中开发的附加模块。从并行编程到web和数据库的子例程和类，Python程序员都可以使用。除了SciPy中的数学库之外，所有这些功能都是可用的 scipy.orgscipy.pypiSciPy User GuideRead the doc for SciPy 常用功能常见的子包 Subpackage Description cluster Clustering algorithms 聚类算法在信息理论、目标检测、通信、压缩等领域有着广泛的应用。vq模块只支持矢量量化和k-均值算法。 constants Physical and mathematical constants fftpack Fast Fourier Transform routines integrate Integration and ordinary differential equation solvers interpolate Interpolation and smoothing splines 此子包包含样条函数和类、一维和多维（单变量和多变量）插值类、Lagrange和Taylor多项式插值器以及FITPACK和DFITPACK函数的包装器。 io Input and Output linalg Linear algebra ndimage N-dimensional image processing odr Orthogonal distance regression optimize Optimization and root-finding routines signal Signal processing sparse Sparse matrices and associated routines spatial Spatial data structures and algorithms special Special functions stats Statistical distributions and functions 该模块包含大量的概率分布以及不断增长的统计函数库。每个单变量分布都是rv_连续（rv_离散用于离散分布）的一个子类的实例。 常用检验Fisher检验12from scipy.stats import fisher_exactoddsratio, pvalue = fisher_exact([[8, 2], [1, 5]])]]></content>
      <categories>
        <category>编程拾慧</category>
        <category>python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python包-pysam]]></title>
    <url>%2F0002.%E5%BC%80%E5%8F%91-02.python%2FPython-%E5%8C%85-pysam%2F</url>
    <content type="text"><![CDATA[文件读写文件读写句柄bam_file : bam文件路径 读取文件1ReadBam = pysam.AlignmentFile(bam_file) # 读取文件句柄 写入文件1234567WriteBam = pysam.AlignmentFile(bam_file , &quot;wb&quot;, header=pysamFile.header)for read in samfile.fetch(): if read.is_paired: WriteBam.write(read)pairedreads.close()samfile.close() Bam操作提取特定区域的Reads1234567# 获取染色体chr1 上 100~120bp的readsfor read in samfile.fetch(&apos;chr1&apos;, 100, 120): print(read)samfile.close()pileup(self, contig=None, start=None, stop=None, region=None, reference=None, end=None, **kwargs) 文件函数1234567891011121314151617181920# region : 提取的区域# mapq : 比对质量# baseq ：碱基质量# ref :参考基因组for pileup_column in bam_file.pileup(region=&quot;chr1:1-10&quot;,mapq=mapq , baseq = baseq, stepper=stepper, fastaFile=ref, max_depth=200000, **&#123;&quot;truncate&quot;: True&#125;):# for pileup_read in pileup_column.pileups: aln = pileup_read.alignment read_name = aln.query_name pair = &apos;pe1&apos; if aln.is_read1 else &apos;pe2&apos; strand = &apos;-&apos; if aln.is_reverse else &apos;+&apos; read = Read(read_name, pair, strand) if pileup_read.is_del or pileup_read.is_refskip or (aln.flag &gt; 1024) or (aln.mapping_quality &lt; mapq) or \ aln.query_qualities[pileup_read.query_position] &lt; baseq: continue start_reads[read] = [pileup_read.query_position, aln] Referencereadthedoc]]></content>
      <categories>
        <category>编程拾慧</category>
        <category>python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python包-python-docx撰写word]]></title>
    <url>%2F0002.%E5%BC%80%E5%8F%91-02.python%2FPython-%E5%8C%85-python-docx%E6%92%B0%E5%86%99word%2F</url>
    <content type="text"><![CDATA[用途处于某些上下游对接需求，所以需要频繁的将生信的分析结果整理成word文件，以便以进行信息的传递。所以基于该模块可以更方便的在集群上自动化生成相关的文档示例，用于进行后续的处理。 环境安装使用Python进行doc文档编写的过程中，需要安装下列软件 &amp; 包 docx #对应安装命令 pip install python-docx 简介参考信息https://zhuanlan.zhihu.com/p/82880510 https://blog.csdn.net/weixin_44601149/article/details/106660853?utm_medium=distribute.pc_relevant.none-task-blog-title-1&amp;spm=1001.2101.3001.4242 示例首先构建一个Word模板，在模板中使用双大括号来标记后期word编辑的过程中，需要进行替换的变量 {{var}} 。]]></content>
      <categories>
        <category>编程拾慧</category>
        <category>python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Office处理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python包-pandas]]></title>
    <url>%2F0002.%E5%BC%80%E5%8F%91-02.python%2FPython-%E5%8C%85-Pandas%2F</url>
    <content type="text"><![CDATA[官方资料api参考文档官方文档-en官方文档-cnpandas的 General functions 盖若pandas速查 方法 简介 示例 concat Concatenate pandas objects along a particular axis with optional set logic along the other axes. df_result = pd.concat([dataframe1 , dataframe2]) 常用功能及代码块示例得到一个DataFrame创建一个空的dataframe1234#指定dataframe的所有字段columns=["sampleName","#Gene","Transcript,"cHGVS","pHGVS","Frequence"]#创建一个含字段结构的空数据框DF = pd.DataFrame(columns=columns) 从文件中读取一个dataframe1234567891011121314# 读取ExcelDF = pd.read_excel(file, sheet_name=None)DF = pd.read_csv(file,sep="\t" )# sheet_name 指定 None时，读取所有sheet返回一个字典，key为sheet名； 不指定默认读取第一个sheet）# 读取tsv文件pd.read_csv(filename, sep="\t")# 读取压缩文件df = pd.read_csv('filename.zip')# 指定压缩格式 df = pd.read_csv('filename.zip', compression='zip', header=0, sep=',', quotechar='"')压缩：&#123;'infer'，'gzip'，'bz2'，'zip'，'xz'，无&#125;，默认为'infer'用于对磁盘数据进行实时解压缩。如果'infer'和filepath_or_buffer与路径类似，则从以下扩展名检测压缩：'.gz'，'。bz2'，'。zip'或'.xz'（否则不进行解压缩）。如果使用“ zip”，则ZIP文件必须仅包含一个要读取的数据文件。设置为“无”将不进行解压缩。 读取数据时跳过注释信息在我们实际使用的文件，尤其是一些数据库类型的记录文件中，通过添加注释，文件通常可以更加清晰明了。csv文件中有注释，这种情况是可能存在的。那么，在pandas读取csv文件的时候，如何规避掉注释。 123456789101112#emptya,b,c12,中国，上海# 如果仅指定分隔符，正常读取是不合理的，结果仅有一列而非a,b,c三列，注释当做了列名&gt;&gt;&gt; df = pd.read_csv(&apos;D:/1.csv&apos;, sep=&apos; &apos;)&gt;&gt;&gt; df #empty\na,b,c\n1,2,3a b c12 中国 上海&gt;&gt;&gt; df.columnsIndex([&apos;#empty\na,b,c\n1,2,3&apos;], dtype=&apos;object&apos;) 我们可以通过设置comment参数，指定某一行是注释，则该行就不会被解析1234&gt;&gt;&gt; df = pd.read_csv('D:/1.csv', sep=' ', comment='#')&gt;&gt;&gt; df a b c0 12 中国 上海 获取DataFrame的信息查看头尾数据123456# 查看开头的N（默认值5） 行df.head()# 查看开头的10行df.head(10) # 查看结尾的N（默认值5） 行df.tail() 随机查看数据12# 随机查看N（默认1 ）行数据df.sample(N) 逐行读取dataframe的每行DataFrame.iterrows()12for index, row in df.iterrows(): print row[&quot;c1&quot;], row[&quot;c2&quot;] DataFrame.itertuples()1234for row in df.itertuples(index=True, name=&apos;Pandas&apos;): print getattr(row, &quot;c1&quot;), getattr(row, &quot;c2&quot;) # 取值方式 - print(row.c1) itertuples()应该比iterrows()快 获取指定cell的数据信息 iloc123456# 获取第i行，c1字段的内容df.iloc[i][&apos;c1&apos;]#也可以通过遍历index获取全部行的特定信息列for i in range(0, len(df)): print df.iloc[i][&apos;c1&apos;], df.iloc[i][&apos;c2&apos;] 将DataFrame转为List 略麻烦，但是更高效，1234567891011121314from collections import namedtuple def myiter(d, cols=None): if cols is None: v = d.values.tolist() cols = d.columns.values.tolist() else: j = [d.columns.get_loc(c) for c in cols] v = d.values[:, j].tolist() n = namedtuple(&apos;MyTuple&apos;, cols) for line in iter(v): yield n(*line) 筛选DataFrame的数据单个条件筛选12345678910$ print(df)name age sexTim 10 男Sam 20 女Tom 30 NaN$ df[df[&quot;age&quot;]&gt;15]name age sexSam 20 女Tom 30 NaN 多个条件筛选存在多个比较条件的时候，需要注意 多个条件同时满足不能用and，使用 &amp; 多个条件满足其中一个即可，不能使用or，使用 | 每个条件要使用 小括号 123456789$ print(df)name age sexTim 10 男Sam 20 女Tom 30 NaN$ df[(df[&quot;age&quot;]&gt;15) &amp; (df[&quot;age&quot;]&lt;25&gt;)]name age sexSam 20 女 筛选常用的数值函数123456df.eq() # 等于相等 ==df.ne() # 不等于 !=df.le() # 小于等于 &gt;=df.lt() # 小于 &lt;df.ge() # 大于等于 &gt;=df.gt() # 大于 &gt; 使用单个数值函数筛选123$ df[(df[&quot;age&quot;] eq 20)]name age sexSam 20 女 筛选常用的字符型函数123包含：str.contains开始：str.startswith结束：str.endswith 示例如下：12345678$ df[(df[&quot;name&quot;].str.contains(&quot;o&quot;))]name age sexTom 30 NaN# 如果字符串所在列存在空值，则可以通过添加参数进行剔除，否则报错$ df[(df[&quot;sex&quot;].str.contains(&quot;男&quot;,na=False))]name age sexTim 10 男 基于索引筛选情况比较少，但是特殊情况也会用到12$ df[df.index == 1]Sam 20 女 筛选存在缺失值的行123$ df[df.isnull().values==True]name age sexTom 30 NaN 更强自定义化的筛选定义一个函数，进行复杂的逻辑判断；使用apply对dataframe进行系统化批量的处理123456789def checkfunction(x,y,z): if int(x) &gt;= 100: if y in (&apos;*&apos;, &apos;-&apos;): return True elif int(y) &gt;= 1 and int(z) &gt;= 1: return True return FalseFilterData = RawData[RawData.apply(lambda x: checkfunction(x[&quot;tag_x&quot;],x[&quot;tag_y&quot;],x[&quot;tag_z&quot;]), axis=1)] 更改 dataframe 的内容整个dataframe的全局替换12345# 将所有na替换为特定的valuesdata.fillna(value=values,inplace=True)# 通过使用正则进行文本的全局替换data.replace(&quot;\r\n&quot;,&quot;&lt;br&gt;&quot;,regex=True,inplace=True) # 将数据中的换行符统一替换成 &lt;br&gt; .regex：是否使用正则，若不适用，则只能进行整体的替换。 整列改为相同的值1DF[&apos;sampleName&apos;] = &quot;S1&quot; # 将数据框DF的sampleName列都改为 &quot;S1&quot; 根据特定条件修改某一列的值调用DataFrame.apply()方法，可以作用于 Series 或者整个 DataFrame，它自动遍历整个 Series 或者 DataFrame, 对每一个元素运行指定的函数。更改前最好对数据进行确认，如果在数据中不存在满足条件的记录，导致更改操作处理的是一个空的dataframe 会导致报错 123456789101112# 使用lambda函数进行操作df[&apos;label&apos;]=df.id.apply(lambda x: 1 if &apos;M&apos; in x else 0)# 使用预定义的函数进行处理def valuation_formula(x, y): return x * y * 0.5df[&apos;price&apos;] = df.apply(lambda row: valuation_formula(row[&apos;x&apos;], row[&apos;y&apos;]), axis=1)# 如果id列值包含‘L’,那么就将label列中对应的值从1替换成0：# 先判断是否存在满足条件的数据记录，如果有在进行更改if len(df[df[&apos;id&apos;].str.contains(&apos;L&apos;)]) &gt; 0: df.loc[df[&apos;id&apos;].str.contains(&apos;L&apos;),&apos;label&apos;]=0 更新DataFrame某一列（值位于另一个DataFrame）12345678import pandas as pddf1=pd.DataFrame(&#123;&apos;id&apos;:[1,2,3],&apos;name&apos;:[&apos;Andy1&apos;,&apos;Jacky1&apos;,&apos;Bruce1&apos;]&#125;)df2=pd.DataFrame(&#123;&apos;id&apos;:[1,2],&apos;name&apos;:[&apos;Andy2&apos;,&apos;Jacky2&apos;]&#125;)s = df2.set_index(&apos;id&apos;)[&apos;name&apos;]df1[&apos;name&apos;] = df1[&apos;id&apos;].map(s).fillna(df1[&apos;name&apos;]).astype(str)print(df1) 将dataframe中的某一列中的文本拆分成多行示例：123df=df.drop(&apos;cont&apos;, axis=1).join(df[&apos;cont&apos;].str.split(&apos;/&apos;, expand=True).stack().reset_index(level=1, drop=True).rename(&apos;tag&apos;)) df=df[&apos;cont&apos;].str.split(&apos;/&apos;, expand=True).stack().reset_index(level=0).set_index(&apos;level_0&apos;).rename(columns=&#123;0:&apos;tag&apos;&#125;).join(df.drop(&apos;cont&apos;, axis=1)) 将dataframe中的多列合并成一个新列12345678910111213data#GENE1 GENE2 chr pos RPN2 SRC chr20 35868878 LINC00486 PLXNA1 chr2 33141303 LINC00486 TRRAP chr2 33141470 LINC00486 MAPK3 chr2 33141307 data[&quot;chr_pos&quot;] = data[&apos;chr&apos;].str.cat(data.pos.astype(&apos;str&apos;),sep=&quot;:&quot;) #GENE1 GENE2 chr pos chr_posRPN2 SRC chr20 35868878 chr20:35868878LINC00486 PLXNA1 chr2 33141303 chr2:33141303LINC00486 TRRAP chr2 33141470 chr2:33141470LINC00486 MAPK3 chr2 33141307 chr2:33141307 dataframe的合并按列进行合并 merge （着重关注的是行的合并）12345678910111213141516171819202122232425262728293031323334&gt;&gt;&gt; print(df1) Courses Fee Durationr1 Spark 20000 30daysr2 PySpark 25000 40daysr3 Python 22000 35daysr4 pandas 30000 50days&gt;&gt;&gt; print(df2) Courses Discountr1 Spark 2000r6 Java 2300r3 Python 1200r5 Go 2000# pandas.merge()df3=pd.merge(df1,df2, how=&apos;left&apos;)print(df3)# DataFrame.merge()df3=df1.merge(df2, how=&apos;left&apos;)print(df3) Courses Fee Duration Discount0 Spark 20000 30days 2000.01 PySpark 25000 40days NaN2 Python 22000 35days 1200.03 pandas 30000 50days NaN# 按特定的列进行两个dataframe的合并# Merge DataFrames by Columnsdf3=pd.merge(df1,df2, on=&apos;Courses&apos;, how=&apos;left&apos;)# When column names are differentdf3=pd.merge(df1,df2, left_on=&apos;Courses&apos;, right_on=&apos;Courses&apos;, how=&apos;left&apos;)print(df3) 按行进行拼接12# 将两个dataframe逐行添加df_result = pd.concat([dataframe1 , dataframe2]) dataframe中逐行添加数据1AllSampleFastqQC.loc[len(AllSampleFastqQC)+1] = [SampleDir,SampleDirPath,Chip,lane,barcode,umi,Q20,Q30,GC] dataframe 去重drop_duplicates()函数的语法格式如下：1234df.drop_duplicates(subset=['A','B','C'],keep='first',inplace=True) #subset：表示要进去重的列名，默认为 None。 #keep：有三个可选参数，分别是 first、last、False，默认为 first，表示只保留第一次出现的重复项，删除其余重复项，last 表示只保留最后一次出现的重复项，False 则表示删除所有重复项。 #inplace：布尔值参数，默认为 False 表示删除重复项后返回一个副本，若为 Ture 则表示直接在原数据上删除重复项。]]></content>
      <categories>
        <category>编程拾慧</category>
        <category>python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python包-Numpy记录]]></title>
    <url>%2F0002.%E5%BC%80%E5%8F%91-02.python%2FPython-%E5%8C%85-threading_%E5%A4%9A%E7%BA%BF%E7%A8%8B%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[threading用于提供线程相关的操作，线程是应用程序中工作的最小单元。python当前版本的多线程库没有实现优先级、线程组，线程也不能被停止、暂停、恢复、中断。 threading模块提供的类：Thread, Lock, Rlock, Condition, [Bounded]Semaphore, Event, Timer, local。 threading 模块提供的常用方法： threading.currentThread(): 返回当前的线程变量。 threading.enumerate(): 返回一个包含正在运行的线程的list。正在运行指线程启动后、结束前，不包括启动前和终止后的线程。 threading.activeCount(): 返回正在运行的线程数量，与len(threading.enumerate())有相同的结果。 threading 模块提供的常量：threading.TIMEOUT_MAX 设置threading全局超时时间。 代码实例：1234567891011121314151617181920212223242526272829# coding:utf-8import threadingimport timeISOTIMEFORMAT=&apos;%Y-%m-%d %X&apos;def action(arg): print &apos;sub thread start!the thread name is:%s\r&apos; % threading.currentThread().getName() , print time.strftime( ISOTIMEFORMAT, time.localtime( time.time() ) ) time.sleep(5) print &apos;the arg is:%s\r&apos; %arg , print time.strftime( ISOTIMEFORMAT, time.localtime( time.time() ) )for i in xrange(1000): t =threading.Thread(target=action,args=(i,)) t.setDaemon(True)# print time.strftime( ISOTIMEFORMAT, time.localtime( time.time() ) ) t.start() while True: #判断正在运行的线程数量,如果小于5则退出while循环, #进入for循环启动新的进程.否则就一直在while循环进入死循环 if(len(threading.enumerate()) &lt; 20): break t.join()print &apos;main_thread end! :&apos;,print time.strftime( ISOTIMEFORMAT, time.localtime( time.time() ) ) 线程 多线程模块参考资料]]></content>
      <categories>
        <category>编程拾慧</category>
        <category>python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python包-tkinter记录]]></title>
    <url>%2F0002.%E5%BC%80%E5%8F%91-02.python%2FPython-%E5%8C%85-tkinter%2F</url>
    <content type="text"><![CDATA[常用交互功能创建GUI界面123import tkinter as tktop = tk.Tk() # 创建主控件top.geometry('800x400') #设置初始窗口大小 创建控件Tkinter的提供各种控件，如按钮，标签和文本框，一个GUI应用程序中使用。这些控件通常被称为控件或者部件。12#创建一个按钮控件，点击按钮时执行 subComman（一个独立定义的函数）命令ChooseInputFile = tk.Button(top, text="选择图片所在目录", command=subComman) 目前有15种Tkinter的部件。我们提出这些部件以及一个简短的介绍，在下面的表:|控件|描述||-|-||Button|按钮控件；在程序中显示按钮。||Canvas|画布控件；显示图形元素如线条或文本||Checkbutton|多选框控件；用于在程序中提供多项选择框||Entry|输入控件；用于显示简单的文本内容||Frame|框架控件；在屏幕上显示一个矩形区域，多用来作为容器||Label|标签控件；可以显示文本和位图||Listbox|列表框控件；在Listbox窗口小部件是用来显示一个字符串列表给用户||Menubutton|菜单按钮控件，用于显示菜单项。||Menu|菜单控件；显示菜单栏,下拉菜单和弹出菜单||Message|消息控件；用来显示多行文本，与label比较类似||Radiobutton|单选按钮控件；显示一个单选的按钮状态||Scale|范围控件；显示一个数值刻度，为输出限定范围的数字区间||Scrollbar|滚动条控件，当内容超过可视化区域时使用，如列表框。.||Text|文本控件；用于显示多行文本||Toplevel|容器控件；用来提供一个单独的对话框，和Frame比较类似||Spinbox|输入控件；与Entry类似，但是可以指定输入范围值||PanedWindow|PanedWindow是一个窗口布局管理的插件，可以包含一个或者多个子控件。||LabelFrame|labelframe|是一个简单的容器控件。常用与复杂的窗口布局。||tkMessageBox|用于显示你应用程序的消息框。| 所有控件的公共属性 属性 描述 Dimension 控件大小； Color 控件颜色； Font 控件字体； Anchor 锚点； Relief 控件样式； Bitmap 位图； Cursor 光标； 几何管理 几何方法 描述 pack() 包装； grid() 网格； place() 位置；]]></content>
      <categories>
        <category>编程拾慧</category>
        <category>python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python包-Numpy]]></title>
    <url>%2F0002.%E5%BC%80%E5%8F%91-02.python%2FPython-%E5%8C%85-Numpy%2F</url>
    <content type="text"><![CDATA[基本属性数组 函数 &amp; 属性 功能 示例 array=np.array([1,2,3],dtype=np.float) 构建一个数组，通过dtype可以是指数组的数据格式 np.arange 生成满足数列的数组 np.arange(start,end,sep) np.linspace 生成固定数目的数组 np.linspace(start,end,num) array.reship(m,n) 将array 重构为一个m行n列的矩阵 矩阵 函数 &amp; 属性 功能 示例 \$matrix=np.mat(\$array ) 将一个数组对象转换成矩阵 RandMat=mat(random.rand(4,4)) *.I 求矩阵 matrix.I 的逆 invRandMat=RandMat.I np.eye 创建一个单位矩阵 i=np.eye(4) np.zeros((m,n)) 构建一个值都是0的矩阵 np.ones((m,n)) 构建一个值都是1的矩阵 矩阵的基本运算 矩阵 * 矩阵的逆 = 单位矩阵 （单位矩阵是对角线元素为1 ，其他元素均为0 ）]]></content>
      <categories>
        <category>编程拾慧</category>
        <category>python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python包-Numpy记录]]></title>
    <url>%2F0002.%E5%BC%80%E5%8F%91-02.python%2FPython-%E5%8C%85-vcf%2F</url>
    <content type="text"><![CDATA[官方文档 模块的安装1pip install pyvcf 安装可能的异常记录Error1pyvcf的最新版（0.6.8）安装需要使用use_2to3 （在setuptools&gt;=58 后不支持该参数）所以需要对setuptools进行版本调整，过高过低都会出现该参数缺失无法变异的情况。1pip install setuptools==57.5.0 Error2安装报错12unable to execute &apos;x86_64-conda_cos6-linux-gnu-gcc&apos;: No such file or directoryerror: command &apos;x86_64-conda_cos6-linux-gnu-gcc&apos; failed with exit status 1 环境先安装x86_64-conda_cos6-linux-gnu-gcc（ conda install gxx_linux-64） 在安装相关模块 VCF文件的读写1234567891011import vcf#读取vcf文件 vcf_reader = vcf.Reader(open(&apos;vcf/test/example-4.0.vcf&apos;, &apos;r&apos;))for var in vcf_reader: print (var)# 写入vcf文件vcf_writer = vcf.Writer(open(&apos;/dev/null&apos;, &apos;w&apos;), vcf_reader)for var in vcf_reader: vcf_writer.write_record(var) vcf文件迭代子类的属性1234567891011121314Record.CHROM Record.POSRecord.IDRecord.REFRecord.ALTRecord.QUALRecord.FILTERRecord.INFO # 返回一个字典，可以用Record.INFO[&apos;type&apos;],Record.INFO[&apos;DP&apos;] 键值继续提取Record.FORMAT #返回format列 字符串 如果你的vcf文件中没有FORMAT 返回 &quot;GT:DP:RO:QR:AO:QA:GL&quot;Record.genotype(&quot;cancer&quot;)[&quot;AD&quot;] # 通过样本名称和FORMAT 索引获得样本对应信息。print i.samples # 返回的是三个样 call object 组成的列表。 常用信息1cpra = Record.CHROM+&quot;_&quot;+Record.POS+&quot;_&quot;+Record.REF+&quot;_&quot;+Record.ALT]]></content>
      <categories>
        <category>编程拾慧</category>
        <category>python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python包-lifelines]]></title>
    <url>%2F0002.%E5%BC%80%E5%8F%91-02.python%2Fpython-%E5%8C%85-lifelines%2F</url>
    <content type="text"><![CDATA[参考资料readthedocscsdn生存分析]]></content>
      <categories>
        <category>编程拾慧</category>
        <category>python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python包-python-docx撰写word]]></title>
    <url>%2F0002.%E5%BC%80%E5%8F%91-02.python%2FPython-%E5%8C%85-python-PyMouse_%E6%8E%A7%E5%88%B6%E9%BC%A0%E6%A0%87%2F</url>
    <content type="text"><![CDATA[参考博客 python-sendkeys 模拟键盘事件的模块 导入需要的包 12345import win32apiimport win32conimport win32guifrom ctypes import *import time 设置鼠标双击的函数，通过坐标控制双击位点 1234567 def double_click(x=0,y=0):mouse_move(x,y)time.sleep(0.05) #延迟时间，尤其是在电脑反映不是很快的时候，win32api.mouse_event(win32con.MOUSEEVENTF_LEFTDOWN,0,0,0,0) #点击鼠标win32api.mouse_event(win32con.MOUSEEVENTF_LEFTUP, 0,0,0,0) #抬起鼠标win32api.mouse_event(win32con.MOUSEEVENTF_LEFTDOWN,0,0,0,0)win32api.mouse_event(win32con.MOUSEEVENTF_LEFTUP, 0,0,0,0) 捕获鼠标当前位置函数 123456def get_mouse_position(): po = POINT() windll.user32.GetCursorPos(byref(po)) return int(po.x),int(po.y)class POINT(Structure): fields_=[(&quot;x&quot;,c_ulong),(&quot;y&quot;,c_ulong)] 模拟键盘输入 1win32api.keybd_event(86,0,0,0) 键码表Win32 api函数表附个键位码表： 字母和数字键 数字小键盘的键 功能键 其它键 键 键码 键 键码 键 键码 键 键码 A 65 0 96 F1 112 Backspace 8 B 66 1 97 F2 113 Tab 9 C 67 2 98 F3 114 Clear 12 D 68 3 99 F4 115 Enter 13 E 69 4 100 F5 116 Shift 16 F 70 5 101 F6 117 Control 17 G 71 6 102 F7 118 Alt 18 H 72 7 103 F8 119 Caps Lock 20 I 73 8 104 F9 120 Esc 27 J 74 9 105 F10 121 Spacebar 32 K 75 * 106 F11 122 Page Up 33 L 76 + 107 F12 123 Page Down 34 M 77 Enter 108 – – End 35 N 78 - 109 – – Home 36 O 79 . 110 – – Left Arrow 37 P 80 / 111 – – Up Arrow 38 Q 81 – – – – Right Arrow 39 R 82 – – – – Down Arrow 40 S 83 – – – – Insert 45 T 84 – – – – Delete 46 U 85 – – – – Help 47 V 86 – – – – Num Lock 144 其他未列出的字母和数字键盘为：ord(c)]]></content>
      <categories>
        <category>编程拾慧</category>
        <category>python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[肿瘤NGS检测方法开发方法学 - MSK方法学资料]]></title>
    <url>%2F0001.%E7%9F%A5%E8%AF%86-03.standard%2FStandard-Method-MSK_method%2F</url>
    <content type="text"><![CDATA[参考资料EVALUATION OF AUTOMATIC CLASS III DESIGNATION FOR MSK-IMPACT (Integrated Mutation Profiling of Actionable Cancer Targets)A Hybridization Capture-Based Next-Generation Sequencing Clinical Assay for Solid Tumor Molecular OncologyTumour lineage shapes BRCA-mediated phenotypes 方法介绍样本准备选择标准 样本肿瘤细胞含量超过20% ； 针对MSI检测，肿瘤细胞含量应该超过25% ； DNA需要进行质控和富集，进行方法学测试的DNA 需要达到100ng~250ng； 平均插入片段（Average fragment）应该在200bp左右； 建库前应该存放在-20℃ （The DNA can be stored at 37°C for 10-20 minutes, stored at 2–8°C for 24 hours, or at –20°C for longer periods）。 文库构建…… 表现评估Determination of pipeline thresholdsRequirements on exon coverage were established不同的测序深度下，观测到的变异频率和理论变异频率之间会存在波动，导致检测结果和理论结果不一致（但理论上大概率会处在某个范围内，理论计算如下表 ： 突变实际频率 置信区间 测序深度 检测值范围 10% 95 500 7.5% ~ 13% 10% 95 100x 5.0% ~ 17.6% 为了验证这个理论，使用了10个 正常的FFPE 进行测试，实际测试结果如下： 突变实际频率 置信区间 测序深度 检测值范围 10% 95 平均深度 480x 5.0% ~ 13.9% 测试结果和预测结果相对比较一致。 这个数据支持我们检测10%的目标变异时，使用5%作为检测的阈值 。 Requirements on sample coverageBased on the calculations, 98% of exons can be expected to be sequenced to coverage greater than 100X, when mean sample coverage is 185X (0.54* 185X = 100X). (A 100X minimum coverage threshold per exon is required based on the power calculations, which showed 100X coverage was necessary to call mutations with true underlying mutation frequency 10% or greater, with 95% power at an alpha level of 0.05).To be conservative, a threshold of 200X on mean sample coverage is used to determine if a sample is sequenced to sufficient depth for subsequent analysis. A sample is flagged as being at increased risk of false negatives if its mean coverage is below 200X. Requirements on mutation coverage, allele depth and frequency for positive calls]]></content>
      <categories>
        <category>NGS</category>
        <category>肿瘤检测</category>
        <category>方法学</category>
      </categories>
      <tags>
        <tag>方法学</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[流程图绘制 - diagrams]]></title>
    <url>%2F0003.%E5%BA%94%E7%94%A8-03.software%2FSoftware-%E6%B5%81%E7%A8%8B%E5%9B%BE%E7%BB%98%E5%88%B6-diagrams%2F</url>
    <content type="text"><![CDATA[安装Web在线版本 本地版安装 支持Windows、Mac、Linux等。 也可以使用 VS code进行相关流程图的绘制，需要安装Draw.io VS Code Integration 扩展。 介绍软件的使用，都是可以通过UI进行拖拽操作的，因此整体使用较为简单，在此不进行赘述。但是在此稍微介绍一些整理收集的流程图绘制规范，来帮助我们作出更规范/清晰的流程图。优秀的流程图需要遵循一定的规范，包括符号规范、结构规范、路径规范等。只要熟练掌握这些基础规范，我们每个人都能做出优秀流程图。 流程图是什么？流程图=流程+图。 所谓流程，IS09000系列国际标准中将流程定义为一组将输入转化为输出的相互群或相互作用的活动。流程有六个要素构成，分别是: - 流程的输入资源 - 流程中的若干活动、 - 活动的相互作用、 - 输出结果、 - 顾客、 - 最终流程创造的价值。 一个流程会将这6个要素有序串联起来，而流程图则是承载上述程序的图形载体。根据流程图“流动”信息的不同，又可以细分为产品流程图、数据流程图、程序流程图等，比如： 页面流程图，呈现的是页面跳转顺序； 数据流程图，用于表达数据的流转。 流程图的符号规范流程图中的每个符号都有着特定含义。 流程图的结构规范流程图有三大结构，分别是顺序结构、选择结构和循环结构。 参考信息https://zhuanlan.zhihu.com/p/347119698]]></content>
      <categories>
        <category>software</category>
        <category>Mac</category>
        <category>Windows</category>
      </categories>
      <tags>
        <tag>流程图</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[变异注释功能标准化 - Sequence Ontology]]></title>
    <url>%2F0001.%E7%9F%A5%E8%AF%86-03.standard%2FStandard-Sequence_Ontology%2F</url>
    <content type="text"><![CDATA[参考资料参考网站新版网站github网址 为什么要标准化？]]></content>
      <categories>
        <category>NGS</category>
        <category>肿瘤检测</category>
        <category>标准化</category>
      </categories>
      <tags>
        <tag>注释</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[变异检出标准化-HGVS (Human Genome Variation Society)]]></title>
    <url>%2F0001.%E7%9F%A5%E8%AF%86-03.standard%2FStandard-%E5%8F%98%E5%BC%82%E6%A3%80%E5%87%BA%E6%B3%A8%E9%87%8A%E6%A0%87%E5%87%86%E5%8C%96-HGVS%2F</url>
    <content type="text"><![CDATA[为什么要标准化？简单讲，就是为了让一个特定的突变具有一个唯一的表述方式，一个突变如果不进行标准化，会出现多种不同的表达方式，即MNP（Multi Nucleotide Polymorphism ），这会给大家的交流带来诸多的问题， 试想每个人有自己的习惯，最终一个突变可能会有成百上千种表达方式，这无疑会给研究人员交流带来诸多的困扰。因此标准化是很有必要的，目前主流的标准化原则是涉及尽可能少的碱基，尽可能做对齐。 首先我们来看一下，目前的变异检测多是依靠序列比对，然后会针对不同的比对情况进行变异检出的，我们先来看一下SNV， 如果不标准化，一个相同的变异 CAT =&gt; TGC 的突变，会出现下列所示的多种不同的表述方法。 随着基因检测技术的广泛应用，越来越多与疾病相关的基因和变异被明确。为了让变异注释更程序化、对变异文献的检索更全面化以及对变异的描述更标准化故需要形成统一突变命名规则，HGVS命名规则是目前公认的命名规则（人类基因组变异学会(HGVS)、人类变异项目(HVP)和人类基因组组织(HUGO)授权）。HGVS命名规则从不同层面对变异进行描述，反应变异的位置及对编码的蛋白的影响。参考序列的选择、转录版本号的不同及描述层面（DNA水平、RNA水平及蛋白水平）的不同，均会导致同一变异描述的形式不同。 HGVS规范参考序列只接受NCBI或EBI公共数据库中的参考序列ID，且必须同时包含accession和version信息；如NC_000023.10中，NC_000023代表accession号，10代表version号。参考序列中下划线前的大写字母代表参考序列格式，目前批准的参考序列格式有：|格式|解释|举例||-|-|-||NC_＃|代表完整的基因组序列，标记的类别包括基因组、染色体、细胞器、质粒。|NC_000023.10:g.32407761G&gt;A||LRG_#|Locus Reference Genomic，基因座参考基因组序列。（不能在其他基因组找到的）|NG_012232.1:g.954966C&gt;T||NG_＃|不完整的基因组区域（不转录的假基因或者那些很难自行化注释的基因组区域）|LRG_199:g.954966C&gt;T||NM_＃|编码蛋白的转录本序列。基因检测报告中最常用此作为参考序列。|NM004006.2:c.4375C&gt;T||NR_＃|非编码的转录本序列，包括结构RNAs，假基因转子等。|NR002196.1:n.601G&gt;T||NP_＃|蛋白质序列|NP-003997.1:p.Arg1459*(p.Arg1459Tero| g.代表线性基因组参考序列； o.代表环状基因组参考序列； m.代表线粒体参考序列； c.代表编码DNA参考序列； n.代表非编码DNA参考序列； r.代表RNA参考序列； p.代表蛋白（氨基酸）参考序列。 变异位置 编码区（CDS） 以起始密码子ATG的第一个碱基A开始，并记为c.1，以终止密码子（TAA, TAG, TGA）的最后一个碱基为终点。 内含子区（Intron） 靠近内含子5’末端的变异位点，需依据上游最近外显子的最后一个碱基来定位，如c.87+4，代表上游最近外显子的边界位置为87，变异位点在内含子5’ 端开始的第4个碱基； 靠近内含子3’ 末端的变异位点，要依据下游最近外显子的第一个碱基来定位，如c.88-11， 内含子碱基个数为偶数时，中间碱基平分后按上下游外显子碱基来定位命名，如…,c.87+676, c.87+677, c.87+678, c.88-678, c.88-677, c.88-676, … 内含子碱基个数为奇数时，中间碱基相对于上游外显子最后一个碱基来定位命名，如…,c.87+677, c.87+678, c.87+679, c.88-678, c.88-677, … 非编码区（UTR区）： 起始密码子ATG上游（5’ UTR区）标记为“-”，编号为c.-1, c.-2, c.-3… 终止密码子下游（3’ UTR区）标记为“”，编号为c.1, c.2, c.3… 位于靠近5’ UTR和3’ UTR区的内含子变异位点，命名规则同内含子区，如：5’ UTR区内含子为c.-85+1，c.-84-3等；3’ UTR区内含子为c.37+1，c.38-3等。 参考示意图如下： 变异类型优先级从高到低分别如下： 置换（&gt;）：一个核苷酸被另一个核苷酸替代，使用“&gt;”来表示；例如g.1318G&gt;T； 缺失（del）:一个或多个核苷酸被移除，使用“del”进行描述；例如g.3661_3706del； 倒置（inv）: 与原始序列反向互补的新的核苷酸序列（大于1个核苷酸）替换原始序列，例如由CTCGA变为TCGAG，使用”inv“表示； 重复（dup）：一个或多个核苷酸拷贝直接插入原始序列的下游，使用“dup”表示； 插入（ins）：序列中插入一个或多个核苷酸，并且插入序列并非上游序列拷贝； 缺失-插入（delins/indel）:一个或多个核苷酸被其他核苷酸替代，但并不是发生替代、倒置和转置； 转换（con）：一种特殊类型的缺失-插入，其中替代原始序列的核苷酸序列是来自基因组中另一个位点的序列拷贝； 5、变异描述示例DNA水平 c.76A&gt;C：76位的核苷酸A变异为C； c.82_83delTG：位于82和83位点上的核苷酸TG缺失，ACTTTGTGCC变异为ACTTTGCC（A是第76位）； c.83_84dupTG： ACTTTGTGCC（A为第76位）的83-84位之间插入短的串联重复序列TG，变为ACTTTGTGTGCC； g.333_590con1844_2011：基因组中编号为333-590的核苷酸序列替代1844-2011原有序列，插入其中； g.112_117delinsTG：在基因组序列编号为112-117之间的6个核苷酸被TG替换；多个变异使用”[]”标注变异，并用“；”链接 同一等位基因发生多个变异： c.[76A &gt;C;83G&gt;C]：同一染色体上76位和83位发生两个变异（顺式）； 不同等位基因发生多个变异： c.[76A &gt;C];[83G&gt;C]：两个变异发生在不同染色体上（反式）； 不确定多个变异发生的位置： c.[76A &gt;C](;)[83G&gt;C]：两个变异可能发生在同一染色体，也可能发生在不同染色体上，用(;)来链接； 定义重复序列的核苷酸范围及重复单位的数量，并用“[]”表示 g.123_124[4]:基因组序列中第123-124间的核苷酸重复出现4次； 对于短的/简单的重复，可以展示重复序列 g．123TG[4]:基因组序列中从123位开始TG核苷酸重复出现4次； 当重复序列长度不确定时，使用括号进行指定 g.-128GGC[(600-800)]:基因组编码区上游128位核苷酸处重复插入GGC，重复次数在600-800之间；蛋白质水平 替换：如p.Trp26Cys，表示第26位的Trp被Cys取代（错义突变）；p.Trp26Ter (p.Trp26*)，表示第26位的Trp变为终止密码（无义突变）；p.Cys123=，表示基因突变之后，氨基酸没有发生改变（同义突变）； 缺失：如p.Ala3_Ser5del，表示多肽序列中从第3位的Ala到第5位的Ser发生了缺失； 插入：如p.Lys2_Gly3insGlnSerLys，表示在第2位的Lys和第3位的Gly之间插入了GlnSerLys； 插入缺失：如p.Cys28delinsTrpVal，表示第28位的Cys缺失，同时被TrpVal取代； 重复：如p.Ala2[10]，表示第2位的Ala重复了10次； 移码突变：在起始密码子和终止密码子之间的读码框发生了改变；以“fx”进行表示；如p.Arg97ProfsTer23，表示第97位的Arg是首个发生改变的氨基酸，且Arg变为Pro，同时发生移码突变后，终止密码的位置变为第23位； 变异类型的描述原则一般原则： 所有变异需先从最基本的层面（DNA水平）进行描述，还可从RNA水平和蛋白质水平上进行描述。 用变异的描述是否加“（）” 来说明变异是由实验确定的还是从理论上推导出来的。 所有的变异都应该根据公认的参考序列来描述。 当变异可描述为几种变异类型时，优先级为：(1)替换，(2)删除，(3)倒位，(4)重复，(5)转换，(6)插入。如：当一个变异可以被描述为重复或插入时，根据优先级决定应描述为重复，而不是插入。如：“-ATGCCCA-”插入后变为“-ATGCCCCA-”，应描述为c.7dup，而不是NM_004006.2:c.7_8insC。 在进行变异描述时，基因的描述要采用HGNC的官方基因名。 重复序列变异描述原则：c.123CAG[16]，g.3258796GA[8]对于编码区DNA序列而言，重复序列的描述仅用于重复单元长度为3的倍数的重复序列，即不会影响阅读框的重复单元长度；若重复序列长度不是3的倍数，则不能用该形式描述。如：当CNPTAB基因在编码区的TATA序列后插入TATATATA序列，则对于该插入变异的描述应为NM_024312.4:c.1741_1742insTATATATA，而不是NM_024312.4:c.1738TA[6]。 3’ 端法则：变异的描述需遵循最靠近3’ 端法则。如“-ATGCCCCA-”变异成“-ATGC_CCA-”，根据3’ 端法则应描述为c.7delC，而不是c.5delC。例外：当缺失/重复发生在外显子与外显子衔接处，且衔接处碱基相同，不遵循3’ 端法则。如“..GAT gta..//..cag TCA..”缺失后变为“..GA_ gta..//..cag TCA..”，应描述为NM_004006.2:c.3921del，而不是NM_004006.2:c.3922del；“..GAT gta..//..cag TCA..”重复后变为“..GATT gta..//..cag TCA..”，应描述为NM_004006.2:c.3921dup，而不是NM_004006.2:c.3922dup。 有大量的原始检测软件，在未进行变异注释前是进行的左对齐 delins原则：涉及两个或以上连续核苷酸的替换，描述为delins。若两个变异被一个或多个核苷酸分隔，优先单独描述两个变异，而不采用delins合并描述；若被一个核苷酸分隔的两个变异，共同影响一个氨基酸，则合并描述为delins，如c.142_144delinsTGG (p.Arg48Trp)； 若两个变异中的任何一个为已知的高频变异位点，则需要单独描述两个变异，即NM_004006.1:c.[145C&gt;T;147C&gt;G]，优先于NM_004006.1:c.145_147delinsTGG。(该原则需与解读同事讨论) 起始密码子变异描述：描述取决于变异对蛋白产物改变的结果。 变异后不产生蛋白质：NM_003002.3:p.0 变异对蛋白产物的影响不清楚且无法预测：NM_003002.3:p.? 变异后产生新的起始氨基酸： a）上游：p.Met1ext-5，即原始密码子上游第5位（5’ UTR区）产生了新的起始氨基酸，另可描述为p.Met1extMet-5。 b）下游：p.Leu2_Met124del，即原起始氨基酸丢失且下游产生新的起始氨基酸，导致蛋白的第1到123位氨基酸缺失。终止密码子变异描述：采用“ext”描述 p.Ter110Glnext17(p.110Glnext17)：原终止氨基酸变为谷氨酰胺（Gln），并在下游第17位产生新的终止氨基酸，导致蛋白产物延长17个氨基酸。注：不可描述为p.Ter110GlnextTer17 ，此处的17代表的是位置（3’ UTR区） p.Ter315TyrextAsnLysGlyThrTer(p.315TyrextAsnLysGlyThr) ：原终止氨基酸变为酪氨酸（Tyr），并在下游第5位产生新的终止氨基酸，导致蛋白产物延长5个氨基酸。 p.Ter327Argext?(p.327Argext*?)：原终止氨基酸变为精氨酸（Arg），导致蛋白产物延长，延长的长度未知。 附录1、特殊字符的含义 “+”：c.123+45A&gt;G（代表靠近内含子5’ 端的核苷酸发生变异） “-”：c.124-56C&gt;T（代表靠近内含子3’ 端核苷酸发生变异）；c.-142C&gt;G（代表5’ UTR区） “”：c.32G&gt;A（代表3’ UTR区）；P.Trp41*（代表终止氨基酸） “[]”：代表等位基因，“;”用来分隔变异和等位基因，如g.[123456A&gt;G;345678G&gt;C] 代表顺式，g.[123456A&gt;G];[345678G&gt;C]代表反式， g.123456A&gt;G(;)345678G&gt;C代表这两个变异的顺式反式未知。 “()”：用来表示不确定的或预测的结果，如p.(Ser123Arg) “?”：用来表示核苷酸或氨基酸的位置未知，如g. (?_234567) _ (345678_?)del “^”：代表“或”的意思，如p.Ser124Arg反推核苷酸的改变为c.(370A&gt;C^372C&gt;G^372C&gt;A) ，即AGC变成CGC, AGG或AGA “/”：表示嵌合体（mosaic），如NM_004006.1:c.145=/C&gt;T “//”：表示异源嵌合体（chimeric），如NM_004006.1:c.85=//T&gt;C “|”：代表不是序列的直接改变，而是一种修饰或一种状态的改变，如甲基化。 “::”：用于描述RNA融合转录本和断点连接形成的环状染色体 缩写字符的含义 “fs”代表变异类型为移码变异，主要是针对蛋白水平而言，如p.Arg456GlyfsTer17或p.Arg456Glyfs*17 “ext”代表变异类型为延长，主要是针对起始密码子和终止密码子变异导致的蛋白水平的改变，如p.Met1ext-5 “gom” 表示获得甲基化，如g.12345678_12345901|gom “lom”表示去甲基化，如g.12345678_12345901|lom “met” 表示甲基化，如g.12345678_12345901|met= 软件脚本3’对齐针对变异进行3’对其的工具，可以参考GitHub仓库的脚本 toolkits\03.Deal_mutation\Realn4vcf.py 左对齐目前仍有大量软件本身是进行左对齐的，因此在进行结果比较时可以统一进行左对齐，从而实现变异结果的规范化，进行比较。对齐的命令行1234567891011Commands used are: bcftools norm -f ref.fa in.vcf -O z &gt; out.vcf.gz java -jar GenomeAnalysisTK.jar -T LeftAlignAndTrimVariants --trimAlleles -R ref.fa --variant in.vcf.gz -o out.vcf.gz vt normalize -r ref.vcf.gz -o out.vcf.gzVersions are: bcftools v0.2.0-rc8-5-g0e06231 (using htslib 0.2.0-rc8-6-gd49dfa6) [updated non release development version] GATK v3.1-1-g07a4bf8 vt normalize v0.5 参考资料： WIKIHGVSHGVS Recommendations for the Description of Sequence Variants: 2016 Update]]></content>
      <categories>
        <category>NGS</category>
        <category>肿瘤检测</category>
        <category>标准化</category>
      </categories>
      <tags>
        <tag>变异检测</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[变异检出标准化-HGVS]]></title>
    <url>%2F0001.%E7%9F%A5%E8%AF%86-03.standard%2FStandard-%E5%8F%98%E5%BC%82%E6%A3%80%E5%87%BA%E6%B3%A8%E9%87%8A%E6%A0%87%E5%87%86%E5%8C%96-HGVS-pHGVS%2F</url>
    <content type="text"><![CDATA[参考资料： WIKIHGVSHGVS Recommendations for the Description of Sequence Variants: 2016 Update]]></content>
      <categories>
        <category>NGS</category>
        <category>肿瘤检测</category>
        <category>标准化</category>
      </categories>
      <tags>
        <tag>变异检测</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[R绘图 - ggplot常用参数记录]]></title>
    <url>%2F0002.%E5%BC%80%E5%8F%91-03.R%2FR-ggplot-%E7%94%A8%E5%8F%82%E6%95%B0%E8%AE%B0%E5%BD%95%2F</url>
    <content type="text"><![CDATA[数据读取如果计划使用ggplot绘图，推荐使用数列方式整理数据，数据之间用tab分割。读取后，默认是数据框格式。然后然后根据所要绘制的图片，对数据进行解析。 折叠代码示例 Type Cancer TMB Plasma hepatocellular_carcinoma 9.743589744 Plasma gastric_cancer 4.615384615 Plasma cervical_cancer 1.025641026 Plasma lung_cancer 4.102564103 Plasma lung_cancer 3.58974359 Plasma lung_cancer 5.641025641 Plasma cervical_cancer 1.025641026 Plasma lung_cancer 1.025641026 Plasma hepatocellular_carcinoma 2.564102564 Plasma lung_cancer 2.564102564 Plasma pancreatic_cancer 1.538461538 Plasma lung_cancer 9.743589744 Plasma lung_cancer 1.538461538 Plasma lung_cancer 1.025641026 Plasma lung_cancer 3.58974359 Plasma lung_cancer 3.076923077 Plasma hepatocellular_carcinoma 2.564102564 Plasma colorectal_cancer 1.025641026 Plasma lung_cancer 6.666666667 Plasma lung_cancer 13.84615385 图片类型柱状图：geom_bar（）1position=&quot;dodge&quot; (多组柱状图平行排列，模式是堆叠柱状图) 箱线图： geom_boxplot1aes(fill=Type) （分类分别绘制箱线图） 坐标轴刻度12scale_x_continuous(breaks=seq(start,end,step)) # 设置x轴对应坐标轴的起始、终止和步长信息。也可以使用数组。scale_y_continuous(breaks=seq(start,end,step)) # 设置y轴对应坐标轴的起始、终止和步长信息。也可以使用数组。 标题格式theme对标签文本的格式进行调整 1theme(axos.text.x=element_text(hjust=0.5,size=8,angle=45)) #hjust对应偏移量，size对应文本字体大小，angle对应字体倾斜角度。 标签格式s 图片截取图片绘制完成后，只截取其中的一部分进行展示，通过改名了，可以调整结果途中展示的数据范围。避免因为某些异常值，导致整个图片展示出现偏差。 1coord_cartesian(xlim=c(0,100),ylim=c(10,50)) # 根据x轴y轴的数据，对图片进行截取，仅保留在截取范围内的图片。 添加辅助线添加水平线geom_hline()1geom_hline(yintercept=10) #添加一天y=10的辅助线； 添加辅助标签 geom_text（）12aes(x=1,y=1,label=as.character(&quot;lab_test&quot;)) #在x=1，y=1的位置添加一个标签，标签内容为“lab_test”；col=&quot;black&quot; #设置标签的颜色；]]></content>
      <categories>
        <category>编程拾慧</category>
        <category>R</category>
      </categories>
      <tags>
        <tag>可视化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Software-Mac环境管理软件-Homebrew简介和基本使用]]></title>
    <url>%2F0003.%E5%BA%94%E7%94%A8-03.software%2FSoftware-Mac%E7%8E%AF%E5%A2%83%E7%AE%A1%E7%90%86%E8%BD%AF%E4%BB%B6-Homebrew%E7%AE%80%E4%BB%8B%E5%92%8C%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[Homebrew官网简介Homebrew是一个开源的包管理器，可以帮助在mac上对一些软件或者命令行工具进行管理，从而补充一些mac上本身缺少但是实用中需要的重要命令，比如 wget等 安装默认安装 首先确认Mac已安装Xcode、Command Line Tool。安装Command Line Tool： 1xcode-select --install 然后把下面的代码粘贴到Terminal中执行安装Homebrew。 1/usr/bin/ruby -e &quot;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)&quot; Homebrew 会将软件包安装到独立目录，并将其文件软链接至 /usr/local 。Homebrew 不会将文件安装到它本身目录之外，所以您可将 Homebrew 安装到任意位置。 指定路径安装安装到/usr/local/homebrew mac 打开终端输入 1mkdir homebrew &amp;&amp; curl -L https://github.com/Homebrew/homebrew/tarball/master | tar xz --strip 1 -C homebrew 在bash文件中 12homebrew=/usr/local/homebrew/bin:/usr/local/homebrew/sbinexport PATH=$homebrew:$PATH 最后更新一下 12source .bash_profilebrew update 卸载1234567$ cd `brew --prefix`$ rm -rf Cellar$ brew prune$ rm `git ls-files`$ rm -r Library/Homebrew Library/Aliases Library/Formula Library/Contributions$ rm -rf .git$ rm -rf ~/Library/Caches/Homebrew 使用 安装任意包的命令 12$ brew install &lt;packageName&gt;$ brew install wget #示例：安装wget 卸载任意包 12$ brew uninstall &lt;packageName&gt;$ brew uninstall git #示例：卸载git 查询可用包 1$ brew search &lt;packageName&gt; 查看已安装包列表 1$ brew list 查看任意包信息 1$ brew info &lt;packageName&gt; 更新Homebrew 1$ brew update 查看Homebrew版本 1$ brew -v Homebrew帮助信息 1$ brew -h 所有可以安装的包列表]]></content>
      <categories>
        <category>software</category>
        <category>Mac</category>
      </categories>
      <tags>
        <tag>mac</tag>
        <tag>环境配置</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python 基础知识 - 文件读写]]></title>
    <url>%2F0002.%E5%BC%80%E5%8F%91-02.python%2FPython-%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86-%E6%96%87%E4%BB%B6%E8%AF%BB%E5%86%99%2F</url>
    <content type="text"><![CDATA[文本文件的读写创建一个文件对象1对象名 = open(文件名,&quot;模式&quot;) 最常用的模式有： 1234567r 打开只读文件，该文件必须存在。r+ 打开可读写的文件，该文件必须存在。w 打开只写文件，若文件存在则文件长度清为0，即该文件内容会消失。若文件不存在则建立该文件。w+ 打开可读写文件，若文件存在则文件长度清为零，即该文件内容会消失。若文件不存在则建立该文件。a 以附加的方式打开只写文件。若文件不存在，则会建立该文件，如果文件存在，写入的数据会被加到文件尾，即文件原先的内容会被保留。a+ 以附加方式打开可读写的文件。若文件不存在，则会建立该文件，如果文件存在，写入的数据会被加到文件尾后，即文件原先的内容会被保留。上述的形态字符串都可以再加一个b字符，如rb、w+b或ab＋等组合，加入b 字符用来告诉函数库打开的文件为二进制文件，而非纯文字文件。 文件对象的方法读取：1234file.read() #读取整个文件，如果文件超过内存2倍，会报错file.read(N) #读取N bytes的数据file.readline() #读取一行file.readlines() #读取所有行，存到列表中，每个元素是一行； 写入：1file.write(&quot;text &quot;) #向文件对象file中写入内容； 关闭：1file.close(); 使用上下文管理器(with…as…),通过缩进确定代码块，确定文件的使用范围， 在使用文件开始时，自动执行特殊方法 enter() 在使用文件结束后，自动执行特殊方法 exit() 特殊方法，完成文件的关闭1234with open(&quot;new.txt&quot;, &quot;w&quot;) as f: #使用文件管理器打开文件 print(f.closed) f.write(&quot;Hello World!&quot;)print(f.closed) #缩进结束时，文件使用结束，自动关闭文件]]></content>
      <categories>
        <category>编程拾慧</category>
        <category>python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python 基础知识 - 文件读写]]></title>
    <url>%2F0002.%E5%BC%80%E5%8F%91-02.python%2FPython-%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86-%E8%84%9A%E6%9C%AC%E5%8A%A0%E5%AF%86%2F</url>
    <content type="text"><![CDATA[Python解释器在执行代码的过程中，会首先生成.pyc文件，然后再解释执行.pyc中的内容，解释器也能直接执行.pyc文件。 .pyc文件是一个二进制的文件，是不具备可读性的。 假如我们发到客户环境时，是.pyc文件，而不是.py，那么是不是就可以保护我们的Python代码？ 想要做到这一点，并不难。Python标准库就提供了一个名叫compileall的库，使用它就可以做到。12345python -m compileall # 对目录下的所有py文件进行变异python -m py_compile file.pypython -m py_compile /root/src/&#123;file1,file2&#125;.py 编译成pyc文件。也可以写份脚本来做这事： 12import py_compilepy_compile.compile(&apos;path&apos;) //path是包括.py文件名的路径 用1python -O -m py_compile file.py 编译成pyo文件。1.其中的 -m 相当于脚本中的import，这里的-m py_compile 相当于上面的 import py_compile2.-O 如果改成 -OO 则是删除相应的 pyo文件，具体帮助可以在控制台输入 python -h 查看 ======================== from:http://blogold.chinaunix.net/u3/93255/showart_1944929.html什么是pyc文件 pyc是一种二进制文件，是由py文件经过编译后，生成的文件，是一种byte code，py文件变成pyc文件后，加载的速度有所提高，而且pyc是一种跨平台的字节码，是由python的虚拟机来执行的，这个是类似于JAVA或者.NET的虚拟机的概念。pyc的内容，是跟python的版本相关的，不同版本编译后的pyc文件是不同的，2.5编译的pyc文件，2.4版本的 python是无法执行的。什么是pyo文件pyo是优化编译后的程序 python -O 源文件即可将源程序编译为pyo文件 什么是pyd文件pyd是python的动态链接库。 为什么需要pyc文件 这个需求太明显了，因为py文件是可以直接看到源码的，如果你是开发商业软件的话，不可能把源码也泄漏出去吧？所以就需要编译为pyc后，再发布出去。当然，pyc文件也是可以反编译的，不同版本编译后的pyc文件是不同的，根据python源码中提供的opcode，可以根据pyc文件反编译出 py文件源码，网上可以找到一个反编译python2.3版本的pyc文件的工具，不过该工具从python2.4开始就要收费了，如果需要反编译出新版本的pyc文件的话，就需要自己动手了（俺暂时还没这能力^–^）,不过你可以自己修改python的源代码中的opcode文件，重新编译 python，从而防止不法分子的破解。生成单个pyc文件 python就是个好东西，它提供了内置的类库来实现把py文件编译为pyc文件，这个模块就是 py_compile 模块。 使用方法非常简单，如下所示，直接在idle中，就可以把一个py文件编译为pyc文件了。(假设在windows环境下)1234567import py_compilepy_compile.compile(r&apos;H:\game\test.py&apos;)compile函数原型：compile(file[, cfile[, dfile[, doraise]]]) file 表示需要编译的py文件的路径 cfile 表示编译后的pyc文件名称和路径，默认为直接在file文件名后加c 或者 o，o表示优化的字节码 dfile 这个参数英文看不明白，请各位大大赐教。(鄙视下自己)原文：it is used as the name of the source file in error messages instead of file doraise 可以是两个值，True或者False，如果为True，则会引发一个PyCompileError，否则如果编译文件出错，则会有一个错误，默认显示在sys.stderr中，而不会引发异常 (来自python2.5文档)批量生成pyc文件 一般来说，我们的工程都是在一个目录下的，一般不会说仅仅编译一个py文件而已，而是需要把整个文件夹下的py文件都编译为pyc文件，python又为了我们提供了另一个模块：compileall 。使用方法如下：123import compileallcompileall.compile_dir(r&apos;H:\game&apos;) 也可以直接用命令行编译一个目录下的文件，如：# python -m compileall /root/src/ 这样就把game目录，以及其子目录下的py文件编译为pyc文件了。嘿嘿，够方便吧。来看下compile_dir函数的说明：1compile_dir(dir[, maxlevels[, ddir[, force[, rx[, quiet]]]]]) dir 表示需要编译的文件夹位置 maxlevels 表示需要递归编译的子目录的层数，默认是10层，即默认会把10层子目录中的py文件编译为pyc ddir 英文没明白，原文：it is used as the base path from which the filenames used in error messages will be generated。 force 如果为True，则会强制编译为pyc，即使现在的pyc文件是最新的，还会强制编译一次，pyc文件中包含有时间戳，python编译器会根据时间来决定，是否需要重新生成一次pyc文件 rx 表示一个正则表达式，比如可以排除掉不想要的目录，或者只有符合条件的目录才进行编译 quiet 如果为True，则编译后，不会在标准输出中，打印出信息 (来自python2.5文档) 总结 通过上面的方法，可以方便的把py文件编译为pyc文件了，从而可以实现部分的源码隐藏，保证了python做商业化软件时，保证了部分的安全性吧，继续学习下，看怎么修改opcode。]]></content>
      <categories>
        <category>编程拾慧</category>
        <category>python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python 进阶知识-调用c++实现局部性能调优]]></title>
    <url>%2F0002.%E5%BC%80%E5%8F%91-02.python%2FPython-%E8%BF%9B%E9%98%B6%E7%9F%A5%E8%AF%86-%E8%B0%83%E7%94%A8c%2B%2B%E5%AE%9E%E7%8E%B0%E5%B1%80%E9%83%A8%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%98%2F</url>
    <content type="text"><![CDATA[待整理补齐的坑https://python3-cookbook.readthedocs.io/zh_CN/latest/c15/p02_write_simple_c_extension_module.html#id1https://blog.csdn.net/m0_46429066/article/details/108491148]]></content>
      <categories>
        <category>编程拾慧</category>
        <category>python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python 包的发布]]></title>
    <url>%2F0002.%E5%BC%80%E5%8F%91-02.python%2FPython-%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86-%E8%84%9A%E6%9C%AC%E5%8F%91%E5%B8%83%2F</url>
    <content type="text"><![CDATA[账号注册要进行Python包的发布，首先需要在 pypi 注册一个账号。]]></content>
      <categories>
        <category>编程拾慧</category>
        <category>python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python 基础知识 - 序列的方法]]></title>
    <url>%2F0002.%E5%BC%80%E5%8F%91-02.python%2FPython-%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86-%E5%BA%8F%E5%88%97%E7%9A%84%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[快速教程中，我们了解了最基本的序列(sequence)。回忆一下，序列包含有定值表(tuple)和表(list)。此外，字符串(string)是一种特殊的定值表。表的元素可以更改，定值表一旦建立，其元素不可更改。 任何的序列都可以引用其中的元素(item)。 下面的内建函数(built-in function)可用于序列(表，定值表，字符串)：s为一个序列12345len(s) 返回： 序列中包含元素的个数min(s) 返回： 序列中最小的元素max(s) 返回： 序列中最大的元素all(s) 返回： True, 如果所有元素都为True的话any(s) 返回： True, 如果任一元素为True的话 下面的方法主要起查询功能，不改变序列本身, 可用于表和定值表：1234sum(s) 返回：序列中所有元素的和# x为元素值，i为下标(元素在序列中的位置)s.count(x) 返回： x在s中出现的次数s.index(x) 返回： x在s中第一次出现的下标 由于定值表的元素不可变更，下面方法只适用于表：12345678# l为一个表, l2为另一个表l.extend(l2) 在表l的末尾添加表l2的所有元素l.append(x) 在l的末尾附加x元素l.sort() 对l中的元素排序l.reverse() 将l中的元素逆序l.pop() 返回：表l的最后一个元素，并在表l中删除该元素del l[i] 删除该元素(以上这些方法都是在原来的表的上进行操作，会对原来的表产生影响，而不是返回一个新表。) 下面是一些用于字符串的方法。尽管字符串是定值表的特殊的一种，但字符串(string)类有一些方法是改变字符串的。这些方法的本质不是对原有字符串进行操作，而是删除原有字符串，再建立一个新的字符串，所以并不与定值表的特点相矛盾。12345678910111213141516171819202122232425262728293031323334353637383940#str为一个字符串，sub为str的一个子字符串。s为一个序列，它的元素都是字符串。width为一个整数，用于说明新生成字符串的宽度。str.count(sub) 返回：sub在str中出现的次数str.find(sub) 返回：从左开始，查找sub在str中第一次出现的位置。如果str中不包含sub，返回 -1str.index(sub) 返回：从左开始，查找sub在str中第一次出现的位置。如果str中不包含sub，举出错误str.rfind(sub) 返回：从右开始，查找sub在str中第一次出现的位置。如果str中不包含sub，返回 -1str.rindex(sub) 返回：从右开始，查找sub在str中第一次出现的位置。如果str中不包含sub，举出错误str.isalnum() 返回：True， 如果所有的字符都是字母或数字str.isalpha() 返回：True，如果所有的字符都是字母str.isdigit() 返回：True，如果所有的字符都是数字str.istitle() 返回：True，如果所有的词的首字母都是大写str.isspace() 返回：True，如果所有的字符都是空格str.islower() 返回：True，如果所有的字符都是小写字母str.isupper() 返回：True，如果所有的字符都是大写字母str.split([sep, [max]]) 返回：从左开始，以空格为分割符(separator)，将str分割为多个子字符串，总共分割max次。将所得的子字符串放在一个表中返回。可以str.split(&apos;,&apos;)的方式使用逗号或者其它分割符str.rsplit([sep, [max]]) 返回：从右开始，以空格为分割符(separator)，将str分割为多个子字符串，总共分割max次。将所得的子字符串放在一个表中返回。可以str.rsplit(&apos;,&apos;)的方式使用逗号或者其它分割符str.join(s) 返回：将s中的元素，以str为分割符，合并成为一个字符串。str.strip([sub]) 返回：去掉字符串开头和结尾的空格。也可以提供参数sub，去掉位于字符串开头和结尾的sub str.replace(sub, new_sub) 返回：用一个新的字符串new_sub替换str中的substr.capitalize() 返回：将str第一个字母大写str.lower() 返回：将str全部字母改为小写str.upper() 返回：将str全部字母改为大写str.swapcase() 返回：将str大写字母改为小写，小写改为大写str.title() 返回：将str的每个词(以空格分隔)的首字母大写str.center(width) 返回：长度为width的字符串，将原字符串放入该字符串中心，其它空余位置为空格。str.ljust(width) 返回：长度为width的字符串，将原字符串左对齐放入该字符串，其它空余位置为空格。str.rjust(width) 返回：长度为width的字符串，将原字符串右对齐放入该字符串，其它空余位置为空格。]]></content>
      <categories>
        <category>编程拾慧</category>
        <category>python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python 基础知识]]></title>
    <url>%2F0002.%E5%BC%80%E5%8F%91-02.python%2FPython-%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86-%E5%AF%B9%E8%B1%A1%E5%AD%97%E5%85%B8%2F</url>
    <content type="text"><![CDATA[基本板书函数 命令 作用 range(N) 生成从０到N-1共 Ｎ个数字的列表 continue 跳过本轮进入下一轮循环（相当于Perl的 next） break 终止循环 （相当于Perl的 last） def 定义函数（相当于Perl的 sub） dir() dir()用来查询一个类或者对象所有属性 print dir(list) help() help()用来查询的说明文档 print help(list) 对象通过class 创建一个对象；对于对象的属性可以直接在class 块中进行赋值；同时可以通过def 对对象添加一些方法； 123456789101112class Bird(object): ##括号中为该对象的父类，如果是object则表明是顶级类，没有父类have_feather = Trueway_of_reproduction = ‘egg’class Chicken(Bird): ###Chicken的父类是Bird，所以Chicken将会集成父类Bird的所有属性和动作way_of_move = ‘walk’possible_in_KFC = Truedef show_laugh(self):#self 用于内部的调用；参数传递时不会传递selfprint self.laughdef laugh_100th(self):for i in range(100):self.show_laugh() #通过self 调用了方法： show_laugh 特殊方法(特殊的方法特点是名称前后各有2个下划线（__）)：init_() 是一个特殊方法；如果在类中定义了这个方法，则在创建对象时，python会在对象创建后自动调用这个方法，完成对象创建后，会直接打印 ‘print ‘We are happy birds.’,more_words’ 12345class happyBird(Bird):def __init__(self,more_words):print &apos;We are happy birds.&apos;,more_wordssummer = happyBird(&apos;Happy,Happy!&apos;) 字典字典的创建1dic = &#123;&quot;key_a&quot;:&quot;value_1&quot;,&quot;key_b&quot;:&quot;value_1&quot;&#125; 或者先构建一个空字典，然后想字典中添加值； 12345dic=&#123;&#125; #构建一个空的字典；dic[&quot;key&quot;] = &quot;value&quot;for 循环默认遍历字典的keyfor i in dicprint i 字典的常用方法|示例命令|功能||-|-||print dic.keys() | 返回dic所有的键||print dic.values()| 返回dic所有的值||print dic.items()| 返回dic所有的元素（键值对）||dic.clear() | 清空dic，dict变为{}||del dic[“key”] | # 删除字典元素||print len(dic)| len查询元素总数|]]></content>
      <categories>
        <category>编程拾慧</category>
        <category>python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python 编码规范]]></title>
    <url>%2F0002.%E5%BC%80%E5%8F%91-02.python%2FPython-%E7%BC%96%E7%A0%81%E8%A7%84%E8%8C%83%2F</url>
    <content type="text"><![CDATA[item detail PEP 8 Title Style Guide for Python Code Version c451868df657 Last-Modified 2016-06-08 10:43:53 -0400 (Wed, 08 Jun 2016) Author Guido van Rossum , Barry Warsaw , Nick Coghlan Status Active Type Process Content-Type text/x-rst Created 05-Jul-2001 Post-History 05-Jul-2001, 01-Aug-2013 Introduction 介绍本文提供的Python代码编码规范基于Python主要发行版本的标准库。Python的C语言实现的C代码规范请查看相应的PEP指南1。 这篇文档以及PEP 257（文档字符串的规范）改编自Guido原始的《Python Style Guide》一文，同时添加了一些来自Barry的风格指南2。 这篇规范指南随着时间的推移而逐渐演变，随着语言本身的变化，过去的约定也被淘汰了。 许多项目有自己的编码规范，在出现规范冲突时，项目自身的规范优先。 A Foolish Consistency is the Hobgoblin of Little Minds 尽信书,则不如无书Guido的一条重要的见解是代码阅读比写更加频繁。这里提供的指导原则主要用于提升代码的可读性，使得在大量的Python代码中保持一致。就像PEP 20提到的，“Readability counts”。 这是一份关于一致性的风格指南。这份风格指南的风格一致性是非常重要的。更重要的是项目的风格一致性。在一个模块或函数的风格一致性是最重要的。 然而，应该知道什么时候应该不一致，有时候编码规范的建议并不适用。当存在模棱两可的情况时，使用自己的判断。看看其他的示例再决定哪一种是最好的，不要羞于发问。 特别是不要为了遵守PEP约定而破坏兼容性！ 几个很好的理由去忽略特定的规则： 当遵循这份指南之后代码的可读性变差，甚至是遵循PEP规范的人也觉得可读性差。 与周围的代码保持一致（也可能出于历史原因），尽管这也是清理他人混乱（真正的Xtreme Programming风格）的一个机会。 有问题的代码出现在发现编码规范之前，而且也没有充足的理由去修改他们。 当代码需要兼容不支持编码规范建议的老版本Python。 Code lay-out 代码布局Indentation 缩进每一级缩进使用4个空格。 续行应该与其包裹元素对齐，要么使用圆括号、方括号和花括号内的隐式行连接来垂直对齐，要么使用挂行缩进对齐3。当使用挂行缩进时，应该考虑到第一行不应该有参数，以及使用缩进以区分自己是续行。 推荐：1234567891011121314# 与左括号对齐foo = long_function_name(var_one, var_two, var_three, var_four)# 用更多的缩进来与其他行区分def long_function_name( var_one, var_two, var_three, var_four): print(var_one)# 挂行缩进应该再换一行foo = long_function_name( var_one, var_two, var_three, var_four) 不推荐：123456789# 没有使用垂直对齐时，禁止把参数放在第一行foo = long_function_name(var_one, var_two, var_three, var_four)# 当缩进没有与其他行区分时，要增加缩进def long_function_name( var_one, var_two, var_three, var_four): print(var_one) 四空格的规则对于续行是可选的。可选：1234# 挂行缩进不一定要用4个空格foo = long_function_name( var_one, var_two, var_three, var_four) 当if语句的条件部分长到需要换行写的时候，注意可以在两个字符关键字的连接处（比如if），增加一个空格，再增加一个左括号来创造一个4空格缩进的多行条件。这会与if语句内同样使用4空格缩进的代码产生视觉冲突。PEP没有明确指明要如何区分i发的条件代码和内嵌代码。可使用的选项包括但不限于下面几种情况：123456789101112131415# 没有额外的缩进if (this_is_one_thing and that_is_another_thing): do_something()# 增加一个注释，在能提供语法高亮的编辑器中可以有一些区分if (this_is_one_thing and that_is_another_thing): # Since both conditions are true, we can frobnicate. do_something()# 在条件判断的语句添加额外的缩进if (this_is_one_thing and that_is_another_thing): do_something() （可以参考下面关于是否在二进制运算符之前或之后截断的讨论）在多行结构中的大括号/中括号/小括号的右括号可以与内容对齐单独起一行作为最后一行的第一个字符，就像这样：12345678my_list = [ 1, 2, 3, 4, 5, 6, ]result = some_function_that_takes_arguments( &apos;a&apos;, &apos;b&apos;, &apos;c&apos;, &apos;d&apos;, &apos;e&apos;, &apos;f&apos;, ) 或者也可以与多行结构的第一行第一个字符对齐，就像这样：12345678my_list = [ 1, 2, 3, 4, 5, 6,]result = some_function_that_takes_arguments( &apos;a&apos;, &apos;b&apos;, &apos;c&apos;, &apos;d&apos;, &apos;e&apos;, &apos;f&apos;,) Tabs or Spaces？ 制表符还是空格？空格是首选的缩进方式。制表符只能用于与同样使用制表符缩进的代码保持一致。Python3不允许同时使用空格和制表符的缩进。混合使用制表符和空格缩进的Python2代码应该统一转成空格。当在命令行加入-t选项执行Python2时，它会发出关于非法混用制表符与空格的警告。当使用–tt时，这些警告会变成错误。强烈建议使用这样的参数。 Maximum Line Length 行的最大长度所有行限制的最大字符数为79。 没有结构化限制的大块文本（文档字符或者注释），每行的最大字符数限制在72。 限制编辑器窗口宽度可以使多个文件并行打开，并且在使用代码检查工具(在相邻列中显示这两个版本)时工作得很好。 大多数工具中的默认封装破坏了代码的可视化结构，使代码更难以理解。避免使用编辑器中默认配置的80窗口宽度，即使工具在帮你折行时在最后一列放了一个标记符。某些基于Web的工具可能根本不提供动态折行。一些团队更喜欢较长的行宽。如果代码主要由一个团队维护，那这个问题就能达成一致，可以把行长度从80增加到100个字符（更有效的做法是将行最大长度增加到99个字符），前提是注释和文档字符串依然已72字符折行。 Python标准库比较保守，需要将行宽限制在79个字符（文档/注释限制在72）。 较长的代码行选择Python在小括号，中括号以及大括号中的隐式续行方式。通过小括号内表达式的换行方式将长串折成多行。这种方式应该优先使用，而不是使用反斜杠续行。 反斜杠有时依然很有用。比如，比较长的，多个with状态语句，不能使用隐式续行，所以反斜杠是可以接受的： 123with open(&apos;/path/to/some/file/you/want/to/read&apos;) as file_1, \ open(&apos;/path/to/some/file/being/written&apos;, &apos;w&apos;) as file_2: file_2.write(file_1.read()) （请参阅前面关于多行if-语句的讨论，以获得关于这种多行with-语句缩进的进一步想法。）另一种类似情况是使用assert语句。确保在续行进行适当的缩进。 Should a line break before or after a binary operator? 在二元运算符之前应该换行吗？几十年来，推荐的风格是在二元运算符之后中断。但是这回影响可读性，原因有二：操作符一般分布在屏幕上不同的列中，而且每个运算符被移到了操作数的上一行。下面例子这个情况就需要额外注意，那些变量是相加的，那些变量是相减的：123456# 不推荐: 操作符离操作数太远income = (gross_wages + taxable_interest + (dividends - qualified_dividends) - ira_deduction - student_loan_interest) 为了解决这种可读性的问题，数学家和他们的出版商遵循了相反的约定。Donald Knuth在他的Computers and Typesetting系列中解释了传统规则：“尽管段落中的公式总是在二元运算符和关系之后中断，显示出来的公式总是要在二元运算符之前中断”4。遵循数学的传统能产出更多可读性高的代码：123456# 推荐：运算符和操作数很容易进行匹配income = (gross_wages + taxable_interest + (dividends - qualified_dividends) - ira_deduction - student_loan_interest) 在Python代码中，允许在二元运算符之前或之后中断，只要本地的约定是一致的。对于新代码，建议使用Knuth的样式。 Blank Lines 空行顶层函数和类的定义，前后用两个空行隔开。 类里的方法定义用一个空行隔开。 相关的功能组可以用额外的空行（谨慎使用）隔开。一堆相关的单行代码之间的空白行可以省略（例如，一组虚拟实现 dummy implementations）。 在函数中使用空行来区分逻辑段（谨慎使用）。 Python接受control-L（即^L）换页符作为空格；许多工具把这些字符当作页面分隔符，所以你可以在文件中使用它们来分隔相关段落。请注意，一些编辑器和基于Web的代码阅读器可能无法识别control-L为换页，将在其位置显示另一个字形。 Source File Encoding 源文件编码Python核心发布版本中的代码总是以UTF-8格式编码（或者在Python2中用ASCII编码）。 使用ASCII（在Python2中）或UTF-8（在Python3中）编码的文件不应具有编码声明。 在标准库中，非默认的编码应该只用于测试，或者当一个注释或者文档字符串需要提及一个包含内ASCII字符编码的作者名字的时候；否则，使用\x,\u,\U , 或者 \N 进行转义来包含非ASCII字符。 对于Python 3和更高版本，标准库规定了以下策略（参见 PEP 3131）：Python标准库中的所有标识符必须使用ASCII标识符，并在可行的情况下使用英语单词（在许多情况下，缩写和技术术语是非英语的）。此外，字符串文字和注释也必须是ASCII。唯一的例外是（a）测试非ASCII特征的测试用例，以及（b）作者的名称。作者的名字如果不使用拉丁字母拼写，必须提供一个拉丁字母的音译。 鼓励具有全球受众的开放源码项目采取类似的政策。 Imports 导入导入通常在分开的行，例如：123456#推荐: import osimport sys#不推荐: import sys, os 但是可以这样：1from subprocess import Popen, PIPE 导入总是位于文件的顶部，在模块注释和文档字符串之后，在模块的全局变量与常量之前。导入应该按照以下顺序分组： 标准库导入 相关第三方库导入 本地应用/库特定导入 你应该在每一组导入之间加入空行。 推荐使用绝对路径导入，如果导入系统没有正确的配置（比如包里的一个目录在sys.path里的路径后），使用绝对路径会更加可读并且性能更好（至少能提供更好的错误信息）:123import mypkg.siblingfrom mypkg import siblingfrom mypkg.sibling import example 然而，显示的指定相对导入路径是使用绝对路径的一个可接受的替代方案，特别是在处理使用绝对路径导入不必要冗长的复杂包布局时：12from . import siblingfrom .sibling import example 标准库要避免使用复杂的包引入结构，而总是使用绝对路径。 不应该使用隐式相对路径导入，并且在Python 3中删除了它。 当从一个包含类的模块中导入类时，常常这么写：12from myclass import MyClassfrom foo.bar.yourclass import YourClass 如果上述的写法导致名字的冲突，那么这么写：12import myclassimport foo.bar.yourclass 然后使用“myclass.MyClass”和“foo.bar.yourclass.YourClass”。 避免通配符的导入（from import *），因为这样做会不知道命名空间中存在哪些名字，会使得读取接口和许多自动化工具之间产生混淆。对于通配符的导入，有一个防御性的做法，即将内部接口重新发布为公共API的一部分（例如，用可选加速器模块的定义覆盖纯Python实现的接口，以及重写那些事先不知道的定义）。 当以这种方式重新发布名称时，以下关于公共和内部接口的准则仍然适用。 Module level dunder names 模块级的“呆”名像all , author , version 等这样的模块级“呆名“（也就是名字里有两个前缀下划线和两个后缀下划线），应该放在文档字符串的后面，以及除from future 之外的import表达式前面。Python要求将来在模块中的导入，必须出现在除文档字符串之外的其他代码之前。比如：12345678910111213&quot;&quot;&quot;This is the example module.This module does stuff.&quot;&quot;&quot;from __future__ import barry_as_FLUFL__all__ = [&apos;a&apos;, &apos;b&apos;, &apos;c&apos;]__version__ = &apos;0.1&apos;__author__ = &apos;Cardinal Biggles&apos;import osimport sys String Quotes 字符串引号在Python中，单引号和双引号字符串是相同的。PEP不会为这个给出建议。选择一条规则并坚持使用下去。当一个字符串中包含单引号或者双引号字符的时候，使用和最外层不同的符号来避免使用反斜杠，从而提高可读性。 对于三引号字符串，总是使用双引号字符来与PEP 257中的文档字符串约定保持一致。 Whitespace in Expressions and Statements 表达式和语句中的空格Pet Peeves 不能忍受的事情在下列情况下，避免使用无关的空格： 紧跟在小括号，中括号或者大括号后。 12Yes: spam(ham[1], &#123;eggs: 2&#125;)No: spam( ham[ 1 ], &#123; eggs: 2 &#125; ) 紧贴在逗号、分号或者冒号之前。 12Yes: if x == 4: print x, y; x, y = y, xNo: if x == 4 : print x , y ; x , y = y , x 然而，冒号在切片中就像二元运算符，在两边应该有相同数量的空格（把它当做优先级最低的操作符）。在扩展的切片操作中，所有的冒号必须有相同的间距。例外情况：当一个切片参数被省略时，空格就被省略了。 12345ham[1:9], ham[1:9:3], ham[:9:3], ham[1::3], ham[1:9:]ham[lower:upper], ham[lower:upper:], ham[lower::step]ham[lower+offset : upper+offset]ham[: upper_fn(x) : step_fn(x)], ham[:: step_fn(x)]ham[lower + offset : upper + offset] 不推荐1234ham[lower + offset:upper + offset]ham[1: 9], ham[1 :9], ham[1:9 :3]ham[lower : : upper]ham[ : upper] 紧贴在函数参数的左括号之前。 12Yes: spam(1)No: spam (1) 紧贴索引或者切片的左括号之前。 12Yes: dct[&apos;key&apos;] = lst[index]No: dct [&apos;key&apos;] = lst [index] 为了和另一个赋值语句对齐，在赋值运算符附件加多个空格。 推荐 123x = 1y = 2long_variable = 3 不推荐： 123x = 1y = 2long_variable = 3 Other Recommendations 其他建议 避免在尾部添加空格。因为尾部的空格通常都看不见，会产生混乱：比如，一个反斜杠后面跟一个空格的换行符，不算续行标记。有些编辑器不会保留尾空格，并且很多项目（像CPython）在pre-commit的挂钩调用中会过滤掉尾空格。 总是在二元运算符两边加一个空格：赋值（=），增量赋值（+=，-=），比较（==,&lt;,&gt;,!=,&lt;&gt;,&lt;=,&gt;=,in,not,in,is,is not），布尔（and, or, not）。 如果使用具有不同优先级的运算符，请考虑在具有最低优先级的运算符周围添加空格。有时需要通过自己来判断；但是，不要使用一个以上的空格，并且在二元运算符的两边使用相同数量的空格。 推荐： 12345i = i + 1submitted += 1x = x*2 - 1hypot2 = x*x + y*yc = (a+b) * (a-b) 不推荐 12345i=i+1submitted +=1x = x * 2 - 1hypot2 = x * x + y * yc = (a + b) * (a - b) 在制定关键字参数或者默认参数值的时候，不要在=附近加上空格。推荐 12def complex(real, imag=0.0): return magic(r=real, i=imag) 不推荐12def complex(real, imag = 0.0): return magic(r = real, i = imag) 功能型注释应该使用冒号的一般性规则，并且在使用-&gt;的时候要在两边加空格。（参考下面的功能注释得到能够多信息） 推荐12def munge(input: AnyStr): ...def munge() -&gt; AnyStr: ... 不推荐12def munge(input:AnyStr): ...def munge()-&gt;PosInt: ... 当给有类型备注的参数赋值的时候，在=两边添加空格（仅针对那种有类型备注和默认值的参数）。推荐：12def munge(sep: AnyStr = None): ...def munge(input: AnyStr, sep: AnyStr = None, limit=1000): ... 不推荐12def munge(input: AnyStr=None): ...def munge(input: AnyStr, limit = 1000): ... 复合语句(同一行中的多个语句)通常是不允许的。推荐：12345if foo == &apos;blah&apos;: do_blah_thing()do_one()do_two()do_three() 不推荐12if foo == &apos;blah&apos;: do_blah_thing()do_one(); do_two(); do_three() 虽然有时候将小的代码块和 if/for/while 放在同一行没什么问题，多行语句块的情况不要这样用，同样也要避免代码行太长！不推荐123if foo == &apos;blah&apos;: do_blah_thing()for x in lst: total += xwhile t &lt; 10: t = delay() 不要12345678910if foo == &apos;blah&apos;: do_blah_thing()else: do_non_blah_thing()try: something()finally: cleanup()do_one(); do_two(); do_three(long, argument, list, like, this)if foo == &apos;blah&apos;: one(); two(); three() Comments 注释与代码相矛盾的注释比没有注释还糟，当代码更改时，优先更新对应的注释！ 注释应该是完整的句子。如果一个注释是一个短语或句子，它的第一个单词应该大写，除非它是以小写字母开头的标识符(永远不要改变标识符的大小写！)。 如果注释很短，结尾的句号可以省略。块注释一般由完整句子的一个或多个段落组成，并且每句话结束有个句号。 在句尾结束的时候应该使用两个空格。 当用英文书写时，遵循Strunk and White （译注：《Strunk and White, The Elements of Style》）的书写风格。 在非英语国家的Python程序员，请使用英文写注释，除非你120%的确信你的代码不会被使用其他语言的人阅读。 Block Comments 块注释块注释通常适用于跟随它们的某些（或全部）代码，并缩进到与代码相同的级别。块注释的每一行开头使用一个#和一个空格（除非块注释内部缩进文本）。 块注释内部的段落通过只有一个#的空行分隔。 Inline Comments 行内注释有节制地使用行内注释。行内注释是与代码语句同行的注释。行内注释和代码至少要有两个空格分隔。注释由#和一个空格开始。事实上，如果状态明显的话，行内注释是不必要的，反而会分散注意力。比如说下面这样就不需要：1x = x + 1 # Increment x 但有时，这样做很有用： 1x = x + 1 # Compensate for border Documentation Strings 文档字符串编写好的文档说明（也叫“docstrings”）的约定在PEP 257中永恒不变。 要为所有的公共模块，函数，类以及方法编写文档说明。非公共的方法没有必要，但是应该有一个描述方法具体作用的注释。这个注释应该在def那一行之后。 PEP 257 描述了写出好的文档说明相关的约定。特别需要注意的是，多行文档说明使用的结尾三引号应该自成一行，例如： 1234&quot;&quot;&quot;Return a foobangOptional plotz says to frobnicate the bizbaz first.&quot;&quot;&quot; 对于单行的文档说明，尾部的三引号应该和文档在同一行。 Naming Conventions 命名规范Python库的命名规范很乱，从来没能做到完全一致。但是目前有一些推荐的命名标准。新的模块和包（包括第三方框架）应该用这套标准，但当一个已有库采用了不同的风格，推荐保持内部一致性。 Overriding Principle 最重要的原则那些暴露给用户的API接口的命名，应该遵循反映使用场景而不是实现的原则。 Descriptive: Naming Styles 描述：命名风格有许多不同的命名风格。这里能够帮助大家识别正在使用什么样的命名风格，而不考虑他们为什么使用。以下是常见的命名方式： b（单个小写字母） B（单个大写字母） lowercase 小写字母 lower_case_with_underscores 使用下划线分隔的小写字母 UPPERCASE 大写字母 UPPER_CASE_WITH_UNDERSCORES 使用下划线分隔的大写字母 CapitalizedWords（或者叫 CapWords，或者叫CamelCase 驼峰命名法 —— 这么命名是因为字母看上去有起伏的外观5）。有时候也被称为StudlyCaps。注意：当在首字母大写的风格中用到缩写时，所有缩写的字母用大写，因此，HTTPServerError 比 HttpServerError 好。 mixedCase（不同于首字母大写，第一个单词的首字母小写） Capitalized_Words_With_Underscores（巨丑无比！） 也有用唯一的短前缀把相关命名组织在一起的方法。这在Python中不常用，但还是提一下。比如，os.stat()函数中包含类似以st_mode，st_size，st_mtime这种传统命名方式命名的变量。（这么做是为了与 POSIX 系统的调用一致，以帮助程序员熟悉它。）X11库的所有公共函数都加了前缀X。在Python里面没必要这么做，因为属性和方法在调用的时候都会用类名做前缀，函数名用模块名做前缀。另外，下面这种用前缀或结尾下划线的特殊格式是被认可的（通常和一些约定相结合）： _single_leading_underscore：（单下划线开头）弱“内部使用”指示器。比如 from M import * 是不会导入以下划线开始的对象的。 single_trailing_underscore_：（单下划线结尾）这是避免和Python内部关键词冲突的一种约定，比如：Tkinter.Toplevel(master, class_=’ClassName’) double_leading_underscore：（双下划线开头）当这样命名一个类的属性时，调用它的时候名字会做矫正（在类FooBar中，boo变成了_FooBar__boo；见下文）。 double_leading_and_trailing_underscore：（双下划线开头，双下划线结尾）“magic”对象或者存在于用户控制的命名空间内的属性，例如：init,import或者file。除了作为文档之外，永远不要命这样的名。 Prescriptive: Naming Conventions 约定俗成：命名约定Names to Avoid 应避免的名字永远不要使用字母‘l’（小写的L），‘O’（大写的O），或者‘I’（大写的I）作为单字符变量名。在有些字体里，这些字符无法和数字0和1区分，如果想用‘l’，用‘L’代替。 Package and Module Names 包名和模块名模块应该用简短全小写的名字，如果为了提升可读性，下划线也是可以用的。Python包名也应该使用简短全小写的名字，但不建议用下划线。当使用C或者C++编写了一个依赖于提供高级（更面向对象）接口的Python模块的扩展模块，这个C/C++模块需要一个下划线前缀（例如：_socket） Class Names 类名类名一般使用首字母大写的约定。在接口被文档化并且主要被用于调用的情况下，可以使用函数的命名风格代替。注意，对于内置的变量命名有一个单独的约定：大部分内置变量是单个单词（或者两个单词连接在一起），首字母大写的命名法只用于异常名或者内部的常量。 Exception Names 异常名因为异常一般都是类，所有类的命名方法在这里也适用。然而，你需要在异常名后面加上“Error”后缀（如果异常确实是一个错误）。 Global Variable Names 全局变量名（我们希望这一类变量只在模块内部使用。）约定和函数命名规则一样。通过 from M import * 导入的模块应该使用all机制去防止内部的接口对外暴露，或者使用在全局变量前加下划线的方式（表明这些全局变量是模块内非公有）。 Function Names 函数名函数名应该小写，如果想提高可读性可以用下划线分隔。大小写混合仅在为了兼容原来主要以大小写混合风格的情况下使用（比如 threading.py），保持向后兼容性。 Function and method arguments 函数和方法参数始终要将 self 作为实例方法的的第一个参数。始终要将 cls 作为类静态方法的第一个参数。如果函数的参数名和已有的关键词冲突，在最后加单一下划线比缩写或随意拼写更好。因此 class_ 比 clss 更好。（也许最好用同义词来避免这种冲突） Method Names and Instance Variables 方法名和实例变量遵循这样的函数命名规则：使用下划线分隔小写单词以提高可读性。在非共有方法和实例变量前使用单下划线。通过双下划线前缀触发Python的命名转换规则来避免和子类的命名冲突。Python通过类名对这些命名进行转换：如果类 Foo 有一个叫 a 的成员变量， 它无法通过 Foo.a 访问。（执着的用户可以通过 Foo._Foo__a 访问。）一般来说，前缀双下划线用来避免类中的属性命名与子类冲突的情况。注意：关于__names的用法存在争论（见下文）。 Constants 常量常量通常定义在模块级，通过下划线分隔的全大写字母命名。例如： MAX_OVERFLOW 和 TOTAL。 Designing for inheritance 继承的设计始终要考虑到一个类的方法和实例变量（统称：属性）应该是共有还是非共有。如果存在疑问，那就选非共有；因为将一个非共有变量转为共有比反过来更容易。公共属性是那些与类无关的客户使用的属性，并承诺避免向后不兼容的更改。非共有属性是那些不打算让第三方使用的属性；你不需要承诺非共有属性不会被修改或被删除。我们不使用“私有（private）”这个说法，是因为在Python中目前还没有真正的私有属性（为了避免大量不必要的常规工作）。另一种属性作为子类API的一部分（在其他语言中通常被称为“protected”）。有些类是专为继承设计的，用来扩展或者修改类的一部分行为。当设计这样的类时，要谨慎决定哪些属性时公开的，哪些是作为子类的API，哪些只能在基类中使用。贯彻这样的思想，一下是一些让代码Pythonic的准则： 公共属性不应该有前缀下划线。 如果公共属性名和关键字冲突，在属性名之后增加一个下划线。这比缩写和随意拼写好很多。（然而，尽管有这样的规则，在作为参数或者变量时，‘cls’是表示‘类’最好的选择，特别是作为类方法的第一个参数。）注意1：参考之前的类方法参数命名建议 对于单一的共有属性数据，最好直接对外暴露它的变量名，而不是通过负责的 存取器（accessor）/突变（mutator） 方法。请记住，如果你发现一个简单的属性需要成长为一个功能行为，那么Python为这种将来会出现的扩展提供了一个简单的途径。在这种情况下，使用属性去隐藏属性数据访问背后的逻辑。注意1：属性只在new-style类中起作用。注意2：尽管功能方法对于类似缓存的负面影响比较小，但还是要尽量避免。注意3：属性标记会让调用者认为开销（相当的）小，避免用属性做开销大的计算。 如果你的类打算用来继承的话，并且这个类里有不希望子类使用的属性，就要考虑使用双下划线前缀并且没有后缀下划线的命名方式。这会调用Python的命名转换算法，将类的名字加入到属性名里。这样做可以帮助避免在子类中不小心包含了相同的属性名而产生的冲突。注意1：只有类名才会整合进属性名，如果子类的属性名和类名和父类都相同，那么你还是会有命名冲突的问题。注意2：命名转换会在某些场景使用起来不太方便，例如调试，getattr()。然而命名转换的算法有很好的文档说明并且很好操作。注意3：不是所有人都喜欢命名转换。尽量避免意外的名字冲突和潜在的高级调用。 ################################################ Public and internal interfaces 公共和内部的接口任何向后兼容保证只适用于公共接口，因此，用户清晰地区分公共接口和内部接口非常重要。 文档化的接口被认为是公开的，除非文档明确声明它们是临时或内部接口，不受通常的向后兼容性保证。所有未记录的接口都应该是内部的。 为了更好地支持内省（introspection），模块应该使用all属性显式地在它们的公共API中声明名称。将all设置为空列表表示模块没有公共API。 即使通过all设置过，内部接口（包，模块，类，方法，属性或其他名字）依然需要单个下划线前缀。 如果一个命名空间（包，模块，类）被认为是内部的，那么包含它的接口也应该被认为是内部的。 导入的名称应该始终被视作是一个实现的细节。其他模块必须不能间接访问这样的名称，除非它是包含它的模块中有明确的文档说明的API，例如 os.path 或者是一个包里从子模块公开函数接口的 init 模块。 Programming Recommendations 编程建议 代码应该用不损害其他Python实现的方式去编写（PyPy，Jython，IronPython，Cython，Psyco 等）。比如，不要依赖于在CPython中高效的内置字符连接语句 a += b 或者 a = a + b。这种优化甚至在CPython中都是脆弱的（它只适用于某些类型）并且没有出现在不使用引用计数的实现中。在性能要求比较高的库中，可以种 ”.join() 代替。这可以确保字符关联在不同的实现中都可以以线性时间发生。 和像None这样的单例对象进行比较的时候应该始终用 is 或者 is not，永远不要用等号运算符。另外，如果你在写 if x 的时候，请注意你是否表达的意思是 if x is not None。举个例子，当测试一个默认值为None的变量或者参数是否被设置为其他值的时候。这个其他值应该是在上下文中能成为bool类型false的值。 使用 is not 运算符，而不是 not … is 。虽然这两种表达式在功能上完全相同，但前者更易于阅读，所以优先考虑。 12345推荐：if foo is not None:不推荐：if not foo is None: 当使用富比较（rich comparisons，一种复杂的对象间比较的新机制，允许返回值不为-1,0,1）实现排序操作的时候，最好实现全部的六个操作符（eq, ne, lt, gt, ge）而不是依靠其他的代码去实现特定的比较。为了最大程度减少这一过程的开销， functools.total_ordering() 修饰符提供了用于生成缺少的比较方法的工具。PEP 207 指出Python实现了反射机制。因此，解析器会将 y &gt; x 转变为 x &lt; y，将 y &gt;= x 转变为 x &lt;= y，也会转换x == y 和 x != y的参数。sort() 和 min()方法确保使用&lt;操作符，max()使用&gt;操作符。然而，最好还是实现全部六个操作符，以免在其他地方出现冲突。 始终使用def表达式，而不是通过赋值语句将lambda表达式绑定到一个变量上。1234推荐：def f(x): return 2*x不推荐：f = lambda x: 2*x 第一个形式意味着生成的函数对象的名称是“f”而不是泛型“&lt; lambda &gt;”。这在回溯和字符串显示的时候更有用。赋值语句的使用消除了lambda表达式优于显式def表达式的唯一优势（即lambda表达式可以内嵌到更大的表达式中）。 从Exception继承异常，而不是BaseException。直接继承BaseException的异常适用于几乎不用来捕捉的异常。 设计异常的等级，要基于扑捉异常代码的需要，而不是异常抛出的位置。以编程的方式去回答“出了什么问题？”，而不是只是确认“出现了问题”（内置异常结构的例子参考 PEP 3151 ） 类的命名规范适用于这里，但是你需要添加一个“Error”的后缀到你的异常类，如果异常是一个Error的话。非本地流控制或者其他形式的信号的非错误异常不需要特殊的后缀。 适当地使用异常链接。在Python 3里，为了不丢失原始的根源，可以显式指定“raise X from Y”作为替代。 当故意替换一个内部异常时（Python 2 使用“raise X”， Python 3.3 之后 使用 “raise X from None”），确保相关的细节转移到新的异常中（比如把AttributeError转为KeyError的时候保留属性名，或者将原始异常信息的文本内容内嵌到新的异常中）。 在Python 2中抛出异常时，使用 rasie ValueError(‘message’) 而不是用老的形式 raise ValueError, ‘message’。 第二种形式在Python3 的语法中不合法 使用小括号，意味着当异常里的参数非常长，或者包含字符串格式化的时候，不需要使用换行符。 当捕获到异常时，如果可以的话写上具体的异常名，而不是只用一个except: 块。比如说： 1234try: import platform_specific_moduleexcept ImportError: platform_specific_module = None 如果只有一个except: 块将会捕获到SystemExit和KeyboardInterrupt异常，这样会很难通过Control-C中断程序，而且会掩盖掉其他问题。如果你想捕获所有指示程序出错的异常，使用 except Exception: （只有except等价于 except BaseException:）。两种情况不应该只使用‘excpet’块： 如果异常处理的代码会打印或者记录log；至少让用户知道发生了一个错误。 如果代码需要做清理工作，使用 raise..try…finally 能很好处理这种情况并且能让异常继续上浮。 当给捕捉的异常绑定一个名字时，推荐使用在Python 2.6中加入的显式命名绑定语法：1234try: process_data()except Exception as exc: raise DataProcessingFailedError(str(exc)) 为了避免和原来基于逗号分隔的语法出现歧义，Python3只支持这一种语法。 当捕捉操作系统的错误时，推荐使用Python 3.3 中errno内定数值指定的异常等级。 另外，对于所有的 try/except 语句块，在try语句中只填充必要的代码，这样能避免掩盖掉bug。 123456789101112131415推荐：try: value = collection[key]except KeyError: return key_not_found(key)else: return handle_value(value)不推荐：try: # Too broad! return handle_value(collection[key])except KeyError: # Will also catch KeyError raised by handle_value() return key_not_found(key) 当代码片段局部使用了某个资源的时候，使用with 表达式来确保这个资源使用完后被清理干净。用try/finally也可以。 无论何时获取和释放资源，都应该通过单独的函数或方法调用上下文管理器。举个例子：123456推荐：with conn.begin_transaction(): do_stuff_in_transaction(conn)不推荐：with conn: do_stuff_in_transaction(conn) 第二个例子没有提供任何信息去指明enter和exit方法在事务之后做出了关闭连接之外的其他事情。这种情况下，明确指明非常重要。 返回的语句保持一致。函数中的返回语句都应该返回一个表达式，或者都不返回。如果一个返回语句需要返回一个表达式，那么在没有值可以返回的情况下，需要用 return None 显式指明，并且在函数的最后显式指定一条返回语句（如果能跑到那的话）。 123456789101112131415161718192021推荐：def foo(x): if x &gt;= 0: return math.sqrt(x) else: return Nonedef bar(x): if x &lt; 0: return None return math.sqrt(x)不推荐：def foo(x): if x &gt;= 0: return math.sqrt(x)def bar(x): if x &lt; 0: return return math.sqrt(x) 使用字符串方法代替字符串模块。 字符串方法总是更快，并且和unicode字符串分享相同的API。如果需要兼容Python2.0之前的版本可以不用考虑这个规则。 使用 ”.startswith() 和 ”.endswith() 代替通过字符串切割的方法去检查前缀和后缀。 startswith()和endswith()更干净，出错几率更小。比如： 12推荐: if foo.startswith(&apos;bar&apos;):糟糕: if foo[:3] == &apos;bar&apos;: 对象类型的比较应该用isinstance()而不是直接比较type。 12正确: if isinstance(obj, int):糟糕: if type(obj) is type(1): 当检查一个对象是否为string类型时，记住，它也有可能是unicode string！在Python2中，str和unicode都有相同的基类：basestring，所以你可以这样：1if isinstance(obj, basestring): 注意，在Python3中，unicode和basestring都不存在了（只有str）并且bytes类型的对象不再是string类型的一种（它是整数序列） 对于序列来说（strings，lists，tuples），可以使用空序列为false的情况。 12345正确: if not seq: if seq:糟糕: if len(seq): if not len(seq): 书写字符串时不要依赖单词结尾的空格，这样的空格在视觉上难以区分，有些编辑器会自动去掉他们（比如 reindent.py （译注：re indent 重新缩进）） 不要用 == 去和True或者False比较：123正确: if greeting:糟糕: if greeting == True:更糟: if greeting is True: Function Annotations 功能注释[PEP 484的引入，功能型注释的风格规范有些变化。 为了向前兼容，在Python3代码中的功能注释应该使用 PEP 484的语法规则。（在前面的章节中对注释有格式化的建议。） 不再鼓励使用之前在PEP中推荐的实验性样式。 然而，在stdlib库之外，在PEP 484中的实验性规则是被鼓励的。比如用PEP 484的样式标记大型的第三方库或者应用程序，回顾添加这些注释是否简单，并观察是否增加了代码的可读性。 Python的标准库代码应该保守使用这种注释，但新的代码或者大型的重构可以使用这种注释。 如果代码希望对功能注释有不同的用途，建议在文件的顶部增加一个这种形式的注释：1# type: ignore 这会告诉检查器忽略所有的注释。（在 PEP 484中可以找到从类型检查器禁用投诉的更细粒度的方法。） 像linters一样，类型检测器是可选的可独立的工具。默认情况下，Python解释器不应该因为类型检查而发出任何消息，也不应该基于注释改变它们的行为。 不想使用类型检测的用户可以忽略他们。然而，第三方库的用户可能希望在这些库上运行类型检测。为此， PEP 484 建议使用存根文件类型：.pyi文件，这种文件类型相比于.py文件会被类型检测器读取。存根文件可以和库一起，或者通过typeshed repo6独立发布（通过库作者的许可） 对于需要向后兼容的代码，可以以注释的形式添加功能型注释。参见PEP 484的相关部分7。 参考pep8 Python PEP8 编码规范中文版 PEP 7, Style Guide for C Code, van Rossum Barry’s GNU Mailman style guide http://barry.warsaw.us/software/STYLEGUIDE.txt 挂行缩进是一种类型设置样式，其中除第一行之外，段落中的所有行都缩进。在Python中，这个术语是用来描述一种风格：在被括号括起来的语句中，左括号是这一行最后一个非空格字符，随后括号内的内容每一行进行缩进，直到遇到右括号。 Donald Knuth’s The TeXBook, pages 195 and 196 http://www.wikipedia.com/wiki/CamelCase Typeshed repo https://github.com/python/typeshed Suggested syntax for Python 2.7 and straddling code https://www.python.org/dev/peps/pep-0484/#suggested-syntax-for-python-2-7-and-straddling-code]]></content>
      <categories>
        <category>编程拾慧</category>
        <category>python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python 基础知识]]></title>
    <url>%2F0002.%E5%BC%80%E5%8F%91-02.python%2FPython-%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86-%E5%B0%86%E8%84%9A%E6%9C%AC%E6%89%93%E5%8C%85%E5%88%B6%E4%BD%9C%E6%88%90%E5%8F%AF%E6%89%A7%E8%A1%8C%E7%A8%8B%E5%BA%8F%2F</url>
    <content type="text"><![CDATA[在支持PIP的情况下pip install pyinstaller安装 pyinstaller 后执行pyinstaller --version查看所安装的版本 pyinstaller : 打包可执行文件的主要命令，详细用法下面会介绍。 pyi-archive_viewer : 查看可执行包里面的文件列表。 pyi-bindepend : 查看可执行文件依赖的动态库（.so或.dll文件） pyi-... : 等等。 打包程序pyinstaller mycript.py然后会看到新增加了两个目录build和dist，dist下面的文件就是可以发布的可执行文件，对于上面的命令你会发现dist目录下面有一堆文件，各种都动态库文件和myscrip可执行文件。有时这样感觉比较麻烦，需要打包dist下面的所有东西才能发布，万一丢掉一个动态库就无法运行了，好在pyInstaller支持单文件模式，只需要执行： pyinstaller -F mycript.py 你会发现dist下面只有一个可执行文件，这个单文件就可以发布了，可以运行在你正在使用的操作系统类似的系统的下面。 可能问题 注意点12341.windows系统的版本和位数 （mac系统和linux 没有进行测试过）2.python3的版本和位数3.pyqt5的版本和位数 （如果pip安装，则位数同python3）4.pyinstaller的版本和位数（一般pip安装，无需考虑位数） 为程序添加图标运行出现cmd窗口取消cmd窗口弹出的参考方式如下： 方法一：pyinstaller -F mycode.py --noconsole 方法二：pyinstaller -F -w mycode.py （-w就是取消窗口） pyinstaller打成的包，可以在64位操作系统使用，无法在32位操作系统使用1234567坑的成因：python存在64位版本和32位版本。64位版本打成的包，只能在64位操作系统使用。32位版本打成的包，即可以在64位操作系统使用，也可以在32位操作系统使用。解决方案：重新安装32位版本的python，进行开发。 pyinstaller打成的包，可以在win7以上操作系统使用，无法在xp操作系统使用12345坑的成因：python3 从3.5版本开始，就已经不支持xp操作系统了。解决方案：重新安装3.4版本的python，进行开发。 pyqt5应用，开发运行时是正常，但pyinstaller打成的包，界面失真变丑。12345678坑的成因：pyinstaller 不支持最新版本的pyqt5。解决方案：重新安装低版本的pyqt5，进行开发。（当前推荐：5.8.2版本）命令pip uninstall pyqt5pip install pyqt5==5.8.2 pyqt5应用，开发运行时是正常，无法打包成功或打包成功但pyinstaller打成的包，无法运行，提示failed to execute script xxx。12345678坑的成因：（同坑3）pyinstaller 不支持最新版本的pyqt5。解决方案：（同坑3）重新安装低版本的pyqt5，进行开发。（当前推荐：5.8.2版本）命令pip uninstall pyqt5pip install pyqt5==5.8.2 pyqt5、pyqt5-tools 安装失败12345坑的成因：你的python3可能是最新版本，pyqt5、pyqt5-tools、pyqtchart还不支持最新版本的python3解决方案：重新安装低版本的python3，进行开发。（当前推荐：3.6.6版本） pyqtchart、pyqtdatavisualization 安装失败123456坑的成因：pyqtchart、pyqtdatavisualization对pyqt5的版本有依赖需求。解决方案：针对pyqt5的版本进行安装。命令如： pip install pyqtchart==5.8 打包opencv 过程中，部分包加载异常12345678ImportError: OpenCV loader: missing configuration file: ['config.py']. Check OpenCV installation.解决方案：import cv2print(cv2.__file__) # 这里我得到的是 D:\ProgramData\Anaconda3\lib\site-packages\cv2\__init__.py# 在使用 pyinstaller 时，加入 paths 选项：pyinstaller main.py -F --paths="D:\ProgramData\Anaconda3\lib\site-packages\cv2"]]></content>
      <categories>
        <category>编程拾慧</category>
        <category>python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[两组数据差异显著性检验]]></title>
    <url>%2F0001.%E7%9F%A5%E8%AF%86-02.Math%2F%E4%B8%A4%E7%BB%84%E6%95%B0%E6%8D%AE%E5%B7%AE%E5%BC%82%E6%98%BE%E8%91%97%E6%80%A7%E6%A3%80%E9%AA%8C%2F</url>
    <content type="text"><![CDATA[简述 1、如果两组数每组数的个数&lt;30，且已知方差服从正态分布，可以比较2组数的均值是否显著不同，用t检验； 2、如果两组数每组数的个数≥30，也可以比较2组数的均值是否显著不同，用z检验； 3、如果两组数每组数的分布未知，可以比较2组数是否显著性同分布，可以用非参数检验 Mann-Whitney U test进行； 4、如果两组数已知都服从正态分布，可以比较2组数的方差是否显著相同，用F检验； K-S检验K-S检验（Kolmogorov-Smirnov检验），K-S检验不仅能够检验单个总体是否服从某一理论分布，还能够检验两总体分布是否存在显著差异。其原假设是：两组独立样本来自的两总体的分布无显著差异。 K-S检验以变量的秩作为分析对象，检验两个独立样本群体，或者一个样本群体和一个特定标准分布之间的关系。K-S就是对两组数据的累积分布进行比较，寻找两个群体累积分布曲线之前的最大值作为D值。获得D值后，查表确定临界值。]]></content>
      <categories>
        <category>统计知识</category>
      </categories>
      <tags>
        <tag>显著性检验</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[NGS测序原理]]></title>
    <url>%2F0001.%E7%9F%A5%E8%AF%86-01.NGS%2F2018-04-27.NGS%E6%B5%8B%E5%BA%8F%E5%8E%9F%E7%90%86%2F</url>
    <content type="text"><![CDATA[# 参考资料： 百度文库 华大基因：知学云]]></content>
      <categories>
        <category>NGS</category>
        <category>原理</category>
      </categories>
      <tags>
        <tag>测序原理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hexo博客文章优化]]></title>
    <url>%2F0003.%E5%BA%94%E7%94%A8-99.other%2Fhexo-%E6%89%A9%E5%B1%95%E5%8A%9F%E8%83%BD-%E6%96%87%E7%AB%A0%E6%B7%BB%E5%8A%A0%E6%8A%98%E5%8F%A0%E5%8A%9F%E8%83%BD%2F</url>
    <content type="text"><![CDATA[博客文章添加折叠功能博客文章中，有些内容篇幅较大，但是可能对一部分读者来说，并不会特别关注，所以这些数据的展示，可能会直接导致整篇博文变得臃肿，而对另一部分读者，这部分详细的介绍却非常重要，因此不能简单的缩减博文，所以这个时候，针对文章中这部分篇幅较大的内容，增加折叠功能可以很好的解决这类问题。 在main.js中添加折叠jsnext主题的主要js位于 themes/next/source/js/src/post-details.js在里面找合适的位置，添加如下代码： 12345678$(document).ready(function()&#123; $(document).on(&apos;click&apos;, &apos;.fold_hider&apos;, function()&#123; $(&apos;&gt;.fold&apos;, this.parentNode).slideToggle(); $(&apos;&gt;:first&apos;, this).toggleClass(&apos;open&apos;); &#125;); //默认情况下折叠 $(&quot;div.fold&quot;).css(&quot;display&quot;,&quot;none&quot;);&#125;); 自定义内建标签在主题scripts下添加一个tags.js, 位于themes/next/scripts/tags.js 123456789101112131415161718192021222324/* @haohuawu 修复 Nunjucks 的 tag 里写 ```代码块```，最终都会渲染成 undefined 的问题 https://github.com/hexojs/hexo/issues/2400*/const rEscapeContent = /&lt;escape(?:[^&gt;]*)&gt;([\s\S]*?)&lt;\/escape&gt;/g;const placeholder = &apos;\uFFFD&apos;;const rPlaceholder = /(?:&lt;|&amp;lt;)\!--\uFFFD(\d+)--(?:&gt;|&amp;gt;)/g;const cache = [];function escapeContent(str) &#123; return &apos;&lt;!--&apos; + placeholder + (cache.push(str) - 1) + &apos;--&gt;&apos;;&#125;hexo.extend.filter.register(&apos;before_post_render&apos;, function(data) &#123; data.content = data.content.replace(rEscapeContent, function(match, content) &#123; return escapeContent(content); &#125;); return data;&#125;);hexo.extend.filter.register(&apos;after_post_render&apos;, function(data) &#123; data.content = data.content.replace(rPlaceholder, function() &#123; return cache[arguments[1]]; &#125;); return data;&#125;); 再继续添加一个themes/next/scripts/fold.js 12345678/* global hexo */// Usage: &#123;% fold ???? %&#125; Something &#123;% endfold %&#125;function fold (args, content) &#123; var text = args[0]; if(!text) text = &quot;点击显/隐&quot;; return &apos;&lt;div&gt;&lt;div class=&quot;fold_hider&quot;&gt;&lt;div class=&quot;close hider_title&quot;&gt;&apos; + text + &apos;&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;fold&quot;&gt;\n&apos; + hexo.render.renderSync(&#123;text: content, engine: &apos;markdown&apos;&#125;) + &apos;\n&lt;/div&gt;&lt;/div&gt;&apos;;&#125;hexo.extend.tag.register(&apos;fold&apos;, fold, &#123;ends: true&#125;); 最后，添加几个自定义样式，位置 themes/next/source/css/_custom/custom.styl 12345678910.hider_title&#123; font-family: &quot;Microsoft Yahei&quot;; cursor: pointer;&#125;.close:after&#123; content: &quot;▼&quot;;&#125;.open:after&#123; content: &quot;▲&quot;;&#125; 最后，在我们需要折叠的地方前后添加便签，示例用法： 折叠示例代码 123&#123;% fold 点击显/隐内容 %&#125;something you want to fold, include code block.&#123;% endfold %&#125; 参考博客 Hexo博文置顶（自定义排序）HEXO默认是按照时间顺序排一条线，然后按照时间顺序来决定显示的顺序的。按照网上的教程整理了一份方法。 使用的是top属性，top值越高，排序越在前，不设置top值得博文按照时间顺序排序。修改Hexo文件夹下的node_modules/hexo-generator-index/lib/generator.js 打开在最后添加如下javascript代码代码 12345678910111213posts.data = posts.data.sort(function(a, b) &#123;if(a.top &amp;&amp; b.top) &#123; // 两篇文章top都有定义 if(a.top == b.top) return b.date - a.date; // 若top值一样则按照文章日期降序排 else return b.top - a.top; // 否则按照top值降序排&#125;else if(a.top &amp;&amp; !b.top) &#123; // 以下是只有一篇文章top有定义，那么将有top的排在前面（这里用异或操作居然不行233） return -1;&#125;else if(!a.top &amp;&amp; b.top) &#123; return 1;&#125;else return b.date - a.date; // 都没定义按照文章日期降序排)&#125;; 更改以后，在写博客的时候，添加top属性就可以啦；]]></content>
      <categories>
        <category>hexo</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[密钥设置实现免密码访问]]></title>
    <url>%2F0003.%E5%BA%94%E7%94%A8-02.Linux%2F2018-04-19.%E5%AF%86%E9%92%A5%E8%AE%BE%E7%BD%AE%E5%AE%9E%E7%8E%B0%E5%85%8D%E5%AF%86%E7%A0%81%E8%AE%BF%E9%97%AE%2F</url>
    <content type="text"><![CDATA[进行项目分析等工作，经常要链接服务器，登陆过程中总会需要我们输入密码，如果频率比较低，还好，但是每天的重复输入，总归会浪费我们大量的时间。另一方面部分工具或任务，可能不方便进行交互式的密码输入，因此通过使用密钥来实现免密码展现出较大的优势。 通过公钥与私钥创建公钥与私钥对在本地机器上运行 1ssh-keygen 将公钥复制到远程目录123ssh-copy-id -i ~/.ssh/id_rsa.pub remote-host# 会提示输入远程服务器的密码# 密码输入后，会将key写到远程机器的 ~/.ssh/authorized_key.文件中 配置完成进行测试1ssh remote-host 不需要输入密码即可登录到远程服务器 sshpass 工具先在机器A上安装 sshpass 工具，然后使用 12sshpass -p passwd ssh(scp) ** 或者 sshpass -f file ssh(scp) ** 其中 -p 直接指定密码，-f 则从文件中读取密码。]]></content>
      <categories>
        <category>环境配置</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[R绘图-ggplot-箱线图绘制]]></title>
    <url>%2F0002.%E5%BC%80%E5%8F%91-03.R%2FR-ggplot-%E7%AE%B1%E7%BA%BF%E5%9B%BE%E7%BB%98%E5%88%B6%2F</url>
    <content type="text"><![CDATA[数据处理过程中，好的数据展示，可以帮助我们更好的理解数据，发现数据之间的关系，记录下各种常见的绘图方式。 结果示例箱线图上方标注的为每个样品对应的中位数，可以根据需要进行调整。 绘图命令1Rscript ../ggplot_boxplot.R -i ggplot_boxplot.demo.data -o ggplot_boxplot.demo.data.png -X &quot;gene&quot; -Y &quot;depth&quot; 输入文件Demo数据 123456789101112131415161718192021222324252627282930313233Cluster Value IDKIT 0.85192541182175 1KIT 0.864711404642792 1KIT 1.12599180249189 1KIT 0.634586693569092 1KIT 1.16825284052483 1KIT 0.68284662568039 1KIT 0.579859358706082 1KIT 1.04258938018236 1KIT 0.753529870275851 1KIT 0.806402805442452 1KIT 0.951605801848777 1KIT 0.941741338875395 1KIT 0.880335826035274 1KIT 0.998844046813315 1。。。。BRCA1 1.20329612016347 2BRCA1 1.16319047052218 2BRCA1 0.779834117567836 2BRCA1 0.79095228570136 2BRCA1 0.835548863196455 2BRCA1 1.52532474830366 2BRCA1 1.13679352537476 2BRCA1 0.598936767501402 2BRCA1 1.1724422960551 2BRCA1 0.924227501396964 2BRCA1 0.691209808851361 2BRCA1 0.73454091210198 2。。 程序目录R脚本 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107library(&apos;getopt&apos;);library (&apos;ggplot2&apos;)library(MASS)library(plyr)#-----------------------------------------------------------------# getting parameters#-----------------------------------------------------------------#get options, using the spec as defined by the enclosed list.#we read the options from the default: commandArgs(TRUE).spec = matrix(c( &apos;help&apos; , &apos;h&apos;, 0, &quot;logical&quot;, &apos;infile&apos; , &apos;i&apos;, 1, &quot;character&quot;, &apos;outfile&apos; , &apos;o&apos;, 1, &quot;character&quot;, &apos;title&apos; , &apos;T&apos; , 2 , &quot;character&quot;, &apos;x.lab&apos; , &apos;X&apos;, 2, &quot;character&quot;, &apos;y.lab&apos; , &apos;Y&apos;, 2, &quot;character&quot;, &apos;type&apos; , &apos;t&apos;, 2, &quot;character&quot; ), byrow=TRUE, ncol=4);opt = getopt(spec);# define usage functionprint_usage &lt;- function(spec=NULL)&#123; cat(getopt(spec, usage=TRUE)); cat(&quot;Usage example: \n&quot;) cat(&quot;Rscript ggplot_boxplot.R -i input_tab -o Demo_out -t png -X xtest -Y ytest -W 100 -H 100 infile format： Cluster Value ID Clus_A 0.51 1 Clus_A 0.31 1 . . . Clus_Z 0.42 21 Clus_Z 0.72 21Options: --help -h NULL get this help--infile -i character the input file [forced]--outfile -o character the prefix for output graph [forced]--title -T character the Title for the picture (default:Title)--x.lab -X character the lab for x in SubPlot (default:xlab)--y.lab -Y character the lab for y in SubPlot (default:ylab)--type -t character save format(png,tiff,jpeg,svg,pdf default:png)\n&quot;) q(status=1);&#125;# if help was asked for print a friendly message# and exit with a non-zero error codeif ( !is.null(opt$help) ) &#123; print_usage(spec) &#125;if ( is.null(opt$infile) ) &#123; print_usage(spec) &#125;if ( is.null(opt$outfile)) &#123; print_usage(spec) &#125;if ( is.null(opt$type) ) &#123; opt$type=&quot;png&quot; &#125;if ( is.null(opt$x.lab) ) &#123; opt$x.lab=&quot;xlab&quot; &#125;if ( is.null(opt$y.lab) ) &#123; opt$y.lab=&quot;ylab&quot; &#125;if ( is.null(opt$title) ) &#123; opt$title=&apos;Title&apos;&#125;Args &lt;- commandArgs();file_tab=read.table(opt$infile,header=F,sep=&quot;\t&quot;);out_file=paste(opt$outfile,opt$type,sep=&quot;.&quot;)if(opt$type == &quot;png&quot;) &#123; png(file=out_file) &#125; #, 400*length(file_tab[1,]) , 400*length(file_tab[,1])) &#125;if(opt$type == &quot;tiff&quot;) &#123;tiff(file=out_file) &#125; #, 400*length(file_tab[1,]) , 400*length(file_tab[,1])) &#125;if(opt$type == &quot;jpeg&quot;) &#123;jpeg(file=out_file) &#125; #, 400*length(file_tab[1,]) , 400*length(file_tab[,1])) &#125;if(opt$type == &quot;pdf&quot;) &#123; pdf(file=out_file) &#125; #, 20*length(file_tab[1,]) , 20*length(file_tab[,1])) &#125;data=read.table(opt$infile ,header=T)data2=ddply(data,&quot;Cluster&quot;,summarise,Median=round(median(Value),3))ggplot(data,aes(x=reorder(Cluster,Value),y=Value,fill=Cluster)) +geom_boxplot() +theme(panel.grid.major =element_blank(), panel.grid.minor = element_blank(), panel.background = element_blank(), axis.line = element_line(colour = &quot;black&quot;),axis.title.x=element_text(size=25), axis.title.y =element_text(size=25), title=element_text(size=25), axis.text.x=element_text(angle=30,hjust=1,size=15))+ #设置x轴，数据标签的样式labs(x=opt$x.lab ,y=opt$y.lab) + # 设置x轴和y轴现实的标注geom_text(data=data2,aes(x=Cluster,y=2.5,colour=Cluster,label=Median,vjust=-0.2)) + # 在y=2.5的位置添加标签批注coord_cartesian(ylim=c(0,3)) # 对绘制的图片 根据x轴和y轴进行截取#####ggplot(data,aes(x=reorder(Cluster,Value),y=Value,fill=Cluster)) +### 绘图类型 ####geom_boxplot() + # 绘制箱线图### 样式 ####theme(panel.grid.major =element_blank(), # 去除主网格线#panel.grid.minor = element_blank(), #去除次网格线#panel.background = element_blank(), #去除背景色（默认为灰色）#axis.line = element_line(colour = &quot;black&quot;)，#将x轴和y轴的框线设置为黑色#axis.title.x=element_text(size=25), #设置x轴的标题样式#axis.title.y =element_text(size=25), #设置y轴的标题样式#title=element_text(size=25), #设置主标题样式#axis.text.x=element_text(angle=30,hjust=1,size=15)) + #设置x轴，数据标签的样式### 设置坐标轴标注 ####labs(x=opt$x.lab ,y=opt$y.lab) + # 设置x轴和y轴现实的标注### 添加标签 ####geom_text(data=data2,aes(x=Cluster,y=2.5,colour=Cluster,label=Median,vjust=-0.2)) + # 在y=2.5的位置添加标签批注### 对图片进行截取 ####coord_cartesian(ylim=c(0,3)) # 对绘制的图片 根据x轴和y轴进行截取dev.off()]]></content>
      <categories>
        <category>编程拾慧</category>
        <category>R</category>
      </categories>
      <tags>
        <tag>可视化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux配置文件~/.bashrc设置]]></title>
    <url>%2F0003.%E5%BA%94%E7%94%A8-02.Linux%2FLinux-%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6-bashrc%E8%AE%BE%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[常见配置https://blog.csdn.net/bangemantou/article/details/7682272 ## shell变量 使用set命令显示所有的变量 使用env命令只显示环境变量 局部变量PS1PS1变量主要设置Bash 提示符所显示的信息可将一些换码序列插入到PS1变量中、它们成为提示信息的一部分、常用换码序列如下：|换码符号|含义||-|-||\d|系统当前的日期、d应该是date的第1个字母||\t|系统当前的时间、t应该是time的第1个字母||\h|简短形式的主机名、h应该是host（主机）的第1个字母||\u|当前用户名、u应该是user的第1个字母||\w|当前的工作目录、w应是working directory的第1个字母||!|当前命令的历史编号、！为执行历史命令的第1个字符||$|如果是普通用户显示$、而如果是root用户显示#||\l|显示shell终端设备的基本名、l应该是line的第1个字母| 除了上述常用转换码外，也可以通过环境变量值设置展示内容（例如conda环境等）参考配置12345678export PS1=&quot;\[\033]0;\h$\w\007\033[36m\]\u@\h[\d] \[\033[33m\]\w\[\033[0m\]\n$ &quot;# 显示样式如下liubo4@tj-login-24-4[一 10月 24] /ifstj1/B2C_RD_P1/$PS1=&quot;($CONDA_DEFAULT_ENV)\[\033]0;\h$\w\007\033[36m\]\u@\h[\d] \[\033[33m\]\w\[\033[0m\]\n$ &quot;# 显示样式如下(base)liubo4@tj-login-24-4[六 11月 12] /ifstj2/B2C_RD_H1/ 别名alias 使用alias命令为history命令创建别名h 使用alias命令为rm -i创建一个名为del的别名 使用不带任何参数的alias命令列出所有的别名 取消别名命令为：unalias 别名的名字12alias h=historyalias del=‘rm -i’ 相关问题~/.bashrc不能自动source最近更换了一个集群，更改了配置文件，却发现每次登陆都需要手动source，~/.bashrc不能自动执行，表示手动用了几次，发现每次这样简直不能忍～。查了一些资料来解决这个问题。 缺少~/.bash_profile创建 ~/.bash_profile 文件，并在文件开始位置添加如下内容：。 12345# .bash_profile# Get the aliases and functionsif [ -f ~/.bashrc ]; then . ~/.bashrcfi 终端颜色配置1export PS1=&quot;\[\033]0;\h$\w\007\033[36m\]\u@\h[\d] \[\033[33m\]\w\[\033[0m\]\n$ &quot; 中文支持异常集群中文支持出现异常，配置中文支持123LANG=&quot;zh_CN.UTF-8&quot; SYSFONT=&quot;latarcyrheb-sun16&quot; SUPPORTED=&quot;zh_CN.UTF-8:zh_CN:zh&quot; Linux常用命令安装Reference[https://blog.csdn.net/weixin_38492159/article/details/106464087]]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Bash</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[BASH的基本语法]]></title>
    <url>%2F0003.%E5%BA%94%E7%94%A8-02.Linux%2FLinux-BASH%E7%9A%84%E5%9F%BA%E6%9C%AC%E8%AF%AD%E6%B3%95%2F</url>
    <content type="text"><![CDATA[参考资料 之前更多的是直接使用Linux命令组合，对shell命令的使用比较少，都是有需求临时百度，没有系统的了解过，趁此机会，系统了解一下bash。 目录 最简单的例子 —— Hello World! 关于输入、输出和错误输出 BASH 中对变量的规定（与 C 语言的异同） BASH 中的基本流程控制语法 函数的使用 1.最简单的例子 —— Hello World!几乎所有的讲解编程的书给读者的第一个例子都是 Hello World 程序，那么我们今天也就从这个例子出发，来逐步了解 BASH。 用 vi 编辑器编辑一个 hello 文件如下： 123#!/bin/bash #第一行说明文件的类型，Linux系统根据 &quot;#!&quot; 及该字串后面的信息确定该文件的类型# This is a very simple example #在 BASH 程序中从“#”号（注意：后面紧接着是“!”号的除外）开始到行尾的多有部分均被看作是程序的注释。echo Hello World #bash的执行命令 如何执行该程序呢？有两种方法：一种是显式制定 BASH 去执行： bash hello或sh hello （这里 sh 是指向 bash 的一个链接，“lrwxrwxrwx 1 root root 4 Aug 20 05:41 /bin/sh -&gt; bash”） 或者可以先将 hello 文件改为可以执行的文件，然后直接运行它，此时由于 hello 文件第一行的 “#! /bin/bash” 的作用，系统会自动用/bin/bash 程序去解释执行 hello 文件的： 123#! bashchmod +x hello./hello 此处没有直接 “$ hello”是因为当前目录不是当前用户可执行文件的默认目录，而将当前目录“.”设为默认目录是一个不安全的设置。 需要注意的是，BASH 程序被执行后，实际上 Linux 系统是另外开设了一个进程来运行的。 2. 关于输入、输出和错误输出在 Linux 系统中：标准输入(stdin)默认为键盘输入；标准输出(stdout)默认为屏幕输出；标准错误输出(stderr)默认也是输出到屏幕（上面的 std 表示 standard）。在 BASH 中使用这些概念时一般将标准输出表示为 1，将标准错误输出表示为 2。 12345# 不常用的方法n&lt;&amp;- #表示将 n 号输入关闭 &lt;&amp;- #表示关闭标准输入（键盘）n&gt;&amp;- #表示将 n 号输出关闭&gt;&amp;- #表示将标准输出关闭 33. BASH 中对变量的规定BASH 中的变量都是不能含有保留字，不能含有 “-“ 等保留字符，也不能含有空格。 简单变量在 BASH 中变量定义是不需要的，没有 “int i” 这样的定义过程。如果想用一个变量，只要他没有在前面被定义过，就直接可以用，当然你使用该变量的第一条语句应该是对他赋初值了，如果你不赋初值也没关 系，只不过该变量是空（ 注意：是 NULL，不是 0 ）。不给变量赋初值虽然语法上不反对，但不是一个好的编程习惯。好了我们看看下面的例子：]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Bash</tag>
        <tag>Shell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[samtools-安装]]></title>
    <url>%2F0003.%E5%BA%94%E7%94%A8-03.software%2FSoftware-samtools-%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[#1. 环境配置 12345#报错缺少时 curses.h: No such file or directoryyum install ncurses-devel ncurses#报错缺少时 bzlib.h: No such file or directoryyum install bzip2-devel.x86_64 #2. 软件安装1234567wget -c https://github.com/samtools/samtools/releases/download/1.9/samtools-1.9.tar.bz2tar xvf samtools-1.9.tar.bz2cd samtools-1.9/./configure --prefix=~/biosoft/samtools-1.9makemake install 3. 异常处理##3.1 samtools打开的文件句柄数目超出Linux限制 问题描述在使用Samtools进行排序时，遇到如下情况，程序中断，从截图中可以看到，samtools一共需要合并1032个bam文件，但是在合并到1020个文件时，程序打开文件失败了。因为常用samtools的时候，我们都知道这个sort.1020.bam 文件，其实是samtools在进行排序处理过程中，生成的中间文件。而这个报错是在合并时出现的（意味着该文件已经生成）。所以经过查找，发现是Linux文件句柄存在限制导致的问题。 12$ ulimit -n1024 # Linux系统设置的，一个程序支持打开的文件句柄数 本次问题中，Linux系统支持最多打开1024个文件，但是需要排序的文件超过了这个数字，因为由于句柄超出，软件处理失败。 解决方案其实Linux是有文件句柄限制的，一般都是1024，因此我们需要把这个值改大一些。这个1024是当前用户给准备要运行的程序的限制。 针对非Root用户1234567ulimit -n 2048 # 句柄数调整为2048。 #调整当前工作窗口，重启工作窗口后会重置。``` 2. 针对具有root权限的用户``` $ cat &gt;&gt; /etc/security/limits.conf soft nofile 1000000hard nofile 1000000 将ulimit 值添加到/etc/profile文件中（适用于有root权限登录的系统）为了每次系统重新启动时，都可以获取更大的ulimit值，将ulimit 加入到/etc/profile 文件底部。]]></content>
      <categories>
        <category>NGS</category>
        <category>software</category>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>软件安装</tag>
        <tag>环境配置</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[R安装]]></title>
    <url>%2F0002.%E5%BC%80%E5%8F%91-03.R%2FR-%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[R，作为数据计算和统计的经典工具，很多行业的数据分析和挖掘都跟它息息相关。熟悉R，对传统行业的数据的体量和分析会有一个初步的认识。了解R，我们先从安装它开始。Windows和Mac下有专门的安装程序，可以从https://www.r-project.org/， 可以直接安装。Linux下也可以通过相应的yum或者apt-get进行安装。然而，有些情况下，如Linux软件中心带的R程序如果太old，无法与其它的程序比如Scala或者Java兼容，则需要手工编译源码进行安装。这种繁琐的环节最好不要遇上，否则会比较��，此文用来纪念这个过程。R程序本身有很多依赖，建议参考本文先把依赖都装上去，然后再build R源程序。或者也可以直接编译R源代码，需要什么依赖安装什么依赖。 yum install -y readline-devel gcc*yum install libXt-devel -y 1.zlibwget http://ncu.dl.sourceforge.net/project/libpng/zlib/1.2.8/zlib-1.2.8.tar.gztar -zxvf zlib-1.2.8.tar.gzcd zlib-1.2.8./configure –prefix=/opt/zlib-1.2.8make &amp;&amp; make install 2.bzipwget http://www.bzip.org/1.0.6/bzip2-1.0.6.tar.gztar -zxvf bzip2-1.0.6.tar.gzcd bzip2-1.0.6make -f Makefile-libbz2_somake cleanmakemake install PREFIX=/opt/bzip2-1.0.6cd /opt/ 3. xzwget http://tukaani.org/xz/xz-5.2.2.tar.gztar xzvf xz-5.2.2.tar.gzcd xz-5.2.2./configure –prefix=/opt/xz-5.2.2make -j3 &amp; make install 4. pcrewget http://fossies.org/linux/misc/pcre-8.39.tar.gztar -zxvf pcre-8.39.tar.gzcd pcre-8.39./configure –prefix=/opt/pcre-8.39 –enable-utf8make &amp; make install 5. openssl(不是必须的，如果机子上已经安装则可以跳过)yum install openssl* 6. CURLwget http://www.execve.net/curl/curl-7.50.1.tar.gztar zxvf curl-7.50.1.tar.gzcd curl-7.50.1./configure —prefix=/opt/curl-7.50.1make &amp;&amp; make install 7.更新链接lib库和PATH路径echo /opt/xz-5.2.2/lib &gt;&gt; /etc/ld.so.confecho /opt/pcre-8.39/lib &gt;&gt; /etc/ld.so.confecho ‘export PATH=/opt/R-3.3.1/bin:${PATH}:/opt/curl-7.50.1/bin’ &gt;&gt; /root/.bashrcsource /root/.bashrc 8. 安装R程序wget http://mirrors.xmu.edu.cn/CRAN/src/base/R-3/R-3.3.1.tar.gztar -zxvf R-3.3.1.tar.gzcd R-3.3.1./configure –prefix=/opt/R-3.3.1 –enable-R-shlib LDFLAGS=”-L/opt/zlib-1.2.8/lib -L/opt/bzip2-1.0.6/lib -L/opt/xz-5.2.2/lib -L/opt/pcre-8.39/lib -L/opt/curl-7.50.1/lib” CPPFLAGS=”-I/opt/zlib-1.2.8/include -I/opt/bzip2-1.0.6/include -I/opt/xz-5.2.2/include -I/opt/pcre-8.39/include -I/opt/curl-7.50.1/include”ldconfigmaketouch doc/NEWS.pdf（Install R的过程中，遇到了一个NEWS.pdf找不到，用这个办法绕过的）make install 安装成功后，可以通过以下办法进行测试。root@cu01 R-3.3.1]# lsbin include lib lib64 share[root@cu01 R-3.3.1]# R R version 3.3.1 (2016-06-21) – “Bug in Your Hair”Copyright (C) 2016 The R Foundation for Statistical ComputingPlatform: x86_64-pc-linux-gnu (64-bit) R是自由软件，不带任何担保。在某些条件下你可以将其自由散布。用’license()’或’licence()’来看散布的详细条件。 R是个合作计划，有许多人为之做出了贡献.用’contributors()’来看合作者的详细情况用’citation()’会告诉你如何在出版物中正确地引用R或R程序包。 用’demo()’来看一些示范程序，用’help()’来阅读在线帮助文件，或用’help.start()’通过HTML浏览器来看帮助文件。用’q()’退出R.]]></content>
      <categories>
        <category>编程拾慧</category>
        <category>环境配置</category>
        <category>R</category>
      </categories>
      <tags>
        <tag>R</tag>
        <tag>软件安装</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Bedtools 的使用]]></title>
    <url>%2F0003.%E5%BA%94%E7%94%A8-03.software%2FSoftware-bedtools%2F</url>
    <content type="text"><![CDATA[巧用annotate命令，更好的了解我们的检测范围]]></content>
      <categories>
        <category>NGS</category>
        <category>software</category>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>变异检测</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[GATK4初探 - 1]]></title>
    <url>%2F0003.%E5%BA%94%E7%94%A8-03.software%2FSoftware-GATK4%E5%88%9D%E6%8E%A2-1%2F</url>
    <content type="text"><![CDATA[germline somatic detail GATK4 简介；参考地址WorkflowWDL语法Best Practices GATK4.0的改动 Re-engineered for speed、 scalability and versatillity Expanded scope of analysis to more variant types Reproducible best practices worlflowsGATK4。0 协议Under BSD3.0 streamlined arhciecture（overall efficlency）Intel Genomics Kernel Library （speed）Intel GenomicsDB （scalability）Apache Spark support（robust parallelism）Google Dataproc and GCS support（cloud execution）Versatility of data traversal （analysis scpe） 变异检出Geretic changes in individuals relative to a reference genome Germline（inherited） Somatic（cancer） Reference genome= a standardized genomic sequenceHuman genome reference sequence Previous standard hg19/b37 New sandard ： hg38 变异检出造成干扰的原因： 噪音、污染、纯度、 GATK4 对应不同变异检出的程序： Germline SOMATIC SNPs&amp;Indel HaplotypeCaller GVCF MuTect2 CNV GATK gCNV（beta） GATK CNV +aCNV Structure GATK SVDiscovery（beta） Planned GATK bestPractices 单样本变异检出算法 Mutect 支持但样本的SNV检出（假阳性多） GATK workflowsGithub流程Script使用WDL编写 变异检测过程；Step1 Identify ActiveRegions]]></content>
      <categories>
        <category>NGS</category>
        <category>software</category>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>变异检测</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Software-MANTIS调研]]></title>
    <url>%2F0003.%E5%BA%94%E7%94%A8-03.software%2FSoftware-MSI-MANTIS%E8%B0%83%E7%A0%94%2F</url>
    <content type="text"><![CDATA[简介MANTIS (Microsatellite Analysis for Normal-Tumor InStability) 是一个从成对的BAM文件中检测微卫星不稳定性的程序。 为了进行分析，程序需要一个肿瘤BAM和一个匹配的正常BAM文件(使用相同的管道生成)来确定两个样本之间的不稳定性得分。 推荐使用较长的读数(理想情况下，100bp或更长)，因为较短的读数不太可能完全覆盖微卫星位点，并且在质量控制过滤器失败后将被丢弃。MANTIS 于2017年发表在Oncotarget ( Performance evaluation for rapid detection of pan-cancer microsatellite instability with MANTIS) 如果需要使用该软件，可以通过Github 进行获取。 软件使用软件的安装安装软件的依赖包该软件是基于Python开发的，软件的运行需要安装下列Python库 NumPy(v1.6.2) Pysam(v0.8.3) 使用方法天津集群安装，conda环境调试成功使用环境Manti （先执行命令 source activate Manti）在模拟环境中进行分析，测试命令：1234python /share/udata/liubo/Software/MANTIS-master/mantis.py -b loci.bed --genome hg19.fa -n normal.bam -t cancer.bam -o MANTIS.txt --threads 8 # threads设置软件运行的线程数。 参考基因组和bed文件，可以使用配置未见的方式存储到配置文件中。引用配置文件使用 -cfg 参数。123#配置文件格式genome = /path/to/reference/genome.fastabedfile = /path/to/my/loci.bed 软件原理 阅读bed文件，获得目标区域，然后根据基因组信息，把这些靶标位点比对到参考基因组上，建立索引（通过0和1）； 每次针对一个靶标区域，工具首先从normal和cancer样本比对后的bam文件中抽提覆盖这个区域的reads，然后对这些reads进行一个初步的质控过滤，其中过滤标准如下： 1.确保这些reads都达到了合格满意的序列长度,默认：35（参数：mrl） 2.每个base的平均质量值得分达到要求的最小值，默认：25（参数：mrq） 3.覆盖的整个的靶标区域。 对通过初步过滤的reads单独进行分析，检测微卫星结构的起始位点，以及总的重复次数（通过起始位点开始，匹配微卫星结构的匹配次数） 一旦确定了微卫星的repeat次数以后，对数据进行第二次的质控， 1.确定在reads序列末端前loci没有被打断。 2.确定微卫星区域中的每个base的平均质量值都比较高，且达到标准，默认：30（mlq） 针对tumor和normal文件，分别统计不同的repeat counts的reads支持数，获得每个靶标区域的结构重复的reads支持数,reads支持数过低的repeat count 会被移除，默认：3（mrr） 确定repeat counts数以后，对每个靶标区域进行质控。将重复序列长度中偏离标准值过大的repeat count数移除，默认3倍标准差（参数sd ） ，会进行移除。 然后针对normal和cancer，检查每个靶标的总reads数目，确保reads的数目足够获得一个统计学显著的分布，达不到标准的loci会被丢弃。默认：30（mlc） 利用最终剩下的repeat count数据，进行得分计算，获得每个样本的一个不稳定得分。计算方法每个loci单独计算，分别对normal和cancer进行标准化（利用样本在loci的总reads数，将每个repeat的reads支持数，标准化为一个分数，从而量化在两个样本在深度和覆盖度上的差异），根据标准化的reads count计算stepwise： 一旦每个区域的得分计算出来以后，所有区域的不稳定性得分的平局值就可以计算得到，从而用一个数值来反映样本的不稳定程度。（得分范围是,值越大，越不稳定，值越小越稳定）]]></content>
      <categories>
        <category>NGS</category>
        <category>software</category>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>MSI</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CNV检测-调研]]></title>
    <url>%2F0003.%E5%BA%94%E7%94%A8-03.software%2FSoftware-CNV%E6%A3%80%E6%B5%8B-%E8%B0%83%E7%A0%94%2F</url>
    <content type="text"><![CDATA[germline CNV software ###CNV主要方法 ###1. Read-depth 2. Read-pair 3. Split-read 4. Assembly 软件汇总 👍🏻CODEX2(2018年更新版本提高灵敏度，CODEX：2016年) 使用语言：R 数据类型：WES/WGS，靶向扩增子测序 算法特点：应用基于对数线性分解的归一化，基于泊松似然的递归分割算法 特点：专为种系和体细胞 CNV 调用而设计 限制：0.2 &lt; GC &lt; 0.8，Target length &gt; 20 bp，median target coverage &gt; 20 × ，mappability &gt; 0.9 参考文献4，认为CODEX上一代虽然识别出较多common CNV但是灵敏度低。 👍🏻DECoN（两篇文献支持假阴性最低） 使用语言：R 数据类型：靶向扩增子测序 算法特点：beta-二项式分布，加入参考集优化的过程 特点：适用于exon-base的panel测序，可以识别single exon CNV。基于ExomeDepth的基础上修改的。新增了染色体上第一个外显子区域的变异检测，HMM模型中增加了exon之间距离的因素。允许自动选择控制样本。 输入文件：bam、bed、ref 缺点：定义R包版本依赖，需要严格版本控制。需要确保其他分析不会更改其版本。 👍🏻CNVkit（现有CNV software） 使用语言：Python 数据类型：WES/WGS，靶向扩增子 算法特点：不仅考虑目标区域的归一化 Read Counts，还考虑非目标区域的归一化 Read Count。 特点：专为种系和体细胞 CNV 调用而设计。可以通过修改method参数使其应用于WGS和靶向扩增子测序数据。获得每个bin的log2copy ratio。标准化过程补充偏好性矫正，包括GC含量，repeat-masked比例。 限制：排除poor mappable regions 👍🏻GATK gCNV（优化了XHMM） 使用语言：Java、Python 数据类型：WES/WGS，靶向扩增子测序 算法特点：负二项式分布，针对RC归一化和HMM call CNV自洽性做了优化。 特点：主要分成两个部分：模型创建和sample calling。 👎🏻ExomeDepth（不考虑理由是DECoN基于此进行了优化） 使用语言：R 数据类型：WES 算法特点：beta-二项式分布，加入参考集优化的过程 软件特点：专为种系和体细胞 CNV 调用而设计 限制：ead mapq &gt; 20, max distance between target border and the middle of paired read to include read into region 300 bp， Transition probability to CNV 0.0001， Expected CNV length 50 kb 👎🏻XHMM（GATK gCNV优化过，不考虑） 数据类型：WES 算法特点：主成分分析来降低噪声，在 Z-RPKM 上使用隐马尔可夫模型 限制：至少50样本, 0.1 &lt; GC &lt; 0.9, 10 bp &lt; target &lt; 10 kbp, mean coverage &gt; 10 × across all samples, average targets 6, distance between targets 70 kb, average rate of CNV occurrence in the exome 10–8 参考文献4，认为其对gCNV敏感性低。 👎🏻CONTRA 使用语言：Python、R 数据类型：WES 特点：专注于外显子级的CNV识别 限制：Include regions at least 10-bp long with coverage &gt; 10 👎🏻PatternCNV 特点：专注于外显子级的CNV识别，专为种系和体细胞 CNV 调用而设计 Bin size 10，mapq &gt; 20 👎🏻EXCAVATOR2 算法特点：不仅考虑目标区域的归一化 RC，还考虑非目标区域的归一化 RC。 软件特点：专为种系和体细胞 CNV 调用而设计 限制：Read mapq &gt; 1，Min number of targets in CNV：4 参考文献4，认为其对gCNV敏感性低。 👎🏻exomeCopy 使用语言：R 数据类型：WES 算法特点：负二项式分布 precision：4%（参考文献1） recall：27%（参考文献1） 限制：mapq &gt; 1, overlap to include read into region—1 bp, median value for background, transition probability to CNV 1e-4 Transition probability to normal state 0.05 👎🏻CANOES 使用语言：R 算法特点：负二项式分布 precision：3.9%（参考文献1） recall：0.2%（参考文献1） 限制：至少15个样本, average targets：6, distance between targets ：70 kb, average rate of CNV occurrence in the exome：10–8 👎🏻cn.MOPS 数据类型：WES/WGS 算法特点：混合使用泊松模型和贝叶斯方法 限制：至少6个样本，Minimum segments 5 缺点：假阴性偏高 👎🏻FishingCNV 算法特点：主成分分析降低噪声，使用CBS对背景进行归一化覆盖率比较 限制：Read mapq &gt; 15，Base quality 10，RPKM &gt; 3，FDR adjusted pvalue 0.05 👎🏻HMZDelFinder ###### 排除 ###### 限制：只能检测loss 👎🏻ExonDel ##### 排除 ##### 限制：只能检测CNV loss 👎🏻CLAMMS 使用语言：C 数据类型：WES 算法特点：加入了参考集优化的过程 限制：0.3 &lt; GC &lt; 0.7，mappability &gt; 0.75 👎🏻CoNIFER 数据类型：WES 算法特点：使用奇异分解执行系统偏差校正 限制：至少50个样本，Probes with median RPKM across samples &gt; 1, samples with a standard deviation of SVD-ZRPKM &lt; 0.5 👎🏻ClinCNV： 使用语言：R 数据类型：WES/WGS 深度：可分析低深度数据（1x） 可检测类别：germline、somatic 需提供数据：bed（常规三列 + GC 含量）、coverage（染色体、坐标、平均测序深度） 👎🏻DeAnnCNV 特点：可以在线使用，还可以进行变异注释 限制：CNV evidence threshold &gt; 80 ？？？？ 缺点：检测CNV很少 参考文献1：Benchmarking germline CNV calling tools from exome sequencing data 验证数据（gold standard）：NA12878 FishingCNV (1210 CNV) 和 exomeCopy (845 CNV) 数量最多； DeAnnCNV (2 CNV)数量最少； CONTRA、EXCAVATOR2、ExomeDepth 和 PatternCNV （ 200-300 ）； #在 CONTRA 和 PatternCNV 的情况下，这些是单外显子 CNV。 其他算法平均检测到 26 个变化。 CNV长度： CNV 的总长度从 50 kb 到 1304 Mb（神经病啊 这么长） 这表明需要过滤某些工具产生的调用，特别是 FishingCNV 和 exomeCopy。 （ExomeDepth、CONTRA、CANOES、CLAMMS、CNVkit、CODEX、FishingCNV、HMZDelFinder 和 PatternCNV）发现了小于 1 kb 的变异 CNVkit、CODEX、CANOES、EXCAVATOR2 和 FishingCNV 是少数能够同时检测 2 到 3 个目标区域的小 CNV 和长变异（超过 1 Mb）的算法 precision、recall和F1-score 参考文献2：A comparison of tools for copy-number variation detection in germline whole exome and whole genome sequencing data 验证数据（gold standard）：NA12878，in-house GB01–GB08 and GB09–GB38（from CytoScan HD SNP-array） CODEX call 出的CNV最少；GATK gCNV call出的CNV最多。 CODEX 检测到的数量与NA12878相近，但不等于全为真阳性 CLC Genomics Workbench 和 cn.MOPS检测到很多long CNV（&gt; 10，000bp） GATK gCNV识别出的片段主要集中在 &lt;500bp (WES)和500-1000bp（WGS）。比其他CNV检测出了更短的CNV cn.MOPS, CNVnator, Control-FREEC 在WGS样本中识别到了比其他software更多的 &gt;1,000 bp length CNVs 。然而标准品中一半的CNV，都为500bp以下 precision和recall（未进行过滤） GATK gCNV recall最高（both in WGS and WES）,随后是lumpy，DELLY，cnMOPS，Manta。但是低于31%的precision。 CNVkit在过滤后对WES表现出较高的precision和recall，而其他tools对wes数据的recall都不高。然而这个表现只针对NA12878样本。 结论gCNV recall表现最好 参考文献3：GATK gCNV: accurate germline copy-number variant discovery from sequencing read-depth 验证数据（gold standard）：从Genome STRiP获得的一个经过人工验证和FDR控制的callset 许多基于读长的CNV caller试图通过PCA降噪或回归消除系统偏差，或者通过对样本和基因组区域进行预聚类消除。随后使用隐马尔可夫模型或非参数变化点检测算法对CNV进行检测。 关键的是，这些方法在数据归一化和检测之间缺乏自洽性，导致前者无意中去除了引号，导致后者灵敏度降低。 GATK gCNV：有原则的贝叶斯方法，用来学习大型队列的读深数据全局和特定样本的偏差。负二项式分布与分层HMM相结合。偏差建模提高了自洽性。 比较了两个软件，一个是XHMM，一个是CODEX。发现GATK gCNV要比其他两个敏感度高将近20%。特异性高50% 参考文献4： CODEX2: full-spectrum copy number variation detection by high-throughput DNA sequencing、 验证数据（gold standard）：HapMap3、Conrad et.al、McCarroll et.al、1000GP wgs CODEX的升级版，提升了敏感度sensitivity XHMM、EXCAVATOR对common CNV检测缺少敏感性；CLAMMS也是precision高，但是sensitivity低。 CODEX可以检测到更多的common CNV，但是sensitivity低； CODEX2在四个验证集中召回率分别是92.8% 60.7% 79.2% 66.2%，同时特异性有显著提高。 参考文献5：Evaluation of CNV detection tools for NGS panel data in genetic diagnostics 验证数据（gold standard）：ICR96 exon CNV validation series , panelcnDataset, In-house MiSeq ，In-House HiSeq DECoN在各个样本集表现稳定 cn.MOPs(panelcn.MOPS是否是它的一个功能还是一个专门应用panel的版本)，默认参数下，假阴性有些高。 ExomeDepth和CODEX2也表现较好。CODEX2在默认参数下，有一个验证集敏感度偏低。优化参数后回到正常水平。 建议将CNV添加到流程前，针对数据集进行参数优化，作者开发了一个R包 https://github.com/TranslationalBioinformaticsIGTP/CNVbenchmarkeR 参考文献6：Free-access copy-number variant detection tools for targeted nextgeneration sequencing data 验证数据（gold standard）：模拟数据 4179 exons ranging in size from 53 bp to 17,155 bp （位于所有染色体except chr18） 三种类型：homozygous deletions (DEL-HO), heterozygous deletions (DEL-HT), and duplications (DUP) 名列前茅的分别是DECoN, exomeDepth, exomeCNV 比较300X和50X数据差距，除了CNVkit，其余软件均在高深度数据表现更好 DECoN假阴性最低]]></content>
      <categories>
        <category>NGS</category>
        <category>编程拾慧</category>
        <category>software</category>
        <category>R</category>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>CNV</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CNV检测-DECoN]]></title>
    <url>%2F0003.%E5%BA%94%E7%94%A8-03.software%2FSoftware-CNV%E6%A3%80%E6%B5%8B-DeCon%2F</url>
    <content type="text"><![CDATA[Publish原文链接PDF: Accurate clinical detection of exon copy number variants in a targeted NGS panel using DECoN DECoN v1.0.0 Documentation DECoN软件Git 背景二代靶向测序(NGS)芯片越来越多地应用于临床基因组学，以提高基因检测的能力、通量和降低检测成本。在靶向捕获测序芯片中进行整个外显子的缺失和扩增是非常有挑战，特别是针对单个外显子的CNV检测。本文献提供了一个针对外显子靶向捕获数据进行外显子水平拷贝数变异的工具（DECoN）。累计用到2016例样本，其中96例（10 samples with exon CNVs in BRCA1, six with exon CNVs in BRCA2, and 15 samples with exon CNVs in one of eight other genes: TP53, SDHB, MLH1, MSH2, NSD1, EZH2, WT1 and FH）评估集合，和1920临床测试集合；样本使用的是淋巴瘤的外周血和唾液。 MethodDECoN（Detection of Exon Copy Number ) 是基于ExomeDepth (v.1.0.0) 进行开发修改的,其中有两个比较重要的改动。 可以检测染色体上第一个（Bed文件中定义的）外显子发生的变异，之前的版本不支持； HMM转移矩阵的概率是基于外显子的距离计算的 ，在这里如果两个外显子在染色体上的距离太远的话，那他们将作为两个独立的变异进行处理。这部分升级也在后面应用到ExomeDepth (v.1.1.0) 了。同时DECoN还增加了一些其他的功能，例如对依赖包进行标准化，保证临床应用过程中，不同实验室的结果一致性。 4.1 Reading BAM files to generate coverage metricDECoN的输入是一系列的Bam文件列表和一个bed文件（描述需要计算覆盖度矩阵的exon区域），然后去计算Bed文件中的每个外显子的FPKM（fragment per kilobase and million base pairs ) 获得每个外显子的覆盖度矩阵。 DECoN使用这个矩阵进行外显子层面的CNV检测. FPKM 的计算方式如下:FPKM = C/(N*L)其中C是map到目标外显子的Reads数目（单位条）；其中N是一个样本能比对到基因组上的总Reads数目（单位Million）其中L是目标外显子的碱基长度（单位base） eg: For example, consider a sample with a total of 20 million mapped read pairs of which 200 map to an exon which is 100 bases long: FPKM = 200/(20*0.1)Thus FPKM for this exon in this sample is 100. 4.2 Running quality checks外显子和样本的评估都是基于它们的平均覆盖率水平。当覆盖率较低时，检测的准确性将受到影响，因此在解释结果时应谨慎行事。 样本也根据它们与其他样本的相关性进行评估。如果样本与集合中其他样本的相关性不高，则很可能在整个目标中出现次优检测。下面给出了支持此质量标志的建议默认阈值。 Minimum correlation threshold the minimum correlation between a test sample and any other sample for the test sample to be considered well-correlated. The default value is 0.98. Minimum coverage thresholdthe minimum median coverage for any sample (measured across all exons in the target) or exon (measured across all samples) to be considered well-covered. The default value is 100. Calling exon CNVsThe HMM transition probabilities are altered from ExomeDepth v1.0.0. to depend upon the distance between exons, so that exons adjacent in the list of targeted regions are treated independently if they are located so far apart on the chromosome that the probability of a germline variant spanning both exons is negligible, specifically: The probability of transitioning into a CNV state (from normal to deletion or from normal to duplication) is given by a constant transition probability specified by the user (set as default to .01). The probability of transitioning to a normal state from a CNV state (from deletion to normal or from duplication to normal) is given by a baseline probability scaled by the distance between exons. If the distance between these exons is 0, then this scaling factor is simply 1, but as the distance increases, the scaling factor tends to 0. This is given by **exp(−𝑙𝐸)∗1/𝑡** where 𝑙 is the distance from the previous exon; E is the expected CNV length in basepairs; and t is the baseline probability of returning to a normal state from a deletion/duplication. These values are set as E=50000 and t=.5. 软件安装测试过程发现，DeCon在R3.6.1版本，会存在较多兼容性问题，建议按官方推荐使用R3.1.2 1conda create -n r3.1.2 -c conda-forge r-base=3.1.2]]></content>
      <categories>
        <category>NGS</category>
        <category>编程拾慧</category>
        <category>software</category>
        <category>R</category>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>CNV</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[蜜蜂群图绘制]]></title>
    <url>%2F0002.%E5%BC%80%E5%8F%91-03.R%2FR-%E8%9C%9C%E8%9C%82%E7%BE%A4%E5%9B%BE%E7%BB%98%E5%88%B6%2F</url>
    <content type="text"><![CDATA[输出结果 绘图命令1bee_plot_with_color(&quot;../../Desktop/seqencing depth.csv&quot;,&quot;xlab&quot;,&quot;ylab&quot;) 函数代码函数代码 12345678910111213141516171819202122bee_plot_with_color=function(In_file,xlab_text,ylab_text)&#123;library(beeswarm)read.csv(In_file,header=F)-&gt;a#biocLite(c(&quot;beeswarm&quot;,&quot;ggplot2&quot;))label_tag=a[1,]label_tag=as.matrix(label_tag)a=a[-1,]ncol(a)-&gt;num1mat=c()for(i in 1:num1)&#123;as.numeric(as.matrix(a[,i]))-&gt;temptemp[!is.na(temp)]-&gt;templength(temp)-&gt;numrep(i,num)-&gt;label_temprbind(mat,cbind(temp,label_temp))-&gt;mat&#125;beeswarm(mat[,1]~mat[,2], pch = 1,col = rainbow(10),labels=label_tag ,xlab=xlab_text , ylab=ylab_text)#pch对应的不同数值可以调整散点图中点的样式；boxplot(mat[,1]~mat[,2],add=T,names=label_tag)-&gt;S&#125; 输入文件：每列对应一类数据，head行，对应类的名称，下面每行是该类的具体数据，最终针对每类数据绘制对应的散点图和4分位图。demo数据]]></content>
      <categories>
        <category>编程拾慧</category>
        <category>R</category>
      </categories>
      <tags>
        <tag>可视化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[文件格式说明 - Bed]]></title>
    <url>%2F0001.%E7%9F%A5%E8%AF%86-03.standard%2Ffileformat-bed%2F</url>
    <content type="text"><![CDATA[定义Browser Extensible Data (BED) is a whitespace-delimited file format, where each file consists of one or more lines Each line describes discrete genomic features by physical start and end position on a linear chromosome. The file extension for the BED format is .bed. 官方文档 格式说明格式概览一个完整的bed文件包含如下的12列信息(可以通过标注bedn只使用其中的前n列信息) Col Field Type Regex or range Brief description 1 chrom String [[:alnum:]_]{1,255} Chromosome name 2 chromStart Int [0, 264 − 1] Feature start position 3 chromEnd Int [0, 264 − 1] Feature end position 4 name String [^\t]{0,255} Feature description 5 score Int [0, 1000] A numerical value 6 strand String [-+.] Feature strand 7 thickStart Int [0, 264 − 1] Thick start position 8 thickEnd Int [0, 264 − 1] Thick end position 9 itemRgb Int,Int,Int ([0, 255], [0, 255], [0, 255]) \ 0 Display color 10 blockCount Int [0, chromEnd − chromStart]5 Number of blocks 11 blockSizes List[Int] ([[:digit:]]+,){blockCount−1}[[:digit:]]+,?6 Block sizes 12 blockStarts List[Int] ([[:digit:]]+,){blockCount−1}[[:digit:]]+,? Block start positions 各列内容的详细介绍如下： 坐标(Coordinates) chrom: The name of the chromosome or scaffold where the feature is present. Limitingonly to word characters only, instead of all non-whitespace characters, makes BED filesmore portable to varying environments which may make different assumptions about allowedcharacters. The name must be between 1 and 255 characters long, inclusive. chromStart: Start position of the feature on the chromosome or scaffold. chromStart must bean integer greater than or equal to 0 and less than the total number of bases of the chromo-some to which it belongs. If the size of the chromoschromosomeome is unknown, then chromStart mustbe less than or equal to $2^{64}$ − 1, which is the maximum size of an unsigned 64-bit integer. chromEnd: End position of the feature on the chromosome or scaffold. chromEnd must bean integer greater than or equal to the value of chromStart and less than or equal to the totalnumber of bases in the chromosome to which it belongs. If the size of the chromosomeis unknown, then chromEnd must be less than or equal to $2^{64}$ − 1, the maximum size of anunsigned 64-bit integer. 简单属性(Simple attributes) name: String that describes the feature. The name must be 0 to 255 non-tab characters. Thename must not be empty or contain whitespace, unless all fields in file are delimited exclusivelyusing single tab characters. A visual representation of the BED format may display the namenext to the feature. score: Integer between 0 and 1000, inclusive. If the feature has no score information, then 0should be used as a default value. A visual representation of the BED format may shadefeatures differently depending on their score. strand: Strand that the feature appears on. The strand may either refer to the + (sense orcoding) strand or the - (antisense or complementary) strand. If the feature has no strandinformation or unknown strand, then a dot (.) must be used. Display attributes thickStart: Start position at which the feature is visualized with a thicker or accented display.This value must be an integer between chromStart and chromEnd, inclusive. There is nospecified default value for thickStart. thickEnd: End position at which the feature is visualized with a thicker or accented display.This value must be an integer greater than or equal to thickStart and less than or equalto chromEnd, inclusive. In BED files with fewer than 7 fields, the whole feature has thickdisplay. In BED7+ files, to achieve the same effect, set thickStart equal to chromStartand thickEnd equal to chromEnd. If this field is not specified but thickStart is, then theentire feature has thick display. There is no specified default value for thickEnd. itemRgb: A triple of integers that determines the color of this feature when visualized. Thetriple is three integers separated by commas. Each integer is between 0 and 255, inclusive. Tomake a feature black, itemRgb may be a single 0, which is visualized identically to a featurewith itemRgb of 0,0,0. Blocks blockCount: Number of blocks in the feature. blockCount must be an integer greater than 0.blockCount is mandatory in BED12+ files. Null or empty blockCount are not allowed,because blockSizes and blockStarts rely on blockCount. A visual representation of the BEDformat may have blocks appear thicker than the rest of the feature. blockSizes: Comma-separated list of length blockCount containing the size of each block. Theremust be no spaces before or after commas. There may be a trailing comma after the lastelement of the list. blockSizes is mandatory in BED12+ files. Null or empty blockSizes is notallowed, because blockStarts cannot be verified without blockSizes. blockStarts: Comma-separated list of length blockCount containing each block’s start position,relative to chromStart. There must not be spaces before or after the commas. There maybe a trailing comma after the last element of the list. Each element in blockStarts is pairedwith the corresponding element in blockSizes. Each blockStarts element must be an integerbetween 0 and chromEnd−chromStart, inclusive. For each couple i of (blockStartsi, blockSizesi),the quantity chromStart + blockStartsi + blockSizesi must be less or equal to chromEnd. Theseconditions enforce that each block is contained within the feature. The first block muststart at chromStart and the last block must end at chromEnd. Moreover, the blocks must notoverlap. The list must be sorted in ascending order. blockStarts is mandatory in BED12+files. Null or empty blockStarts is not allowed. 术语和概念 （Terminology and concepts）0-start, half-open coordinate system: bed区间的位置描述为0起始，区间为左闭右开区间。A coordinate system where the first base starts at position 0, and the start of the interval is included but the end is not. For example, for a sequence of bases ACTGCG, the bases given by the interval [2, 4) are TG. BEDn: 表示文件包含bed的前n个字段（总计12个字段）A file with the first n fields of the BED format. For example, BED3 means a file with only the first 3 fields; BED12 means a file with all 12 fields. BEDn+: 表示包含前n个bed的定义字段，后续跟随一些自定义字段。A file that has n fields of the BED format, followed by any number of fields of custom data defined by a user. BEDn+m: 表示包含前n个bed的定义字段，后续跟随m个自定义字段。A file that has a custom tab-delimited format starting with the first n fields of the BED format, followed by m fields of custom data defined by a user. For example, BED6+4 means a file with the first 6 fields of the BED format, followed by 4 user-defined fields block: Linear subfeatures within a feature. Usually used to designate exons chromosome: 染色体编号 A sequence of nucleobases with a name. In this specification, “chromosome” may also describe a named scaffold that does not fit the biological definition of a chromosome. Often, chromosomes are numbered starting from 1. There are also often sex chromosomes such as W, X, Y, and Z, mitochondrial chromosomes such as M, and possibly scaffolds from an unknown chromosome, often labeled Un. The name of each chromosome is often prefixed with chr. Examples of chromosome names include chr1, 21, chrX, chrM, chrUn, chr19_KI270914v1_alt, and chrUn_KI270435v1. feature: A linear region of a chromosome with specified properties. For example, a file’s features might all be peaks called from ChIP-seq data, or transcript. field: Data stored as non-tab text. All fields are 7-bit US ASCII. file: Sequence of one or more lines. line: String terminated by a line separator, in one of the following classes. Either a data line, a comment line, or a blank line. Discussed more fully in subsection 1.3 line separator: Either carriage return, line feed, or carriage return followed by line feed. The same line separator must be used throughout the file. 示意12 [Bedwen]]]></content>
      <categories>
        <category>知识沉淀</category>
        <category>文件格式</category>
      </categories>
      <tags>
        <tag>文件格式</tag>
        <tag>Bed</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[文件格式说明 - Generic Feature Format Version 3 (GFF3)]]></title>
    <url>%2F0001.%E7%9F%A5%E8%AF%86-03.standard%2Ffileformat-gff3%2F</url>
    <content type="text"><![CDATA[简介Annotating Genomes with GFF3 or GTF filesGeneric Feature Format Version 3 (GFF3) v1.26GFF3 format Formatting requirements[1] seqid in GFF3/GTF column 1 should match the corresponding FASTA or ASN.1 file that is being annotated. For assemblies already in GenBank, seqids will be matched to their corresponding accessions if they are the same as what was used in the original submission. [The seqid is the text between the ‘&gt;’ and the first space in the fasta definition line; do not include the ‘&gt;’ in the GFF file] [2] contig, supercontig, chromosome and similar landmark features are not required and will be ignored. [3] multi-exon mRNA and other RNA features can be represented using either: [a] child exon features [b] child five_prime_UTR, CDS, and three_prime_UTR features [c] multiple RNA feature rows with the same ID Furthermore, whereas the GFF3 specifications require that all rows of a multi-exon CDS feature use the same ID, some commonly used software deviates from this requirement. To allow for deviations from the specifications, for eukaryotes the GenBank software assumes that multiple CDS rows with the same Parent attribute represent parts of the same CDS feature. Multiple CDS features for the same gene need to be annotated by using a separate mRNA Parent feature for each, so there is always a 1:1 relationship of mRNA to CDS, like in the following schematic:123456789gene1 ================================ ID=gene1mRNA1 ================================ ID=mRNA1;Parent=gene1five_prime_UTR == Parent=mRNA1CDS1 ==....=====...........== Parent=mRNA1 (3 rows)three_prime_UTR ====== Parent=mRNA1mRNA2 ================================ ID=mRNA2;Parent=gene1exon ==== Parent=mRNA2CDS2 ==....................== Parent=mRNA2 (2 rows)exon ======== Parent=mRNA2 [4] GFF3 ID attributes are required for interpreting parent-child feature relationships and that is their only role here. They are not automatically used for the locus_tag qualifier, so if the ID is applicable as the locus_tag, it should be copied into that attribute with the appropriate formatting. However, if no transcript_id, or protein_id qualifiers are present, then the GFF3 ID attribute will be used as the basis of those qualifiers, as described in point [5c] below. These qualifiers do not appear in the flatfile view, so if the GFF3 IDs are meant to be seen in that view, then they should be copied into a ‘note’ attribute with the appropriate formatting. [5] GFF3 Name attributes are ignored.]]></content>
      <categories>
        <category>知识沉淀</category>
        <category>文件格式</category>
      </categories>
      <tags>
        <tag>文件格式</tag>
        <tag>gff3</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[文件格式说明 - Vcf]]></title>
    <url>%2F0001.%E7%9F%A5%E8%AF%86-03.standard%2Ffileformat-vcf%2F</url>
    <content type="text"><![CDATA[简介VCF (Variant Call Format) version 4.1The VCF specification is no longer maintained by the 1000 Genomes Project. The group leading the management and expansion of the format is the Global Alliance for Genomics and Health Data Working group file format team, http://ga4gh.org/#/fileformats-teamThe main version of the specification can be found on https://github.com/samtools/hts-specsThis is under continued development, please check the hts-specs page for the most recent specification A PDF of the v4.1 spec is http://samtools.github.io/hts-specs/VCFv4.1.pdfA PDF of the v4.2 spec is http://samtools.github.io/hts-specs/VCFv4.2.pdfVCFTools host a discussion list about the specification called vcf-spec http://sourceforge.net/p/vcftools/mailman/ REF:http://blog.sina.com.cn/s/blog_12d5e3d3c0101qv1u.htmlhttp://samtools.github.io/hts-specs/VCFv4.2.pdfhttp://samtools.github.io/bcftools/bcftools.html VCF（Variant Call Format）文件示例（VCFv4.2）123456789101112131415161718192021222324##fileformat=VCFv4.2##fileDate=20090805##source=myImputationProgramV3.1##reference=file:///seq/references/1000GenomesPilot-NCBI36.fasta##contig=&lt;ID=20,length=62435964,assembly=B36,md5=f126cdf8a6e0c7f379d618ff66beb2da,species=&quot;Homo sapiens&quot;,taxonomy=x&gt;##phasing=partial##INFO=&lt;ID=NS,Number=1,Type=Integer,Description=&quot;Number of Samples With Data&quot;&gt;##INFO=&lt;ID=DP,Number=1,Type=Integer,Description=&quot;Total Depth&quot;&gt;##INFO=&lt;ID=AF,Number=A,Type=Float,Description=&quot;Allele Frequency&quot;&gt;##INFO=&lt;ID=AA,Number=1,Type=String,Description=&quot;Ancestral Allele&quot;&gt;##INFO=&lt;ID=DB,Number=0,Type=Flag,Description=&quot;dbSNP membership, build 129&quot;&gt;##INFO=&lt;ID=H2,Number=0,Type=Flag,Description=&quot;HapMap2 membership&quot;&gt;##FILTER=&lt;ID=q10,Description=&quot;Quality below 10&quot;&gt;##FILTER=&lt;ID=s50,Description=&quot;Less than 50% of samples have data&quot;&gt;##FORMAT=&lt;ID=GT,Number=1,Type=String,Description=&quot;Genotype&quot;&gt;##FORMAT=&lt;ID=GQ,Number=1,Type=Integer,Description=&quot;Genotype Quality&quot;&gt;##FORMAT=&lt;ID=DP,Number=1,Type=Integer,Description=&quot;Read Depth&quot;&gt;##FORMAT=&lt;ID=HQ,Number=2,Type=Integer,Description=&quot;Haplotype Quality&quot;&gt;#CHROM POS ID REF ALT QUAL FILTER INFO FORMAT NA00001 NA00002 NA0000320 14370 rs6054257 G A 29 PASS NS=3;DP=14;AF=0.5;DB;H2 GT:GQ:DP:HQ 0|0:48:1:51,51 1|0:48:8:51,51 1/1:43:5:.,.20 17330 . T A 3 q10 NS=3;DP=11;AF=0.017 GT:GQ:DP:HQ 0|0:49:3:58,50 0|1:3:5:65,3 0/0:41:320 1110696 rs6040355 A G,T 67 PASS NS=2;DP=10;AF=0.333,0.667;AA=T;DB GT:GQ:DP:HQ 1|2:21:6:23,27 2|1:2:0:18,2 2/2:35:420 1230237 . T . 47 PASS NS=3;DP=13;AA=T GT:GQ:DP:HQ 0|0:54:7:56,60 0|0:48:4:51,51 0/0:61:220 1234567 microsat1 GTC G,GTCT 50 PASS NS=3;DP=9;AA=G GT:GQ:DP 0/1:35:4 0/2:17:2 1/1:40:3 其中每列对应的含义 title 含义 CHROM： 表示变异位点是在哪个contig 里call出来的，如果是人类全基因组的话那就是chr1…chr22，chrX,Y,M。 POS： 参考基因组位置，第一个碱基的位置是1。按数字升序排列，允许有多条记录有相同的位置。变异位点相对于参考基因组所在的位置，如果是indel，就是第一个碱基所在的位置。 ID： 如果call出来的SNP存在于dbSNP数据库里，就会显示相应的dbSNP里的rs编号。 REF和REF： 在这个变异位点处，参考基因组中所对应的碱基和研究对象基因组中所对应的碱基。 QUAL： 可以理解为所call出来的变异位点的质量值。Q=-10lgP，Q表示质量值；P表示这个位点发生错误的概率。因此，如果想把错误率从控制在90%以上，P的阈值就是1/10，那lg（1/10）=-1，Q=（-10）*（-1）=10。同理，当Q=20时，错误率就控制在了0.01。 FILTER： 理想情况下，QUAL这个值应该是用所有的错误模型算出来的，这个值就可以代表正确的变异位点了，但是事实是做不到的。因此，还需要对原始变异位点做进一步的过滤。无论你用什么方法对变异位点进行过滤，过滤完了之后，在FILTER一栏都会留下过滤记录，如果是通过了过滤标准，那么这些通过标准的好的变异位点的FILTER一栏就会注释一个PASS，如果没有通过过滤，就会在FILTER这一栏提示除了PASS的其他信息。如果这一栏是一个“.”的话，就说明没有进行过任何过滤。 到现在，我们就可以解释上面的例子：chr1：873762 是一个新发现的T/G变异，并且有很高的可信度（qual=5231.78）。chr1：877664 是一个已知的变异为A/G 的SNP位点，名字rs3828047，并且具有很高的可信度（qual=3931.66）。chr1：899282 是一个已知的变异为C/T的SNP位点，名字rs28548431，但可信度较低（qual=71.77）。chr1：974165 是一个已知的变异为T/C的SNP位点，名字rs9442391，但是这个位点的质量值很低，被标 成了“LowQual”，在后续分析中可以被过滤掉。 其中最后面两列是相对应的，每一个tag对应一个或者一组值，如：chr1：873762，GT对应0/1；AD对应173,141；DP对应282；GQ对应99；PL对应255,0,255。 GT： 表示这个样本的基因型，对于一个二倍体生物，GT值表示的是这个样本在这个位点所携带的两个等位基因。0表示跟REF一样；1表示表示跟ALT一样；2表示第二个ALT。当只有一个ALT 等位基因的时候，0/0表示纯和且跟REF一致；0/1表示杂合，两个allele一个是ALT一个是REF；1/1表示纯和且都为ALT； The most common format subfield is GT (genotype) data. If the GT subfield is present, it must be the first subfield. In the sample data, genotype alleles are numeric: the REF allele is 0, the first ALT allele is 1, and so on. The allele separator is ‘/‘ for unphased genotypes and ‘|’ for phased genotypes.0 - reference call1 - alternative call 12 - alternative call 2AD： 对应两个以逗号隔开的值，这两个值分别表示覆盖到REF和ALT碱基的reads数，相当于支持REF和支持ALT的测序深度。DP： 覆盖到这个位点的总的reads数量，相当于这个位点的深度（并不是多有的reads数量，而是大概一定质量值要求的reads数）。PL: 对应3个以逗号隔开的值，这三个值分别表示该位点基因型是0/0，0/1，1/1的没经过先验的标准化Phred-scaled似然值（L）。如果转换成支持该基因型概率（P）的话，由于L=-10lgP，那么P=10^（-L/10），因此，当L值为0时，P=10^0=1。因此，这个值越小，支持概率就越大，也就是说是这个基因型的可能性越大。GQ： 表示最可能的基因型的质量值。表示的意义同QUAL。 举个例子说明一下：chr1 899282 rs28548431 C T [CLIPPED] GT:AD:DP:GQ:PL 0/1:1,3:4:25.92:103,0,26在这个位点，GT=0/1，也就是说这个位点的基因型是C/T；GQ=25.92，质量值并不算太高，可能是因为cover到这个位点的reads数太少，DP=4，也就是说只有4条reads支持这个地方的变异；AD=1,3，也就是说支持REF的read有一条，支持ALT的有3条；在PL里，这个位点基因型的不确定性就表现的更突出了，0/1的PL值为0，虽然支持0/1的概率很高；但是1/1的PL值只有26，也就是说还有10^(-2.6)=0.25%的可能性是1/1；但几乎不可能是0/0，因为支持0/0的概率只有10^(-10.3)=5*10-11。 各个版本的vcf之间的差异v4.1 -&gt; v4.2 的变动特征]]></content>
      <categories>
        <category>知识沉淀</category>
        <category>文件格式</category>
      </categories>
      <tags>
        <tag>文件格式</tag>
        <tag>Vcf</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[文件格式说明 - SAM/BAM]]></title>
    <url>%2F0001.%E7%9F%A5%E8%AF%86-03.standard%2Ffileformat-SAM.BAM%2F</url>
    <content type="text"><![CDATA[Sam 格式文献Sam 格式博客Sam 格式说明 Bam 头文件头文件是一行行以 @ 开头的注释文件，文件记录了比对是所用的参考序列信息，序列的名称和序列的长度（@SQ），同时也记录了比对、Markdup、重比对等过程所使用的软件以及对应的软件版本（@PG），同时还会有记录的样本信息（@RG）；如果有需要更改样本的头文件，可以通过Samtools rehead# 定义一个新的头文件，来替代原来的头文件. Bam头文件示例 Bam 头文件：123456789101112@HD VN:1.0 GO:none SO:coordinate@SQ SN:chr1 LN:249250621@SQ SN:chr2 LN:243199373@SQ SN:chr3 LN:198022430@SQ SN:chr4 LN:191154276@SQ SN:chr5 LN:180915260@SQ SN:chr6 LN:171115067@SQ SN:chr7 LN:159138663@RG ID:cancer PL:illumina PU:FCHF3HCBCX2 LB:17D0846999-98 SM:cancer CN:BGI@PG ID:MarkDuplicates PN:MarkDuplicates VN:1.98(1547) CL:net.sf.picard.sam.MarkDuplicates INPUT=[/THL4/home/bgi_thcancer/liubo/Project/BRAC_CY/2018-05-29/17D0846999-98/cancer/3_markdup_bam/17D0846999-98_sort.bam] OUTPUT=/THL4/home/bgi_thcancer/liubo/Project/BRAC_CY/2018-05-29/17D0846999-98/cancer/3_markdup_bam/17D0846999-98_sort_markdup.bam METRICS_FILE=/THL4/home/bgi_thcancer/liubo/Project/BRAC_CY/2018-05-29/17D0846999-98/cancer/3_markdup_bam/17D0846999-98_sort_markdup.bam.metrics REMOVE_DUPLICATES=false ASSUME_SORTED=true VALIDATION_STRINGENCY=LENIENT PROGRAM_RECORD_ID=MarkDuplicates PROGRAM_GROUP_NAME=MarkDuplicates MAX_SEQUENCES_FOR_DISK_READ_ENDS_MAP=50000 MAX_FILE_HANDLES_FOR_READ_ENDS_MAP=8000 SORTING_COLLECTION_SIZE_RATIO=0.25 READ_NAME_REGEX=[a-zA-Z0-9]+:[0-9]:([0-9]+):([0-9]+):([0-9]+).* OPTICAL_DUPLICATE_PIXEL_DISTANCE=100 VERBOSITY=INFO QUIET=false COMPRESSION_LEVEL=5 MAX_RECORDS_IN_RAM=500000 CREATE_INDEX=false CREATE_MD5_FILE=false@PG ID:bwa PN:bwa VN:0.6.2-r126@PG ID:GATK IndelRealigner VN:2.3-9-ge5ebf34 CL:knownAlleles=[(RodBinding name=knownAlleles source=/THL4/home/bgi_thcancer/pipeline/chip_1.7M/db/aln/dbsnp/dbSNP132_1000GIndel_merge_for_realgn.txt)] targetIntervals=/THL4/home/bgi_thcancer/liubo/Project/BRAC_CY/2018-05-29/17D0846999-98/cancer/4_realign_bam/17D0846999-98_cancer.intervals LODThresholdForCleaning=5.0 consensusDeterminationModel=USE_READS entropyThreshold=0.15 maxReadsInMemory=150000 maxIsizeForMovement=3000 maxPositionalMoveAllowed=200 maxConsensuses=30 maxReadsForConsensuses=120 maxReadsForRealignment=20000 noOriginalAlignmentTags=false nWayOut=null generate_nWayOut_md5s=false check_early=false noPGTag=false keepPGTags=false indelsFileForDebugging=null statisticsFileForDebugging=null SNPsFileForDebugging=null Bam 内容部分Bam 的内容部分，包含了所有序列的比对信息，包括比对位置，比对质量值查看方式 samtools view ;在Bam/Sam输出的结果中每一行都包括十二项通过Tab分隔，从左到右分别是： 序列的名称 概括出一个合适的标记，各个数字分别代表 1 read是pair中的一条（read表示本条read，mate表示pair中的另一条read） 2 pair一正一负完美的比对上 4 这条read没有比对上 8 mate没有比对上 16 这条read反向比对 32 mate反向比对 64 这条read是read1 128 这条read是read2 256 第二次比对 512 比对质量不合格 1024 read是PCR或光学副本产生 2048 辅助比对结果 假如说标记为以上列举出的数目，就可以直接推断出匹配的情况。假如说标记不是以上列举出的数字，比如说 83=（64+16+2+1），就是这几种情况值和。 参考序列的名字 在参考序列上的位置 mapping qulity?? 越高则位点越独特bowtie2有时并不能完全确定一个短的序列来自与参考序列的那个位置，特别是对于那些比较简单的序列。但是bowtie2会给出一个值来显示出 这个段序列来自某个位点的概率值，这个值就是mapping qulity。Mapping qulity的计算方法是：Q=-10log10p，Q是一个非负值，p是这个序列不来自这个位点的估计值。假如说一条序列在某个参考序列上找到了两个位点，但是其中一个位点的Q明显大于另一个位点的Q值，这条序列来源于前一个位点的可能性就比较大。Q值的差距越大，这独特性越高。Q值的计算方法来自与SAM标准格式，请查看SAM总结。 代表比对结果的CIGAR字符串，如37M1D2M1I，这段字符的意思是37个匹配，1个参考序列上的删除，2个匹配，1个参考序列上的插入。M代表的是alignment match(可以是错配) “M”表示 match或 mismatch； “I”表示 insert； “D”表示 deletion； “N”表示 skipped（跳过这段区域）； “S”表示 soft clipping（被剪切的序列存在于序列中）； “H”表示 hard clipping（被剪切的序列不存在于序列中）； “P”表示 padding； “=”表示 match； “X”表示 mismatch（错配，位置是一一对应的）； mate 序列所在参考序列的名称 mate 序列在参考序列上的位置 估计出的片段的长度，当mate 序列位于本序列上游时该值为负值。 read的序列 ASCII码格式的序列质量 可选的区域 AS:i? 匹配的得分 XS:i? 第二好的匹配的得分 YS:i? mate 序列匹配的得分 XN:i? 在参考序列上模糊碱基的个数 XM:i? 错配的个数 XO:i? gap open的个数 XG:i? gap 延伸的个数 NM:i? 经过编辑的序列 YF:i? 说明为什么这个序列被过滤的字符串 YT:Z MD:Z? 代表序列和参考序列错配的字符串]]></content>
      <categories>
        <category>知识沉淀</category>
        <category>文件格式</category>
      </categories>
      <tags>
        <tag>Bam</tag>
        <tag>Sam</tag>
        <tag>文件格式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Perl脚本的调试]]></title>
    <url>%2F0002.%E5%BC%80%E5%8F%91-01.perl%2FPerl-%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86-%E8%84%9A%E6%9C%AC%E8%B0%83%E8%AF%95%2F</url>
    <content type="text"><![CDATA[编程中错误不可避免，调试能够帮助我们发现有问题的代码段。在网上看了一下Perl脚本调试，发现其实很多东西并不需要，而且那么多也没人看。 下面简单整理一下。 1.进入debug。 使用-d，进入debug状态。例：perl -d Perl程序名称。 2.设置断点 b:设置断点。例：b 行号； c:程序执行到下一个断点处，或执行到指定行。例：c ；c 行号； d:删除一个断点。例：d 断点所在行号； D:删除所有断点。例：D； L:列出所有断点。例：L。 3.程序调试 n：执行下一行，跳过方法； s：执行下一行，如果是方法则进入方法体。# 所有引用的第三方包也会逐语句进行执行 T：程序的调用栈回退一级。 a：给程序的某一行加一个附加操作。在执行该行语句前先执行附加的操作。 例：a 行号 命令 。 R：重新启动正在调试的程序。 w：显示某行周围一窗（一屏）文件内容。 例: w 行号。 4.查看变量值 p：查看变量值。例：p 变量名； x：查看变量值并结构化显示。例：x 变量名。 W：监视变量值。被监视的变量在发生改变时，会打印输出。 例： W 变量名 （无变量则删除所有监控）。 V： 包名 变量名列表：显示指定包内的所有（或部分）变量的值。 **注：V、X命令中的变量名列表以空格分隔且变量名前应去掉$、@或% ** 使用工具进行调试 类似浏览器的F12功能，Perl也提供了调试工具ptkdb。使用该工具需要两个模型包：Tk和ptkdb。 使用如下命令可以安装着两个包 perl -MCPAN -e&apos;install Tk&apos; perl -MCPAN -e&apos;install Devel::ptkdb&apos;]]></content>
      <categories>
        <category>编程拾慧</category>
        <category>Perl</category>
      </categories>
      <tags>
        <tag>Perl</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Perl包：包安装工具-CPAN]]></title>
    <url>%2F0002.%E5%BC%80%E5%8F%91-01.perl%2FPerl-%E5%8C%85-cpan%E5%8C%85%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[镜像配置几个主要的CPAN站点有： 国内： ftp://freesoft.cgi.gov.cn/pub/languages/perl/CPAN http://cpan.qz.fj.cn/ 国外： http://www.cpan.org/ http://www.perl.com/CPAN-local/ 镜像配置CPAN (The Comprehensive Perl Archive Network) 镜像源的配置文件为 MyConfig.pm（一般位于 ~/.cpan/CPAN/MyConfig.pm），可使用包管理脚本 cpan 进行修改。123456- 生成镜像配置文件# 确保 MyConfig.pm 配置文件存在，如不存在则自动生成PERL_MM_USE_DEFAULT=1 perl -MCPAN -e &apos;mkmyconfig&apos;# 不使用默认配置，手动确认各个配置选项perl -MCPAN -e &apos;mkmyconfig&apos; 在 CPAN Shell 中手动设置镜像 在命令行中执行 cpan 进入 cpan shell：12345678910111213141516171819202122cpan shell -- CPAN exploration and modules installationEnter &apos;h&apos; for help.# 列出当前的镜像设置cpan[1]&gt; o conf urllist# 将本站镜像加入镜像列表首位# 注：若已在列表中则可跳过本步直接退出，修改列表不会执行自动去重cpan[2]&gt; o conf urllist unshift https://mirrors.tuna.tsinghua.edu.cn/CPAN/# 或将本站镜像加入镜像列表末尾# 注：本命令和上面的命令执行一个即可，修改列表不会执行自动去重cpan[3]&gt; o conf urllist push https://mirrors.tuna.tsinghua.edu.cn/CPAN/# 或清空镜像列表，仅保留本站cpan[4]&gt; o conf urllist https://mirrors.tuna.tsinghua.edu.cn/CPAN/# 保存修改后的配置至 MyConfig.pmcpan[5]&gt; o conf commit# 退出 cpan shellcpan[6]&gt; quit cpan安装perl包12perl -MCPAN -e shellinstall Set::IntervalTree 通过conda安装perl包不区分大小写、加上了perl前缀、中间以短横线连接12# conda install perl-module_nameconda install -c bioconda perl-set-intervaltree 下载包本地安装有些时候无法正常在线安装可以在 https://metacpan.org/pod/DBI]]></content>
      <categories>
        <category>编程拾慧</category>
        <category>Perl</category>
      </categories>
      <tags>
        <tag>Perl</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Perl包：包管理常用命令]]></title>
    <url>%2F0002.%E5%BC%80%E5%8F%91-01.perl%2FPerl-%E5%8C%85-%E5%B8%B8%E7%94%A8%E6%9F%A5%E8%AF%A2%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[包查询列出所有系统中已经安装的perl模块可以通过以下三种方式，查询系统中已经安装的所有perl模块 12345perldoc perllocalinstmodsh # 显示模块find `perl -e 'print "@INC"'` -name '*.pm' #显示模块对应的集群目录 查询单个模块查询 DBD::mysql 为例1perldoc -l DBD::mysql 查询安装的perl模块的版本号查询 DBD::mysql 为例 1perl -MDBD::mysql -e &apos;print DBD::mysql-&gt;VERSION. &quot;\n&quot;&apos; 输出系统中所有已安装的perl模块及版本12345678910111213#!/usr/bin/perluse strict;use ExtUtils::Installed;my $inst = ExtUtils::Installed-&gt;new();my @modules = $inst-&gt;modules();foreach (@modules) &#123; my $ver = $inst-&gt;version($_) || "???"; printf("%-22s -Version- %-22s\n", $_, $ver);&#125;exit;]]></content>
      <categories>
        <category>编程拾慧</category>
        <category>Perl</category>
      </categories>
      <tags>
        <tag>Perl</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Perl包：Statistics_Descriptive 数据统计]]></title>
    <url>%2F0002.%E5%BC%80%E5%8F%91-01.perl%2FPerl-%E5%8C%85-Statistics_Descriptive%E6%95%B0%E6%8D%AE%E7%BB%9F%E8%AE%A1%2F</url>
    <content type="text"><![CDATA[模块名： Statistics::Descriptive 使用方法简介构建对象12use Statistics::Descriptivemy $stat = Statistics::Descriptive::Full-&gt;new(); 导入数据12$stat-&gt;add_data(@a); # 导入数组$stat-&gt;add_data($a); # 导入数值 数据处理1$stat-&gt;sort_data(); 数据过滤1$stat-&gt;set_outlier_filter($code_ref); # 设置一个过滤函数，对数据进行过滤 计算统计指标12345678910111213141516171819202122my $mean = $stat-&gt;mean();#平均值my $variance = $stat-&gt;variance();#方差my $num = $stat-&gt;count();#data的数目my $standard_deviation=$stat-&gt;standard_deviation();#标准差my $sum=$stat-&gt;sum();#求和my $min=$stat-&gt;min();#最小值my $mindex=$stat-&gt;mindex();#最小值的indexmy $max=$stat-&gt;max();#最大值my $maxdex=$stat-&gt;maxdex();#最大值的indexmy $range=$stat-&gt;sample_range();#最小值到最大值print &quot;Number of Values = $num\n&quot;, &quot;Mean = $mean\n&quot;, &quot;Variance = $variance\n&quot;, &quot;standard_deviation = $standard_deviation\n&quot;, &quot;sum =$sum\n&quot;, &quot;min =$min\n&quot;, &quot;mindex=$mindex\n&quot;, &quot;max=$max\n&quot;, &quot;maxdex=$maxdex\n&quot;, &quot;range=$range\n&quot;;]]></content>
      <categories>
        <category>编程拾慧</category>
        <category>Perl</category>
      </categories>
      <tags>
        <tag>Perl</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[测序数据库-SRA]]></title>
    <url>%2F0001.%E7%9F%A5%E8%AF%86-01.NGS%2F2017-01-22.%E6%B5%8B%E5%BA%8F%E6%95%B0%E6%8D%AE%E5%BA%93-SRA%2F</url>
    <content type="text"><![CDATA[目前三个高通量数据合并针对动物，植物，微生物的划分进行去除；将目前的动物，植物．微生物三个高通量测序数据库进行整合；合并为一个数据库 高通量测序数据库 :合并原因： 有些项目有可能是存在跨物种的情况，进行分类的时候会产生问题（数据冗余或数据的缺失）； 物种的分类进行无法直接获取，如果区分需要人工整理，后期自动更新受影响； 提供物种检索后，数据库的合并不会收到影响； 高通量测序数据库 类别 NCBI DDBJ EBI 项目 PRJNAxxx / SRPxxx PRJDxxxxx 样品 SAMN03085625 / SRSxxx SAMDxxxxxx 实验 SRXxxxx DRAxxxxxx http://trace.ddbj.nig.ac.jp/bioproject/index_e.htmlhttp://trace.ddbj.nig.ac.jp/biosample/index_e.htmlhttp://trace.ddbj.nig.ac.jp/dra/index_e.html 数据获取 项目获取页面 NCBI项目项目ID 拼出xml文件下载路径：http://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=bioproject&amp;retmode=xml&amp;id=301661 样品获取页面 NCBI样品进入样品页面，获得样品的uid，拼出xml文件下载路径:http://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=biosample&amp;retmode=xml&amp;id=1047767 特殊案例 一个样品对应多个项目 一个样品对应多个实验 样品没有对应项目信息 一个实验对应多套测序数据]]></content>
      <categories>
        <category>知识沉淀</category>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>数据库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[生信相关软件索引]]></title>
    <url>%2F0003.%E5%BA%94%E7%94%A8-03.software%2F0000-00-01.software%2F</url>
    <content type="text"><![CDATA[常见文件格式 终端软件 下机数据质控 SOAPNuke 变异检测 功能软件 安装 bcftools 1.2, htslib-1.2.1, tabix12345678wget https://github.com/samtools/bcftools/releases/download/1.2/bcftools-1.2.tar.bz2tar xvf bcftools-1.2.tar.bz2cd bcftools-1.2makemake installcd htslib-1.2.1makemake install]]></content>
      <categories>
        <category>index</category>
        <category>NGS</category>
        <category>software</category>
      </categories>
      <tags>
        <tag>Software</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[生信相关公共数据库]]></title>
    <url>%2F0001.%E7%9F%A5%E8%AF%86-04.Database%2F0000-00-00.%E7%94%9F%E4%BF%A1%E7%9B%B8%E5%85%B3%E5%85%AC%E5%85%B1%E6%95%B0%E6%8D%AE%E5%BA%93%2F</url>
    <content type="text"><![CDATA[整体数据库列表|数据库|地址|简介||-|-|-||TCGA-GDC|https://portal.gdc.cancer.gov/|TCGA官网||GEPIA|http://gepia.cancer-pku.cn/|包含TCGA和GTEx|的9736个肿瘤和8687个正常对照样本的RNA-seq数据。可提供在线分析。||cBioPortal|http://www.cbioportal.org/|数据包括MUT（Mutation突变），CNA(Copy|Number|Alterations,拷贝数变化），EXP（mRNA|Expression，mRNA表达）和PORT/RPPA（Protein/|phosphoprotein|level，蛋白表达或磷酸化变化），部分数据含有临床信息。||MethHC|http://methhc.mbc.nctu.edu.tw/php/index.php|目前包含由Illumina|HumanMethylation450K|BeadChip产生的6548个DNA甲基化数据和由18个人癌症中的RNA-seq|/|miRNA-seq产生的12|567个mRNA|/|microRNA表达数据。||WebMEV|http://mev.tm4.org/#/welcome|可上传数据或下载TCGA或GEO数据||DriverDBv2|http://driverdb.tms.cmu.edu.tw/driverdbv2/index.php|肿瘤驱动基因查询||NCBI-gene|https://www.ncbi.nlm.nih.gov/gene|是分子生物学，生物化学，和遗传学知识的存储和分析的自动系统。||GeneCards|https://www.genecards.org/|收录关于人的蛋白质编码基因、假基因、RNA基因、遗传基因座、基因簇和未分类的基因等详细信息。||ICGC|https://icgc.org/|收集了50种不同癌症类型（或亚型）的肿瘤数据，其中包括基因异常表达，体细胞突变，表观遗传修饰，临床数据等。ICGC包括亚洲、澳大利亚、欧洲、北美和南美17个行政区的89项目，包括25000个癌症基因组。||HPA|https://www.proteinatlas.org/ |包括三大亚图谱：组织图谱、细胞图谱和病理图谱。可交互式展示数据。||UCSC|http://genome.ucsc.edu/|包含多个重要物种基因组草图，与ENCODE同步更新。可在线分析。||OncoKB|https://oncokb.org/ |包含有关554种癌症基因特定改变的详细信息，还有1级（FDA批准）、2级（标准护理）的治疗信息，3级临床证据和生物学证据。||MalaCards|https://www.malacards.org/|疾病相关基因查询||GEO|https://www.ncbi.nlm.nih.gov/gds|芯片数据库||SRA|https://www.ncbi.nlm.nih.gov/sra/|测序数据库||ArrayExpress|https://www.ebi.ac.uk/arrayexpress/|欧洲版GEO||DAVID|http://david.abcc.ncifcrf.gov/tools.jsp|功能富集分析||String|http://string-db.org/|蛋白互作查询+功能富集分析||GSEA|http://software.broadinstitute.org/gsea/index.jsp|功能富集分析| 规范化数据库 HGVS(Human Genome Variation Society) 人类基因组突变描述格式规范 主要的可变剪切注释 HGMD(The Human Gene Mutation Database,HGMD® ) 肿瘤相关数据库 TCGA肿瘤基因组图谱（TCGA）计划是由美国National Cancer Institute（NCI）和National Human Genome ResearchInstitute（NHGRI）于2006年联合启动的项目，作为目前最大的癌症基因信息数据库，收录33个癌种其中10个罕见癌种，及29种癌症器官，1万多个肿瘤样本，27万多份文件，含有多模式基因组学、表观基因组学和蛋白质组学数据。数据包括全基因组不同遗传特征的测量，如同一基因的DNA拷贝数、DNA甲基化、mRNA表达、SNP等。参考文献：The Cancer Genome Atlas (TCGA): an immeasurable source of knowledge cBioportalcBioPortal数据库整合了126个肿瘤基因组研究的数据，包括TCGA和ICGC等大型的肿瘤研究项目，涵盖了两万八千例标本的数据，此外部分样品还包括了临床预后等表型的信息。cBioPortal用于探索，可视化和分析多维癌症基因组学数据。将癌症组织和细胞系的分子谱分析数据简化为易于理解的遗传，表观遗传，基因表达和蛋白质组学事件。查询界面与定制数据存储相结合，使研究人员能够以交互方式探索样本，基因和途径的基因改变，并在基础数据中提供时将这些与临床结果联系起来。提供来自多个平台的基因水平数据的图形摘要，网络可视化和分析，生存分析，以患者为中心的查询和软件程序化访问。 Cosmic(Catalogue Of Somatic Mutations In Cancer)是目前世界上最大和最综合性的数据库，可以帮助我们探索癌症患者体细胞突变的功能及影响。癌症相关的体细胞位点，是整个网站的核心，收录了来自不同研究机构和数据库的体细胞突变数据，并提供了方便的浏览，检索，下载功能。 Cell Lines Projec对癌症研究中常用的细胞系样本进行深入研究，分析其突变信息。相比COSMIC, 整个项目中涵盖的变异数据会少一点。 COSMIC-3D 通过交互式的网页，展现了基因突变导致的蛋白结构域的变化。在搜索框中输入一个具体的基因名称或者蛋白名称，可以查看具体的记录。 Cancer Gene Census在癌症研究中，找到相关的突变基因是最核心的目的之一。通过对各种癌症进行调研，整理了一份癌症相关的突变基因列表，这份列表就是Cancer Gene Census,简称CGC。 在CGC种，将所有的癌症相关基因分成两类: Tier1 : 对于这部分基因，有充分的证据表明，正是由于这些基因的突变，导致癌症的进一步发生。 Tier2 : 对于这部分基因，只能说在癌症中检测到了大量该基因的突变，但是并没有充分证据表明该基因突变对癌症发生的影响。 My cancer Genome我的癌症基因组包含有关癌症相关基因，蛋白质和其他生物标志物类型的分子生物标志物在癌症中使用抗癌疗法的临床影响的信息。 ICGCICGC（International Cancer Genome Consortium，国际肿瘤基因组协作组），主要目标是全面阐明导致全球人类疾病负担的多种癌症中存在的基因组变化。收集了50种不同癌症类型（或亚型）的肿瘤数据，其中包括基因异常表达，体细胞突变，表观遗传修饰，临床数据等。ICGC包括亚洲、澳大利亚、欧洲、北美和南美17个行政区的89项目，包括25000个癌症基因组 OncoKBOncoKB是由Memorial Sloan Kettering癌症中心（MSK）维护的全面的精准肿瘤学知识库，包含来自FDA，NCCN或ASCO，ClinicalTrials.gov和科学文献的专业指导方针和建议，治疗策略，肿瘤专家或肿瘤协会共识，参考文献等信息。OncoKB目前包含有关554种癌症基因特定改变的详细信息，还有1级（FDA批准）、2级（标准护理）的治疗信息，3级临床证据和生物学证据。 人群数据库 ExAC/gnomAD.汇总了来自60,706个个体的完整外显子序列，并在这些志愿者的同意下，通过外显子集成联合（Exome Aggregation consortium ，ExAC，生物通译)共享这些序列信息。 1000 Genomes Project dbSNP Exome Variant Server :在欧洲和非洲裔美国血统的几个大群体的外显子组测序期间发现的变体数据库。 功能预测数据库 Polyphen2 SIFT MutationAssessor用Mutation-Assessor软件来看突变位点对基因或者蛋白功能的影响, MutationTaster [PhyloP]生成蛋白模型图 [PhastCons49]计算序列保守性 Human Splicing Finder为了更好地理解导致剪接缺陷的内含子和外显子突变，决定创建Human Splicing Finder网站。 该工具旨在帮助研究前mRNA剪接[更多关于剪接背景]。 MaxEntScan预测突变的软件？类似注释（内含子等） NetGene2A service producing neural network predictions of splice sites in human, C. elegans and A. thaliana DNA. The prediction output for both server and mail server consist of the prediction for both direct (+) and complementary (-) strand. The output lists the predictions for donor and acceptor sites in the submitted sequence, as well as branchpoint predictions (for A. thaliana only). NNSplice剪切位点预测。 GeneSplicer剪切位点预测。 其他数据库 UCSC ClinVarclinvar的注释，可以寻找出对应的基因变异信息，发生频率，表型，临床意义，评审状态以及染色体位置等。 HGMD疾病基因突变注释数据库，收集文献发表的基因变异与疾病的关系信息 GEOGEO数据库全称GENE EXPRESSION OMNIBUS，是由美国国立生物技术信息中心NCBI创建并维护的基因表达数据库，用于从任何物种或人造的来源检索基因表达数据。它创建于2000年，收录了世界各国研究机构提交的来自microarray，高密度寡核苷酸array（HAD），杂交膜（filter）和SAGE的许多类型的基因表达数据，目前已经发表的论文，论文中涉及到的基因表达检测的数据可以通过此数据库中找到。作为一个公共数据集合含有一系列预先计算的数据的定义和描述，以及用于交互检索和分析表达数据的在线工具。 MethHCMethHC专注于人类疾病的异常甲基化。MethHC整合了来自TCGA的DNA甲基化数据，基因表达数据和microRNA表达数据。MethHC目前包含由Illumina HumanMethylation450K BeadChip产生的6548个DNA甲基化数据和由18个人癌症中的RNA-seq / miRNA-seq产生的12 567个mRNA / microRNA表达数据。 HPA人类蛋白质表达图集（The human protein atlas）涵盖了17000个不同蛋白及26009种不同抗体的蛋白质水平分析。现在人类蛋白质图谱共包括三大亚图谱：组织图谱、细胞图谱和病理图谱。组织图谱包含了人类基因在RNA和蛋白质水平的表达信息。其中，蛋白质表达信息是来自免疫组化分析结果，依赖于无数商业化或者自制的抗体。细胞图谱包含人类细胞内蛋白质的空间信息。病理图谱涵盖了17种主要癌症类型、大约8000名病人的信息。病理图谱的一个创新是交互式生存散点图，可以以交互形式展示病人的生存数据。参考文献文档及文献：http://www.proteinatlas.org/about/publications StringString(search tool for the retrival of interacting genes/proteins)基因、蛋白质相互作用关系检索工具可以获取独特的，覆盖范围广的实验以及预测的相互作用关系信息。string提供的相互作用关系主要基于confidence score（可靠指数），以及其他附属信息，比如提供蛋白质域和3D结构。目前包括1100+个物种的5200+万蛋白质。构建蛋白质蛋白质相互作用网络可以用于过滤和评估功能性基因组学的数据，以及为注释蛋白质的结构、功能和进化性。 GeneCards基本覆盖了几大数据库对于基因的分析数据，是人类基因的综合数据库。该数据库整合了125个网站的基因数据中心的数据（包括HUGO(Human Gene Nomenclature Committee)、GDB(Genome Database)、MGD(Mouse Genome Database)等）。由以色列魏茨曼科学研究所维护的关于基因及其产物以及生物医学应用的文献库。GeneCards提供简明的基因组，蛋白质组，转录，遗传和功能上所有已知和预测的人类基因。GeneCards中的信息功能信息包括指向疾病的关系，突变和多态性，基因表达，基因功能，途径，蛋白质与蛋白质相互作用，相关的药物及化合物和切割等先进的研究抗体的试剂和工具等，重组蛋白，克隆，表达分析和RNAi试剂等。（还有各种数据库ID相互转换） GeneCards以卡片的形式给出结果，列出所查询基因的 1、官方名称，GDB同义列表、小鼠中的同源物、细胞遗传学定位、基因产物名称、产物功能，如在细胞中的作用、表达方式、定位、与其他蛋白质的同源性及其在疾病中的作用等； 2、相关基因家族； 3、相关疾病列表； 4、有关的研究论文； 5、医学应用，如根据该基因的有关知识而建立的新的治疗与诊断方法等。]]></content>
      <categories>
        <category>index</category>
        <category>知识沉淀</category>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>数据库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Vim常用命令]]></title>
    <url>%2F0003.%E5%BA%94%E7%94%A8-02.Linux%2FLinux-Vim-%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[在Vim中，光标的移动控制 命令 说明 H 左移动 J 下移动 K 上移动 L 右移动 w 移动到下一个单词 b 移动到上一个单词 行内跳转 命令 说明 w 到下一个单词的开头 e 到下一个单词的结尾 b 到前一个单词的开头 ge 到前一个单词的结尾 0或^ 到行头 $ 到行尾 f&lt;字母&gt; 向后搜索&lt;字母&gt;并跳转到第一个匹配的位置(非常实用) F&lt;字母&gt; 向前搜索&lt;字母&gt;并跳转到第一个匹配的位置 t&lt;字母&gt; 向后搜索&lt;字母&gt;并跳转到第一个匹配位置之前的一个字母(不常用) T&lt;字母&gt; 向前搜索&lt;字母&gt;并跳转到第一个匹配位置之后的一个字母(不常用) 切换为编辑状态的命令 命令 说明 i 在当前光标处进行编辑 I 在行首插入 A 在行末插入 a 在光标后插入编辑 o 在当前行后插入一个新行 O 在当前行前插入一个新行 cw 从光标所在位置开始插入编辑，同时删除该行中光标后面的文本 命令行模式下，退出Vim 命令 说明 :q! 强制退出，不保存编辑内容 :q 直接退出（仅在未更改文本内容时可用） :wq 或 :x 保存并退出 :wq! 强制保存并退出 :w （文件路径）将文档另存为文件路径，如果没有文件路径则保存原文件 :saveas 文件路径将文件另存为（文件路径） shift + zz 在普通模式下直接退出Vim，（对文件进行的更改会被保存） 其他快捷操作 命令 说明 .（小数点） 重复上一次的操作]]></content>
      <categories>
        <category>编程拾慧</category>
        <category>Linux</category>
        <category>Vim</category>
      </categories>
      <tags>
        <tag>Vim</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[云计算技术及性能优化]]></title>
    <url>%2F0003.%E5%BA%94%E7%94%A8-01.cloud_computing%2F0000-02-01.cloud_computing%2F</url>
    <content type="text"><![CDATA[]]></content>
      <categories>
        <category>云计算</category>
      </categories>
      <tags>
        <tag>流程开发</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[BGI工作索引]]></title>
    <url>%2F0000-01-00.BGI%E5%B7%A5%E4%BD%9C%E7%B4%A2%E5%BC%95%2F</url>
    <content type="text"><![CDATA[培训材料 知识点 指南参考]]></content>
      <categories>
        <category>index</category>
      </categories>
      <tags>
        <tag>BGI</tag>
      </tags>
  </entry>
</search>
