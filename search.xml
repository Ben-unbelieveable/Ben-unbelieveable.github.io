<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Manifold-Constrained Hyper-Connections]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-25.LLM-%E6%96%87%E7%8C%AE%E8%A7%A3%E6%9E%90%2F20251231.deepseek_v4%2F</url>
    <content type="text"><![CDATA[2025 年 12 月 31 日，DeepSeek 发布了论文《mHC：流形约束的超连接》（Manifold-Constrained Hyper-Connections）点击获取pdf原文，提出了一种新的残差连接设计，用于解决超连接（HC）在大模型扩展下的不稳定性和难以扩展的问题。]]></content>
      <categories>
        <category>算法原理</category>
      </categories>
      <tags>
        <tag>deepseek</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[1205.机器学习-强化学习-1.Q-Learning]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-20.MachineLearn%2F1205.%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0-1.Q-Learning%2F</url>
    <content type="text"><![CDATA[什么是 Q-Learning ？Q学习是强化学习中基于价值的学习算法。 假设机器人必须越过迷宫并到达终点。有地雷，机器人一次只能移动一个地砖。如果机器人踏上矿井，机器人就死了。机器人必须在尽可能短的时间内到达终点。 得分/奖励系统如下： 机器人在每一步都失去1点。这样做是为了使机器人采用最短路径并尽可能快地到达目标。 如果机器人踩到地雷，则点损失为100并且游戏结束。 如果机器人获得动力⚡️，它会获得1点。 如果机器人达到最终目标，则机器人获得100分。 现在，显而易见的问题是：我们如何训练机器人以最短的路径到达最终目标而不踩矿井？ 什么是 Q-Table ？Q-Table只是一个简单查找表的奇特名称，我们计算每个州的最大预期未来奖励。基本上，这张表将指导我们在每个州采取最佳行动。每个非边缘区块将有四个动作数。当机器人处于某种状态时，它可以向上或向下或向右或向左移动。 所以，让我们在Q-Table中对这个环境进行建模。在Q表中，列是动作，行是状态。每个Q表得分将是机器人在该状态下采取该行动时将获得的最大预期未来奖励。这是一个迭代过程，因为我们需要在每次迭代时改进Q-Table。 但问题是： 我们如何计算Q表的值？ 值是可用的还是预定义的？ 为了学习Q表的每个值，我们使用Q-Learning算法。 Q-Learning 的数学依据Q-Fuction 所述 Q-Fuction 使用Bellman方程和采用两个输入：状态（小号）和动作（一个）。 使用上面的函数，我们得到表中单元格的Q值。当我们开始时，Q表中的所有值都是零。有一个更新值的迭代过程。当我们开始探索环境时，通过不断更新表中的Q值， Q函数为我们提供了更好和更好的近似。 现在，让我们了解更新是如何进行的。 Q-Learning 算法的过程详解第1步：初始化Q表我们将首先构建一个Q表。有n列，其中n =操作数。有m行，其中m =状态数。我们将值初始化为0。在我们的机器人示例中，我们有四个动作（a = 4）和五个状态（s = 5）。所以我们将构建一个包含四列五行的表。 步骤2和3：选择并执行操作这些步骤的组合在不确定的时间内完成。这意味着此步骤一直运行，直到我们停止训练，或者训练循环停止，如代码中所定义。我们将根据Q-Table选择状态中的动作（a）。但是，如前所述，当剧集最初开始时，每个Q值都为0。 因此，现在探索和开发权衡的概念发挥作用。我们将使用一种叫做epsilon贪婪策略的东西。 一开始，$ε$利率会更高。机器人将探索环境并随机选择动作。这背后的逻辑是机器人对环境一无所知。 随着机器人探索环境，epsilon率降低，机器人开始利用环境。 在探索过程中，机器人逐渐变得更有信心估计Q值。 对于机器人示例，有四种操作可供选择：向上，向下，向左和向右。 我们现在开始训练 – 我们的机器人对环境一无所知。所以机器人选择随机动作，说对了。 Q-Learning 执行和操作我们现在可以使用Bellman方程更新Q值，使其处于开始和向右移动。 步骤4和5：评估现在我们采取了行动并观察了结果和奖励。我们需要更新功能 $Q(s，a)$。Q-Learning 评估 在机器人游戏的情况下，重申得分/奖励结构是：功率 = +1 ; 我的 = -100 ; 结束 = +100 ;我们将一次又一次地重复这一过程，直到学习停止。通过这种方式，Q表将会更新。 在Q-learning中，我们维护一张Q值表，表的维数为：状态数S * 动作数A，表中每个数代表在当前状态S下可以采用动作A可以获得的未来收益的折现和。我们不断的迭代我们的Q值表使其最终收敛，然后根据Q值表我们就可以在每个状态下选取一个最优策略。 referenceQ-Learning强化学习入门：基本思想和经典算法]]></content>
      <categories>
        <category>机器学习</category>
        <category>强化学习</category>
      </categories>
      <tags>
        <tag>Machine-Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[1205.机器学习-强化学习-0.概述]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-20.MachineLearn%2F1205.%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0-0.%E6%A6%82%E8%BF%B0%2F</url>
    <content type="text"><![CDATA[概念定义 强化学习（Reinforcement learning ，RL）是机器学习的三驾马车之一。讨论的问题是一个智能体(agent) 怎么在一个复杂不确定的环境(environment) 里面去极大化它能获得的奖励。通过感知所处环境的状态(state) 对 动作(action) 的 反应(reward)， 来指导更好的动作，从而获得最大的 收益(return)，这被称为在交互中学习，这样的学习方法就被称作强化学习。 强化学习主要有以下几个特点： 试错学习：强化学习一般没有直接的指导信息，Agent 要以不断与 Environment 进行交互，通过试错的方式来获得最佳策略(Policy)。 延迟回报：强化学习的指导信息很少，而且往往是在事后（最后一个状态(State)）才给出的。比如 围棋中只有到了最后才能知道胜负。 强化学习基本元素 环境(Environment) 是一个外部系统，智能体处于这个系统中，能够感知到这个系统并且能够基于感知到的状态做出一定的行动。 智能体(Agent) 是一个嵌入到环境中的系统，能够通过采取行动来改变环境的状态。 状态(State)/观察值(Observation)：状态是对世界的完整描述，不会隐藏世界的信息。观测是对状态的部分描述，可能会遗漏一些信息。 动作(Action)：不同的环境允许不同种类的动作，在给定的环境中，有效动作的集合经常被称为动作空间(action space)，包括离散动作空间(discrete action spaces)和连续动作空间(continuous action spaces)，例如，走迷宫机器人如果只有东南西北这 4 种移动方式，则其为离散动作空间;如果机器人向 360◦ 中的任意角度都可以移动，则为连续动作空间。 奖励(Reward)：是由环境给的一个标量的反馈信号(scalar feedback signal)，这个信号显示了智能体在某一步采 取了某个策略的表现如何。 应用场景 术语策略(Policy)策略是智能体用于决定下一步执行什么行动的规则。 可以是确定性的，一般表示为：$\mu$:$\alpha_t=\mu(\alpha_t|s_t)$，其中$\mu(\alpha_t|s_t)$表示在状态s_t下，策略$\mu$所选择的动作是$\alpha_t$的概率。 也可以是随机的，一般表示为 $\mu$:$\alpha_t~\pi(\cdot|s_t)$: 状态转移(State Transition)状态转移，可以是确定的也可以是随机的，一般认为是随机的，其随机性来源于环境。可以用状态密度函数来表示：$p(s’|s,a)=P(S’=s’|S=s, A=a)$环境可能会变化，在当前环境和行动下，衡量系统状态向某一个状态转移的概率是多少，注意环境的变化通常是未知的。 回报(Return)回报又称cumulated future reward，一般表示为 $U$，定义为$U_t=\sum_{k=0}^{\infty}R_{t+k}$ 其中, $R_{t+k}$表示第t+k时刻的奖励。agent的目标就是让Return最大化。未来的奖励不如现在等值的奖励那么好（比如一年后给100块不如现在就给），所以$R_t+1$的权重应该小于$R_t$。因此，强化学习通常用discounted return（折扣回报，又称cumulative discounted future reward），取$\lambda$为discount rate（折扣率），$\lambda \in (0,1] $，则有，$U_t=\sum_{k=0}^{\infty}\lambda^kR_{t+k}$ 价值函数(Value Function)举例来说，在象棋游戏中，定义赢得游戏得1分，其他动作得0分，状态是棋盘上棋子的位置。仅从1分和0分这两个数值并不能知道智能体在游戏过程中到底下得怎么样，而通过价值函数则可以获得更多洞察。价值函数使用期望对未来的收益进行预测，一方面不必等待未来的收益实际发生就可以获知当前状态的好坏，另一方面通过期望汇总了未来各种可能的收益情况。使用价值函数可以很方便地评价不同策略的好坏。 状态价值函数(State-value Function)：用来度量给定策略$\pi$的情况下，当前状态$s_t$的好坏程度。 动作价值函数(Action-value Function)：用来度量给定状态$s_t$和给定策略$\pi$下，执行动作$a_t$的好坏程度。算法算法分类 按照环境是否已知划分：免模型学习（Model-Free） vs 有模型学习（Model-Based） Model-free 就是不去学习和理解环境，环境给出什么信息就是什么信息，常见的方法有policy optimization和Q-learning。 Model-Based 是去学习和理解环境，学会用一个模型来模拟环境，通过模拟的环境来得到反馈。Model-Based相当于比Model-Free多了模拟环境这个环节，通过模拟环境预判接下来会发生的所有情况，然后选择最佳的情况。 一般情况下，环境都是不可知的，所以这里主要研究无模型问题。 按照学习方式划分：在线策略（On-Policy） vs 离线策略（Off-Policy） On-Policy 是指agent必须本人在场， 并且一定是本人边玩边学习。典型的算法为Sarsa。 Off-Policy 是指agent可以选择自己玩， 也可以选择看着别人玩， 通过看别人玩来学习别人的行为准则， 离线学习同样是从过往的经验中学习， 但是这些过往的经历没必要是自己的经历， 任何人的经历都能被学习，也没有必要是边玩边学习，玩和学习的时间可以不同步。典型的方法是Q-learning，以及Deep-Q-Network。 按照学习目标划分：基于策略（Policy-Based）和基于价值（Value-Based）。 Policy-Based 的方法直接输出下一步动作的概率，根据概率来选取动作。但不一定概率最高就会选择该动作，还是会从整体进行考虑。适用于非连续和连续的动作。常见的方法有Policy gradients。 Value-Based 的方法输出的是动作的价值，选择价值最高的动作。适用于非连续的动作。常见的方法有Q-learning、Deep Q Network和Sarsa。 更为厉害的方法是二者的结合：Actor-Critic，Actor根据概率做出动作，Critic根据动作给出价值，从而加速学习过程，常见的有A2C，A3C，DDPG等。 经典算法经典算法：Q-learning，Sarsa，DQN，Policy Gradient，A3C，DDPG，PPO下面我们挑选一些有代表性的算法进行讲解： 基于表格、没有神经网络参与的Q-Learning算法 基于价值(Value-Based)的Deep Q Network（DQN）算法 基于策略(Policy-Based)的Policy Gradient（PG）算法 结合了Value-Based和Policy-Based的Actor Critic算法。 referenceOpenAI: Kinds of RL Algorithms强化学习入门：基本思想和经典算法详解经典强化学习算法，搞定“阿尔法狗”下围棋]]></content>
      <categories>
        <category>机器学习</category>
        <category>强化学习</category>
      </categories>
      <tags>
        <tag>Machine-Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[1204.机器学习-集成学习-2.Boosting-LightBGM]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-20.MachineLearn%2F1204.%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0-2.Boosting-4.LightBGM%2F</url>
    <content type="text"><![CDATA[LightGBM简介GBDT (Gradient Boosting Decision Tree) 是机器学习中一个长盛不衰的模型，其主要思想是利用弱分类器（决策树）迭代训练以得到最优模型，该模型具有训练效果好、不易过拟合等优点。GBDT不仅在工业界应用广泛，通常被用于多分类、点击率预测、搜索排序等任务；在各种数据挖掘竞赛中也是致命武器，据统计Kaggle上的比赛有一半以上的冠军方案都是基于GBDT。而LightGBM（Light Gradient Boosting Machine）是一个实现GBDT算法的框架，支持高效率的并行训练，并且具有更快的训练速度、更低的内存消耗、更好的准确率、支持分布式可以快速处理海量数据等优点。 1.1 LightGBM提出的动机常用的机器学习算法，例如神经网络等算法，都可以以mini-batch的方式训练，训练数据的大小不会受到内存限制。而GBDT在每一次迭代的时候，都需要遍历整个训练数据多次。如果把整个训练数据装进内存则会限制训练数据的大小；如果不装进内存，反复地读写训练数据又会消耗非常大的时间。尤其面对工业级海量的数据，普通的GBDT算法是不能满足其需求的。 LightGBM提出的主要原因就是为了解决GBDT在海量数据遇到的问题，让GBDT可以更好更快地用于工业实践。 1.2 XGBoost的缺点及LightGBM的优化XGBoost的缺点在LightGBM提出之前，最有名的GBDT工具就是XGBoost了，它是基于预排序方法的决策树算法。这种构建决策树的算法基本思想是：首先，对所有特征都按照特征的数值进行预排序。其次，在遍历分割点的时候用O(#data)的代价找到一个特征上的最好分割点。最后，在找到一个特征的最好分割点后，将数据分裂成左右子节点。 这样的预排序算法的优点是能精确地找到分割点。但是缺点也很明显：首先，空间消耗大。这样的算法需要保存数据的特征值，还保存了特征排序的结果（例如，为了后续快速的计算分割点，保存了排序后的索引），这就需要消耗训练数据两倍的内存。其次，时间上也有较大的开销，在遍历每一个分割点的时候，都需要进行分裂增益的计算，消耗的代价大。最后，对cache优化不友好。在预排序后，特征对梯度的访问是一种随机访问，并且不同的特征访问的顺序不一样，无法对cache进行优化。同时，在每一层长树的时候，需要随机访问一个行索引到叶子索引的数组，并且不同特征访问的顺序也不一样，也会造成较大的cache miss。 LightGBM的优化为了避免上述XGBoost的缺陷，并且能够在不损害准确率的条件下加快GBDT模型的训练速度，lightGBM在传统的GBDT算法上进行了如下优化： 基于Histogram的决策树算法。 单边梯度采样 Gradient-based One-Side Sampling(GOSS)：使用GOSS可以减少大量只具有小梯度的数据实例，这样在计算信息增益的时候只利用剩下的具有高梯度的数据就可以了，相比XGBoost遍历所有特征值节省了不少时间和空间上的开销。 互斥特征捆绑 Exclusive Feature Bundling(EFB)：使用EFB可以将许多互斥的特征绑定为一个特征，这样达到了降维的目的。 带深度限制的Leaf-wise的叶子生长策略：大多数GBDT工具使用低效的按层生长 (level-wise) 的决策树生长策略，因为它不加区分的对待同一层的叶子，带来了很多没必要的开销。实际上很多叶子的分裂增益较低，没必要进行搜索和分裂。LightGBM使用了带有深度限制的按叶子生长 (leaf-wise) 算法。 直接支持类别特征(Categorical Feature) 支持高效并行 Cache命中率优化 下面我们就详细介绍以上提到的lightGBM优化算法。 LightGBM的基本原理2.1 基于Histogram的决策树算法（1）直方图算法Histogram algorithm应该翻译为直方图算法，直方图算法的基本思想是：先把连续的浮点特征值离散化成$k$个整数，同时构造一个宽度为$k$的直方图。在遍历数据的时候，根据离散化后的值作为索引在直方图中累积统计量，当遍历一次数据后，直方图累积了需要的统计量，然后根据直方图的离散值，遍历寻找最优的分割点。图：直方图算法 直方图算法简单理解为：首先确定对于每一个特征需要多少个箱子（bin）并为每一个箱子分配一个整数；然后将浮点数的范围均分成若干区间，区间个数与箱子个数相等，将属于该箱子的样本数据更新为箱子的值；最后用直方图（#bins）表示。看起来很高大上，其实就是直方图统计，将大规模的数据放在了直方图中。 我们知道特征离散化具有很多优点，如存储方便、运算更快、鲁棒性强、模型更加稳定等。对于直方图算法来说最直接的有以下两个优点： 内存占用更小：直方图算法不仅不需要额外存储预排序的结果，而且可以只保存特征离散化后的值，而这个值一般用 $8$位整型存储就足够了，内存消耗可以降低为原来的$\frac{1}{8}$ 。也就是说XGBoost需要用$32$位的浮点数去存储特征值，并用$32$位的整形去存储索引，而 LightGBM只需要用$8$位去存储直方图，内存相当于减少为$\frac{1}{8}$ ； 计算代价更小：预排序算法XGBoost每遍历一个特征值就需要计算一次分裂的增益，而直方图算法LightGBM只需要计算$k$次（ $k$可以认为是常数），直接将时间复杂度从$O(#data#feature)$降低到$O(k#feature)$，而我们知道$#data &gt;&gt; k$。 当然，Histogram算法并不是完美的。由于特征被离散化后，找到的并不是很精确的分割点，所以会对结果产生影响。但在不同的数据集上的结果表明，离散化的分割点对最终的精度影响并不是很大，甚至有时候会更好一点。原因是决策树本来就是弱模型，分割点是不是精确并不是太重要；较粗的分割点也有正则化的效果，可以有效地防止过拟合；即使单棵树的训练误差比精确分割的算法稍大，但在梯度提升（Gradient Boosting）的框架下没有太大的影响。 （2）直方图做差加速LightGBM另一个优化是Histogram（直方图）做差加速。一个叶子的直方图可以由它的父亲节点的直方图与它兄弟的直方图做差得到，在速度上可以提升一倍。通常构造直方图时，需要遍历该叶子上的所有数据，但直方图做差仅需遍历直方图的k个桶。在实际构建树的过程中，LightGBM还可以先计算直方图小的叶子节点，然后利用直方图做差来获得直方图大的叶子节点，这样就可以用非常微小的代价得到它兄弟叶子的直方图。注意：XGBoost 在进行预排序时只考虑非零值进行加速，而 LightGBM 也采用类似策略：只用非零特征构建直方图。 2.2 带深度限制的 Leaf-wise 算法在Histogram算法之上，LightGBM进行进一步的优化。首先它抛弃了大多数GBDT工具使用的按层生长 (level-wise) 的决策树生长策略，而使用了带有深度限制的按叶子生长 (leaf-wise) 算法。 XGBoost 采用 Level-wise 的增长策略，该策略遍历一次数据可以同时分裂同一层的叶子，容易进行多线程优化，也好控制模型复杂度，不容易过拟合。但实际上Level-wise是一种低效的算法，因为它不加区分的对待同一层的叶子，实际上很多叶子的分裂增益较低，没必要进行搜索和分裂，因此带来了很多没必要的计算开销。 LightGBM采用Leaf-wise的增长策略，该策略每次从当前所有叶子中，找到分裂增益最大的一个叶子，然后分裂，如此循环。因此同Level-wise相比，Leaf-wise的优点是：在分裂次数相同的情况下，Leaf-wise可以降低更多的误差，得到更好的精度；Leaf-wise的缺点是：可能会长出比较深的决策树，产生过拟合。因此LightGBM会在Leaf-wise之上增加了一个最大深度的限制，在保证高效率的同时防止过拟合。 2.3 单边梯度采样算法Gradient-based One-Side Sampling 应该被翻译为单边梯度采样（GOSS）。GOSS算法 从减少样本的角度出发，排除大部分小梯度的样本，仅用剩下的样本计算信息增益，它是一种在减少数据量和保证精度上平衡的算法。 AdaBoost中，样本权重是数据重要性的指标。然而在GBDT中没有原始样本权重，不能应用权重采样。幸运的是，我们观察到GBDT中每个数据都有不同的梯度值，对采样十分有用。即梯度小的样本，训练误差也比较小，说明数据已经被模型学习得很好了，直接想法就是丢掉这部分梯度小的数据。然而这样做会改变数据的分布，将会影响训练模型的精确度，为了避免此问题，提出了GOSS算法。 GOSS是一个样本的采样算法，目的是丢弃一些对计算信息增益没有帮助的样本留下有帮助的。根据计算信息增益的定义，梯度大的样本对信息增益有更大的影响。因此，GOSS在进行数据采样的时候只保留了梯度较大的数据，但是如果直接将所有梯度较小的数据都丢弃掉势必会影响数据的总体分布。所以，GOSS首先将要进行分裂的特征的所有取值按照绝对值大小降序排序（XGBoost一样也进行了排序，但是LightGBM不用保存排序后的结果），选取绝对值最大的 $a100%$ 个数据，。然后在剩下的较小梯度数据中随机选择 $b 100%$ 个数据。接着将这$b100% $ 个数据乘以一个常数 $\frac{1-a}{b}$, 这样算法就会更关注训练不足的样本，而不会过多改变原数据集的分布。最后使用这$(a+b)100%$ 个数据来计算信息增益。下图是GOSS的具体算法。 2.4 互斥特征捆绑算法高维度的数据往往是稀疏的，这种稀疏性启发我们设计一种无损的方法来减少特征的维度。通常被捆绑的特征都是互斥的（即特征不会同时为非零值，像one-hot），这样两个特征捆绑起来才不会丢失信息。如果两个特征并不是完全互斥（部分情况下两个特征都是非零值），可以用一个指标对特征不互斥程度进行衡量，称之为冲突比率，当这个值较小时，我们可以选择把不完全互斥的两个特征捆绑，而不影响最后的精度。互斥特征捆绑算法（Exclusive Feature Bundling, EFB）指出如果将一些特征进行融合绑定，则可以降低特征数量。这样在构建直方图时的时间复杂度从$O(#data#feature)$ 变为$O(#data#bundle)$ ，这里 $#bundle$指特征融合绑定后特征包的个数，且 $#bundle$ 远小于 $#feature$。 针对这种想法，我们会遇到两个问题： 怎么判定哪些特征应该绑在一起（build bundled）？ 怎么把特征绑为一个（merge feature）？ （1）解决哪些特征应该绑在一起将相互独立的特征进行绑定是一个 NP-Hard 问题，LightGBM的EFB算法将这个问题转化为图着色的问题来求解，将所有的特征视为图的各个顶点，将不是相互独立的特征用一条边连接起来，边的权重就是两个相连接的特征的总冲突值，这样需要绑定的特征就是在图着色问题中要涂上同一种颜色的那些点（特征）。此外，我们注意到通常有很多特征，尽管不是100％相互排斥，但也很少同时取非零值。 如果我们的算法可以允许一小部分的冲突，我们可以得到更少的特征包，进一步提高计算效率。经过简单的计算，随机污染小部分特征值将影响精度最多 ，$O([(1-\gamma)n]^{-2/3})$, $\gamma$ 是每个绑定中的最大冲突比率，当其相对较小时，能够完成精度和效率之间的平衡。具体步骤可以总结如下： 构造一个加权无向图，顶点是特征，边有权重，其权重与两个特征间冲突相关； 根据节点的度进行降序排序，度越大，与其它特征的冲突越大； 遍历每个特征，将它分配给现有特征包，或者新建一个特征包，使得总体冲突最小。 算法允许两两特征并不完全互斥来增加特征捆绑的数量，通过设置最大冲突比率 $\gamma$ 来平衡算法的精度和效率。EFB 算法的伪代码如下所示：算法3的时间复杂度是 $O(#feature^2)$ ，训练之前只处理一次，其时间复杂度在特征不是特别多的情况下是可以接受的，但难以应对百万维度的特征。为了继续提高效率，LightGBM提出了一种更加高效的无图的排序策略：将特征按照非零值个数排序，这和使用图节点的度排序相似，因为更多的非零值通常会导致冲突，新算法在算法3基础上改变了排序策略。 （2）解决怎么把特征绑为一捆特征合并算法，其关键在于原始特征能从合并的特征中分离出来。绑定几个特征在同一个bundle里需要保证绑定前的原始特征的值可以在bundle中识别，考虑到histogram-based算法将连续的值保存为离散的bins，我们可以使得不同特征的值分到bundle中的不同bin（箱子）中，这可以通过在特征值中加一个偏置常量来解决。比如，我们在bundle中绑定了两个特征A和B，A特征的原始取值为区间[0,10)，B特征的原始取值为区间[0,20），我们可以在B特征的取值上加一个偏置常量10，将其取值范围变为[10,30），绑定后的特征取值范围为 [0, 30），这样就可以放心的融合特征A和B了。具体的特征合并算法如下所示： LightGBM的工程优化我们将论文《Lightgbm: A highly efficient gradient boosting decision tree》中没有提到的优化方案，而在其相关论文《A communication-efficient parallel algorithm for decision tree》中提到的优化方案，放到本节作为LightGBM的工程优化来向大家介绍。 3.1 直接支持类别特征实际上大多数机器学习工具都无法直接支持类别特征，一般需要把类别特征，通过 one-hot 编码，转化到多维的0/1特征，降低了空间和时间的效率。但我们知道对于决策树来说并不推荐使用 one-hot 编码，尤其当类别特征中类别个数很多的情况下，会存在以下问题： 会产生样本切分不平衡问题，导致切分增益非常小（即浪费了这个特征）。使用 one-hot编码，意味着在每一个决策节点上只能使用one vs rest（例如是不是狗，是不是猫等）的切分方式。例如，动物类别切分后，会产生是否狗，是否猫等一系列特征，这一系列特征上只有少量样本为 1，大量样本为 0，这时候切分样本会产生不平衡，这意味着切分增益也会很小。较小的那个切分样本集，它占总样本的比例太小，无论增益多大，乘以该比例之后几乎可以忽略；较大的那个拆分样本集，它几乎就是原始的样本集，增益几乎为零。比较直观的理解就是不平衡的切分和不切分没有区别。 会影响决策树的学习。因为就算可以对这个类别特征进行切分，独热编码也会把数据切分到很多零散的小空间上，如下图左边所示。而决策树学习时利用的是统计信息，在这些数据量小的空间上，统计信息不准确，学习效果会变差。但如果使用下图右边的切分方法，数据会被切分到两个比较大的空间，进一步的学习也会更好。下图右边叶子节点的含义是X=A或者X=C放到左子节点，其余放到右子节点。 而类别特征的使用在实践中是很常见的。且为了解决one-hot编码处理类别特征的不足，LightGBM优化了对类别特征的支持，可以直接输入类别特征，不需要额外的0/1展开。LightGBM采用 many-vs-many 的切分方式将类别特征分为两个子集，实现类别特征的最优切分。假设某维特征有 k 个类别，则有$2^{k-1}-1$ 种可能，时间复杂度为 $O(2^k)$，LightGBM 基于 Fisher的《On Grouping For Maximum Homogeneity》论文实现了$O(klogkk)$的时间复杂度。 算法流程如下图所示，在枚举分割点之前，先把直方图按照每个类别对应的label均值进行排序；然后按照排序的结果依次枚举最优分割点。从下图可以看到，$\frac{Sum(y)}{Count(y)}$ 为类别的均值。当然，这个方法很容易过拟合，所以LightGBM里面还增加了很多对于这个方法的约束和正则化。图：LightGBM求解类别特征的最优切分算法 在Expo数据集上的实验结果表明，相比0/1展开的方法，使用LightGBM支持的类别特征可以使训练速度加速8倍，并且精度一致。更重要的是，LightGBM是第一个直接支持类别特征的GBDT工具。 3.2 支持高效并行（1）特征并行特征并行的主要思想是不同机器在不同的特征集合上分别寻找最优的分割点，然后在机器间同步最优的分割点。XGBoost使用的就是这种特征并行方法。这种特征并行方法有个很大的缺点：就是对数据进行垂直划分，每台机器所含数据不同，然后使用不同机器找到不同特征的最优分裂点，划分结果需要通过通信告知每台机器，增加了额外的复杂度。 LightGBM 则不进行数据垂直划分，而是在每台机器上保存全部训练数据，在得到最佳划分方案后可在本地执行划分而减少了不必要的通信。具体过程如下图所示。 （2）数据并行传统的数据并行策略主要为水平划分数据，让不同的机器先在本地构造直方图，然后进行全局的合并，最后在合并的直方图上面寻找最优分割点。这种数据划分有一个很大的缺点：通讯开销过大。如果使用点对点通信，一台机器的通讯开销大约为 $O(#machine #feature #bin)$ ；如果使用集成的通信，则通讯开销为$O(2 #feature #bin)$ 。LightGBM在数据并行中使用分散规约 (Reduce scatter) 把直方图合并的任务分摊到不同的机器，降低通信和计算，并利用直方图做差，进一步减少了一半的通信量。具体过程如下图所示。 （3）投票并行基于投票的数据并行则进一步优化数据并行中的通信代价，使通信代价变成常数级别。在数据量很大的时候，使用投票并行的方式只合并部分特征的直方图从而达到降低通信量的目的，可以得到非常好的加速效果。具体过程如下图所示。 大致步骤为两步： 本地找出 Top K 特征，并基于投票筛选出可能是最优分割点的特征； 合并时只合并每个机器选出来的特征。 3.3 Cache命中率优化XGBoost对cache优化不友好，如下图所示。在预排序后，特征对梯度的访问是一种随机访问，并且不同的特征访问的顺序不一样，无法对cache进行优化。同时，在每一层长树的时候，需要随机访问一个行索引到叶子索引的数组，并且不同特征访问的顺序也不一样，也会造成较大的cache miss。为了解决缓存命中率低的问题，XGBoost 提出了缓存访问算法进行改进。 而 LightGBM 所使用直方图算法对 Cache 天生友好： 首先，所有的特征都采用相同的方式获得梯度（区别于XGBoost的不同特征通过不同的索引获得梯度），只需要对梯度进行排序并可实现连续访问，大大提高了缓存命中率； 其次，因为不需要存储行索引到叶子索引的数组，降低了存储消耗，而且也不存在 Cache Miss的问题。 4. LightGBM的优缺点4.1 优点这部分主要总结下 LightGBM 相对于 XGBoost 的优点，从内存和速度两方面进行介绍。 （1）速度更快 LightGBM 采用了直方图算法将遍历样本转变为遍历直方图，极大的降低了时间复杂度； LightGBM 在训练过程中采用单边梯度算法过滤掉梯度小的样本，减少了大量的计算； LightGBM 采用了基于 Leaf-wise 算法的增长策略构建树，减少了很多不必要的计算量； LightGBM 采用优化后的特征并行、数据并行方法加速计算，当数据量非常大的时候还可以采用投票并行的策略； LightGBM 对缓存也进行了优化，增加了缓存命中率； （2）内存更小 XGBoost使用预排序后需要记录特征值及其对应样本的统计值的索引，而 LightGBM 使用了直方图算法将特征值转变为 bin 值，且不需要记录特征到样本的索引，将空间复杂度从 $O(2*#data)$ 降低为 $O(#bin)$，极大的减少了内存消耗； LightGBM 采用了直方图算法将存储特征值转变为存储 $bin$ 值，降低了内存消耗； LightGBM 在训练过程中采用互斥特征捆绑算法减少了特征数量，降低了内存消耗。 4.2 缺点 可能会长出比较深的决策树，产生过拟合。因此LightGBM在Leaf-wise之上增加了一个最大深度限制，在保证高效率的同时防止过拟合； Boosting族是迭代算法，每一次迭代都根据上一次迭代的预测结果对样本进行权重调整，所以随着迭代不断进行，误差会越来越小，模型的偏差（bias）会不断降低。由于LightGBM是基于偏差的算法，所以会对噪点较为敏感； 在寻找最优解时，依据的是最优切分变量，没有将最优解是全部特征的综合这一理念考虑进去； 实例数据集和代码均在GitHub reference精通梯度提升算法https://zhuanlan.zhihu.com/p/99069186https://lightgbm.cn/en/stable/index.html LightGBM论文解读：[1]. Ke G, Meng Q, Finley T, et al. Lightgbm: A highly efficient gradient boosting decision tree[C]//Advances in Neural Information Processing Systems. 2017: 3146-3154. [2]. Taifeng Wang分享LightGBM的视频，地址：https://v.qq.com/x/page/k0362z6lqix.html [3]. 开源|LightGBM：三天内收获GitHub 1000+ 星，地址：https://mp.weixin.qq.com/s/M25d_43gHkk3FyG_Jhlvog [4]. Lightgbm源论文解析：LightGBM: A Highly Efficient Gradient Boosting Decision Tree，地址：https://blog.csdn.net/anshuai_aw1/article/details/83048709 [5]. 快的不要不要的lightGBM - 王乐的文章 - 知乎 https://zhuanlan.zhihu.com/p/31986189 [6]. 『 论文阅读』LightGBM原理-LightGBM: A Highly Efficient Gradient Boosting Decision Tree，地址：https://blog.csdn.net/shine19930820/article/details/79123216 LightGBM算法讲解：[7]. 【机器学习】决策树（下）——XGBoost、LightGBM（非常详细） - 阿泽的文章 - 知乎 https://zhuanlan.zhihu.com/p/87885678 [8]. 入门 | 从结构到性能，一文概述XGBoost、Light GBM和CatBoost的同与不同，地址：https://mp.weixin.qq.com/s/TD3RbdDidCrcL45oWpxNmw [9]. CatBoost vs. Light GBM vs. XGBoost，地址：https://towardsdatascience.com/catboost-vs-light-gbm-vs-xgboost-5f93620723db [10]. 机器学习算法之LightGBM，地址：https://www.biaodianfu.com/lightgbm.html LightGBM工程优化：[11]. Meng Q, Ke G, Wang T, et al. A communication-efficient parallel algorithm for decision tree[C]//Advances in Neural Information Processing Systems. 2016: 1279-1287. [12]. Zhang H, Si S, Hsieh C J. GPU-acceleration for Large-scale Tree Boosting[J]. arXiv preprint arXiv:1706.08359, 2017. [13]. LightGBM的官方GitHub代码库，地址：https://github.com/microsoft/LightGBM [14]. 关于sklearn中的决策树是否应该用one-hot编码？ - 柯国霖的回答 - 知乎 https://www.zhihu.com/question/266195966/answer/306104444 LightGBM实例：[15]. LightGBM使用，地址：https://bacterous.github.io/2018/09/13/LightGBM%E4%BD%BF%E7%94%A8/ [16]. LightGBM两种使用方式 ，地址：https://www.cnblogs.com/chenxiangzhen/p/10894306.html LightGBM若干问题的思考：[17]. GBDT、XGBoost、LightGBM的区别和联系，地址：https://www.jianshu.com/p/765efe2b951a [18]. xgboost和lightgbm的区别和适用场景，地址：https://www.nowcoder.com/ta/review-ml/review?page=101]]></content>
      <categories>
        <category>机器学习</category>
        <category>集成学习</category>
      </categories>
      <tags>
        <tag>Machine-Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[二阶泰勒展开]]></title>
    <url>%2F01.%E7%9F%A5%E8%AF%86-02.Math%2F%E4%BA%8C%E9%98%B6%E6%B3%B0%E5%8B%92%E5%B1%95%E5%BC%80%2F</url>
    <content type="text"><![CDATA[核心思想泰勒展开的核心思想是：用一个多项式函数来近似地表示一个复杂函数。这个多项式在某个点（展开点）附近与原始函数具有非常相似的行为。 一阶泰勒展开就是我们熟悉的线性近似，它只考虑函数值和一阶导数（斜率），得到一个切线。 二阶泰勒展开在线性近似的基础上，增加了二阶导数（曲率） 的信息，从而能更好地捕捉函数的弯曲程度，通常比线性近似更精确。 二阶泰勒展开是利用函数在某一点的函数值、一阶导数和二阶导数来近似原函数的方法。相比于一阶展开（线性近似），它引入了曲率（Curvature）的概念，能够更精准地捕捉函数的局部形状，常用于无约束优化算法（如牛顿法）中寻找极小值点。]]></content>
      <categories>
        <category>知识沉淀</category>
      </categories>
      <tags>
        <tag>统计学</tag>
        <tag>Machine-Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[遗传病-遗传模式详解]]></title>
    <url>%2F01.%E7%9F%A5%E8%AF%86-11.%E9%81%97%E4%BC%A0%E7%97%85%2F%E9%81%97%E4%BC%A0%E7%97%85-%E9%81%97%E4%BC%A0%E6%A8%A1%E5%BC%8F%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[(一). 孟德尔规律遗传病通常将以孟德尔遗传规律表现的单基因遗传病按不同遗传模式分为： 常染色体显性遗传 (autosomal dominant inheritance)，简称常显或AD致病基因位于常染色体上，且由单个等位基因突变即可起病的遗传方式。常见的亚型包括： ①完全显性(正常纯合子AA和杂合子Aa患者在表型上无甚差别，如家族性腺瘤样息肉病)； ②不完全显性(杂合子Aa患者表型介于显性纯合子患者与正常人之间，常表现为轻病型，如软骨发育不良、家族性高胆固醇血症等)； ③不规则显性(由于某种原因可使杂合子Aa的显性基因不表现出相应的症状，如多指畸形、马方综合征等)； ④共显性(等位基因之间无显性与隐性之分，在杂合体时都能表现两种基因作用，如血型系统的抗原表达、人类白细胞抗原等)； ⑤延迟显性(杂合子Aa在生命早期显性基因并不表达，待一定年龄后才表达，如遗传性舞蹈病等)； ⑥从性显性(杂合子在不同性别中的表现型不同，如秃发等)。 常染色体隐性遗传 (autosomal recessive inheritance)，简称常隐或AR。位于常染色体上的致病基因在杂合状态Aa时不表现相应的疾病(称为携带者)，而只有在纯合子aa时才致病，如苯丙酮尿症、胱氨酸尿症、遗传性高度近视等。 X连锁显性遗传(X-linked inheritance)，简称XL位于X染色体上的致病基因随X染色体而传递疾病。包括: X连锁显性遗传(X-linked dominant inheritance)，简称XD。 (抗D佝偻病、i遗传性肾炎等) X连锁隐性遗传(X-linked recessive inheritance)，简称XR。 (血友病、i进行性肌营养不良等)。 Y连锁遗传(Y-linked inheritance)。定位于Y染色体的致病基因随Y染色体上而传递疾病，故亦称全男性遗传。如性别决定基因(SRY基因)突变所致的性反转等。 线粒体遗传 (mitochondrial inheritance) 。(二). 遗传印迹非孟德尔规律的遗传病是另一类新型遗传现象，即遗传印迹，亦称基因组印迹(genomic1imprinting)，是指控制某一表型的一对等位基因，因亲源不同而呈差异性表达，即机体转录来自亲本一方的等位基因，而与自身性别无关。如Prader-Willi综合征(母源单亲二体)和Angelman综合征(父源单亲二体)。印迹遗传还影响某些遗传病的表现度、外显率等发病特点。 References中国遗传咨询网]]></content>
      <categories>
        <category>遗传病</category>
        <category>性染色体</category>
      </categories>
      <tags>
        <tag>遗传病</tag>
        <tag>概念解析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[5024.大模型-模型优化-Ranking-00.概述]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-25.LLM-%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86%2F5024.%E5%A4%A7%E6%A8%A1%E5%9E%8B-%E6%A8%A1%E5%9E%8B%E4%BC%98%E5%8C%96-Ranking-00.%E6%A6%82%E8%BF%B0%2F</url>
    <content type="text"><![CDATA[在信息检索（IR）和推荐系统中，Ranking（排序） 是决定用户体验的核心环节。它不仅仅是给文档打分，更是将海量数据转化为用户可感知的有序列表的过程。根据技术演进的脉络，主流 Ranking 技术主要分为 传统算法 和 机器学习排序 (LTR) 两大阵营。 核心架构核心架构：从“投票”到“学习” 传统排序算法 (Traditional)基于图论或统计学规则，强调网页质量与权威性。 PageRank: Google 的基石，通过链接投票衡量网页重要性。 TF-IDF: 基于关键词频率与逆文档频率的文本匹配度计算。 BM25: 现代搜索引擎召回阶段的标准算法，结合了词频饱和度和文档长度惩罚。 PageRankGoogle 在 1998 年提出的革命性算法，解决了早期搜索引擎“关键词堆砌”的问题。是 Google 的基石，通过链接投票衡量网页重要性。123核心思想：民主表决 (Democratic Voting)阻尼因子：d (通常取 0.85)公式：PR(A) = (1-d) + d * Σ(PR(Ti)/C(Ti)) LambdaRank微软研究院提出的列表级排序算法，是目前的工业界标配。 梯度构造 (Gradient Construction)计算每个文档位置的变化对最终评估指标（如 NDCG）的梯度影响（Lambda 值）。 模型训练 (Model Training)使用梯度提升树（GBDT，如 XGBoost/LightGBM）来拟合这些梯度，从而直接优化 NDCG 指标。 机器学习排序 (LTR,Learn to Rank)这是目前大厂（如 Baidu, Google, ByteDance）的主流方案。LTR 将排序问题转化为一个监督学习问题，核心在于如何定义损失函数。利用历史点击/转化数据训练模型，预测用户偏好。 逐点法 (Pointwise)原理：将排序问题转化为单个文档的打分任务。通常使用线性回归模型 假设我们有 $n$ 个搜索结果，每个结果有 $m$ 个特征，特征向量表示为 $\mathbf{x}i = [x{i1}, x_{i2}, \cdots, x_{im}]，i = 1, 2, \cdots, n$。相关性得分表示为 $y_i$​。线性回归模型的公式为：$y_i = \mathbf{w}^T\mathbf{x}_i + b$其中，$\mathbf{w} = [w_1, w_2, \cdots, w_m]^T$ 是权重向量，bbb 是偏置项。 输入：Query + Document。输出：该文档的相关性得分（Score）。缺点：忽略了文档间的相对关系，可能导致同一 Query 下所有文档得分都很高但无区分度。 逐对法 (Pairwise)原理：关注文档对之间的胜负关系（A 是否比 B 更相关）。通常使用排序支持向量机（RankSVM）。对于两个搜索结果 $\mathbf{x}_i$​ 和 $\mathbf{x}_j$​，如果 $\mathbf{x}_i$​ 比 $\mathbf{x}j$​ 更相关，则 $y{ij} = 1$；否则 $y_{ij} = -1$。RankSVM的目标是找到一个超平面 $\mathbf{w}^T\mathbf{x} + b = 0$，使得不同相关性的结果对能够被正确分类。 输入：Query + (Doc A, Doc B)。输出：判断 A 应该排在 B 前面的概率。优势：直接优化了排序的相对顺序，效果优于 Pointwise。代表：RankNet, LambdaMART 列表法 (Listwise)原理：直接优化整个文档列表的排序质量。常用的是LambdaRank算法。其目标是最小化一个排序损失函数（如 NDCG）。输入：Query + Document List。输出：优化后的文档排序。优势：考虑了全局排序效果，效果优于 Pointwise 和 Pairwise。代表：LambdaRank, ListNet 常见框架LightGBM：是一个快速、高效的梯度提升框架，支持Learning to Rank任务。XGBoost：也是一个强大的梯度提升库，可用于排序学习。RankLib：是一个专门用于Learning to Rank的开源库，提供了多种排序算法的实现。 评估指标如何知道排序好不好？我们需要量化指标。 Precision@K: 前 K 个结果中有多少是相关的（准确率）。 Recall@K: 所有的相关结果中有多少出现在了前 K 个（召回率）。 NDCG@K (Normalized Discounted Cumulative Gain): 考虑了位置权重（越靠前越重要）且进行了归一化，是衡量排序质量的“黄金标准”。 指标 核心逻辑 特点 适用场景 NDCG@K 计算 DCG（折损累计增益）并除以 IDCG 进行归一化。 极度重视 Top-1, Top-2 的位置，受理想最大值影响大。对长尾敏感。 推荐系统、问答系统（用户只看前几条）。 MAP@K 先计算每个 Query 的 AveP（平均精度），再对所有 Query 取平均。 对长列表（Top-K 大）更友好，强调在不同截断长度下的综合表现。只关注是否出现，不关注位置（即便在k的尾部） 搜索、推荐系统、问答系统（用户只看前几条）、多标签分类。 NDCG@KNDCG (Normalized Discounted Cumulative Gain) 是推荐系统和信息检索中衡量排序质量的“黄金标准”。它解决了 DCG（折损累计增益）无法跨不同查询长度进行比较的问题，通过将实际得分归一化到 [0, 1] 区间来实现这一目标。不同于普通评估方法的二分类，NDCG 可以对文档的相关性进行进一步的分级。 核心公式与逻辑NDCG 的计算核心在于两个步骤： 首先计算实际排序下的 $DCG$， 然后计算理想排序下的 $IDCG$， 取比值,进行标准化 $NDCG = DCG/IDCG$。 可以看到评价的核心就是DCG。针对不同的排序计算DCG越接近 IDCG结果越好。而为了量化评估不同数据集合的排序效果,所以引入了标准化的过程。 DCG@k计算逻辑 定义每个文档的相关度。为每个文档分配一个相关性评分 rel_i​（如 0, 1, 2, 3…）。这是评价的基础。$$DCG_k=\sum_{i=1}^k (2^{rel_i}-1) * \frac{1}{log_2(i+1)}$$其中，$2^{rel_i}-1$是计算每个文档的绝对相关性(绝对指的是排序无关)。在使用中可以根据业务场景进行转换计算方式。$\frac{1}{log_2(i+1)}$ 则是考虑排序位置后进行的加权(排序靠后的影响降低),越靠前的文档权重越高。第一个权重为1，排第三的权重为0.5，排第七的权重为0.33，以此类推。只要我们知道了理想排序（相关性高的排在前面），和实际排序（排序策略的结果)后，就可以带入共识计算 对应的DCG@K。然后进行标准化结果。 MAP (Mean Average Precision)MAP (Mean Average Precision) 是衡量排序质量的另一座高峰。虽然它与 NDCG 都是“位置敏感”的指标，但它们的计算逻辑和侧重点截然不同。 核心逻辑和计算方式MAP 的计算可以拆解为两个步骤：AveP (单次查询) -&gt; MAP (全局平均)。 Step 1: 计算 AveP (Average Precision)对于每一个查询 $q$，遍历检索结果列表前 K 个检索结果：当遇到一个相关文档时，计算当前的 Precision（已检出的相关文档数 / 已检出的文档总数）。将这个 Precision 值累加。最终，AveP = 累加的 Precision 和 / 相关文档总数。$$AveP =\frac{1}{K} \sum_{k=1}^K(\frac{RelatedDocNum_{topk}}{k} I_k) $$$I_k$ 是一个指示器，用于指示当前文档是否相关。如果相关，$I_k=1$；否则，$I_k=0$。 Step 2: 计算 MAP将所有查询的 AveP 值求平均。$$MAP=\frac{1}{Q} * \sum_{q=1}^QAveP(q)$$其中 Q 是查询的数量，再一次方法评估种，我们需要评估计算多个不同的查询，而不是一个查询，Q指的是每个独立的查询结果。 HR（Hit Ratio）命中率$$ HR@K=\frac{NumberOfHits@K}{TotalTargetItems} ​$$分母是所有测试集合，分子表示每个用户top-K列表中属于测试集合个数的和举例：有3个用户，三个用户在测试集中的商品个数为$6,8,10$，得到的top5商品在测试集中的商品个数分别为$2,3,4$，那么$HR@5=\frac{2+3+4}{6+8+10}=0.375$ MRR(Mean Reciprocal Rank,MRR) 平均倒数排名关注推荐的项目是否靠前,每个关注检索结果越靠前，倒数值越大。所有结果都在前面可以获得最大的MRR值。$$ MRR=\frac 1N\sum_{i=1}^N\frac 1{p_i} $$​N: 预期目标点击的总数$p_i$​:$item_i$在推荐列表中出现的位置，如果没有出现就是 $p_i\rarr\infty$ 排序算法优劣相比直接使用规则系统，LTR 有以下优势： 优势 劣势 效果显著：相比纯规则系统，LTR 能大幅提升点击率和转化率。 冷启动难：新物品没有历史数据，难以获得高排名。 适应性强：可以处理复杂的非线性特征交互。 数据稀疏性：用户只点击第一个结果，导致负样本（未点击的高质量文档）难以获取。 可解释性：模型参数可以解释为特征的重要性，方便理解和调试。 计算成本：深度学习排序模型（Deep Ranking）推理耗时较长。 可扩展性：可以集成新的特征或模型，适应不断变化的需求。]]></content>
      <categories>
        <category>LLM</category>
      </categories>
      <tags>
        <tag>LLM</tag>
        <tag>Ranking</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[1001.特征工程-数据编码-图嵌入]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-20.MachineLearn%2F1001.%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B-%E6%95%B0%E6%8D%AE%E7%BC%96%E7%A0%81-%E5%9B%BE%E5%B5%8C%E5%85%A5%2F</url>
    <content type="text"><![CDATA[当数据呈现非欧几里得结构（如社交网络、推荐系统中的用户-物品关系）时，传统的向量空间方法失效，需要使用图嵌入技术。 DeepWalk 原理: 类似于 Word2Vec，通过随机游走 (Random Walk) 生成节点序列，再用 Skip-gram 学习向量。 特点: 简单易用，但随机游走策略固定，灵活性较差。 Node2Vec 原理: 在 DeepWalk 基础上引入 p 和 q 参数控制游走策略。 pp: 控制返回倾向（结构同质性）。 qq: 控制远跳倾向（结构随机性）。 特点: 能够灵活平衡“同质性”与“结构性”，在节点分类和链接预测任务中表现更优。 LINE (Large-scale Information Network Embedding) 原理: 基于矩阵分解，优化一阶和二阶相似度的定义。 特点: 特别适合处理有向图和带权图，计算效率远高于 DeepWalk。 123# 3. 获取图嵌入 (Node2Vec - 伪代码示意)# 通常需要调用 graph-tool 或 networkx 库配合训练model_node2vec.fit(graph_data)]]></content>
      <categories>
        <category>machine_learning</category>
      </categories>
      <tags>
        <tag>machine_learning</tag>
        <tag>特征工程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[1001.特征工程-数据编码-自然语言处理]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-20.MachineLearn%2F1001.%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B-%E6%95%B0%E6%8D%AE%E7%BC%96%E7%A0%81-%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86-BERT%2F</url>
    <content type="text"><![CDATA[Embedding（嵌入）技术的核心目标是将高维、稀疏的数据（如文本、图像、图结构）映射为低维、稠密的向量空间，从而捕捉数据间的语义关系。根据应用场景的不同，主要分为自然语言处理 (NLP)、图神经网络 (GNN) 和 多模态学习 三大领域。 自然语言处理过程中，文本的向量化，是其中重要的一环。NLP 领域的 Embedding 发展经历了从静态到动态、从单词到句子的演变。 静态词向量：Word2Vec / GloVe 原理: 基于统计学或神经网络模型，通过预测上下文或共现矩阵来学习词向量。 适用: 文本分类、信息检索。 特点: 训练速度快，但无法区分同一词在不同语境下的含义（如 “Apple” 是水果还是公司）。 动态上下文：ELMo / BERT 原理: 利用 RNN 或 Transformer 捕捉双向上下文，生成动态的词向量。 适用: 问答系统、复杂语义理解。 特点: 同一个词在不同句子中会有不同的向量表示，精度极高。 句子/段落：Sentence-BERT (SBERT) 原理: 基于 BERT 的 Siamese/Triplet 结构，直接对齐句子对的向量空间。 适用: 语义相似度搜索、聚类。 特点: 相比原生 BERT，推理速度提升数百倍。 技术 核心机制 优势 劣势 Word2Vec 局部窗口预测 训练极快，适合大规模语料 缺乏上下文感知，无法处理歧义 GloVe 全局矩阵分解 结合了全局统计信息和局部窗口 计算复杂度较高 BERT 双向 Transformer 深度理解语义，效果 SOTA 训练慢，推理时需同时输入两个句子，效率低 SBERT Siamese 网络 推理极速 (5秒 vs BERT 65小时) 需要额外微调，参数量大 示例代码：123456789import gensim# 1. 获取静态词向量 (Word2Vec)model_word2vec = gensim.models.Word2Vec.load("word2vec.model")vec = model_word2vec.wv['apple']# 2. 获取句子嵌入 (Sentence-BERT)from sentence_transformers import SentenceTransformermodel_sbert = SentenceTransformer('paraphrase-MiniLM-L6-v2')sentences = ["Hello world", "Hi there"]embeddings = model_sbert.encode(sentences) # 输出形状: [2, 768] BERTBERT（Bidirectional Encoder Representations from Transformers）是 Google 在 2018 年提出的革命性预训练模型。即双向Transformer的Encoder，因为decoder是不能获要预测的信息的。模型的主要创新点都在pre-train方法上，它只使用了transformer的encoder部分，它的整体框架是由多层transformer的encoder堆叠而成的。每一层的encoder则是由一层muti-head-attention和一层feed-forword组成，大的模型有24层，每层16个attention，小的模型12层，每层12个attention。每个attention的主要作用是通过目标词与句子中的所有词汇的相关度，对目标词重新编码。所以每个attention的计算包括三个步骤：计算词之间的相关度，对相关度归一化，通过相关度和所有词的编码进行加权求和获取目标词的编码。在通过attention计算词之间的相关度时，首先通过三个权重矩阵对输入的序列向量(512*768)做线性变换，分别生成query、key和value三个新的序列向量，用每个词的query向量分别和序列中的所有词的key向量做乘积，得到词与词之间的相关度，然后这个相关度再通过softmax进行归一化，归一化后的权重与value加权求和，得到每个词新的编码。 BERT模型输入在BERT中，输入的向量是由三种不同的embedding求和而成，分别是： wordpiece embedding：单词本身的向量表示。WordPiece是指将单词划分成一组有限的公共子词单元，能在单词的有效性和字符的灵活性之间取得一个折中的平衡。 position embedding：将单词的位置信息编码成特征向量。因为我们的网络结构没有RNN 或者LSTM，因此我们无法得到序列的位置信息，所以需要构建一个position embedding。构建position embedding有两种方法：BERT是初始化一个position embedding，然后通过训练将其学出来；而Transformer是通过制定规则来构建一个position embedding segment embedding：用于区分两个句子的向量表示。这个在问答等非对称句子中是用区别的。 BERT模型的输入就是wordpiece token embedding + segment embedding + position embedding，如图所示： 网络结构BERT的主要结构是transformer（如图1所示），一个BERT预训练模型的基础结构是标准transformer结构的 encoder 部分，一个标准transformer结构如图2所示，其中左边的部分就是BERT中使用的encoder部分。 一个transformer的encoder单元由一个multi-head-Attention + Layer Normalization + feedforword + Layer Normalization 叠加产生，BERT的每一层由一个这样的 encoder 单元构成。在比较大的BERT模型中，有24层encoder，每层中有16个Attention，词向量的维度是1024。在比较小的BERT模型中，有12层encoder，每层有12个Attention，词向量维度是768。在所有情况下，将feed-forward/filter 的大小设置为 4H（H为词向量的维度），即H = 768时为3072，H = 1024时为4096。这种transformer的结构可以使用上下文来预测mask的token，从而捕捉双向关系。 self-attention Layerself-attention出现的原因a、为了解决RNN、LSTM等常用于处理序列化数据的网络结构无法在GPU中并行加速计算的问题b、由于每个目标词是直接与句子中所有词分别计算相关度(attention)的，所以解决了传统的RNN模型中长距离依赖的问题。通过attention，可以将两个距离较远的词之间的距离拉近为1直接计算词的相关度，而传统的RNN模型中，随着距离的增加，词之间的相关度会被削弱。 模型的输入就是前面BERT模型的输入，即 X=(batch_size, max_len, embedding)，假设batch_size=1，输入的句子长度为512，每个词的向量表示的长度为768，那么整个模型的输入就是一个512*768的tensor。 单个self-attention 的计算过程每一次的self-attention的计算涉及到三个中间权重矩阵Wq,Wk,Wv，他们分别对输入的X进行线性变换，生成query、key和value这三个新的tensor，整个的计算步骤如 下： step 1：输入X分别与Wq,Wk,Wv矩阵相乘，得到Q,K,V。step 2：Q，K_T矩阵相乘，得到X中各个词之间的相关度，并scale（为了防止结果过大，除以他们维度的均方根）。step 3：将第二步的相关度通过Softmax函数归一化，得到归一化后各个词与其他词的相关度。step 4：将第三步的相关度矩阵与 V 相乘，即加权求和，得到每个词新的向量编码。 计算图如下所示：在BERT小模型中，每个head的神经元个数是64，12个head总的神经元的个数即为768，也就是模型介绍时说的H=768。在上图中单个的的Wq,Wk,Wv都是76864的矩阵，那么Q,K,V则都是51264的矩阵，Q，K_T相乘后的相关度矩阵则为512512，归一化后跟V相乘后的z矩阵的大小则为51264，这是一个attention计算出的结果。12个attention则是将12个51264大小的矩阵横向concat，得到一个512768大小的多头输出，这个输出再接一层768的全连接层，最后就是整个muti-head-attention的输出了。 multi-head attention的计算Multi-Head Self-Attention将多个不同单头的Self-Attention输出Concat成一条，然后再经过一个全连接层降维输出。例如，一个self-attention计算的输出为output_0 = (batch_size, max_len, w_length)，那么n个attention进行concat之后，输出就为output_sum = (batch_size, max_len,n * w_length)，这个concat的结果再连一层全连接层即为整个multi-head attention的输出。如下图所示，右边的部分即为一个multi-head attention的计算过程，其中的h指的是attention的个数，即上面例子中的n。 Layer NormalizationSelf-Attention的输出会经过Layer Normalization，为什么选择Layer Normalization而不是Batch Normalization？ 此时，我们应该先对我们的数据形状有个直观的认识，当一个batch的数据输入模型的时候，形状是长方体如图所示，大小为(batch_size, max_len, embedding)，其中batch_size为batch的批数，max_len为每一批数据的序列最大长度，embedding则为每一个单词或者字的embedding维度大小。而Batch Normalization是在batch间选择同一个位置的值做归一化，相当于是对batch里相同位置的字或者单词embedding做归一化，Layer Normalization是在一个Batch里面的每一行做normalization，相当于是对每句话的embedding做归一化。显然，LN更加符合我们处理文本的直觉。如下图所示。模型优化之Batch Normalization模型优化之Layer Normalization BERT 每一层的学习bert从浅层到高层可以分别学习到surface，短语级别的，句法级别的，和语义级别的信息；长程依赖需要更多层进行建模；What does BERT learn about the structure of language?译本:BERT的每一层学习到了哪些语义信息理解BERT每一层都学到了什么 模型预训练训练任务 masked language model随机掩盖掉一些单词，然后通过上下文预测该单词。BERT中有15%的wordpiece token会被随机掩盖，这15%的token中80%用[MASK]这个token来代替，10%用随机的一个词来替换，10%保持这个词不变。这种设计使得模型具有捕捉上下文关系的能力，同时能够有利于token-level tasks例如序列标注。 Q：为什么选中的15%的wordpiece token不能全部用 [MASK]代替，而要用 10% 的 random token 和 10% 的原 token[MASK] 是以一种显式的方式告诉模型『这个词我不告诉你，你自己从上下文里猜』，从而防止信息泄露。如果 [MASK] 以外的部分全部都用原 token，模型会学到『如果当前词是 [MASK]，就根据其他词的信息推断这个词；如果当前词是一个正常的单词，就直接抄输入』。这样一来，在 finetune 阶段，所有词都是正常单词，模型就照抄所有词，不提取单词间的依赖关系了。以一定的概率填入 random token，就是让模型时刻堤防着，在任意 token 的位置都需要把当前 token 的信息和上下文推断出的信息相结合。这样一来，在 finetune 阶段的正常句子上，模型也会同时提取这两方面的信息，因为它不知道它所看到的『正常单词』到底有没有被动过手脚的。 Q：最后怎么利用[MASK] token做的预测？最终的损失函数只计算被mask掉的token的，每个句子里 [MASK] 的个数是不定的。实际代码实现是每个句子有一个 maximum number of predictions，取所有 [MASK] 的位置以及一些 PADDING 位置的向量拿出来做预测（总共凑成 maximum number of predictions 这么多个预测，是定长的），然后再用掩码把 PADDING 盖掉，只计算[MASK]部分的损失。 next sentence prediction语料中50%的句子，选择其相应的下一句一起形成上下句，作为正样本；其余50%的句子随机选择一句非下一句一起形成上下句，作为负样本。这种设定，有利于sentence-level tasks，例如问答。注意：作者特意说了语料的选取很关键，要选用document-level的而不是sentence-level的，这样可以具备抽象连续长序列特征的能力。 模型训练设置pre-train阶段（1）256个句子作为一个batch,每个句子最多512个token。（2）迭代100万步。（3）总共训练样本超过33亿。（4）迭代40个epochs。（5）用adam学习率， 1 = 0.9, 2 = 0.999。（6）学习率头一万步保持固定值，之后线性衰减。（7）L2衰减，衰减参数为0.01。（8）drop out设置为0.1。（9）激活函数用GELU代替RELU。（10）Bert base版本用了16个TPU，Bert large版本用了64个TPU，训练时间4天完成。 论文定义了两个版本，一个是base版本，一个是large版本。 Large版本（ L=24, H=1024, A=16, Total Parameters=340M）。 base版本 （ L=12, H=768 , A=12, Total Parameters=110M）。L代表网络层数，H代表隐藏层数，A代表self attention head的数量。 因为序列长度太大（512）会影响训练速度，所以90%的steps都用seq_len=128训练，余下的10%步数训练512长度的输入。 fine-tune 阶段微调阶段根据不同任务使用不同网络模型。在微调阶段，大部分模型的超参数跟预训练时差不多，除了batchsize，学习率，epochs。微调参数建议： Batch size: 16, 32 Learning rate (Adam): 5e-5, 3e-5, 2e-5 Number of epochs: 3, 4 模型特点 使用transformer作为算法的主要框架，transformer能更彻底的捕捉语句中的双向关系； 使用了mask language model 和next sentence prediction的多任务训练目标，是一个自监督的过程，不需要数据的标注； 使用tpu这种强大的机器训练了大规模的预料，是NLP的很多任务达到了全新的高度。 BERT本质上是在海量语料的基础上，通过自监督学习的方法为单词学习一个好的特征表示。该模型的优点是可以根据具体的人物进行微调，或者直接使用预训练的模型作为特征提取器。 参考资料[1] Bengio Y, Ducharme R, Vincent P, et al. A neural probabilistic language model[J]. Journal of machine learning research, 2003, 3(Feb): 1137-1155. [2] Devlin J, Chang M-W, Lee K, et al. Bert: Pre-training of deep bidirectional transformers for language understanding[J]. arXiv preprint arXiv:1810.04805, 2018. [3] Vaswani A, Shazeer N, Parmar N, et al. Attention is all you need[C]. Advances in neural information processing systems, 2017: 5998-6008. BERT: Pre-training of Deep Bidirectional Transformers for Language UnderstandingNLP必读：十分钟读懂谷歌BERT模型BERT的原理与应用论文解读:BERT模型及fine-tuning]]></content>
      <categories>
        <category>machine_learning</category>
      </categories>
      <tags>
        <tag>machine_learning</tag>
        <tag>特征工程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MCP开发-01.从0开发一个MCP]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-12.MCP%E5%BC%80%E5%8F%91%2FMCP%E5%BC%80%E5%8F%91-01.%E4%BB%8E0%E5%BC%80%E5%8F%91%E4%B8%80%E4%B8%AAMCP%2F</url>
    <content type="text"><![CDATA[参考资料：https://blog.csdn.net/2401_84204207/article/details/149294973https://juejin.cn/post/7538733004224774186 如何开发一个MCP我们从0开始开发一个MCP，需要做的工作本身并不复杂。 创建一个MCP项目 在服务器端运行我们的MCP服务项目 在LLM中配置MCP信息，实现功能的调用。 只要有变成基础，我一直把所有的变成过程，都拆解成2个部分， 如何和外界交互，包括如何读取解析外部数据，以及保存和写入结果文件。 一个是使用语言内部的逻辑进行数据的处理。当然复杂的项目，需要考虑项目结构，模块划分，功能服用，结构优化，以及基于项目逻辑的并行串行和效率提升，但这都是跨越具体语言的编码思维，不是要给初学者需要考虑到，随着任何一门语言的深入应用，你都会一点点发现这些工作的意义和价值，并建立自己的逻辑，而且这些逻辑往往是可以通用的。 一步步实现自己的第一个MCP创建一个MCP项目代码在这里我们提供一个简单的模块功能，计算两个数的和。 1234567891011121314151617181920212223from mcp.server.fastmcp import FastMCP# 初始化MCP服务实例mcp = FastMCP("sum_int_tool")@mcp.tool()def sum(a: int,b: int) -&gt; int: """ 两个数相加求和 Args: a (int): 整数a b (int): 整数b Returns: int: 两个数的和 """ return a+bif __name__ == '__main__': # 启动MCP服务，使用标准输入输出作为传输方式 mcp.settings.host = "0.0.0.0" mcp.settings.port = 8000 mcp.run(transport='stdio') print("MCP服务已启动，等待工具调用...") 本地调用MCP配置MCP服务在支持MCP的 AI工具（如 trae）中，可以通过简单的JSON配置来注册和使用本地服务。 1234567891011&#123; &quot;mcpServers&quot;: &#123; &quot;sum_int_tool&quot;: &#123; &quot;command&quot;: &quot;python&quot;, &quot;args&quot;: [ &quot;D:\\gitlab_sync\\MCP\\0.demo.py&quot; ], &quot;autoApprove&quot;: [] &#125; &#125;&#125; 配置完成后，可以在工具中看到服务状态。 试MCP服务这时候，我们就可以提交一个任务，来看看能否成功的调用我们开发的MCP服务。可以看到大模型识别到了我们新创建的MCP服务，并使用MCP协议进行了调用，完成了我们需要的功能。 远程调用MCP服务MCP 支持三种数据传输方式：stdio、sse、streamable-http。 stdio (标准输入/输出) 这是最基础和通用的传输方式。它通过标准输入流读取数据，并向标准输出流写入结果，非常适合本地开发、命令行工具 (CLI) 和代理插件的集成。 sse (服务器发送事件) SSE 基于标准的 HTTP 协议，用于解决远程服务的访问问题。在此模式下，MCP 服务作为一个独立的 Web 服务器运行，能够处理来自多个客户端的并发连接。 streamable-http (可流式传输的HTTP) 这是MCP协议在新版本中引入的新传输机制，作为 SSE 的强大替代方案。它同样基于 HTTP POST/GET 请求，并利用服务器发送事件 (SSE) 的能力来高效地流式传输多条服务器消息。在我们测试阶段，使用的是stdio模式，更适合开发和访问，但是在需要远程启用MCP服务时，我们可以切换到sse 或者streamable-http模式。 切换传输方式只需在服务入口文件中修改一行代码，即可切换传输协议。123# 启动MCP服务，使用HTTP作为传输方式mcp.run(transport='http')mcp.run(transport='sse')]]></content>
      <categories>
        <category>LLM</category>
      </categories>
      <tags>
        <tag>MCP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[概念-基因结构介绍]]></title>
    <url>%2F01.%E7%9F%A5%E8%AF%86-01.NGS%2F%E6%A6%82%E5%BF%B5-%E5%9F%BA%E5%9B%A0%E7%BB%93%E6%9E%84%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[今天被问到一个问题，基因结构注释中的 CDS1\CDS2\5U1E 是什么意思。简单来说是编码区，但是想想好像从未系统的梳理过基因结构注释结果中的各个概念。所以借此梳理以下。 基因组指一个生物体或细胞内所有遗传物质的总和。它不仅包含基因，还包含非基因序列（如端粒、着丝粒等） DNA即脱氧核糖核酸，是基因组的化学本质。 核DNA 染色体 编码区: 直接参与蛋白质合成 Exon: 基因中被转录并保留在成熟 mRNA 中的片段。 5U1E: 位于起始密码子之前。影响核糖体结合及翻译效率 CDS ： 严格指 mRNA 上从起始密码子到终止密码子之间、能被翻译成蛋白质的核苷酸序列。它对应于基因组中的外显子序列（除去 UTR 部分） 3U1E：位于终止密码子之后。包含 miRNA 结合位点，调控 mRNA 稳定性和翻译效率 Intron：基因中位于两个外显子之间的间隔序列。在转录后被剪接体切除，不进入成熟 mRNA。 非编码区: 不直接编码蛋白质，但负责调控和结构功能 启动子 (Promoter)：通常位于基因转录起始位点的上游 (Upstream)（5’端附近）；功能：RNA 聚合酶识别并结合的位置，决定转录是否开始 增强子 (Enhancer)：可以位于基因的 上游、下游，甚至内含子中。无方向性。功能：像“加速器”一样显著增强启动子的活性，使基因在特定时空高表达。机制：通过染色质环化（Looping）物理接触启动子发挥作用。 线粒体DNA 编码区（CDS） 以起始密码子ATG的第一个碱基A开始，并记为c.1，以终止密码子（TAA, TAG, TGA）的最后一个碱基为终点。 内含子区（Intron） 靠近内含子5’末端的变异位点，需依据上游最近外显子的最后一个碱基来定位，如c.87+4，代表上游最近外显子的边界位置为87，变异位点在内含子5’ 端开始的第4个碱基； 靠近内含子3’ 末端的变异位点，要依据下游最近外显子的第一个碱基来定位，如c.88-11， 内含子碱基个数为偶数时，中间碱基平分后按上下游外显子碱基来定位命名，如…,c.87+676, c.87+677, c.87+678, c.88-678, c.88-677, c.88-676, … 内含子碱基个数为奇数时，中间碱基相对于上游外显子最后一个碱基来定位命名，如…,c.87+677, c.87+678, c.87+679, c.88-678, c.88-677, … 非编码区（UTR区）： 起始密码子ATG上游（5’ UTR区）标记为“-”，编号为c.-1, c.-2, c.-3… 终止密码子下游（3’ UTR区）标记为“”，编号为c.1, c.2, c.3… 位于靠近5’ UTR和3’ UTR区的内含子变异位点，命名规则同内含子区，如：5’ UTR区内含子为c.-85+1，c.-84-3等；3’ UTR区内含子为c.37+1，c.38-3等。 参考示意图如下：]]></content>
      <categories>
        <category>NGS</category>
        <category>基因</category>
      </categories>
      <tags>
        <tag>基础概念</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MCP开发-00.什么是MCP]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-12.MCP%E5%BC%80%E5%8F%91%2FMCP%E5%BC%80%E5%8F%91-00.%E4%BB%80%E4%B9%88%E6%98%AFMCP%2F</url>
    <content type="text"><![CDATA[为什么选择 MCP？MCP 帮助你在 LLM 的基础上构建代理（agents）和复杂的工作流。LLM 经常需要与数据和工具集成，而 MCP 提供了： 持续增长的预构建集成列表，LLM 可直接使用 灵活切换不同的 LLM 提供商和厂商 在你的基础设施内安全地处理数据的最佳实践 原本的大模型，只能进行文本的生成，不具备操作能力，而MCP为我们提供了一个标准化的接口规范，让大模型可以直接调用满足该规范的外延扩展功能，直接进行文件的操作、程序的执行，数据库的增删改查等等一系列操作，从而极大地提高了LLM所能实现功能的想象力。 MCP 是什么？MCP (Model Context Protocol) 是一种专为大型语言模型 (LLM) 设计的协议，为应用程序向 LLM 提供上下文的方式进行了标准化，旨在打通模型与外部数据源、工具及服务之间的壁垒，实现无缝集成。 你可以将 MCP 想象成 AI 应用程序的 USB-C 接口。就像 USB-C 为设备连接各种外设和配件提供了标准化的方式一样，MCP 为 AI 模型连接各种数据源和工具提供了标准化的接口。 MCP 协议文档中文版文档地址 MCP的架构介绍MCP应用架构​根据 MCP 协议的规定，在 MCP 协议中有以下对象： MCP Hosts: 如 Claude Desktop、IDE 或 AI 工具，希望通过 MCP 访问数据的程序； MCP Clients: 维护与服务器一对一连接的协议客户端； MCP Servers: 轻量级程序，通过标准的 Model Context Protocol 提供特定能力； 本地数据源: MCP 服务器可安全访问的计算机文件、数据库和服务； 远程服务: MCP 服务器可连接的互联网上的外部系统（如通过 APIs）； MCP 支持的数据传输方式 stdio (标准输入/输出) 这是最基础和通用的传输方式。它通过标准输入流读取数据，并向标准输出流写入结果，非常适合本地开发、命令行工具 (CLI) 和代理插件的集成。 sse (服务器发送事件) SSE 基于标准的 HTTP 协议，用于解决远程服务的访问问题。在此模式下，MCP 服务作为一个独立的 Web 服务器运行，能够处理来自多个客户端的并发连接。 streamable-http (可流式传输的HTTP) 这是MCP协议在新版本中引入的新传输机制，作为 SSE 的强大替代方案。它同样基于 HTTP POST/GET 请求，并利用服务器发送事件 (SSE) 的能力来高效地流式传输多条服务器消息。 MCP服务架构 目前可用的MCP协议和框架 OpenAI Agents SDKOpenAI 官方推出的 Agent 构建工具，支持 MCP 原生接入，提供如 MCPServerStdio 和 MCPServerSse 等类，适合生产环境使用，是早期 Swarm 实验的成熟版本。安装命令：pip install openai-agents Composio with OpenAI一个轻量级 SDK，用于将 OpenAI Agent 与 Composio 的托管式 MCP 服务器集成，自动完成工具注册、身份验证与通信对接。安装命令：pip install composio-openai openai mcp-agent by LastMile AI一个可组合的简单框架，基于 MCP 协议和工作流模式构建代理，同时兼容 OpenAI 的 Swarm 多代理编排思路，但对模型类型不设限。安装命令：pip install mcp-agent MCP Python SDK官方推出的 Python SDK，完整实现了 MCP 协议规范，提供快速创建 MCP 服务器的类（如 FastMCP）以及客户端连接组件。安装命令：pip install “mcp[cli]” MCP TypeScript SDK官方 TypeScript/Node SDK，适用于 JavaScript/TypeScript 生态，可用 McpServer 快速构建 MCP 服务端及客户端。安装命令：npm install @modelcontextprotocol/sdk Google ADKGoogle 开源的 Agent 开发工具包（ADK），原生集成 MCP 服务器与工具支持，还可接入其多智能体运行框架。安装命令：pip install google-adk CopilotKit MCP 支持一行命令即可将前端变成 MCP 客户端，快速连接任意 MCP 兼容服务器，实现工具共享、代理协作、多智能体编排。启动命令：npx copilotkit@latest init -m MCP LangChain MCP Adapters将 MCP 工具封装成 LangChain 可识别组件，便于在 LangGraph 等代理工作流中调用，非常适合已有 LangChain 项目的开发者。安装命令：pip install langchain-mcp-adapters Strands Agents SDK由 AWS 开源的 Agent 构建框架，支持 MCP，并兼容主流模型平台：Amazon Bedrock、Anthropic、LiteLLM、Llama、Ollama、OpenAI 等。安装命令：pip install strands-agents strands-agents-tools fast-agent支持 MCP 协议的全功能框架，覆盖工具调用、采样、多模态（图片/PDF）输入等，兼容 OpenAI 和 Anthropic 模型。安装命令：pip install fast-agent-mcp PraisonAI一个主打低代码体验的多智能体框架，支持单行代码接入 MCP，附带丰富文档与示例，支持 Brave、GitHub、Perplexity、Slack 等集成。安装命令：pip install praisonaiagents mcp Semantic Kernel微软推出的智能体编排 SDK，现已通过官方适配器支持 MCP 工具注册与调用，配合 Semantic Kernel Pipeline 使用效果更佳。安装命令：pip install semantic-kernel 支持MCP的工具]]></content>
      <categories>
        <category>LLM</category>
      </categories>
      <tags>
        <tag>MCP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Git-8.cherry-pick的使用]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-09.%E7%89%88%E6%9C%AC%E6%8E%A7%E5%88%B6%2FGit-8.cherry-pick%E7%9A%84%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[各种业务需求的存在，经常会导致我们存在不同的项目分支来应对不同的实际业务场景，开发分支上的某些更新不能上线，导致我们只上线开发分支中的某几个commit。这时候，我们就可以使用cherry-pick命令，将某个开发分支的commit提交到正式分支。或者将某个重复的更改同步到多个不同的业务分支上。 merge其他分支的某个commit首先在开发分支上获取要合并commit的commitID，1git log --oneline 然后切换到目标分支进行merge 12345git checkout main # 切换到要进行修改的分支git cherry-pick a1b2c3d4 # 合并 dev分支上的一个commit提交（-n 参数只合并变更不提交）git cherry-pick &lt;起始提交&gt;^..&lt;结束提交&gt; # 合并dev分支上一个范围的连续commit提交git cherry-pick &lt;commit1&gt; &lt;commit2&gt; &lt;commit3&gt; # 合并dev分支上多个独立的commitgit push origin main 常用的参数 12345678910111213141516171819202122git cherry-pick -h用法：git cherry-pick [选项] &lt;提交号&gt;... 或：git cherry-pick &lt;子命令&gt; --quit 终止反转或拣选操作 --continue 继续反转或拣选操作（基于当前合并的commit,继续合并下一个commit） --abort 取消反转或拣选操作 -n, --no-commit 不要自动提交 -e, --edit 编辑提交说明 -s, --signoff 添加 Signed-off-by: 签名 -m, --mainline &lt;n&gt; 父编号 --rerere-autoupdate update the index with reused conflict resolution if possible --strategy &lt;策略&gt; 合并策略 -X, --strategy-option &lt;选项&gt; 合并策略的选项 -x 追加提交名称 --ff 允许快进式 --allow-empty 保留初始化的空提交 --allow-empty-message 允许提交说明为空 --keep-redundant-commits 保持多余的、空的提交]]></content>
      <categories>
        <category>编程拾慧</category>
        <category>Git</category>
      </categories>
      <tags>
        <tag>培训</tag>
        <tag>Git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2.团队多元化建设]]></title>
    <url>%2F06.%E8%81%8C%E5%9C%BA-%E6%B2%9F%E9%80%9A%2F2.%E5%9B%A2%E9%98%9F%E5%A4%9A%E5%85%83%E5%8C%96%E5%BB%BA%E8%AE%BE%2F</url>
    <content type="text"><![CDATA[贝尔宾团队贝尔宾通过研究确定了九种有效促进团队绩效的行为集群，被称为贝尔宾团队角色。九种团队角色又可以被划分为三大类：思考型（Thinking）、行动型（Action）、社交型（Social）。 每个团队角色都有自己的贡献和缺点。贝尔宾衡量的是行为非个性。 明确我们的贝尔宾团队角色后，我们可以确保使用各自的优势来协作，并且以最佳方式来管理我们的弱势。 横向团队的四个发展阶段组建阶段状态： 彼此缺乏了解、对项目、分工、定位缺乏认识解决方式：引导成员熟悉彼此，加强沟通 ；明确各自的工作职责 磨合阶段状态： 彼此有一定的了解； 凭借资历、经验强调立场、行为和地位 ； 挑战管理者 ；解决方式： 时常召开团队会议，理解合作的意义和目标 规范阶段状态： 熟悉彼此的工作职责； 熟悉自己的定位解决方式： 激励、赞许等方式，认可努力和付出； 培育团队合作精神提高团队能力 合作阶段状态： 成员间产生了默契解决方式：朝最终目标努力 ； 制定、完成短期目标提高团队自信。]]></content>
      <categories>
        <category>职场培训</category>
      </categories>
      <tags>
        <tag>职场培训</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Software-污染识别-VerifyBamID]]></title>
    <url>%2F03.%E5%BA%94%E7%94%A8-01.software_in_NGS%2FSoftware-%E6%B1%A1%E6%9F%93%E8%AF%86%E5%88%AB-VerifyBamID%2F</url>
    <content type="text"><![CDATA[VerifyBamID 是一款软件，用于验证特定文件中的读取是否与个体（或一组个体）先前已知的基因型匹配，并检查读取是否因两个样本混合而受到污染。当外部基因型可用时， verifyBamID 可以检测样本污染和交换。当外部基因型不可用时， verifyBamID 仍然可以稳健地检测样本交换。githubverifyBamID2 文章：Ancestry-agnostic estimation of DNA sample contamination from sequence reads 软件简介使用方法12345VerifyBamID --DisableSanityCheck \ --SVDPrefix 1000g.phase3.10k.b37.exome.vcf.gz.dat \ --Reference hg19.genome.fa \ --BamFile $final_bam \ --Output $qc_dir/$sampleID 以下内容由AI解析完善 检测原理一、整体执行逻辑 载入参考/基因型信息（GenMatrixBinary） 根据 --SVDPrefix / 指定的 VCF/二进制前缀载入变异位点和/或已处理的矩阵/频率信息。实现里会： 读取 VCF（或预处理数据），按位点提取等位基频（AF）、位点基本信息及（当可用）参考样本的基因型/个体 ID。 过滤非常染色体位点（只保留 autosomes）。 应用阈值过滤（minAF、minCallRate 等）。 为后续步骤建立每个位点的等位基因频率或 genotype matrix。 在 BAM 上做 pileup（BamPileBases） 在上一步的位点集合上，对输入 BAM 做 pileup，按位点收集观测到的碱基、对应的碱基质量（Phred）、mapping quality、read group、以及各类过滤（SAM flags、minMapQ、minQ、maxDepth、是否忽略 overlapping pair 等）。 将 Phred 质量转换成错误率（e = 10^(-Q/10)），并对有上限的 Q（maxQ）做截断；实现中有 fPhred2Err[] 数组用于快速查表。 每个位点计算观测数据的似然（基于基因型+测序误差+参考偏差+污染模型） 对每个位点，基于该位点的等位基因频率/参考基因型信息和 pileup 的碱基/质量，计算在不同假设下（例如样本基因型为 REF/REF、REF/ALT、ALT/ALT；或在含污染 f 的混合模型下）观测到这些碱基的概率（位点似然）。 关键构件： 基于 Phred 得到单个碱基的“发射概率”：P(observed base | sampled allele) = 1 - e 当匹配，否则 ≈ e/3（或实现中对“其他碱基/插入”等有特殊处理）。 基于基因型的“抽样等位基因概率”：当基因型为 het 时，读到 Ref 或 Alt 的概率通常为 0.5/0.5，但实现允许“reference bias”参数来调整这些采样概率（源码中用 pSN 矩阵保存 Pr(sampled allele | true genotype)）。 当考虑污染（freemix）时，位点的观测被建模为两个个体的混合：观测 = (1 - f) * 源样本 + f * 污染者。实现里对污染者可能使用群体等位基频（按 AF 假设 Hardy-Weinberg）或在有参考个体时用具体个体基因型来对污染进行建模/积分。 位点似然是对该位点所有读的发射概率（通常相乘，取对数求和以避免下溢）。 全基因组（全位点集合）上对污染率 f、参考偏差等参数的估计与假设检验 通过在 f 的取值网格（VerifyBamIDArgs.grid，命令行默认 0.05）上计算全局 log-likelihood，先做粗略的网格搜索找出较好区域，再做局部优化（或 EM）来得到最大似然估计（MLE），这就是 freemix（估计的污染比例）。 若程序允许估计 reference-bias 或其他参数（由 bFreeMixOnly、bFreeRefBiasOnly、bFreeFull 等控制），会联合或交替估计这些参数以最大化总体似然。 计算对比统计量，例如： LLR（log-likelihood ratio）或 delta log-likelihood：比较 f = 0 (无污染假设) 与 f = mle 的对数似然差，用于衡量污染信号强度。 p-value / z-score（若实现提供）或其他基于似然比的显著性度量。 “自检 / 匹配参考个体”（self / best） 当 VCF 中包含个体基因型时，VerifyBamID 能评估该 BAM 是否来自 VCF 中给定或任一参考个体： 逐位点将测序似然与参考个体基因型进行比对，计算样本与参考个体相同/不同的似然或 posterior，从而输出“是否与某个 VCF 个体匹配”以及最佳匹配（best match）的 ID 和相应分数。 这一步与污染估计可以同时给出：“这个 BAM 看起来像哪个参考样本？” 以及“是否有显著污染”。 二、主要输出（摘要）与每个输出的计算/含义 下面列出 VerifyBamID 的常见主要结果项及其计算逻辑（不同版本在命名/格式上可能有差异，但基本统计是如下几类）： FREEMIX（或 mlefMix / estimated contamination f） 意义：估计的外源污染比例 f（0-1），表示测序读中有多少比例来自非目标样本的 DNA。 计算逻辑：最大化全位点总体似然得到的污染参数 MLE。总体似然为各位点似然的乘积（或对数和），每个位点的似然在污染模型下为： P(obs | f) = sum_{g_source} P(g_source | prior) * [ (1 - f) * P(obs | g_source) + f * P(obs | contaminant) ] — 其中 P(obs | g) 又是各读的发射概率乘积；contaminant 的成分可用群体 AF 折算成基因型分布并对其积分求和（即把污染者视为从群体中随机抽取）。 实现上常用网格搜索（指定 grid）+ 局部优化；输出通常是 MLE 的数值（如 0.023 表示 ~2.3% 污染）。 LLK / deltaLLK / LLR（对数似然/似然比） 意义：度量模型拟合改善程度，例如将 f = 0 与 f = mle 的对数似然差，或将“样本与某参考个体为同一人”的假设对比其它假设。 计算逻辑：对数似然 = sum_over_sites log P(obs_site | model_params)。LLR = 2*(logL(mle) - logL(null)) 或直接给出 logL 差值，用于判断是否显著偏离无污染假设。 BEST MATCH / SELF 判定（bestOut / selfOut） 意义：如果你传入了含基因型的 VCF，可得出样本最可能对应的 VCF 个体（best match），以及与给定样本 ID 的 self-check（样本是否与给定 ID 匹配）。 计算逻辑：对于每个参考个体 i，计算 P(obs | genotype_i)（或含污染模型下的观测似然），选取使似然最大的个体并输出相应统计（例如匹配概率、位点不一致数、覆盖位点数等）。 位点/总体覆盖深度与位点数（nMarkers、nBases、DP、mean depth） 意义：用于说明用于估计的位点数量、总读数或平均深度，及过滤后实际参与计算的位点数。 计算逻辑：在 pileup 步骤统计，考虑过滤条件（minMapQ、minQ、maxDepth、exclude flags、非 autosome 的跳过等），并累积有效读数与有效位点。 Call Rate / site call statistics 意义：在 VCF 与 BAM 的交叉位点中有多少位点可用（达到最小覆盖/质量），以及用于匹配/估计的有效位点比例。 计算逻辑：在读取 VCF 时按 minCallRate 过滤位点；在 BAM pileup 时按 minQ/minMapQ/maxDepth 等过滤读并决定该位点是否被接受为“可调用”。 Reference-bias 参数（若估计：pRefRef、pRefHet、pRefAlt） 意义：描述同一真实基因型下，测序/对齐导致读到参考/替代等位基因的偏向；若存在系统性偏向（例如对参考等位基因更容易比对/呼出），会影响基因型似然计算与污染估计。 计算逻辑：在模型中把“读到哪个等位基因”的概率从均等（het 情况下通常 0.5/0.5）改为可调参数 pSN；程序可以在自由/固定的设置下用最大似然估计这些参数，使得总体似然最大。源码中 pSN[i*3+j] 存储 Pr(sampled=i | true=j)。 质量控制/诊断项（例如不一致位点数、各等位基因读数分布、各 ReadGroup 统计等） 意义：帮助判断污染是否受某一文库/运行影响、是否需要按 read-group 分析等。 计算逻辑：由 pileup 阶段按 read group 分组统计碱基计数、深度、质量分布等；并在模型里可输出按 RG 的 contamination estimate（若实现支持）。 三、关键数学细节（更具体的概率表达式） 单碱基发射概率（简化表达）： e = 10^{-Q/10} （实现中 Q 超过 maxQ 时有截断） 若 sampled allele = a，observed base = b： P(b | sampled allele = a) = (1 - e) if b equals a ≈ e/3 if b different（或按 implementation 细节分配到“其他”类别） 基因型到 sampled allele 的概率（含 reference-bias）： pSN 表示 Pr(sampled allele = i | true genotype = j)；例如若 true genotype = homozygous ref，则 sampled allele 非 ref 的概率很低（由 pRefRef/pRefAlt 参数控制） 若 het，则默认 0.5/0.5，但可被 pRefHet 调整为偏向 ref 或 alt 含污染模型的位点似然（概念式）： P(obs_site | f) = sum_{g_source} P(g_source) prod_{reads r} [ (1 - f) P(read_r | g_source) + f * P(read_r | contaminant_model) ]contaminant_model 可以是基于群体 AF 的基因型分布积分（若不知道污染者是谁），或基于某参考个体的基因型（若做 pairwise 匹配）。 全局对数似然 = sum_over_sites log P(obs_site | params)。最大化这个值得到参数估计。 四、命令中特别选项的影响--DisableSanityCheck：跳过一些输入/文件一致性检查，程序会更快直接运行，但可能忽略格式/匹配错误。 --SVDPrefix 1000g.phase3.10k.b37.exome.vcf.gz.dat：表明使用预处理的 1000G（或对应）位点/矩阵信息作为参考（用于 AF、样本匹配或 SVD/投影以找 best match）。 --Reference hg19.genome.fa：参考基因组用于 alignment 相关的位点定位或某些比对相关过滤。 其它参数（默认为 VerifyBamIDArgs 中的值）：grid、minQ、maxDepth、minMapQ、genoError、minAF、minCallRate 等，都会影响位点筛选与似然计算。 五、给用户的实用说明（如何解读结果）FREEMIX（估计污染率）是最重要的输出之一：一个小值（如 &lt; 0.01）通常表明污染可以忽略；较大值（&gt; ~0.02–0.05）需要关注和可能的样本重做/去污处理。 LLR（或 delta log-likelihood）说明污染估计相较于无污染模型的支持度。数值越大，越支持存在污染。 BEST MATCH / SELF check：如果你有 VCF 的个体 ID，查看最佳匹配和匹配概率可以检验样本 ID 对应性（样本错配会在这里显著表现）。 覆盖深度/位点数：小的有效位点数或低平均深度会降低污染估计的精度（不可靠）。 若程序同时估计 reference-bias，应检查该参数是否显著偏离中性（0.5），大偏差说明技术偏向可能需要考虑在下游分析中纠正。 六、典型输出文件/行（示例性的说明）程序通常会在输出文件（你用 --Output sampleID 指定的前缀）或控制台打印一行或几行摘要，包括：sample ID / FREEMIX / LLR / #sites used / mean depth / bestMatchID / bestMatchScore 等。不同发布版具体字段名会有差别，但上面提到的统计项是核心。]]></content>
      <categories>
        <category>NGS</category>
        <category>software</category>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>污染</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[高效团队建设]]></title>
    <url>%2F06.%E8%81%8C%E5%9C%BA-%E6%B2%9F%E9%80%9A%2F1.%E9%AB%98%E6%95%88%E5%9B%A2%E9%98%9F%E5%BB%BA%E8%AE%BE%2F</url>
    <content type="text"><![CDATA[群体 vs 团队 团队的特征团队是一群人有共同目标的人。目标分为总目标和阶段目标，总目标是]]></content>
      <categories>
        <category>职场培训</category>
      </categories>
      <tags>
        <tag>职场培训</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[遗传病-拟常染色体]]></title>
    <url>%2F01.%E7%9F%A5%E8%AF%86-11.%E9%81%97%E4%BC%A0%E7%97%85%2F%E9%81%97%E4%BC%A0%E7%97%85-%E6%8B%9F%E5%B8%B8%E6%9F%93%E8%89%B2%E4%BD%93%2F</url>
    <content type="text"><![CDATA[概念拟常染色体区（Pseudoautosomal Region，简称 PAR），是位于人类 X 染色体和 Y 染色体两端的一段 DNA 序列。虽然它们位于性染色体上，却与常染色体具有相同的同源性，能够在减数分裂时进行交叉互换，因而得名“拟常染色体”。分为PAR1和PAR2两部分，拟常染色体区上已发现至少29个基因，这些基因表达类似常染色体，而非性染色体的伴性遗传模式。 区域在人类基因组中，PAR 被划分为三段（PAR1、PAR2、PAR3），其中最常被提及的是 PAR1 与 PAR2： 拟常染色体区在GRCh38 的位置是: | 名称 | 染色体 | 起始位点 | 结束位点 | 带 | | —- | ——- | ———– | ———– | —- | | PAR1 | X染色体 | 10,001 | 2,781,479 | Xp22 | | PAR1 | Y染色体 | 10,001 | 2,781,479 | Yp11 | | PAR2 | X染色体 | 155,701,383 | 156,030,895 | Xq28 | | PAR2 | Y染色体 | 56,887,903 | 57,217,415 | Yq12 | 拟常染色体区在GRCh37的位置是: | 名称 | 染色体 | 起始位点 | 结束位点 | | —- | ——- | ———– | ———– | | PAR1 | X染色体 | 60,001 | 2,699,520 | | PAR1 | Y染色体 | 10,001 | 2,649,520 | | PAR2 | X染色体 | 154,931,044 | 155,260,560 | | PAR2 | Y染色体 | 59,034,050 | 59,363,566 |PAR1 长约 2.6 Mbp，PAR2 仅约 0.32 Mbp，合计约占 Y 染色体全长的 5%。 意义 染色体配对与重组在男性的第一次减数分裂（精子形成）过程中，X 与 Y 染色体只能在 PAR 区域进行联会（配对）并发生交叉互换。这是性染色体能够正确分离的关键机制。若缺失或异常，常导致不育或染色体异常。 基因遗传方式PAR 区内的基因遵循常染色体的显性遗传规律，能够实现“男→男”传递，这种模式被称为假常染色体遗传（pseudoautosomal inheritance）。例如，位于 PAR 区的 SHOX 基因（负责身高发育）在男性和女性中均可表现出显性遗传特征。 基因数量目前已在 PAR 区鉴定出约 29 个功能基因，涉及生长、发育、免疫等多个生理过程。它们的遗传方式与常染色体基因相同，因而在遗传咨询和疾病预测中具有重要价值。 涉及基因所以拟常染色体区域的基因本身具有特殊性，比如他们会同时存在X染色体和Y染色体上，进而会有两个坐标位置。这也是我们在进行技术开发阶段需要特别注意的。 PAR1以下是目前已发现在人类拟常染色体区PAR1区域的基因，共计16个。在老鼠基因体中，有些PAR1的转基因到了常染色体。 ASMT，控制乙酰复合胺O-甲基转移酶（Acetylserotonin O-methyltransferase），促进褪黑素生合成的酵素产生。 ASMTL，控制乙酰复合胺O-甲基转移酶的相关构型蛋白（Acetylserotonin O-methyltransferase-like protein）的生成。 CD99，可增加T细胞的胞间黏着，控制T细胞的细胞凋亡，且和细胞迁移有关。 CRLF2，和细胞因子的受体有关。 CSF2RA，控制粒细胞-巨噬细胞集落刺激因子（Granulocyte macrophage colony-stimulating factor）之受体的生成，与造血干细胞的诱导分化有关。 SFRS17A。 DHRSXY，翻译出的蛋白质为短链脱氢酶（Short-chain dehydrogenase）的一种。 GTPBP6，过度表现将影响口头表现，为克氏综合征的症状之一。 IL3RA，和三型白细胞介素有关。 P2RY8，和G蛋白偶联受体有关。 PLCXD1。 PPP2R3B，可翻译出一种蛋白磷酸酶（Protein phosphatase 2），该蛋白质为四种主要的丝氨酸-苏氨酸磷酸酶（Ser/Thr phosphatases）之一，也和细胞成长的负回馈有关。 SHOX，控制身体发育。 SLC25A6。 XG，控制红细胞表面的一种抗原。 ZBED1。 PAR2以下是目前已发现在拟常染色体区PAR2区域的基因，共计3个，另有一个受关注的伪基因。 SPRY3 SYBL1 IL9R CXYorf1，现改称WASH6P，被认为是伪基因，但因其靠近端粒而颇受关注。]]></content>
      <categories>
        <category>遗传病</category>
        <category>性染色体</category>
      </categories>
      <tags>
        <tag>遗传病</tag>
        <tag>概念解析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[遗传病-LOH、ROH、LCSH、AOH相关概念]]></title>
    <url>%2F01.%E7%9F%A5%E8%AF%86-11.%E9%81%97%E4%BC%A0%E7%97%85%2F%E9%81%97%E4%BC%A0%E7%97%85-LOH.ROH%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5%2F</url>
    <content type="text"><![CDATA[ROH、LCSH、LOH、AOH一直以来很容易被混淆，这些名词究竟有什么区别呢？ 概念 ROH（region of homozygosity）：基因组在拷贝数正常状态下存在的连续且纯合的区域（两个等位基因相同），通常由遗传自共同祖先的相同染色体片段导致，常见于近亲繁殖的个体中， LOH（loss of heterozygosity）：杂合性丢失(原本应为杂合状态的基因组区域（即两个等位基因不同）因一个等位基因丢失或缺失而变为纯合状态)，表现为该区域内不存在杂合的状态；可以是拷贝数为2，也可以是发生缺失拷贝数为1的情况；通常与染色体片段缺失、基因重组或复制错误相关。 AOH（absence of heterozygosity）：杂合性不存在，和LOH一样，可以是拷贝数为2或发生缺失拷贝数为1的情况； LCSH（long continuous stretches of homozygosity）：长连续性纯合片段，与ROH本质是一样的； 形成机制 LOH 多由体细胞突变（如染色体缺失、有丝分裂重组）引起，常见于癌症等疾病中，导致抑癌基因失活。 ROH 通常源于遗传（如父母具有共同祖先），反映血缘同源（Identity by Descent, IBD），用于评估近亲程度或种群遗传结构。当然除了IBD,还可能由于染色体自救导致的单亲二倍体（Uniparental Disomy，UPD)。 IBD概念血缘同源IBD（identity-by descent）：亲属个体基因序列由某一共同祖先基因复制而来，虽然异常同源染色体分别来自父母，但是基因序列完全相同，导致出现纯合区域。这种情况的亲代往往具有不同程度的近亲血缘关系，大多数由于近亲结婚导致。 当ROH累及多条染色体优先考虑IBD：基于ROH计算基因组近交系数 (FROH)：$$F_{ROH} = \frac{\sum{L_{ROH}}}{L_{auto}}$$常染色体上 ≥5Mb的ROH长度总和 与常染色体总长的比值（2881Mb,GRCh37/h19）为基因组近交系数 ($F_{ROH}$)，可以用来评估亲缘关系；大于25%提示一级亲缘关系；12.5%提示二级亲缘关系；6.25%提示三级亲缘关系；若该比例≥6.25%（ROH片段总长接近180Mb），建议在报告中注明“基因组大片段纯合区域较多，常染色体隐性遗传病的发病风险增高”；小于三级亲缘关系不建议报告。 UPD概念参考资料 UPD(Uniparental Disomy):单亲二体，指的是染色体的两个副本或其部分均从同一父母遗传而来，而不是从两个不同的父母遗传而来。单亲二体及印迹基因又分为： 单亲同二体(isodisomy，iso-UPD):两条染色体来自同一亲本的同一染色体。 单亲异二体(heterodisomy，hetero-UPD):两条染色体来自同一亲本的两条同源染色体。 复合型单亲二体(mix-UPD):部分表现为单亲同二体，部分表现为单亲异二体。 片段性单亲二体(seg-UPD):染色体的一部分表现为UPD。 当ROH累及1~2条染色体优先考虑UPD：单亲二体(uniparental disomy,UPD)的概念在1980年由Eric Engel首先提出，指一个个体的两条同源染色体均遗传自一个亲本，或来自亲本一方的染色体片段被另一方的同源部分取代。 UPD根据染色体来源和组成的不同可以分为：]]></content>
      <categories>
        <category>Tobeinsert</category>
      </categories>
      <tags>
        <tag>Tobeinsert</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Software-动态突变-ExpansionHunter]]></title>
    <url>%2F03.%E5%BA%94%E7%94%A8-01.software_in_NGS%2FSoftware-%E5%8A%A8%E6%80%81%E7%AA%81%E5%8F%98-ExpansionHunter%2F</url>
    <content type="text"><![CDATA[软件仓库： Expansion Hunter: a tool for estimating repeat sizes参考文献： ExpansionHunter: A sequence-graph based tool to analyze variation in short tandem repeat regions 人类基因组中有许多区域由重复的 短单元序列（通常是三聚体）。这样的重复区域可以扩展为尺寸远大于读取长度，从而导致疾病。 脆性 X 综合征 ， ALS ，以及 亨廷顿舞蹈症 都是众所周知的例子。 部署安装参考原文档 算法原理软件开发环境基于 PCR-free WGS 数据，如果处理经过PCR的数据，需要进行评估性能。 结果格式说明Expansion Hunter 生成的 JSON 文件包含样本参数信息（SampleParameters 字段）以及按位点汇总的分析结果信息（ LocusResults 字段）。原文参考结果格式说明]。示例如下：1234567891011121314151617181920212223242526272829303132333435363738394041424344"ATXN2": &#123; //该基因座的预期等位基因数量 "AlleleCount": 2, // 估计基因位点的读取覆盖深度 "Coverage": 76.47368421052632, // 基因位点id "LocusId": "ATXN2", // reads长度 "ReadLength": 100, // 基因型和描述在基因座分析的每个变异的其他信息 "Variants": &#123; "ATXN2": &#123; // CountsOfSpanningReads 已识别的**跨越读取**的摘要，以包含条目 (n, m) 数组形式给出，其中 n 是侧翼读​​取跨越的重复单元数， m 是此类读取的数量 "CountsOfSpanningReads": "(20, 1), (21, 3), (22, 10)", //CountsOfFlankingReads An analog of CountsOfSpanningReads for in-repeat reads "CountsOfFlankingReads": "(0, 3), (1, 12), (2, 3), (3, 8), (4, 5), (5, 10), (6, 8), (7, 8), (8, 9), (9, 5), (10, 6), (11, 11), (13, 8), (14, 7), (15, 8), (16, 6), (17, 4), (18, 3), (19, 9), (20, 5), (21, 10), (22, 1)", // CountsOfInrepeatReads An analog of CountsOfSpanningReads for spanning reads "CountsOfInrepeatReads": "(33, 1)", //Genotype Repeat genotype given by the size of each repeat allele "Genotype": "22/35", //GenotypeConfidenceInterval Size confidence interval for each repeat allele "GenotypeConfidenceInterval": "22-22/34-36", //ReferenceRegion 0-based half-open reference coordinates of the repeat region (chrom:start-end) "ReferenceRegion": "chr12:112036755-112036823", //RepeatUnit Repeat unit in the reference orientation "RepeatUnit": "CTG", //VariantId Unique variant identifier "VariantId": "ATXN2", //VariantType Always set to "Repeat" "VariantType": "Repeat", //VariantSubtype Either "Repeat" or "RareRepeat" "VariantSubtype": "Repeat", &#125; &#125;&#125;, 配套工具全基因组 STR 目录 含有与已知致病基因具有相似特性的多态性重复序列， 功能性 STRREViewer, a tool for visualizing alignments of reads in regions containing tandem repeatsREViewer ，一种用于可视化包含串联重复的区域中的读取比对的工具 其他验证检测方法 方法 荧光PCR-毛细管电泳法 (CE) 一代测序法 (Sanger) 二代测序法 (NGS / MPS) 技术原理 基于片段长度分离DNA，通过荧光信号检测 基于双脱氧链终止法，直接读取DNA碱基序列 基于大规模并行测序，直接读取海量DNA碱基序列 核心信息 长度多态性 (Repeat Unit Number) 序列多态性 (Base Sequence) 序列多态性 (Base Sequence + 长度) 分辨率 高 (可区分1-4 bp的差异) 极高 (黄金标准，可识别单个碱基差异) 超高 (可同时检测长度和序列变异) 优势 1. 技术成熟稳定，全球标准; 2. 自动化程度高，通量大 ;3. 成本相对较低;4. 流程标准化，数据库完善;5. 数据分析简单快捷 1. 准确性最高，是验证其他方法的“金标准”;2. 序列结果明确，无歧义;3. 可检测侧翼区SNP; 1. 信息量最丰富：能发现“等位基因丢失”;2. 高通量：可同时检测数百个基因座(STR+SNP);3. 卓越的混合样本分析能力;4. 更适合降解DNA (测序读长短) 劣势 1. 无法区分序列变异 (等位基因丢失);2. 对高度降解DNA的分析能力有限;3. 复杂混合样本解析困难 1. 通量极低，一次反应只能测一个片段;2. 成本高昂 (按片段收费);3. 操作繁琐，耗时漫长;4. 完全不适用于常规STR分型筛查 1. 初始设备和试剂成本;2. 数据分析复杂，需生物信息学支持;3. 标准化仍在进行中;4. 数据存储和管理挑战大 检测通量 高 (一次运行可检测16-24个STR基因座) 极低 (一次运行只能检测一个片段的序列) 超高 (一次运行可检测数百个样本的数百个基因座) 成本效益 高 (适合大规模常规检测) 低 (仅适合疑难样本的靶向验证) 初期投入高，但单位数据成本低 (适合大批量样本的多基因座检测) 主要应用场景 法医DNA数据库建设、亲子鉴定、个体识别 (绝对主流) CE结果的验证、疑难等位基因的序列确认、新STR基因座的发现与验证 疑难案件检验 (降解检材、复杂混合样本)、祖先推断/表型预测 (同步测SNP)、研究领域、未来数据库扩展]]></content>
      <categories>
        <category>NGS</category>
        <category>software</category>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>动态突变</tag>
        <tag>STR</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Software-资源监控-]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-00.%E7%BC%96%E7%A8%8B%E8%BE%85%E5%8A%A9%E5%B7%A5%E5%85%B7%2FSoftware-%E8%B5%84%E6%BA%90%E7%9B%91%E6%8E%A7-process_perf%2F</url>
    <content type="text"><![CDATA[process_perf 一个资源监控项目，可以直接下载编译文件 使用方式1./process_perf -p 28208 # 要监控任务的 pid PS: 要监控的进程和监控任务要在同一个终端运行。]]></content>
      <categories>
        <category>software</category>
        <category>Windows</category>
      </categories>
      <tags>
        <tag>软件工具</tag>
        <tag>资源监控</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[R-rd文件的处理]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-03.R%2FR-rd%E6%96%87%E4%BB%B6%E7%9A%84%E5%A4%84%E7%90%86%2F</url>
    <content type="text"></content>
      <categories>
        <category>编程拾慧</category>
        <category>R</category>
      </categories>
      <tags>
        <tag>Tobeinsert</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Software-遗传-亲缘关系分析-SampleSimilarity]]></title>
    <url>%2F03.%E5%BA%94%E7%94%A8-01.software_in_NGS%2FSoftware-%E9%81%97%E4%BC%A0-%E4%BA%B2%E7%BC%98%E5%85%B3%E7%B3%BB%E5%88%86%E6%9E%90-SampleSimilarity%20%2F</url>
    <content type="text"><![CDATA[开源生物信息分析工具ngs-bits的SampleSimilarity 可以用于进行家系分析。 概念说明Identity by State (IBS): Identity by state is an easy, generally applicable method to measure similarity between unrelated individuals. IBS simply considers the similarity between genotypes at each locus and averages over all the loci of interest. 参考来源 Sample Similarity 计算几个衡量样本相似性的指标： overlap_percent: Overlap of the variant lists, i.e. percentage of variants found in both samples - not considering the genotype (only in VCF and GSvar mode). correlation: Correlation of variant genotypes for variants found in both samples. ibs0: Percentage of variants with zero IBS 基因型完全不一样, e.g. AA and CC (only in BAM mode). ibs2: Percentage of variants with complete IBS 基因型完全一样, e.g. AA and AA. 原理简介计算两个样本基因型相关性步骤： 提取两个样本都检出SNP的位置； 编码基因型，HOM：1，HET：0.5 计算皮尔逊相关性： $$correlation = \frac{\sum_i [(X_i - X_m) (Y_i - Y_m)]}{\delta(X) \delta(Y)}$$ 其中 $X_m$， $Y_m$表示编码后的序列平均值，\delta(X) \delta(Y)表示编号后序列标准差$$overlap_percentage = \frac{X_{snp} \cap Y_{snp}}{min(X_{count}, Y_{count})}$$ 其中$X_{snp} \cap Y_{snp}$表示X，Y样本SNP交集，$min(X_{count}, Y_{count})$ 表示两个样本SNP个数较小的一个值$$ibs2 = \frac{X_{isnp} \cap Y_{isnp}}{min(X_{count}, Y_{count})}$$ 与 $overlap_{percentage}$ 计算公式类似，只不过SNP交集是考虑杂合性，即只统计基因型完全相同的SNP，如CC和CC，AG和AG 使用方法安装1234conda config --add channels biocondaconda install ngs-bitspip install -r requirements.txt 软件的使用整体比较简单，输入文件为VCF文件，输出文件可以指定一个txt文件，也可以不指定，默认输出为stdout。1234567SampleSimilarity -in 25D03227091*/vcf/*final.vcf.gz -skip_multi# 屏幕输出结果示例#file1 file2 overlap_percent correlation ibs2_percent count1 count2 comments25D03227091_0.final.vcf.gz 25D03227091_1.final.vcf.gz 94.91 0.9566 40.53 143172 17866825D03227091_0.final.vcf.gz 25D03227091_2.final.vcf.gz 94.40 0.9576 40.31 143172 17431225D03227091_1.final.vcf.gz 25D03227091_2.final.vcf.gz 92.26 0.9756 38.05 178668 174312 结果格式说明： 每两个vcf文件的样本对，后续对应，共突变的位点比例，样本的位点相关性，基因型完全相同的位点比例，样本1的位点个数，样本2的位点个数，注释。 一个外部封装封装后的项目代码仓库路径: https://gitlab.genomics.cn/bioinfo/ss 。封装主要对输入文件进行了一些区分，明文指定了父亲、母亲、先证者（孩子）的样本，然后回根据 SampleSimilarity 返回的相关性，进行样本亲缘关系的判定，来确定输入的样本关系是否存在异常。 历史测评数据显然的，亲缘关系越接近的样本，相关性会越高，历史测试数据也支持这个结论，历史测试的不同关系的样本间的具体数据表现如下： 相同样本相关性一般应该在0.95以上；父母与子女的相关性一般应该在0.55以上；兄弟姐妹的相关性在0.67左右。父母这类无血缘关系的样本一般在0.44左右（数据主要来源于中国人样本，针对不同族裔会有差别）。 统计数据如下： relationship count mean start end sibling 34 0.6783 0.5847 0.7401 couple 604 0.449 0.4225 0.4769 grandparent 4 0.5001 0.4585 0.5417 nephew 2 0.4875 0.4262 0.5488 parent-child 1253 0.5466 0.5213 0.5737 relative 4 0.4916 0.4778 0.5086 unknown 16 0.553 0.4945 0.5982 一个更直观的查看方式不同亲缘关系间的阈值$$start: Q1 - 1.5 (Q3 - Q1)$$$$end: Q3 + 1.5 (Q3 - Q1)$$ 一般的经验标准夫妻关系异常：&gt; 0.4769亲子关系异常：&lt; 0.5213]]></content>
      <categories>
        <category>NGS</category>
        <category>software</category>
        <category>Linux</category>
        <category>遗传</category>
      </categories>
      <tags>
        <tag>亲缘分析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Software-变异注释-spliceAI]]></title>
    <url>%2F03.%E5%BA%94%E7%94%A8-01.software_in_NGS%2FSoftware-%E5%8F%98%E5%BC%82%E6%B3%A8%E9%87%8A-spliceAI%2F</url>
    <content type="text"><![CDATA[SpliceAI gitlab ，SpliceAI paper 软件介绍软件的部署适用spliceAI 是一个基于python的软件，可以通过conda或者pip进行安装，安装方式如下：12345pip install spliceai# orconda install -c bioconda spliceaiconda create -n spliceAI -c bioconda spliceai 软件本身分析速度较慢，预计一个变异耗时1s 左右。 结果说明预测：11: 47364709 C&gt;T 为例, SpliceAI=G|OR4F5|0.01|0.08|0.00|0.00|-10|26|-28|-25 参考来源 原件的原理介绍]]></content>
      <categories>
        <category>NGS</category>
        <category>software</category>
      </categories>
      <tags>
        <tag>Annotation</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[向上沟通的技巧]]></title>
    <url>%2F06.%E8%81%8C%E5%9C%BA-%E6%B2%9F%E9%80%9A%2F%E5%90%91%E4%B8%8A%E6%B2%9F%E9%80%9A%E7%9A%84%E6%8A%80%E5%B7%A7%2F</url>
    <content type="text"></content>
      <categories>
        <category>职场培训</category>
      </categories>
      <tags>
        <tag>职场培训</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Database-GenomAD]]></title>
    <url>%2F01.%E7%9F%A5%E8%AF%86-04.Database%2FDatabase-GenomAD%2F</url>
    <content type="text"><![CDATA[GenomAD: Genome Aggregation Database 是由国际研究人员联盟开发的资源，目的是聚合和协调来自各种大型测序项目的外显子组和基因组测序数据，并向更广泛的科学界提供汇总数据。 v4 数据集 (GRCh38) 涵盖 730,947 个外显子组序列和 76,215 个全基因组序列，这些序列来自来自不同祖先的无亲缘个体，是作为各种疾病特异性和群体遗传学研究的一部分进行测序的。 GenomAD数据库]]></content>
      <categories>
        <category>知识沉淀</category>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>数据库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MCP-应用时间]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-20.MachineLearn_%E5%AE%9E%E8%B7%B5%E9%A1%B9%E7%9B%AE%2FMCP-%E5%BA%94%E7%94%A8%E6%97%B6%E9%97%B4%2F</url>
    <content type="text"><![CDATA[MCP服务平台：MCP.modelscope , MCP.so]]></content>
      <categories>
        <category>LLM</category>
      </categories>
      <tags>
        <tag>LLM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Obsidian-常用插件]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-06.markdown%2FObsidian-%E5%B8%B8%E7%94%A8%E6%8F%92%E4%BB%B6%2F</url>
    <content type="text"><![CDATA[官方仓库 代码切页 Obsidian-CodeBlock-Tabs提供代码块的tab标签切页功能 obsidian-excel-to-markdown-table表格自动转markdown格式 advanced-tables-obsidianmarkdown中的表格操作插件 Excalidraw介绍视频 obsidian-iconize remotely-save非官方的一个代码同步插件]]></content>
      <categories>
        <category>备忘录</category>
      </categories>
      <tags>
        <tag>环境配置</tag>
        <tag>markdown</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RAG-本地部署时间]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-05.MachineLearn_%E5%AE%9E%E8%B7%B5%E9%A1%B9%E7%9B%AE%2FRAG-%E6%9C%AC%E5%9C%B0%E9%83%A8%E7%BD%B2%E5%AE%9E%E8%B7%B5%2F</url>
    <content type="text"><![CDATA[tinyRAG 一个手搓实现项目，使用代码，从底层实现了一个 RAG 框架，包含如何进行文档的分块，向量化，建库，基于提问如何在向量库中检索相关的文本块，然后将文本块作为上下文嵌入到prompt中，然后更新问题（加入参考信息）给LLM进行回答。其中的不管是文本导入，文档分块，向量化，数据库中知识块的检索肯定是有一些更复杂完善的实现方案，虽然项目实现相对简单，但是对于初学者具有非常大的帮助，可以快速的将整个RAG的各个环境所执行的操作具象化。快速建立整体框架层面的认知体系。 通过上述项目，我们带RAG有了初步的了解，接下来我们可以参考下面的项目，提供了更多的技术方案，有了基本的理解，我们可以更快的接触一些前沿的方案，来应用到我们的业务场景中。RAG_Techniques 是一个开源项目，集成了30+前沿技术方案，覆盖从基础检索到多模态增强的全场景实现。无论你是AI新手还是资深开发者，这个”RAG技术百科全书”都能帮你突破性能瓶颈！例如：加载csv文件进行RAG的示例知识库存在多个检索结果时，对结果进行重排序，从而使更相关的结果优先级更高对原始问题进行更详细的重构、将原始问题重写为一个更宽泛的问题、将原始问题拆分为多个子问题等多种方式调整问题，从而提高模型的质量，增加知识库的检索准确性。 该项目还有其他多种技术方案，实践中我们可以先进行系统的了解，进而优化我们的项目。]]></content>
      <categories>
        <category>LLM</category>
        <category>RAG</category>
      </categories>
      <tags>
        <tag>LLM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MCP-开发部署时间]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-05.MachineLearn_%E5%AE%9E%E8%B7%B5%E9%A1%B9%E7%9B%AE%2FMCP-%E5%BC%80%E5%8F%91%E9%83%A8%E7%BD%B2%E6%97%B6%E9%97%B4%2F</url>
    <content type="text"></content>
      <categories>
        <category>LLM</category>
        <category>MCP</category>
      </categories>
      <tags>
        <tag>LLM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[01.算法知识-遗传算法]]></title>
    <url>%2F01.%E7%9F%A5%E8%AF%86-14.%E7%AE%97%E6%B3%95%2F01.%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86-%E9%81%97%E4%BC%A0%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"></content>
      <categories>
        <category>machine_learning</category>
        <category>算法</category>
      </categories>
      <tags>
        <tag>machine_learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[引物设计相关工具-Primer3]]></title>
    <url>%2F01.%E7%9F%A5%E8%AF%86-13.%E5%BC%95%E7%89%A9%E8%AE%BE%E8%AE%A1%2F02.%E5%BC%95%E7%89%A9%E8%AE%BE%E8%AE%A1-%E7%9B%B8%E5%85%B3%E5%B7%A5%E5%85%B7-Primer3%2F</url>
    <content type="text"><![CDATA[引物设计工具Primer3Primer3是一个常用的引物设计工具，可以自动计算引物长度、GC含量和Tm值，并提供详细的分析结果。Primer3官网Primer3 参数参考说明 参考引物设计代码1234567891011121314151617181920212223242526272829303132333435import primer3 global_args = &#123; 'PRIMER_NUM_RETURN': 20, 'PRIMER_OPT_SIZE': 22, # 基于Natera 数据更新指标 22 'PRIMER_MIN_SIZE': 16, # 基于Natera 数据更新指标 16 'PRIMER_MAX_SIZE': 32, # 基于Natera 数据更新指标 32 'PRIMER_OPT_TM': 62.0, # 基于Natera 数据更新指标 62 'PRIMER_MIN_TM': 59.0, # 基于Natera 数据更新指标 59 'PRIMER_MAX_TM': 70.0, # 基于Natera 数据更新指标 70暂调位64 'PRIMER_MIN_GC': 45.0, # 基于Natera 数据更新指标 30暂调位40 'PRIMER_MAX_GC': 55.0, # 基于Natera 数据更新指标 70暂调位60 'PRIMER_PRODUCT_SIZE_RANGE': [50, 70], # 指定引物产物的长度范围。 'PRIMER_THERMODYNAMIC_OLIGO_ALIGNMENT': 1, # 该值设为 1， 则程序会使用热力学模型来计算 oligos 形成发夹结构和二聚体的可能性。 'PRIMER_MAX_POLY_X': 5, # 所允许的单核苷酸重复的次数，例如， 默认下 AAAAAA 是不允许的。 'PRIMER_INTERNAL_MAX_POLY_X': 100, # 和上个标签一致，但是 for internal oligo. 'PRIMER_SALT_MONOVALENT': 50.0, # 单价盐离子浓度(mM) 'PRIMER_DNA_CONC': 50.0, # DNA产物浓度(mM) 'PRIMER_MAX_NS_ACCEPTED': 0, # 所允许的 N 碱基的数目。 'PRIMER_MAX_SELF_ANY': 12, # 引物自身进行反向互补 'PRIMER_MAX_SELF_END': 8, # 引物自身进行 3' 端反向互补形成引物二聚体 'PRIMER_PAIR_MAX_COMPL_ANY': 12, # left primer 和 right primer 序列的反向互补 'PRIMER_PAIR_MAX_COMPL_END': 8, # left primer 和 right primer 进行 3' 端反向互补形成引物二聚体 'PRIMER_GC_CLAMP': 1 # 要求 left primer 和 right primer 的 3' 末端序列中有连续指定数目的 Gs 或 Cs 碱基。&#125;seq_args = &#123; 'SEQUENCE_ID': id, 'SEQUENCE_TEMPLATE': seq, 'SEQUENCE_INCLUDED_REGION': [0, len(seq) - 1], 'SEQUENCE_TARGET ':[50,1], # 其中 &lt;start&gt; 是目标第一个碱基的索引， &lt;length&gt; 是其长度。 "SEQUENCE_PRIMER_PAIR_OK_REGION_LIST": [0,48,52,48]&#125;primer3_result = primer3.bindings.design_primers(seq_args, global_args) 由于理想的引物涉及条件可能会出现无法涉及出有效引物的情况，所以可以设置多个严格程度递松的global_args，从最理想的参数到非理想参数进行递归设计，直至寻找到满足预期的备选引物。]]></content>
      <categories>
        <category>引物设计</category>
      </categories>
      <tags>
        <tag>引物设计</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pipeline-container-Singularity-常用命令]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-04.pipeline%2Fpipeline-container-Singularity-%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[运行一个镜像启动服务1singularity instance mysql 查询容器列表1singularity instance list 输出示例如下：123INSTANCE NAME PID IP IMAGEtest1 1234 10.0.0.1 ubuntu.simgtest2 5678 10.0.0.2 ubuntu.simg 停止一个容器1singularity instance stop mysql]]></content>
      <categories>
        <category>Tobeinsert</category>
      </categories>
      <tags>
        <tag>Tobeinsert</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pipeline-frameworks-cromwell-03.数据库配置]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-04.pipeline%2Fpipeline-container-Singularity-%E5%90%AF%E5%8A%A8mysql%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[mysql 镜像获取https://github.com/PSC-PublicHealth/mysql-singularity-container/blob/master/README.md 启动singularity 服务一次执行如下shell sh instance.sh 123456#!/bin/bashmkdir -p $&#123;PWD&#125;/mysql/var/lib/mysql $&#123;PWD&#125;/mysql/run/mysqld/share/app/singularity/3.8.1/bin/singularity instance start \ --bind $&#123;PWD&#125;/mysql/var/lib/mysql/:/var/lib/mysql \ --bind $&#123;PWD&#125;/mysql/run/mysqld:/run/mysqld \ ./mysql.simg mysql sh mysql_server.sh 启动mysql 服务 1/share/app/singularity/3.8.1/bin/singularity run instance://mysql 创建文件 cromwell.sql 123456CREATE DATABASE IF NOT EXISTS cromwell default charset utf8mb4 COLLATE utf8mb4_unicode_ci;CREATE USER IF NOT EXISTS 'cromwell'@'localhost' IDENTIFIED BY 'cromwell';GRANT all privileges ON cromwell.* TO 'cromwell'@'localhost';grant all privileges on *.* to 'root'@'127.0.0.1' identified by 'root';grant all privileges on *.* to 'cromwell'@'127.0.0.1' identified by 'cromwell';flush privileges; sh cromwell_init.sh 创建mysql中的cromwel调度相关表结构 /share/app/singularity/3.8.1/bin/singularity exec instance://mysql mysql &lt; cromwell.sql]]></content>
      <categories>
        <category>Tobeinsert</category>
      </categories>
      <tags>
        <tag>Tobeinsert</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[文档解析-pdf文档解析]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-20.MachineLearn_%E6%89%A9%E5%B1%95%E5%BA%93%2F%E6%96%87%E6%A1%A3%E8%A7%A3%E6%9E%90-pdf%E6%96%87%E6%A1%A3%E8%A7%A3%E6%9E%90%2F</url>
    <content type="text"><![CDATA[PP-DocLayoutPP-DocLayout 系列提供了三个不同尺度的模型 高精度模型：PP-DocLayout-L，GPU 上每个页面端到端推理耗时 13.4 ms 精度和效率均衡模型：PP-DocLayout-M，GPU 上每个页面端到端推理耗时 12.7ms 高效率模型：PP-DocLayout-S，GPU上每个页面处理耗时约 8.1ms MarkerMarker 是一款基于先进 OCR（光学字符识别）技术的文件转换工具。它不仅可以处理 PDF 文件，还能处理各种图像格式（如 PNG、JPEG 等），并将其内容转换为结构化的 Markdown、JSON 或 HTML 格式。无论是文字、表格还是图像，Marker 都能精准识别并转换，确保输出内容的完整性和准确性。 特点 高精度 OCR 识别Marker 采用了最新的 OCR 技术，能够准确识别 PDF 和图像中的文字内容。即使是复杂的排版、多列文本或手写字体，Marker 也能轻松应对，确保转换后的内容与原文件高度一致。 多格式输出Marker 支持将文件转换为多种格式，满足不同场景的需求： • Markdown：适合需要进一步编辑或发布到博客、文档平台的用户。 • JSON：适合开发者和数据分析师，方便进行结构化数据处理。 • HTML：适合需要网页展示或嵌入到网站中的用户。 批量处理Marker 支持批量处理文件，用户可以一次性上传多个 PDF 或图像文件，Marker 会自动将其转换为指定的格式，大大节省了时间和精力。 保持原始格式Marker 不仅能够识别文字内容，还能保留原始文件的格式，如标题、段落、列表、表格等。转换后的 Markdown、JSON 或 HTML 文件能够清晰地反映原文件的结构，方便后续编辑和使用。 跨平台支持Marker 支持多种操作系统，包括 Windows、macOS 和 Linux，用户可以在不同的设备上使用 Marker 进行文件转换。 安装1pip install marker-pdf 使用 命令行使用 12345678# 转换一个文件marker_single /path/to/file.pdf# 转换一个目录下的所有文件marker /path/to/input/folder --workers 4# 使用多个gpu进行转换NUM_DEVICES=4 NUM_WORKERS=15 marker_chunk_convert ../pdf_in ../md_out python调用 123456789from marker.converters.pdf import PdfConverterfrom marker.models import create_model_dictfrom marker.output import text_from_renderedconverter = PdfConverter( artifact_dict=create_model_dict(),)rendered = converter("FILEPATH")text, _, images = text_from_rendered(rendered)]]></content>
      <categories>
        <category>Tobeinsert</category>
      </categories>
      <tags>
        <tag>Tobeinsert</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[5013.大模型-调用api - 多模态调用]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-25.LLM-%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86%2F5013.%E5%A4%A7%E6%A8%A1%E5%9E%8B-%E8%B0%83%E7%94%A8api%20-%20%E5%A4%9A%E6%A8%A1%E6%80%81%E8%B0%83%E7%94%A8%2F</url>
    <content type="text"><![CDATA[openai-api官方文档通义千问API参考 测试链接是否成功1234curl https://api.openai.com/v1/models \ -H "Authorization: Bearer $OPENAI_API_KEY" \ -H "OpenAI-Organization: YOUR_ORG_ID" \ -H "OpenAI-Project: $PROJECT_ID" 12345678from openai import OpenAIclient = OpenAI( api_key=api_key, # 必选 base_url=base_url, # 必选 organization='YOUR_ORG_ID', #可选 project='$PROJECT_ID', # 可选) 发送api请求示例12345678curl https://api.openai.com/v1/chat/completions \ # 部署服务的地址 -H "Content-Type: application/json" \ # 传递数据的格式 -H "Authorization: Bearer $OPENAI_API_KEY" \ -d '&#123; "model": "gpt-4o-mini", # 访问的模型 "messages": [&#123;"role": "user", "content": "Say this is a test!"&#125;], # 模型参数 "temperature": 0.7 # 模型温度 &#125;' 像模型发送上述请求后，你会得到一个完整的json格式的返回，格式如下123456789101112131415161718192021222324252627&#123; "id": "chatcmpl-abc123", "object": "chat.completion", "created": 1677858242, "model": "gpt-4o-mini", "usage": &#123; "prompt_tokens": 13, "completion_tokens": 7, "total_tokens": 20, "completion_tokens_details": &#123; "reasoning_tokens": 0, "accepted_prediction_tokens": 0, "rejected_prediction_tokens": 0 &#125; &#125;, "choices": [ &#123; "message": &#123; "role": "assistant", "content": "\n\nThis is a test!" &#125;, "logprobs": null, "finish_reason": "stop", "index": 0 &#125; ]&#125; 聊天创建对话123456789101112from openai import OpenAIclient = OpenAI()completion = client.chat.completions.create( model="gpt-4o", messages=[ &#123;"role": "developer", "content": "You are a helpful assistant."&#125;, &#123;"role": "user", "content": "Hello!"&#125; ])print(completion.choices[0].message) 对话包含图片上传123456789101112131415161718192021222324from openai import OpenAIclient = OpenAI()response = client.chat.completions.create( model="gpt-4o", messages=[ &#123; "role": "user", "content": [ &#123;"type": "text", "text": "What's in this image?"&#125;, &#123; "type": "image_url", "image_url": &#123; "url": "https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg", &#125; &#125;, ], &#125; ], max_tokens=300,)print(response.choices[0]) 123456789101112131415161718192021222324curl https://api.openai.com/v1/chat/completions \ -H "Content-Type: application/json" \ -H "Authorization: Bearer $OPENAI_API_KEY" \ -d '&#123; "model": "gpt-4o", "messages": [ &#123; "role": "user", "content": [ &#123; "type": "text", "text": "What'\''s in this image?" &#125;, &#123; "type": "image_url", "image_url": &#123; "url": "https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg" &#125; &#125; ] &#125; ], "max_tokens": 300 &#125;' api使用音频文字转音频12345678910from pathlib import Pathimport openaispeech_file_path = Path(__file__).parent / "speech.mp3"response = openai.audio.speech.create( model="tts-1", voice="alloy", input="The quick brown fox jumped over the lazy dog.")response.stream_to_file(speech_file_path) 参数说明| 参数 | 数据类型 | 是否必须 | 说明 || ————— | ——– | ———————————————————————————————————- | ———————————————————————————————————————————————————————————————— || model | string | Required | One of the available TTS models: tts-1 or tts-1-hd || input | string | Required | The text to generate audio for. The maximum length is 4096 characters. || voice | string | Required | The voice to use when generating the audio. Supported voices are alloy, ash, coral, echo, fable, onyx, nova, sage and shimmer. Previews of the voices are available in the Text to speech guide. || response_format | string | Optional Defaults to mp3,The format to audio in. Supported formats are mp3, opus, aac, flac, wav, and pcm. || speed | number | Optional | Defaults to 1 ,The speed of the generated audio. Select a value from 0.25 to 4.0. 1.0 is the default. || Returns | - | - | The audio file content. | 音频转文字12345678from openai import OpenAIclient = OpenAI()audio_file = open("speech.mp3", "rb")transcript = client.audio.transcriptions.create( model="whisper-1", file=audio_file) 翻译12345678from openai import OpenAIclient = OpenAI()audio_file = open("speech.mp3", "rb")transcript = client.audio.translations.create( model="whisper-1", file=audio_file)]]></content>
      <categories>
        <category>LLM</category>
      </categories>
      <tags>
        <tag>LLM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[5013.大模型-调用api - 进阶调用]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-25.LLM-%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86%2F5013.%E5%A4%A7%E6%A8%A1%E5%9E%8B-%E8%B0%83%E7%94%A8api%20-%20%E8%AF%A6%E7%BB%86%E5%8F%82%E6%95%B0%E8%AE%B0%E5%BD%95%2F</url>
    <content type="text"><![CDATA[openai-api官方文档简单使用示例, 提供模型的地址，提供调用的模型和消息，即可调用大模型。1234567891011121314151617from openai import OpenAI# 配置大模型的ip和Keyclient = OpenAI( api_key=api_key, # 必选。没有使用 "EMPTY"，但不能缺省 base_url=base_url, # 必选)# 创建链接response = client.chat.completions.create(model= "/data/tianjin/tf/model/Qwen2.5-VL-72B-Instruct",messages=[ &#123;"role": "user", "content": "who are you!"&#125;])'I am an AI model developed by Alibaba Cloud.'# 'I am a large language model created by researchers at UC Berkeley. I am called Qwen.' 资料参考： text generation,vision, and audio guides.但是实际使用的过程中，这样的效果往往不那么好，所以我们需要进行一些优化。 messages 扩展示例中，message只提供了简单的user,但是如果你使用api调用的话，你能设定的，并不只有通话的消息，而是可以设置更多。比如： system，user，assistant。 system“system”角色有助于通过分配特定行为给聊天助手来创建对话的上下文或范围。例如，如果您希望与ChatGPT在与医学相关的话题范围内进行对话，可以将”system”角色分配给聊天助手，并设置内容为”遗传咨询专家”。然后ChatGPT会表现得像遗传咨询专家一样回答您的问题。”system”角色指示了ChatGPT在对话消息中应该具有哪种个性。通过利用系统角色（System），我们可以创建具有不同个性、专长或行为的助手（Assistant），从而使 AI 的响应与我们的使用案例相匹配。 12345678910response = client.chat.completions.create(temperature=0,model= "/data/tianjin/tf/model/Qwen2.5-VL-72B-Instruct",messages=[ &#123;'role': 'system', 'content': '你是一个资深的遗传分析专家，擅长识别文档和图片，并确认他们是否和要关注的基因变异相关。'&#125;, &#123;"role": "user", "content": "who are you"&#125;])response.choices[0].message.content# 'I am a sophisticated artificial intelligence designed to assist with a wide range of tasks, including genetic analysis, document and image recognition, and identifying gene variants. My capabilities are tailored to provide expert-level support in these areas. How can I assist you today?' user用户角色（User）代表与 AI 模型互动的人类。当我们使用“用户（User）”角色发送消息时，实际上是在模拟用户在对话中的输入。AI 模型将把该消息解释为来自用户，并相应地生成响应。 assistant助手角色（Assistant）代表 AI 模型本身。当 API 返回响应时，它将包括一个“助手（Assistant）”角色的消息。该消息包含由 AI 生成的内容，作为对用户（User）输入的响应。]]></content>
      <categories>
        <category>LLM</category>
      </categories>
      <tags>
        <tag>LLM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[引物设计的整体设计原理]]></title>
    <url>%2F01.%E7%9F%A5%E8%AF%86-13.%E5%BC%95%E7%89%A9%E8%AE%BE%E8%AE%A1%2F01.%E5%BC%95%E7%89%A9%E8%AE%BE%E8%AE%A1-%E6%80%BB%E4%BD%93%E5%8E%9F%E5%88%99%2F</url>
    <content type="text"><![CDATA[引物设计考虑因素引物的基因特性 引物长度：引物长度一般在15-30个碱基之间，常用长度为18-24个碱基。过短的引物可能导致特异性不足，而过长的引物则可能增加成本和非特异性扩增的风险。 GC含量：应控制在40%-60%之间。过高或过低的GC含量会影响引物的稳定性和扩增效率。 Tm值：引物的Tm值（退火温度）应控制在58-60℃之间，以确保引物与模板DNA的稳定结合。两条引物的Tm值尽量接近，相差不超过4℃。 ΔG值：引物3’端的ΔG值应较低（绝对值不超过9），避免在错配位置引发反应。 碱基分布：引物序列中的碱基应随机分布，避免出现连续相同的碱基序列，特别是避免连续4个以上的嘌呤或嘧啶。 引物的特异性和唯一性 序列唯一性：引物序列在基因组中应具有唯一性，避免非特异性扩增。可以通过Primer-BLAST等工具验证引物的唯一性。 避免二级结构：引物不应形成二级结构，如发夹结构或二聚体，这会影响引物与模板的结合以及PCR反应的进行。能量值应低于4.5 kcal/mol。 跨内含子设计：引物最好跨越内含子设计，以区分DNA和RNA样本。 避免重复序列：引物应避免与模板中的重复序列（如短间隔核元素、长间隔核元素等）互补，以防止意外扩增。 引物的3’端和5’端特性 3’端设计：引物的3’端应避免出现过多的G或C，特别是在最后5个碱基内不应有多于2个的G或C。此外，3’端的末位碱基对扩增效率有较大影响，应避免使用A作为末位碱基。 5’端设计：5’端可以进行修饰，但3’端不可修饰。 修饰与标记：根据实验需求，可以在引物的5’端进行修饰，如加入酶切位点、生物素、荧光素等标记物。扩增产物的特性 产物长度：扩增产物长度一般在100-300 bp之间，以确保PCR反应的效率和特异性。 产物特异性：通过熔解曲线分析、电泳或Labchip等方法验证扩增产物的单一性。 其他注意事项 引物生产工艺：引物生产过程中可能出现的错误或杂质可能影响实验的稳定性。 引物验证：设计好的引物需要通过实验验证其特异性和扩增效率。 软件工具推荐 在引物设计过程中，可以使用以下专业软件或在线工具： 软件：Primer Premier、Oligo7、Beacon Designer。 在线工具：Primer-BLAST（NCBI）、Primer3 Plus。 Tm值引物中GC含量对退火温度的具体影响在引物设计中，GC含量对退火温度的具体影响主要体现在以下几个方面： 氢键数量的影响：相较于A-T碱基对的两对氢键，G-C碱基对有三对氢键。因此，当引物的GC含量较高时，需要更高的退火温度来确保引物与目标序列之间的稳定结合。 退火温度的选择：为了保证PCR反应的特异性，较高的退火温度可以减少引物和模板的非特异性结合。因此，在设计引物时，应根据引物的长度及GC含量，在$Tm$值允许的范围内选择合适的退火温度。 实验优化：在实际操作中，最佳退火温度通常通过实验优化确定，以提高产物的特异性和产量。例如，在某些情况下，退火温度可以设置为低于引物Tm值约5℃，以确保有效的退火。 避免高GC含量的问题：当引物的GC含量过高（如超过70%）时，单链的高GC区容易形成稳定的二级结构，这可能会影响引物的退火效率。引物设计中GC含量对退火温度的影响是显著的。高GC含量需要更高的退火温度以确保稳定的结合，而低GC含量则可能需要较低的退火温度。 引物的Tm值计算为了确保PCR反应的最优化，准确计算引物的Tm值至关重要。我们可以总结出几种计算引物Tm值的方法： 简化的公式法：对于长度在20mer以下的引物，可以使用以下公式进行估算：$$T_m=2℃ x (A+T)+4℃ x (G+C)$$这种方法简单易行，适用于初步估计。 更精确的公式法：对于长度超过20mer的引物，可以使用以下公式进行更精确的计算：$$Tm = 81.5 + 0.41 x (GC\%) - 600/L$$其中L为引物的长度。 近邻分析法：这种方法被认为是最可信的，尤其适用于高盐溶液中的杂交情况。它考虑了碱基对之间的相互作用，因此更加精确。 软件工具：使用专门的软件如Oligo 6或在线$Tm$计算器，可以自动计算引物的Tm值，并提供详细的分析结果，包括引物长度、GC含量和Tm值等信息。 在实际操作中，选择合适的计算方法取决于引物的长度和实验的具体需求。例如，对于短引物（&lt;20mer），简化公式法可能足够；而对于长引物或需要高精度的情况，则应采用更复杂的公式或近邻分析法。此外，使用专门的软件工具可以提高计算的准确性和效率。 避免二级结构的策略在引物设计中，避免二级结构的最佳策略包括以下几点： 避开产物的二级结构区：选择扩增片段时应避开二级结构区域，以防止引物重复区DNA二级结构的影响。 避免引物内部出现二级结构：引物设计应避免内部折叠和发夹结构的形成。这可以通过确保引物结构相对简单来实现。 避免引物间的互补性：特别是3’端的互补，以防止形成引物二聚体，产生非特异的扩增条带。 选择合适的GC含量：选择GC含量约50%的引物，并确保四种碱基中缺少一种，以避免二级结构的形成。 确保碱基的随机分布：在引物设计中保持四种碱基的随机分布，有助于提高引物的合成效率和PCR扩增的特异性。 避免引物内同源性或自身同源性：筛选引物序列以避免引物内同源性或自身同源性，这可能导致部分双链螺旋环状结构的形成。 引物3’端设计注意事项引物3’端的设计对PCR扩增效率有显著影响。以下是关于引物3’端设计及其优化方法的详细分析： 引物的3’端应避免G或C的重复，因为这可能导致引物之间的相互结合，形成二聚体或多聚体，从而降低扩增效率。 如果引物3’端过于富含A-T，可能会增加非特异性结合的风险，导致非特异性扩增。因此，建议引物3’端的G-C含量较高，以提高其与模板DNA的亲和力，从而提高PCR效率。 如果两条引物在3’端互补性过高，则更易形成引物二聚体，从而降低PCR的扩增效率和特异性。因此，应尽量减少引物3’端的互补性。 引物3’端应避免出现发夹结构，因为这会影响引物的稳定性和PCR效率。 引物3’端错配时，不同碱基引发效率存在很大差异。例如，当末位碱基为A时，即使在错配的情况下，也能有引发链的合成；而当末位碱基为T时，错配的引发效率大大降低。 如果扩增编码区域，引物3’端不要终止于密码子的第3位，因为密码子的第3位易发生简并，会影响扩增的特异性与效率。 引物的GC含量控制在40%-60%之间，并且正向引物和反向引物的Tm值相差不超过1℃为佳，Tm值调整至55-65℃为佳。 引物修饰与标记引物修饰与标记技术在分子生物学和基因工程中具有广泛的应用，其最新技术和应用趋势主要体现在以下几个方面： 基于CRISPR-Cas9的新技术通过合成具有5’修饰的引物，如带有C6接头（AmC6）或C12接头（AmC12）的胺基，可以显著提高敲入效率近五倍。这些修饰通过N-羟基琥珀酰亚胺（NHS）酯缀合，可以将其他二级修饰添加到接头上，从而提高PCR反应的效率。 LNA（Locked Nucleic Acid）碱基修饰技术能够提高引物与靶标分子的稳定性，并增加引物的熔解温度（Tm值）。这种修饰使得LNA Oligos在更短的长度情况下仍能保持很高的Tm值，从而降低PCR交叉反应的可能性。 生物素标记的引物可用于非放射性免疫分析，检测蛋白质、胞内化学染色、细胞分离、核酸分离以及杂交检测特异性的DNA/RNA序列等。这种标记技术为科学家们提供了更精确、高灵敏度的分子生物学工具。 使用巯基修饰的引物可以显著增强PCR的敏感性和产量。例如，在副溶血弧菌基因组DNA的实验中，这种修饰将PCR敏感性提高了100倍以上，并伴随着扩增子产量的提高。 在PCR扩增过程中，使用荧光标记的引物可以实现高通量的基因分型和检测。例如，利用荧光SSR技术进行PCR扩增时，引物包括一个5’端标记有荧光报告基团（如HEX）的上游引物和一个下游引物，这种方法可以产生带有荧光的PCR产物，便于实时监测和分析。 基于KASP（Knowledge-based Amplification System for Polymorphism）技术的标记引物用于检测小麦粒重相关基因。这些引物包含特定的核苷酸序列，并使用荧光标签序列（如FAM和HEX）进行标记，适用于大量样本的检测和基因分型。 在寡核苷酸合成过程中添加功能性修饰基团，如在3’端添加Inverted dT等封闭修饰，或在中间添加硫代等修饰以避免外源核酸酶降解，这些修饰支持各种不同目的的分子生物学研究。]]></content>
      <categories>
        <category>引物设计</category>
      </categories>
      <tags>
        <tag>引物设计</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python-包-Flask-api开发]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-02.python%2FPython-%E5%8C%85-Flask-api%E5%BC%80%E5%8F%91%2F</url>
    <content type="text"><![CDATA[Flask是一个Python编写的Web 微框架，可以快速实现一个网站或Web服务。Flask官方文档 基础示例12345678910111213from flask import Flask#1、首先我们导入了 Flask 类。 该类的实例将会成为我们的 WSGI 应用。app = Flask(__name__)# 2、接着我们创建一个该类的实例。第一个参数是应用模块或者包的名称。如果你使用 一个单一模块（就像本例），那么应当使用 name ，因为名称会根据这个 模块是按应用方式使用还是作为一个模块导入而发生变化（可能是 ‘main’ ， 也可能是实际导入的名称）。这个参数是必需的，这样 Flask 才能知道在哪里可以 找到模板和静态文件等东西# http://127.0.0.1:6006/hello 其中ip和端口再启动时，会返回根据实际情况调用。@app.route('/hello') # 3、然后我们使用 route() 装饰器来告诉 Flask 触发函数的 URL 。def hello_world():# 4、函数名称被用于生成相关联的 URL 。函数最后返回需要在用户浏览器中显示的信息。 return 'Hello World' if __name__ == '__main__': app.run() # 本地启动服务 简单的理解，当我们访问route()指定的url时，会运行对应的函数，并在页面中返回对应函数的返回值。在上述示例中，访问 http://127.0.0.1:6006/hello 会返回 “Hello World”。 扩展参数app.run()1app.run(host, port, debug, options) 所有参数都是可选的 host：要监听的主机名。 默认为127.0.0.1（localhost）。设置为“0.0.0.0”以使服务器在外部可用 port ：默认值为5000 debug：默认为false。 如果设置为true，则提供调试信息，可以自动重载代码并显示调试信息 options：要转发到底层的Werkzeug服务器。 Flask路由指定现代Web框架使用路由技术来帮助用户记住应用程序URL。可以直接访问所需的页面，而无需从主页导航。 Flask中的route()装饰器用于将URL绑定到函数。例如：123@app.route('/hello')def hello_world(): return 'hello world' 在这里，URL’/ hello’规则绑定到hello_world()函数。 因此，如果用户访问http://localhost:5000/hello URL，hello_world()函数的输出将在浏览器中呈现。 application对象的add_url_rule()函数也可用于将URL与函数绑定，如上例所示，使用route()装饰器的目的也由以下表示：123def hello_world(): return 'hello world'app.add_url_rule('/', 'hello', hello_world) Flask 变量规则通过向规则参数添加变量部分，可以动态构建URL。此变量部分标记为 converter:variable_name。它作为关键字参数转换为指定格式传递给与规则相关联的函数。在以下示例中，route()装饰器的规则参数包含附加到URL’/hello’的。 因此，如果在浏览器中输入http://localhost:5000/hello/chenshifeng作为URL，**则&#39;chenshifeng&#39;将作为参数提供给 hello()函数**。1234567891011121314151617181920212223242526#!/usr/bin/python# -*- coding: UTF-8 -*-"""@author:chenshifeng@file:flask_demo.py@time:2021/03/01"""from flask import Flaskapp = Flask(__name__)@app.route('/hello/&lt;name&gt;') # url第二层作为遍量 name传入函数。def hello_name(name): return 'Hello %s!' % name@app.route('/post/&lt;int:post_id&gt;') # 指定传入的参数是 int 类型def show_post(post_id): # show the post with the given id, the id is an integer return 'Post %d' % post_id@app.route('/path/&lt;path:subpath&gt;') # 指定传入的参数是 path类型def show_subpath(subpath): # show the subpath after /path/ return 'Subpath %s' % subpathif __name__ == '__main__': app.run(debug=True) 支持的格式转换器类型| 转换器 | 描述 || —— | ———————————– || string | （缺省值） 接受任何不包含斜杠的文本 || int | 接受正整数 || float | 接受正浮点数 || path | 类似 string ，但可以包含斜杠 || uuid | 接受 UUID 字符串 | 唯一的 URL / 重定向行为以下两条规则的不同之处在于是否使用尾部的斜杠。:1234567@app.route('/projects/')def projects(): return 'The project page'@app.route('/about')def about(): return 'The about page' projects 的 URL 是中规中矩的，尾部有一个斜杠，看起来就如同一个文件夹。访问一个没有斜杠结尾的 URL 时 Flask 会自动进行重定向，帮你在尾部加上一个斜杠。about 的 URL 没有尾部斜杠，因此其行为表现与一个文件类似。如果访问这个 URL 时添加了尾部斜杠就会得到一个 404 错误。这样可以保持 URL 唯一，并帮助 搜索引擎避免重复索引同一页面。 Flask URL构建url_for()函数对于动态构建特定函数的URL非常有用。该函数接受函数的名称作为第一个参数，以及一个或多个关键字参数，每个参数对应于URL的变量部分。1234567891011121314151617181920212223#!/usr/bin/python# -*- coding: UTF-8 -*-from flask import Flask, redirect, url_forapp = Flask(__name__)@app.route('/admin')def hello_admin(): return 'Hello Admin' @app.route('/guest/&lt;guest&gt;')def hello_guest(guest): return 'Hello %s as Guest' % guest @app.route('/user/&lt;name&gt;')def hello_user(name): if name == 'admin': return redirect(url_for('hello_admin')) else: return redirect(url_for('hello_guest', guest=name)) if __name__ == '__main__': app.run(debug=True) redirect函数用于重定向，实现机制很简单，就是向客户端（浏览器）发送一个重定向的HTTP报文，浏览器会去访问报文中指定的url。 Flask HTTP方法默认情况下，Flask路由只响应GET请求。但是，可以通过为route()装饰器提供方法参数来更改此首选项。| 方法 | 描述 || —— | —————————————————————— || GET | 以未加密的形式将数据发送到服务器，最常见的方法。 || HEAD | 和GET方法相同，但没有响应体。 || POST | 用于将HTML表单数据发送到服务器，POST方法接收的数据不由服务器缓存。 || PUT | 用上传的内容替换目标资源的所有当前表示。 || DELETE | 删除由URL给出的目标资源的所有当前表示。 | 为了演示在URL路由中使用POST方法，首先让我们创建一个HTML表单，并使用POST方法将表单数据发送到URL。将以下脚本另存为login.html1234567891011&lt;html&gt; &lt;body&gt; &lt;form action = "http://localhost:5000/login" method = "post"&gt; &lt;p&gt;Enter Name:&lt;/p&gt; &lt;p&gt;&lt;input type = "text" name = "nm" /&gt;&lt;/p&gt; &lt;p&gt;&lt;input type = "submit" value = "submit" /&gt;&lt;/p&gt; &lt;/form&gt; &lt;/body&gt;&lt;/html&gt; 运行如下的flask代码123456789101112131415161718from flask import Flask, redirect, url_for, requestapp = Flask(__name__) @app.route('/success/&lt;name&gt;')def success(name): return 'welcome %s' % name @app.route('/login',methods = ['POST', 'GET'])def login(): if request.method == 'POST': user = request.form['nm'] return redirect(url_for('success',name = user)) else: user = request.args.get('nm') return redirect(url_for('success',name = user)) if __name__ == '__main__': app.run(debug = True) 表单数据将POST到表单标签的action子句中的URL。 http://localhost/login映射到login()函数。由于服务器通过POST方法接收数据，因此通过以下步骤获得从表单数据获得的“nm”参数的值：表单数据将POST到表单标签的action子句中的URL。 Flask 模板在大型应用中,把业务逻辑和表现内容放在一起,会增加代码的复杂度和维护成本. 模板其实是一个包含响应文本的文件,其中用占位符(变量)表示动态部分,告诉模板引擎其具体的值需要从使用的数据中获取 使用真实值替换变量,再返回最终得到的字符串,这个过程称为’渲染’ Flask 是使用 Jinja2 这个模板引擎来渲染模板 使用模板的好处 视图函数只负责业务逻辑和数据处理(业务逻辑方面) 而模板则取到视图函数的数据结果进行展示(视图展示方面) 代码结构清晰,耦合度低 使用 render_template() 方法可以渲染模板，你只要提供模板名称和需要 作为参数传递给模板的变量就行了。 Flask 会在 templates 文件夹内寻找模板。因此，如果你的应用是一个模块， 那么模板文件夹应该在模块旁边；如果是一个包，那么就应该在包里面：情形 1 : 一个模块:123/application.py/templates /hello.html 情形 2 : 一个包:1234/application /__init__.py /templates /hello.html]]></content>
      <categories>
        <category>Tobeinsert</category>
      </categories>
      <tags>
        <tag>Tobeinsert</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[5030.大模型-数据集-文档分割]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-25.LLM-%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86%2F5030.%E5%A4%A7%E6%A8%A1%E5%9E%8B-%E6%95%B0%E6%8D%AE%E9%9B%86-%E6%96%87%E6%A1%A3%E5%88%86%E5%89%B2%2F</url>
    <content type="text"><![CDATA[一、文档分割方法基于规则的切分方法 字符/固定大小分块：按固定字符数或令牌数切分，简单但可能破坏语义连贯性。例如，指定每块300个字符，并设置重叠（如100字符）以减少信息割裂。 内容感知分块：利用标点符号、段落或标题等结构特征分割。例如，使用NLTK或spaCy进行句子或段落分割。 递归分块：多级分割策略，先按段落或标题分割，若块仍过大则进一步细分，适用于长文本。 特殊文档类型分割：针对Markdown、XML等结构化文档，使用预定义分隔符（如标题层级）分割。 基于语义的切分方法 语义聚类分割：通过嵌入模型（如BERT）计算句子或段落的语义相似度，将相关文本合并为块。例如，LlamaIndex的SemanticSplitterNodeParser。 滑动窗口与相似度阈值：结合滑动窗口和余弦相似度，动态调整分块边界，确保语义连贯性。 基于内容类型的切分针对pdf文件，一个文件中通常包含多种不同的内容组成（图片、表格、正文等等），grobid 可以识别pdf 文件中的内容格式，将pdf分割成图片，表格，正文等。 基于大模型（LLM）的智能分割 问答类文档拆分：通过大模型提取问题，按问题-答案对分割。例如，使用提示词模板让LLM输出问题列表，再根据问题切分文本。 标题提取与分块：利用LLM识别文档中的标题层级，按一级标题切分长文档。例如，缩减文本行后输入大模型提取标题，再按标题分割。 混合方法嵌入与规则结合：例如，先按语义分块，再通过规则优化块大小，兼顾灵活性与效率。 二、成熟工具推荐通用文本分割工具 LangChain文本分割器：支持字符分割（CharacterTextSplitter）、递归分割（RecursiveCharacterTextSplitter）及Markdown/代码分割，适用于大模型开发场景。 Linux命令行工具：如split（按行/字节分割）、csplit（正则匹配分割）、dd（按块分割），适合处理大型纯文本文件。 结构化文档工具 PDF分割 PDF Size Splitter：按大小或页数分割PDF，支持批量操作。 PyPDF2/pdfminer：Python库，可编程解析PDF内容并按段落分割。 表格分割 Excel文件分割器：按行或份数切分Excel文件，支持.xls/.xlsx格式。 语义分割与LLM集成工具 LlamaIndex：提供语义分割器（SemanticSplitter），结合嵌入模型和相似度阈值动态分块。 自定义LLM提示词模板：如通过大模型提取标题或问题，实现智能分块（需结合OpenAI或本地LLM）。 多格式文件处理工具 文本分割器：如西盟TXT分割器、闪电文本分割器，支持按大小、行数或章节分割文本。 音频/视频分割：如传华MP3切割器，按时间点切割音频文件。 三、方法选择与工具对比| 场景 | 推荐方法 | 工具示例 | 优势 || ———————— | ——————- | ——————————- | —————————- || 长文本处理（如论文） | 递归分块 + 语义聚类 | LangChain递归分割器、LlamaIndex | 兼顾结构完整性与语义相关性 || 结构化文档（如Markdown） | 特殊格式分割 | MarkdownTextSplitter | 保留文档层级逻辑 || 问答类文档 | LLM提取问题分块 | 自定义提示词模板 + GPT模型5 | 精准定位问答对，提升检索效率 || 大型文件传输 | 固定大小分块 | Linux split命令、FileSplit | 快速分割，支持合并还原 | Reference https://blog.csdn.net/weixin_45312236/article/details/138537792]]></content>
      <categories>
        <category>LLM</category>
      </categories>
      <tags>
        <tag>LLM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[wifi配置]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-11.raspberrypi%2Fwifi%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"></content>
      <categories>
        <category>编程拾慧</category>
        <category>raspberrypi</category>
      </categories>
      <tags>
        <tag>raspberrypi</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[3.5显示屏驱动安装]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-11.raspberrypi%2F%E6%98%BE%E7%A4%BA%E5%B1%8F%E9%A9%B1%E5%8A%A8%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[一、硬件连接准备好树莓派和3.5寸触摸屏，我这里使用的是3B，其他的型号应该也能兼容。 3.5寸屏幕 二、驱动安装直接上电后，屏幕是无法把内容显示出来的，默认是白屏，需要按照相关的驱动驱动下载github链接`shellgit clone https://github.com/goodtft/LCD-show cd LCD-show/sudo chmod +x LCD35-showsudo ./LCD35-show # 显示3.5寸屏幕，使用3.5寸屏幕时，hdmi会不显示。sudo ./LCD35-show 90 # 显示3.5寸屏幕，屏幕旋转90度。 sudo ./LCD-hdmi # 显示hdmi屏幕` 安装GUI服务xorg是X11的一个实现，而X Window System是一个C/S结构的程序，Xorg只是提供了一个X Server，负责底层的操作当你运行一个程序的时候，这个程序会连接到X server上，由X server接收键盘鼠标输入和负责屏幕输出窗口的移动，窗口标题的样式等等。——引用自百度百科xorg词条。lightdm(Light Display Manager)，轻量级的linux桌面管理器。 Reference参考资料： http://www.lcdwiki.com/zh/3.5inch_RPi_Display]]></content>
      <categories>
        <category>编程拾慧</category>
        <category>raspberrypi</category>
      </categories>
      <tags>
        <tag>raspberrypi</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DataBase-文献数据库-PubMed_PMC]]></title>
    <url>%2F01.%E7%9F%A5%E8%AF%86-04.Database%2FDataBase-%E6%96%87%E7%8C%AE%E6%95%B0%E6%8D%AE%E5%BA%93-PubMed_PMC%2F</url>
    <content type="text"><![CDATA[PMC（PubMed Central）和 PubMed 都是由美国国家生物技术信息中心（NCBI）维护的生物医学领域重要学术资源，但两者功能有所不同。往往也是我们研究过程中跳不过去的几个调研渠道。PubMed是一个免费的文献检索平台，主要收录生物医学领域的论文摘要与引文信息，覆盖包括MEDLINE在内的超过3000万篇文献，用户可通过关键词、作者等方式快速定位相关研究。PMC则是一个开放获取的全文数据库，专门存储同行评审的生物医学与生命科学期刊论文的全文内容，尤其要求受美国国立卫生研究院（NIH）等机构资助的研究成果在此公开共享。PubMed的部分文献会链接至PMC的免费全文，但PMC中的内容也会被整合到PubMed的检索结果中。两者互为补充，PubMed侧重文献发现与摘要检索，PMC则聚焦于全文的开放共享，共同推动科学知识的传播与利用。但是除了进行文章调研，在遗传分析领域，对这类开源全文的依赖更甚。同时为了提高资源检索的速度，记录相关开源资料的获取方式，来提高本地业务的稳定性，同时也满足收集癖（毕竟下载了，就等于看过了(手动狗头~)。 PMC 下载资源PMC 本身直接开源了其收录的文献资源 https://pmc.ncbi.nlm.nih.gov/tools/textmining/ ，可以直接进行下载获取，通过该链接可以批量获取PMC收录的所有全文信息，同时PMC也提供了API 接口：https://www.ncbi.nlm.nih.gov/research/bionlp/APIs/BioC-PMC/通过已知的ID可以直接检索获取全文的信息，示例12345https://www.ncbi.nlm.nih.gov/research/bionlp/RESTful/pmcoa.cgi/BioC_[format]/[ID]/[encoding]# The parameters are:# format: xml or json# ID: PubMed ID (such as 17299597) or PMC ID (such as PMC1790863)# encoding: unicode or ascii 可以通过PMID或 PMC ID直接获取文章信息 PubMedAPIsPubmed提供的 API 接口：使用 efetch 工具，指定返回格式为 XML 或 JSON。 通过文章title查找PMID 1234title="Caution advised in the use of CFTR modulator treatment for individuals harboring specific CFTR variants"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?db=pubmed&amp;term=&#123;title&#125;[Title]# https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?db=pubmed&amp;term=Caution%20advised%20in%20the%20use%20of%20CFTR%20modulator%20treatment%20for%20individuals%20harboring%20specific%20CFTR%20variants[Title] 通过 PMID 获取 DOI 示例 URL：1https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&amp;id=27214204&amp;retmode=xml 操作步骤： 将 PMID列表 替换为具体的 PMID（多个 PMID 用逗号分隔）。 解析返回的 XML/JSON 数据，提取 字段。 示例代码（PMID 12345678 获取 DOI）：1https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&amp;id=12345678&amp;retmode=xml 通过 DOI 反向查找 PMID API 接口：使用 esearch 工具，结合 term 参数搜索 DOI。 示例 URL： 1https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?db=pubmed&amp;term=DOI值[DOI]&amp;retmode=json 操作步骤： 将 DOI值 替换为目标 DOI（需 URL 编码，例如 10.1038/nature12345 → 10.1038%2Fnature12345）。 解析返回的 JSON 数据中的 IdList 字段，获取 PMID。 示例代码（DOI 10.1038/nature12345 查找 PMID）： 1https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?db=pubmed&amp;term=10.1038/nature12345[DOI]&amp;retmode=json ClinVarclinVar提供了可以下载所有变异对应文献PMID的记录https://ftp.ncbi.nlm.nih.gov/pub/clinvar/tab_delimited/var_citations.txt123456789#AlleleID VariationID rs nsv citation_source citation_id15041 2 397704705 PubMed 2574186815041 2 397704705 PubMedCentral 454475315042 3 397704709 PubMed 2061386215044 5 267606829 PubMed 2081838315044 5 267606829 PubMed 2085859915046 7 200401432 PubMed 2532663515046 7 200401432 PubMedCentral 454475315046 214885 200401432 PubMed 25741868 该文件可以通过AlleleID检索获取clinvar收录的所有相关文献,如果我们没有AlleleID，可以下载clinvar.vcf 检索获取。也可以通过variant_summary, 该文件记录了更多的变异信息，除了vcf相关变异描述，还提供了基因、转录本、cHGVS和pHGVS进行检索。]]></content>
      <categories>
        <category>知识沉淀</category>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>数据库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[5003.大模型-架构-DeepSeek-0.概述]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-25.LLM-%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86%2F5003.%E5%A4%A7%E6%A8%A1%E5%9E%8B-%E6%9E%B6%E6%9E%84-DeepSeek-0.%E6%A6%82%E8%BF%B0%2F</url>
    <content type="text"></content>
      <categories>
        <category>LLM</category>
      </categories>
      <tags>
        <tag>LLM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[文档转换-MarkItDown]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-20.MachineLearn_%E6%89%A9%E5%B1%95%E5%BA%93%2F%E6%96%87%E6%A1%A3%E8%BD%AC%E6%8D%A2-MarkItDown%2F</url>
    <content type="text"><![CDATA[Web官方页面]]></content>
      <categories>
        <category>Tobeinsert</category>
      </categories>
      <tags>
        <tag>Tobeinsert</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MobaXterm 配置]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-00.%E7%BC%96%E7%A8%8B%E8%BE%85%E5%8A%A9%E5%B7%A5%E5%85%B7%2FMobaXterm%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[配置需要中转链接登录的远程Session -&gt; SSH -&gt; Network setting -&gt; jump host]]></content>
      <categories>
        <category>环境配置</category>
      </categories>
      <tags>
        <tag>Windows</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[notepad++ 配置]]></title>
    <url>%2F03.%E5%BA%94%E7%94%A8-03.windows%2FNotePad%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[添加Markdownnotepad++是一个十分强大的编辑器，除了可以用来制作一般的纯文字说明文件，也十分适合编写计算机程序代码。Notepad++ 不仅有语法高亮度显示，也有语法折叠功能，并且支持宏以及扩充基本功能的外挂模组。当时对markdown支持不够。这里通过插件与自定义语法让notepad++变成一个markdown书写工具。 下载所需文件 由于GitHub不时抽风，我这里将需要用到的工具打包上传到网盘。下载链接提取码：4hvm 导入语法规则打开Notepad++，点击“语言” 选择“自定义语言格式” 点击“导入”，选择下载并解压后文件夹中的“userDefineLang_markdown.xml”文件。导入完成后重启notepad++，点击“语言”，选择“markdown”即可。 安装实时预览插件打开notepad++，点击“设置”，选择“导入-导入插件”，将之前下载的文件中的“NppMarkdown.dll”导入即可。 打开插件打开notepad++，点击“插件”，选择“NppMarkdown” 在右侧出现的“preview markdown”窗口底部，勾选“live preview” 同时点击“preview”即可。]]></content>
      <categories>
        <category>环境配置</category>
      </categories>
      <tags>
        <tag>Windows</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Software-CNV检测-ExonDepth]]></title>
    <url>%2F03.%E5%BA%94%E7%94%A8-01.software_in_NGS%2FSoftware-CNV%E6%A3%80%E6%B5%8B-ExonDepth%2F</url>
    <content type="text"><![CDATA[ExonDepth githubExonDepth 帮助文档 ExomeDepth 是一个 R 软件包，旨在使用高通量 DNA 序列数据检测遗传拷贝数变异 (CNV)。虽然外显子组包含在包的名称中，但实际上它在较小的面板上表现最佳，因为包的分析利用了（通常）并行运行的大量样本之间的紧密相关结构。这些紧密的相关性正是 ExomeDepth 在为每个测试样本构建参考样本时所寻找的（默认要求对照集中需要存在一个相关性大于0.97的样本，否则失败），并且输出的质量通常会根据相关性结构而变化。 Tool Version Language Availability Methods Number of evaluated parameters Year (paper publication) Citationsa PMID Bench- marked in [14] Atlas-CNV 0 R and Perl program https://github.com/theodorc/Atlas-CNV It normalizes individual read depth data to average read depth per target, converting it to reads per kilobase million (RPKM). It computes log2 scores for each sample/median ratio at every exon, assessing sample quality via SampleQC, checking StDev of log2 scores and analysis of variance (ANOVA) on mean RPKM coverage. 2 2019 14 30890783 No ClearCNV 0.306 Python program https://github.com/bihealth/clear-cnv It utilizes match scores to group samples based on coverage patterns. It employs data normalization, scaled z-scores, and r-scores to identify copy number variations (CNVs) in both multi-exon and single-exon regions. 7 2022 1 35751599 No ClinCNV 1.18.3 R, Java, Python program https://github.com/imgag/ClinCNV ClinCNV employs an algorithm that combines the strengths of circular binary segmentation and hidden Markov model–based techniques to perform multi-sample normalization and CNV calling. 2 2022b 6 – No CNVkit 0.9.10 Python program https://github.com/etal/cnvkit It uses targeted and the nonspecifically captured off-target reads to calculate log2 copy ratios across the genome. 18 2016 1212 27100738 No Cobalt 0.8.0 Python program https://github.com/ARUP-NGS/cobalt It introduces two algorithmic adaptations to improve accuracy in a hidden Markov model. A method for computing target and copy number–specific emission distributions and they perform pointwise maximum posteriori HMM decoding to improve sensitivity for small CNV. 8 2022 0 35854218 No CODEX2 1.3.00 R package https://github.com/yuchaojiang/CODEX2 Based on CODEX package, it models the GC content bias and normalizes the read depth data for CNV detection via a Poisson latent factor model. 8 2018 39 30477554 Yes (v.1.2.0) CoNVaDING 1.2.1 Perl program https://github.com/molgenis/CoNVaDING Combination of ratio scores and Z-scores of the sample of interest compared to the selected normalized control samples. 7 2016 67 26864275 Yes (v.1.2.0) DECoN 2.0.1 R program https://github.com/RahmanTeam/DECoN Modifies ExomeDepth package by altering the hidden Markov model probabilities to depend upon the distance between exons. 3 2016 59 28459104 Yes (v.1.0.1) ExomeDepth 1.1.16 R package https://github.com/vplagnol/ExomeDepth Beta-binomial model with GC correction and hidden Markov model to combine likelihood across exons. 3 2012 516 22942019 Yes (v.1.1.10) GATK-gCNV 4.5.00 Java, Python, R program https://github.com/broadinstitute/gatk It calculates read counts over specified genomic regions per sample; it clusters technically similar samples using principal component analysis to reduce biases and enhance efficiency. After estimating chromosomal ploidy, it denoises read depth, infers CNVs via a unified model using the Viterbi algorithm 35 2023 0 37604963 No pan-elcn.MOPS 1.20.00 R package https://github.com/bioinf-jku/panelcn.mops Adaptation of cn.MOPS package, which decomposes variations in coverage across samples into integer copynumbers and noise by means of its mixture components and Poisson distributions. 13 2017 53 28449315 Yes (v.1.0.0) VisCap 0.8 R program https://github.com/pughlab/VisCap It determines the portion of sequence coverage allocated to genomic intervals and calculates log2 ratios compared to the median of reference samples with a matching test setup.CNV candidates are identified when log2 ratios surpass thresholds set by the user. 2 2016 49 26681316 No]]></content>
      <categories>
        <category>NGS</category>
        <category>software</category>
        <category>编程拾慧</category>
        <category>R</category>
        <category>Linux</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[大模型- AI IDE]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-20.MachineLearn_%E5%AE%9E%E8%B7%B5%E9%A1%B9%E7%9B%AE%2F%E5%A4%A7%E6%A8%A1%E5%9E%8B-%E9%A1%B9%E7%9B%AE%E4%BB%A3%E7%A0%81%E8%87%AA%E5%8A%A8%E5%8C%96%E5%AE%A1%E6%A0%B8%2F</url>
    <content type="text"><![CDATA[截至目前，辅助编码的大模型已经层出不穷 微软旗下的GitHub与OpenAI合作推出的 GitHub Copilot 阿里云的AI编码助手通义灵码 百度推出的基于文心大模型智能编码助手 Comate 亚马逊云科技出品的 CodeWhispere Mistral AI今年7月发布其专为全球开发人员的代码生成任务而构建的最新型号 Codestral Mamba Copilot.Live官网可以抓取解析一个网址然后通过对话的形式，对网页内容进行获取和交互。 GitHub Copilot官网由 GitHub 和 OpenAI 合作开发，专为 Visual Studio Code、Visual Studio、Neovim 和 JetBrains IDE 的用户量身定制。自 2021 年 6 月推出以来，GitHub Copilot 已成为行业标准，超过 37,000 家企业和三分之一的财富 500 强公司正在利用其功能。 Amazon Code Whisperer官网 AI IDEZed AICursor官网帮助文档Cursor仅支持图形用户（Linux支持Ubuntu的图形），没有纯命令行版本。类似vsCode，可以直接引用vscode的扩展模块，支持多个模型（部分模型收费）。可以从零进行项目代码的创建。能自己优化prompt，例如： 常用快捷键 Tab：自动填充 Ctrl+K：编辑代码 Ctrl+L：回答用户关于代码和整个项目的问题，也可以编辑代码（功能最全面），支持逐一文件逐一代码款进行处理更新（accept / reject） Ctrl+i：编辑整个项目代码（跨文件编辑代码） 可以解析整个项目的所有代码，并提供优化建议，优化建议涵盖： 代码组织结构 代码逻辑完善 配置文件管理 日志优化 测试 版本管理 文档检查 代码风格修改方式除了项目结构外，其他的相关调整会逐一文件提供参考（chat模式）。也可以使用composer模式，会直接进行代码的创建和更改。可以让代码自己进行逐级的多次代码完善。 step1 更清晰的项目结构 更好的错误处理 配置管理 更强大的 IO 操作 现代化的构建系统要完成迁移,还需要: 将现有的 ACMG 相关代码移动到 internal/acmg 包中 添加更多单元测试 更新文档 迁移其他辅助功能需要我继续完善其他部分吗？ step2 完整的 ACMG 处理框架 更详细的配置选项 完善的日志系统 基础的单元测试框架接下来需要: 从原有代码迁移具体的 ACMG 判断逻辑 添加更多测试用例 完善文档 添加性能优化需要我继续完善其他部分吗 step3 结构化的变异数据模型 更清晰的 ACMG 处理逻辑 辅助函数的封装 更完善的测试覆盖需要我继续完善其他部分吗？ step4 数据库访问层，支持高效查询 缓存系统，提高性能 性能监控和统计 更完善的主程序流程需要我继续完善其他部分吗？ step5 并发工作池，提高处理效率 批处理功能，优化内存使用 进度报告功能，提供实时反馈 更好的错误处理和上下文管理 step6 配置验证和默认值处理 灵活的输出格式化 统一的错误处理机制 step7 完整的数据验证系统 数据清理和标准化功能 灵活的数据导入功能 直接生成的代码基本无法编译，但是通过项目的自纠错（不人工写代码）可以最终达到能正编译的情况。 aider官网支持 GPT-4o or o1-preview 、 Claude 3.5 Sonnet等多个模型，可以使用本地模型（一定程度应对沙盒~） 以ollama平台模型进行代码优化的部署安装 模型准备以 llama2 为例 12345678910111213# Pull the modelollama pull llama2# Start your ollama serverollama serve# In another terminal window...python -m pip install -U aider-chatexport OLLAMA_API_BASE=http://127.0.0.1:11434 # Mac/Linuxsetx OLLAMA_API_BASE http://127.0.0.1:11434 # Windows, restart shell after setxaider --model ollama_chat/&lt;model&gt; 软件安装 123456789101112# 安装aiderpython -m pip install aider-installaider-install# Change directory into your code basecd /to/your/project# Work with Claude 3.5 Sonnet on your codeaider --model sonnet --anthropic-api-key your-key-goes-here# Work with GPT-4o on your codeaider --model gpt-4o --openai-api-key your-key-goes-here windsurf官网 帮助文档说明文档 优势：可以链接远程服务器 与 Cursor Composer 相类似的，在Windsurf中他叫Cascade， Cascade 有两种模式：编辑(允许修改你的代码) 和 聊天。相比Cursor，Cascade可以直接安装所需要的依赖（Cursor只会提示） 代码更新会比较简洁，只会列出更新的概况和更新的文件，不会直接显示代码内容 模型 Linux版本 命令行版本 自定义prompt 链接远程服务器 git 兼容 Cursor Ubuntu 无 支持定义角色 不支持 差，会生成命令，但是需要点击执行 windsurf 支持 疑似无 不支持定义 支持 可以直接执行系统命令 aider 支持 是 即便是基于命令行版本的aider，也并不适合用于自动化审核，]]></content>
      <categories>
        <category>LLM</category>
      </categories>
      <tags>
        <tag>LLM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Nucleotide_Transformer-building_and_evaluating_robust_foundation_models_for_human_genomics]]></title>
    <url>%2F01.%E7%9F%A5%E8%AF%86-05.Paper%2FNucleotide_Transformer-building_and_evaluating_robust_foundation_models_for_human_genomics%2F</url>
    <content type="text"><![CDATA[Nucleotide Transformer 是一个 DNA 序列预训练基础模型，参数范围从 5000 万到 25 亿个参数，并集成了来自 3,202 个人类基因组的信息和850各其他物种的基因组信息。期实例包括在蛋白质序列上训练语言模型，他们的任务是预测大型蛋白质序列数据集中的隐藏氨基酸。当使用迁移学习将这些蛋白质 LM 应用于下游任务时，即使在数据稀缺的情况下，也表现出了与以前的方法竞争甚至超越预测蛋白质结构和功能等任务的能力。 除了蛋白质序列之外，DNA 序列中编码的依赖性模式在理解基因组过程中发挥着重要作用，从表征调控区域到评估单个变体在单倍型环境中的影响。在此背景下，专门的深度学习 (DL) 模型经过训练可以发现有意义的 DNA 模式。例如，DL 模型已被用于根据 DNA 序列预测基因表达，最近的进展将卷积神经网络和 Transformer 架构相结合，实现编码上游 100 KB 处调控元件。现代基因组学研究产生的大量数据既带来了机遇，也带来了挑战。一方面，跨物种和种群的自然变异的复杂模式很容易获得；另一方面，为了从未标记的数据集中准确提取信号，需要能够处理大规模数据的强大深度学习方法。在核苷酸序列上训练的大型基础模型似乎是应对这一挑战的值得探索的方法。 构建了强大的基础模型来编码基因组序列，称为核苷酸转化器（NT），并提出了系统研究和基准来评估其性能。我们通过构建四个不同大小的不同 LM 开始我们的研究，参数范围从 5 亿到 25 亿不等。这些模型在三个不同的数据集上进行了预训练，包括人类参考基因组、3,202 个不同人类基因组的集合以及来自不同物种的 850 个基因组。训练后，我们以两种方式利用每个模型的表示（嵌入）。为了评估 NT 在适应各种任务时性能的稳定性，我们在一组不同的 18 个基因组策划的预测任务上训练每个模型，并将它们与三个替代 DNA 基础模型以及一个最先进的非基础模型进行比较，使用系统的十倍交叉验证程序。此外，为了扩大我们的评估范围，我们比较了我们表现最好的模型和三个最先进的监督基线模型，这些模型已经针对当前的特定任务进行了优化。为了破译在预训练期间学到的序列特征，我们探索了模型的注意力图和困惑度，并对它们的嵌入进行了数据降维。此外，我们还评估了嵌入通过基于零样本的评分来模拟人类功能上重要的遗传变异影响的能力。扩展了第一组实验的结果，我们开发了第二组四个 LM，参数大小从 5 亿减少到 5000 万，以研究此类模型的缩放定律。 我们成功构建了一个模型，该模型仅用十分之一的参数数量就实现了之前最佳模型的性能，同时感知域大小增加了一倍。 Result构建了具有不同参数大小和数据集的 Transformer 模型： 一个 5 亿参数模型，根据从人类参考基因组中提取的序列进行训练（’人类参考 500M’); 5 亿参数模型（“1000G 500M”）在 3,202 个遗传多样性人类基因组上进行训练； 25 亿参数模型（“1000G 2.5B”）在 3,202 个遗传多样性人类基因组上进行训练； 25 亿参数模型，涵盖来自不同门的 850 个物种（“Multispecies 2.5B”），其中包括 11 种模型生物（图 1c 和补充表 1-4）。为了评估这些模型在预测不同分子表型方面的功效，我们从公开资源中整理了 18 个基因组数据集，包括剪接位点预测任务 (GENCODE)、启动子任务 (真核启动子数据库) 以及组蛋白修饰和增强子任务 (ENCODE)，每个数据集都设计了具有合理的大小，以实现快速和严格的交叉验证程序。 通过两种不同的技术评估了我们的变压器模型：probing and fine-tuning。probing是指使用学习过的LM嵌入来简化 DNA序列作为预测基因组标记模型的输入特征。具体来说，我们使用逻辑回归或由最多两个隐藏层组成的小型多层感知器（MLP）来探测LMs的十个任意选择的层。在微调的情况下，LM头部被替换为分类或回归头部，并使用参数有效技术进行再训练（方法）。为了确保不同模型之间的公平和准确的比较，我们实施了十倍交叉验证策略。 为了将我们预先训练的基础模型方案与该领域的标准监督方法进行比较，我们在 18 个任务（方法）中的每一个上从头开始训练 BPNet 卷积架构的不同变体。 BPNet 架构已广泛应用于基因组学领域，并代表了一种非常强大的默认架构，用于通过监督学习从头开始对小型数据集进行建模。我们观察到原始 BPNet 模型跨任务的强大性能（平均 Matthews 相关系数 (MCC) 为 0.665），我们可以通过将其大小增加到 2800 万个参数（平均 MCC 为 0.683）来改进，这证实了直接监督卷积架构的性能非常好非常适合基因组学任务（图2a，b）。接下来，我们在基准数据集上评估了 NT 模型的探测和微调与这些监督基线模型的比较。如果所得的两个标准差分别重叠或优于报告的基线值，我们认为该模型相当于或优于其他模型。 使用此标准，NT 模型在 5 个任务中与基线 BPNet 模型相匹配，并在 18 个任务中的 8 个中通过单独探测（补充图 1 和补充表 6）超过了基线 BPNet 模型，并且显着优于原始标记的探测。与最近的工作一致，我们观察到最佳性能既依赖于模型又依赖于层（补充表 8）。我们还注意到，如早期工作所示，通过使用最后一层的嵌入永远无法实现最高的模型性能。例如，在增强子类型预测任务中，我们观察到性能最高层和性能最低层之间的相对差异高达 38%，这表明各层学习表示存在显着差异（补充图 3）。与我们的探测策略相比，我们的微调模型要么匹配（n = 6），要么超越（n = 12）18 个基线模型（图 2a、b 和补充表 7 和 9）。值得注意的是，经过微调的 NT 模型优于所探测的模型，而更大且更多样化的模型始终优于较小的模型。这些结果支持针对特定任务微调 NT 基础模型以实现卓越性能的必要性。我们的结果还表明，在以 Multispecies 2.5B 模型为代表的多样化数据集上进行训练，在源自人类检测的多项任务上优于或匹配 1000G 2.5B 模型（图 2a、b）。这意味着增加序列多样性的策略，而不仅仅是增加模型大小，可能会提高预测性能，特别是当计算资源有限时。 微调在之前的工作中尚未得到广泛探索，可能是由于其苛刻的计算要求。我们通过采用最新的参数高效微调技术克服了这一限制，该技术仅需要总模型参数的 0.1%（图 1b 和方法）。这种方法可以在单个 GPU 上进行更快的微调，将所有微调参数的存储需求减少 1,000 倍，同时仍然提供可比的性能。在实践中，我们观察到，尽管在嵌入上使用简单的下游模型明显很简单，但严格的探测比微调更慢且计算量更大。这种差异是由于层选择、下游模型选择和这个 18 个基因组数据集的汇编提供了多样化和强大的选择，可以以统计上严格的方式审查模型在不同任务中的适应性，并与其他 DNA 自监督基础模型超参数的性能进行比较。此外，微调表现出较小的性能差异，增强了该方法的稳健性。总的来说，这种通用方法是通用的，可以适应各种任务，无需调整模型架构或超参数。这与监督模型形成鲜明对比，监督模型通常具有不同的架构，并且需要从头开始对每个任务进行训练。 以无人监督的方式检测已知的基因组元件NT 模型在没有任何监督的情况下学会了区分基因间、内含子、编码和非翻译区（UTR）的基因组序列，尽管不同层的熟练程度不同。Multispecies 2.5B 模型中，在第 1 层观察到基因间区域和基因区域之间的分离最强，其次是第 5 层上的 5’UTR 区域，以及第 21 层上大多数区域之间的分离![[01.知识-05.Paper/attachments/1735609530286.png]] 模型优化成本和性能收益预测 250M 和 500M 参数模型的训练持续时间延长到包含 1 万亿个令牌，与文献中最近的建议保持一致。 ![[01.知识-05.Paper/attachments/Pasted image 20250103150343.png]] 相同的物种训练集上训练后，都使用18个任务的下游数据进行微调和评估，4个不同大小的模型表现（训练迭代过程的表现）如下图： ![[01.知识-05.Paper/attachments/Pasted image 20250103150356.png]] MethodmodelLM 主要是在 NLP 中开发的，用于对口语进行建模。 LM 是给定任意单词序列的标记序列（通常是单词）的概率分布； LM 将返回该句子存在的概率。语言模型之所以受欢迎，是因为它们能够利用大量未标记的数据集来生成通用表示，即使在可用的监督数据很少的情况下，也可以解决下游任务。一种训练 LM 任务模型以预测序列中屏蔽位置最有可能的标记的技术通常称为 masked language modeling (MLM)。受蛋白质研究领域 MLM 获得的结果（其中蛋白质被视为句子，氨基酸被视为单词）的启发，我们应用 MLM 来训练基因组学中的 LM 转换器，将核苷酸序列视为句子和 k 聚体（k = 6）作为单词。 Transformers 是一类在机器学习领域（包括 NLP 和计算机视觉）取得突破的 DL 模型。它们由一个初始嵌入层组成，该初始嵌入层将输入序列中的位置转换为嵌入向量，然后是一系列自注意力层，这些层依次细化这些嵌入。使用 MLM 训练 LM Transformer 的主要技术称为 Transformer 的双向编码器表示 (BERT)。在 BERT 中，序列中的所有位置都可以相互关联，从而允许信息双向流动，这在 DNA 序列的背景下至关重要。在训练过程中，网络的最终嵌入被输入到语言模型头，将其转换为输入序列上的概率分布。 架构我们所有的模型都遵循 encoder-only transformer 架构。嵌入层将输入的序列转换为嵌入向量，然后添加位置编码到每个嵌入向量中，用来向模型提供位置信息。使用科学系的位置编码，最多接受1000个tokens。使用了6-mer的tokens作为序列长度（最多可以处理 6KB =6*1000的序列）。token的向量化由transformer的嵌入层实现。每个transformer层，通过一个标准化层和一个多头自注意力层来转化输入的数据。自注意力层得到的结果和transformer的输入进行整合相加后，数据经过一个新的归一化层和具有GELU激活的两层感知器处理。每个模型的多头自注意力头数、向量为度、隐藏层的神经元数和总层数如下表。![[01.知识-05.Paper/attachments/Pasted image 20250103174622.png]] 感知器最后返回的向量是语言模型头转换成序列中每个位置的现有标记的概率分布。 NT-v2在第二个版本，对模型进行了一系列更改，被证明是有效的： 每个自注意力层，位置向量不使用学习的位置向量，使用旋转向量； 使用门控线性单元和无偏差的swish激活这个改动，让模型接受的token数目提升到2048个，可以支持12kb的上下文。 训练这些模型按照 BERT 方法进行训练。在每个训练步骤中，都会对一批标记化序列进行采样。批量大小根据可用的硬件和模型大小进行调整。我们在 A100 GPU 集群上进行了所有实验，并分别以大小为 14 和 2 的序列批量训练“500M”和“2.5B”参数模型。在序列中，15% 令牌的子集中的 80% 被特殊掩码（“MASK”）令牌替换。用于在人类参考基因组和多物种数据集上进行训练，15% 令牌子集中的另外 10% 被随机选择的标准令牌（与类（‘CLS’）、填充（‘PAD’）或 MASK 令牌不同的任何令牌）替换，如 BERT 中执行的那样。 对于 1000G 数据集上的训练运行，我们跳过了额外的数据增强，因为添加的噪声大于人类基因组中存在的自然突变频率。对于每个批次，损失函数计算为每个选定位置的标记预测概率和真实标记之间的交叉熵损失之和。累积梯度以达到每批 100 万个代币的有效批次大小。我们使用了 Adam 优化器，该优化器具有学习率计划以及指数衰减率和 epsilon 常数的标准值，β= 0.9、β= 0.999 和 ϵ = 1 × 10 。在第一个预热期间，学习率在 16,000 个步骤中在 5 × 10 和 1 × 10 之间线性增加，然后按照平方根衰减下降，直到训练结束。我们稍微修改了 NT-v2 模型的超参数：优化器和学习率计划保持不变；然而，我们将批次大小增加到 512（每批次 100 万个代币）。受 Chinchilla 缩放定律的启发，与其他 DL 模型相比，我们还对 NT-v2 模型进行了更长的持续时间训练。具体来说，我们针对 3000 亿个代币预训练了 NT-v2 50M 和 250M 参数模型，而我们的“250M”和“500M”参数模型则针对多达 1 万亿个代币进行了训练，以了解起作用的缩放法则。相比之下，NT-v1 2.5B 参数模型接受了 3000 亿个令牌的训练，而其 5 亿模型则接受了 500 亿个令牌的训练。最后，我们对 NT-v2 模型使用了以下模型检查点：“50M”和“100M”模型检查点 3000 亿代币，“250M”模型检查点 8000 亿代币，“500M”模型检查点 9000 亿代币’ 模型。 Probing为了探索模型向量化的质量，评估已解决下游任务，训练后，对于每个任务，都会探索模型的每一层，并比较几种下游方法，以评估模型的表示能力。换句话说，给定下游任务的核苷酸序列数据集，我们计算并存储模型10层中每层返回的嵌入向量，然后用每层的嵌入向量作为输入，训练多个下游的模型来解决对应的下游任务。我们测试了逻辑回归（基于scikit-learn默认的超参数）和MLP。作者发现选择超参数的选择，比如学习率、激活函数、隐藏层的层数都会影响最终的表现，我们还对每个下游模型进行了超参数扫描。我们使用了十折交叉验证，训练数据集被分成10分，进行10次训练验证（轮流将其中9份作为训练数据，1份作为测试数据，进行试验）。经过处理后，会产生10个模型，然后对10个模型的性能表现取平均。在评估测试集上性能最佳的模型集之前，使用树结构 Parzen Estimator 求解器运行该过程 100 次，引导超参数空间上的搜索。因此，对于每个下游任务，对于每个预训练模型的十层，在超参数搜索结束时记录测试集上的性能。补充表 8 报告了预训练模型及其层中性能最佳探针的超参数。 这种探索策略训练了 760,000 个下游模型，提供了对训练和使用 LM 的各个方面的详细分析，例如不同层对下游任务性能的作用。 作为基线，我们评估了逻辑回归模型的性能，该模型在将标记传递到转换器层之前将标记化序列作为输入。使用原始标记化序列作为输入比使用向量（其中标记 ID 被单热编码并通过池化层（在序列长度轴上求和或平均））产生了更好的性能。 Fine-tuning除了嵌入向量的探索，还通过 IA technique 进行参数高效的微调。使用这种策略，基于要处理的不同任务，使用一个分类器或者回归头替换大语言模型的head。transformer底层和向量嵌入层的权重参数都是被冻结的，并引入一个新的可以学习权重，对每一个transformer层，我们引入三个科学系的向量 $l_k$, $l_v$ ,$l_{ff}$ ，也就是自注意力机制中的三个向量参数。$$softmax ( \frac{Q(l_k) ⊙ K^T }{\sqrt{d_k}}) (l_v ⊙ V)$$]]></content>
      <categories>
        <category>Tobeinsert</category>
      </categories>
      <tags>
        <tag>Tobeinsert</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Sequence modeling and design from molecular to genome scale with Evo]]></title>
    <url>%2F01.%E7%9F%A5%E8%AF%86-05.Paper%2FPMID39541441-Sequence_modeling_and_design_from_molecular_to_genome_scale_with_Evo%2F</url>
    <content type="text"><![CDATA[DNA是遗传物质，其重要性不言而喻，针对DNA、RNA和蛋白的研究也层出不穷。之前有很多基于Transformer的DNA模型，受限于起本身上下文长度的限制，经常采用寡居核苷酸而不是单碱基所谓模型训练/学习的基本单位。而本问开发的Evo模型，是一个包含7-billion-参数 的模型，训练以用来在全基因尺度上进行DNA序列的生成。该模型的基础架构是基于 StripedHyena architecture。 该模型结合了注意力模型和针对数据的卷积核，来高效的处理长序列数据。Evo的训练集是由 300 bilion 原核全基因组的核酸组成的全基因组数据集上进行训练的，并对输入数据进行单碱基层面的分词。并且发现预测表现伴随着数据规模的增大而获益。 Materials and methodEvo 基于 StripedHyena。 Evo包含32个block，模型宽度为4096维。 每个块包含一个序列混合层，负责沿序列维度处理信息，以及一个通道混合层，侧重于沿模型宽度维度处理信息。 在序列混合层中，Evo使用29个鬣狗层，并在其中等间隔的穿插交错着3个旋转自关注层交错。我们使用参考文献中描述的模态规范形式参数化鬣狗算子中的卷积。 对于通道混合层，Evo使用门控线性单元。 使用均方根归一化每个输入层。 Hyena layers/鬣狗层鬣狗 是一个序列混合器，由短卷积、长卷积和一个数据控制门 构成，结构如下： Self-attention layers / 自注意力层自注意力将输出序列构造为输入元素的加权组合，其中的权重由输入决定。 Positional embeddings / 位置嵌入自注意算子对输入序列中输入嵌入的不同位置没有任何概念。出于这个原因，它通常会辅以位置编码机制。StripedHyena的注意层使用旋转位置嵌入机制（RoPE）来模拟相对位置信息在我们的第二个预训练阶段，我们应用线性位置插值来扩展在第一个预训练阶段（8k序列长度）应用的旋转位置嵌入[Extending context window of large language models via positional interpolation]。当应用于比最初训练时更长的序列时，插值使模型能够继续利用其学习到的表示。我们还测试了其他位置插值方法，但发现它们在数据上的表现略差于线性插值 Tokenization / 分词器Evo使用Python中实现的UTF-8编码，以单核苷酸分辨率对DNA序列进行标记。在预训练期间，Evo使用了一个有效的词汇表，包含四个标记，每个碱基一个，总共有512个字符，这允许在后续的下游任务中扩展词汇表。我们使用额外的字符来启用在使用微调模型生成过程中使用特殊令牌的提示 OpenGenome datasets(i) the Genome Taxonomy Database (GTDB) 的细菌和古细菌数据；(ii) theIMG/VR v4 database (36) 的筛选后的原核病毒；(iii) IMG/PR database (37) 的质粒序列数据。所有的数据均表示直接拿来使用的， 数据均进行过相应的筛选和过滤。因为这部分实际用途不大，有需求查看原文，此处略过。 Training procedure整个Evo的训练过程分两个阶段，第一阶段是使用80k token的上下文，第二阶段上下文提升到131B token（分阶段是为了提高训练的效率）。累计训练了4周 * 2线程 (we trained Evo in stage 1 on 64 NVIDIA H100 GPUs for 2 weeks and on 128 NVIDIA A100 GPUs in stage 2 for an additional 2 weeks.) Dataloading序列打包生成训练数据，每个特定上下文的序列是从完整的序列数据集中随机采样的。一些DNA序列比上下文长度短，这时把多个DNA的序列连接起来，直至长度满足预期（8k 或 131k）。如果DNA序列比上下文长度长，就会截取基因组的子序列。 Hyperparameter tuning and direct model comparisons测试了 7B Transformer++,Hyena 和 StripedHyena, StripedHyena具有整体最低的困惑度和损失函数。 Scaling laws我们通过计算最优协议比较不同类型的架构，旨在评估计算最优前沿的结果。计算优化分析研究给定计算预算（通常以浮点运算（FLOPs）表示）的预训练运行的最佳性能，并通过将部分计算预算优化分配给模型大小和数据集大小来实现。体系结构类型在计算效率以及如何分配计算预算方面有所不同。我们首先通过网格搜索调优Transformer++的学习率和批处理大小等超参数，然后对所有架构使用相同的值，除了观察到数值不稳定的设置。为了解决不稳定性问题，我们逐渐降低学习率并重复实验直到收敛。在所有实验中，我们训练了上下文长度为8192个令牌的模型。对于由总FLOP计数定义的每个计算预算，我们改变了模型大小（600万到10亿个参数）和训练的令牌数量。为了测量模型性能，我们使用困惑度度量，它表明自回归模型在预测序列的下一个标记方面的表现如何，并且与下游任务的性能高度相关。perplexity值越低，性能越好。 Scaling laws procedure1) 确定预算. We use $8 × 10^{18}, 2 × 10^{19}, 4 × 10^{19}, and 8 × 10^{19}$ FLOPs (floating point operations)2) 计算要使用的模型架构处理输入数据所需要的 FLOPs3) 确定每个计算预算下模型的最优分配： a) 模型的大小，选择比较宽的范围，然后计算每个模型大小计算需要处理的相应令牌数量以达到计算预算。根据表S3选择其他超参数。我们通常观察到模型拓扑结构（深度，宽度）的微小变化只会最小程度地影响perplexity。 b) 训练每种大小的模型并记录其表现（例如，在困惑度方面）。 c) 确定最优计算分配：根据之前的分析，我们拟合一个二阶多项式作为（log）模型大小与perplexity的函数，并提取得到的计算最优点作为其最小值。计算最优点在给定的计算预算下确定模型大小和训练令牌的最佳分配。在得出计算最优缩放率之后，我们比较了体系结构并计算了令牌和模型大小的最优分配（图S5）。在图S3中，我们还显示了按架构计算次优模型尺寸的比率。我们量化了计算预算分配到模型或数据集大小的次优分配（例如，为更多令牌训练较小的模型）对困惑缩放的影响。我们估计每个计算预算的计算最优模型大小，然后将其减少一个百分比（偏移量）。相应的perplexity通过IsoFLOP曲线得到（图1F）。与Hyena和StripedHyena相比，Transformer++的复杂度缩放在计算最优边界之外迅速退化。表S3提供了为我们的缩放律分析而训练的模型的架构细节 Transformer++我们使用具有旋转位置嵌入、均方根层归一化的预处理和SwiGLU作为激活函数的现代 decoder-only Transformer架构。SwiGLU的内部宽度是模型宽度的4/3。我们对分组查询注意（GQA）进行了实验（117），发现最终损失的差异很小，这表明该技术可能适合DNA序列建模，以进一步减少推理过程中的内存占用。所有使用Transformer++的缩放结果都不使用GQA。 HyenaHyena基线的设计采用了与Transformer++模型相同的体系结构改进。我们将所有多头自注意层替换为鬣狗层，并对状态维为8的长卷积使用模态规范参数化 Protein function prediction我们使用DMS数据集来测试蛋白质和核苷酸语言模型预测蛋白质功能突变效应的能力。在所有情况下，我们都使用了原始研究作者报告的核苷酸序列。我们的分析仅限于原核和人类蛋白质，其中值得注意的是Evo训练数据集仅包含原核蛋白质序列。为了编译来自原核生物DMS研究的核苷酸信息，我们使用了ProteinGym基准中列出的所有“原核生物”数据集，我们也可以找到原始研究作者报告的核苷酸水平信息。这导致了9项研究：Firnberg等人（118）的黑酰胺酶DMS， Jacquier等人（119）的黑酰胺酶DMS， CcdB DMS(120)，多蛋白热稳定性数据集（121），IF-1 DMS (122), Rnc DMS (123), HaeIII DMS (124), VIM-2 DMS（125）和APH(3 ‘)II DMS（126）。为了编译来自人类DMS研究的核苷酸信息，我们将人类基准中使用的数据集的范围缩小到参考文献（45）中用于基准突变效应预测因子的人类数据集。我们还将分析限制在我们也可以找到原始研究作者报告的核苷酸水平信息的研究中。这导致了六项研究：CBS DMS (127), GDI1 DMS (128), PDE3A DMS (129)， Kotler等人的P53 DMS (130)， Giacomelli等人的P53 DMS（131）和BRCA1 DMS（132）。 我们将Evo（在8k背景下进行预训练）与两个基因组DNA语言模型进行了比较：GenSLM 2.5B，前者使用原核生物基因组序列的密码子词汇表进行训练（15），而Nucleotide Transformer 2B5_multi_species则使用原核和真核生物基因组序列的6 mer核苷酸词汇表进行训练（16）。我们还将Evo与几种在非冗余的蛋白质序列通用语料上训练的蛋白质语言模型进行了比较：CARP 640M（46）、ESM-1v（41）、esm - 2650m、esm - 23b（47）、ProGen2 large和ProGen2 xlarge（48）。对于提供具有多个参数大小的模型的研究，我们选择了可以在不超过GPU内存的情况下使用80 GB NVIDIA H100 GPU对所有基准研究序列进行推理的最大大小。我们还纳入了ESM-2 650M和ProGen2 large，因为这些模型有时在功能预测方面比这些模型的大版本表现得更好（44）。 为了比较核苷酸和蛋白质语言模型，我们使用了原始研究报告的所有独特的核苷酸序列及其相应的适应度值。偶尔，我们观察到报告的核苷酸序列的适应度值与报告的蛋白质序列的适应度值不同；在这种情况下，我们使用报告的核苷酸序列的适应度值，并使用翻译的序列评估蛋白质语言模型。如果由于密码子的使用不同，单个蛋白质序列存在多个核苷酸序列，则在每个唯一核苷酸序列上评估核苷酸语言模型，并在每个唯一核苷酸序列对应的编码序列上评估蛋白质语言模型；这意味着一个蛋白质语言模型可以在一个给定的研究中对相同的蛋白质序列进行多次评估。一些研究报告了涉及终止密码子的突变的适应度值；在这种情况下，我们评估了包含停止密码子的序列的核苷酸语言模型，并将这些例子从蛋白质语言模型基准中排除。我们计算了实验适应度值与序列似然（用于自回归语言模型）或序列伪似然（用于掩码语言模型）之间的Spearman相关性。当使用Evo序列似然对序列进行评分时，我们还将EOS令牌（在预训练数据中用于划分不同序列）前置到完整序列，我们从经验上发现这可以提高零射击性能。我们在零假设下评估了Spearman相关系数的统计显著性，即相关系数来自具有N - 2个自由度的t分布，其中N是我们计算相关性的样本数量。我们使用这个零分布来根据观察到的相关性计算P值。我们使用scipy Python库（https://scipy.org/）来计算这些值。]]></content>
      <categories>
        <category>LLM</category>
        <category>算法</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[5003.大模型-架构-transformer-3.tokenizer]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-25.LLM-%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86%2F5003.%E5%A4%A7%E6%A8%A1%E5%9E%8B-%E6%9E%B6%E6%9E%84-transformer-3.tokenizer%2F</url>
    <content type="text"><![CDATA[由于神经网络模型不能直接处理文本，因此我们需要先将文本转换为数字，这个过程被称为编码 (Encoding)，其包含两个步骤： 使用分词器 (tokenizer) 将文本按词、子词、字符切分为 tokens； 将所有的 token 映射到对应的 token ID。 分词策略根据切分粒度的不同，分词策略可以分为以下几种： 按词切分 (Word-based) 例如直接利用 Python 的 split() 函数按空格进行分词：1234tokenized_text = "Jim Henson was a puppeteer".split()print(tokenized_text)['Jim', 'Henson', 'was', 'a', 'puppeteer'] 这种策略的问题是会将文本中所有出现过的独立片段都作为不同的 token ，从而产生巨大的词表。而实际上很多词是相关的，例如 “dog” 和 “dogs”、“run” 和 “running”，如果给它们赋予不同的编号就无法表示出这种关联性。 词表就是一个映射字典，负责将 token 映射到对应的 ID（从 0 开始）。神经网络模型就是通过这些 token ID 来区分每一个 token。 当遇到不在词表中的词时，分词器会使用一个专门的 [UNK] token 来表示它是 unknown 的。显然，如果分词结果中包含很多 [UNK] 就意味着丢失了很多文本信息，因此一个好的分词策略，应该尽可能不出现 unknown token。 按字符切分 (Character-based) 这种策略把文本切分为字符而不是词语，这样就只会产生一个非常小的词表，并且很少会出现词表外的 tokens 。 但是从直觉上来看，字符本身并没有太大的意义，因此将文本切分为字符之后就会变得不容易理解。这也与语言有关，例如中文字符会比拉丁字符包含更多的信息，相对影响较小。此外，这种方式切分出的 tokens 会很多，例如一个由 10 个字符组成的单词就会输出 10 个 tokens，而实际上它们只是一个词。 因此现在广泛采用的是一种同时结合了按词切分和按字符切分的方式——按子词切分 (Subword tokenization)。 按子词切分 (Subword)高频词直接保留，低频词被切分为更有意义的子词。例如 “annoyingly” 是一个低频词，可以切分为 “annoying” 和 “ly”，这两个子词不仅出现频率更高，而且词义也得以保留。下图展示了对 “Let’s do tokenization!“ 按子词切分的结果： 可以看到，“tokenization” 被切分为了 “token” 和 “ization”，不仅保留了语义，而且只用两个 token 就表示了一个长词。这种策略只用一个较小的词表就可以覆盖绝大部分文本，基本不会产生 unknown token。尤其对于土耳其语等黏着语，几乎所有的复杂长词都可以通过串联多个子词构成。 加载与保存分词器分词器的加载与保存与模型相似，使用 Tokenizer.from_pretrained() 和 Tokenizer.save_pretrained() 函数。例如加载并保存 BERT 模型的分词器：1234from transformers import BertTokenizertokenizer = BertTokenizer.from_pretrained("bert-base-cased")tokenizer.save_pretrained("./models/bert-base-cased/") 同样地，在大部分情况下我们都应该使用 AutoTokenizer 来加载分词器：1234from transformers import AutoTokenizertokenizer = AutoTokenizer.from_pretrained("bert-base-cased")tokenizer.save_pretrained("./models/bert-base-cased/") 调用 Tokenizer.save_pretrained() 函数会在保存路径下创建三个文件： special_tokens_map.json：映射文件，里面包含 unknown token 等特殊字符的映射关系； tokenizer_config.json：分词器配置文件，存储构建分词器需要的参数； vocab.txt：词表，一行一个 token，行号就是对应的 token ID（从 0 开始）。 编码与解码文本前面说过，文本编码 (Encoding) 过程包含两个步骤： 分词：使用分词器按某种策略将文本切分为 tokens； 映射：将 tokens 转化为对应的 token IDs。 下面我们首先使用 BERT 分词器来对文本进行分词：123456789from transformers import AutoTokenizertokenizer = AutoTokenizer.from_pretrained("bert-base-cased")sequence = "Using a Transformer network is simple"tokens = tokenizer.tokenize(sequence)print(tokens)['using', 'a', 'transform', '##er', 'network', 'is', 'simple'] 可以看到，BERT 分词器采用的是子词切分策略，它会不断切分词语直到获得词表中的 token，例如 “transformer” 会被切分为 “transform” 和 “##er”。 然后，我们通过 convert_tokens_to_ids() 将切分出的 tokens 转换为对应的 token IDs：123ids = tokenizer.convert_tokens_to_ids(tokens)print(ids)[7993, 170, 13809, 23763, 2443, 1110, 3014] 还可以通过 encode() 函数将这两个步骤合并，并且 encode() 会自动添加模型需要的特殊 token，例如 BERT 分词器会分别在序列的首尾添加 [CLS] 和 [SEP] ：12345678910from transformers import AutoTokenizertokenizer = AutoTokenizer.from_pretrained("bert-base-cased")sequence = "Using a Transformer network is simple"sequence_ids = tokenizer.encode(sequence)print(sequence_ids)[101, 7993, 170, 13809, 23763, 2443, 1110, 3014, 102] 其中 101 和 102 分别是 [CLS] 和 [SEP] 对应的 token IDs。 注意，上面这些只是为了演示。在实际编码文本时，最常见的是直接使用分词器进行处理，这样不仅会返回分词后的 token IDs，还包含模型需要的其他输入。例如 BERT 分词器还会自动在输入中添加 token_type_ids 和 attention_mask：123456789from transformers import AutoTokenizertokenizer = AutoTokenizer.from_pretrained("bert-base-cased")tokenized_text = tokenizer("Using a Transformer network is simple")print(tokenized_text)&#123;'input_ids': [101, 7993, 170, 13809, 23763, 2443, 1110, 3014, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]&#125; 文本解码 (Decoding) 与编码相反，负责将 token IDs 转换回原来的字符串。注意，解码过程不是简单地将 token IDs 映射回 tokens，还需要合并那些被分为多个 token 的单词。下面我们通过 decode() 函数解码前面生成的 token IDs：123456789101112from transformers import AutoTokenizertokenizer = AutoTokenizer.from_pretrained("bert-base-cased")decoded_string = tokenizer.decode([7993, 170, 11303, 1200, 2443, 1110, 3014])print(decoded_string)decoded_string = tokenizer.decode([101, 7993, 170, 13809, 23763, 2443, 1110, 3014, 102])print(decoded_string)Using a transformer network is simple[CLS] Using a Transformer network is simple [SEP] 解码文本是一个重要的步骤，在进行文本生成、翻译或者摘要等 Seq2Seq (Sequence-to-Sequence) 任务时都会调用这一函数。]]></content>
      <categories>
        <category>LLM</category>
      </categories>
      <tags>
        <tag>LLM</tag>
        <tag>transformer</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[5003.大模型-架构-transformer-2.modle]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-25.LLM-%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86%2F5003.%E5%A4%A7%E6%A8%A1%E5%9E%8B-%E6%9E%B6%E6%9E%84-transformer-2.modle%2F</url>
    <content type="text"><![CDATA[除了像之前使用 AutoModel 根据 checkpoint 自动加载模型以外，我们也可以直接使用模型对应的 Model 类，例如 BERT 对应的就是 BertModel：12from transformers import BertModelmodel = BertModel.from_pretrained("bert-base-cased") 注意，在大部分情况下，我们都应该使用 AutoModel 来加载模型。这样如果我们想要使用另一个模型（比如把 BERT 换成 RoBERTa），只需修改 checkpoint，其他代码可以保持不变。 加载模型所有存储在 HuggingFace Model Hub 上的模型都可以通过 Model.from_pretrained() 来加载权重，参数可以像上面一样是 checkpoint 的名称，也可以是本地路径（预先下载的模型目录），例如：123from transformers import BertModelmodel = BertModel.from_pretrained("./models/bert/") Model.from_pretrained() 会自动缓存下载的模型权重，默认保存到 ~/.cache/huggingface/transformers，我们也可以通过 HF_HOME 环境变量自定义缓存目录。 由于 checkpoint 名称加载方式需要连接网络，因此在大部分情况下我们都会采用本地路径的方式加载模型。部分模型的 Hub 页面中会包含很多文件，我们通常只需要下载模型对应的 config.json 和 pytorch_model.bin，以及分词器对应的 tokenizer.json、tokenizer_config.json 和 vocab.txt。 保存模型保存模型通过调用 Model.save_pretrained() 函数实现，例如保存加载的 BERT 模型：12345from transformers import AutoModelmodel = AutoModel.from_pretrained("bert-base-cased")model.save_pretrained("./models/bert-base-cased/") 这会在保存路径下创建两个文件： config.json：模型配置文件，存储模型结构参数，例如 Transformer 层数、特征空间维度等； pytorch_model.bin：又称为 state dictionary，存储模型的权重。 简单来说，配置文件记录模型的结构，模型权重记录模型的参数，这两个文件缺一不可。我们自己保存的模型同样通过 Model.from_pretrained() 加载，只需要传递保存目录的路径。]]></content>
      <categories>
        <category>LLM</category>
      </categories>
      <tags>
        <tag>LLM</tag>
        <tag>transformer</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[0009.概念-术语-embeddings嵌入]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-20.MachineLearn%2F0009.%E6%A6%82%E5%BF%B5-%E6%9C%AF%E8%AF%AD-embeddings%E5%B5%8C%E5%85%A5%2F</url>
    <content type="text"><![CDATA[嵌入是值或文本、图像和音频等对象的表示，旨在供机器学习模型和语义搜索算法使用。它们根据每个物体可能有或可能没有的因素或特征，以及它们所属的类别，将这些物体转化为数学形式。 从本质上讲，嵌入能让机器学习模型找到相似的对象。给定一张照片或一份文档，使用嵌入的机器学习模型就能找到类似的照片或文档。由于嵌入可以让计算机理解单词与其他对象之间的关系，因此嵌入是人工智能 (AI) 的基础。 例如，这个二维空间右上方的文档可能彼此相关：从技术上讲，嵌入是机器学习模型创建的向量，目的是获取有关每个对象的有意义数据。 什么是机器学习中的向量？在数学中，向量是定义维空间中某一点的数组。从更实际的角度来说，向量就是一串数字，比如 {1989, 22, 9, 180}。每个数字表示对象在指定维度上的位置。 在机器学习中，使用向量可以搜索相似的对象。向量搜索算法只需在向量数据库中找到相近的两个向量即可。 为了更好地理解这一点，想像一下经度和纬度。这两个维度（分别是南北向和东西向）可以指示地球上任何地方的位置。加拿大不列颠哥伦比亚省温哥华市的经纬度坐标可以表示为 {49°15’40”N, 123°06’50”W}。这个包含两个值的列表就是一个简单的向量。 现在，试想找一个离温哥华很近的城市。人们只需查看地图，而机器学习模型则可以查看经纬度（或向量）并找到经纬度相似的地方。本拿比市位于 {49°16’N, 122°58’W}，非常靠近 {49°15’40”N, 123°06’50”W}。因此，模型可以正确地得出本拿比在温哥华附近的结论。给向量添加更多维度 现在，试想找一座不但离温哥华近，而且规模相仿的城市。在这个位置模型中，让我们在经纬度之外添加第三个“维度”：人口规模。可以将人口添加到每个城市的向量，并且可以将人口规模视为 Z 轴，而经纬度则是 Y 轴和 X 轴。 现在，温哥华的向量是 {49°15’40”N, 123°06’50”W, 662,248}。加上这第三个维度，本拿比市就不再与温哥华特别近了，因为它的人口只有 249,125 人。模型可能会找到美国华盛顿州西雅图市，其向量为 {47°36’35”N 122°19’59”W, 749,256**}。 *截至 2021 年。**截至 2022 年。 这是一个相当简单的例子，说明了向量和相似性搜索是如何工作的。但是，为了发挥作用，机器学习模型可能需要生成三个以上的维度，从而产生复杂得多的向量。 维度更多的多维向量例如，模型如何分辨哪些电视节目彼此相似，从而可能被相同的人观看？需要考虑的因素有很多：剧集长度、剧集数量、类型分类、共同观众人数、每个节目中的演员、每个节目的首播年份等等。所有这些都可以是“维度”，每个节目都可以用这些维度上的一个点来表示。 多维向量可以帮助我们确定情景喜剧宋飞正传与恐怖剧星期三是否相似。宋飞正传 1989 年首播，星期三 2022 年首播。两部剧的剧集长度不同，宋飞正传为 22-24 分钟，星期三为 46-57 分钟…等等。通过观察它们的向量，我们可以发现这两部剧在电视节目的维度表示中可能占据截然不同的点。 电视节目 类型 首播年份 剧集长度 季数（至 2023 年） 剧集数（至 2023 年） 宋飞正传 情景喜剧 1989 22-24 9 180 周三 恐怖剧 2022 46-57 1 80 我们可以用向量来表示这些数据，就像我们用向量来表示经纬度一样，但使用更多的值： 宋飞正传的向量：{[情景喜剧], 1989, 22-24, 9, 180}星期三的向量：{[恐怖剧], 2022, 46-57, 1, 8} 机器学习模型可能会认为情景喜剧欢乐酒店》与宋飞正传更为相似。该剧类型相同，于 1982 年首播，每集长度为 21-25 分钟，共播出 11 季，275 集。 宋飞正传》的向量：{[情景喜剧], 1989, 22-24, 9, 180}欢乐酒店的向量：{[情景喜剧], 1982, 21-25, 11, 275} 在我们上面的例子中，城市是经纬度两个维度上的一个点，然后我们添加第三个维度——人口。我们还从五个方面分析了这些电视节目的位置。 在机器学习模型中，电视节目不再是两个、三个或五个维度，可能是一百个或一千个维度的一个点——就看模型想要包含多少个维度。 嵌入如何工作？嵌入是利用深度学习创建向量的过程。“嵌入”是这一过程的输出结果，换句话说，是深度学习模型为该模型进行相似性搜索而创建的向量。嵌入——通过嵌入 API 将左边的文档转换为右边的三维向量就像西雅图和温哥华的纬度和经度值相近、人口数量也相当一样，彼此相近的嵌入可被视为相似。通过使用嵌入，算法可以推荐相关的电视节目，找到相似的地点，或者像在语言模型中那样，识别哪些词语可能会一起使用或彼此相似。 神经网络如何创建嵌入神经网络是模仿人脑结构的深度学习模型。正如大脑是由神经元组成，神经元之间会相互发射电脉冲一样，神经网络也是由虚拟节点组成的，当它们的输入超过给定的阈值时，就会相互通信。 神经网络由若干层组成：输入层、输出层以及这两个层之间任意数量的“隐藏”层。无论模型是如何定义的，隐藏层都能以多种方式转换输入。 创建嵌入是一个隐藏层。这通常发生在其他层处理输入之前。因此，举例来说，人类不需要定义每个电视节目在一百个不同维度上的位置。取而代之，神经网络中的隐藏层会自动完成这项工作。然后，其他隐藏层可以利用这种嵌入对电视节目进行进一步分析，以找到类似的电视节目。最后，输出层可以推荐观众可能想观看的其他节目。 创建这个嵌入层在开始时需要手动操作。程序员可以向神经网络提供如何创建嵌入、包含哪些维度等的示例。最后，嵌入层可以独立运行——尽管程序员可能会继续对模型进行微调，以产生更好的推荐。]]></content>
      <categories>
        <category>machine_learning</category>
      </categories>
      <tags>
        <tag>machine_learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大模型-编程基础-transformers-运行微调模型]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-25.LLM-%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86%2F4031.%E5%A4%A7%E6%A8%A1%E5%9E%8B-%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%80-transformers-%E8%BF%90%E8%A1%8C%E5%BE%AE%E8%B0%83%E6%A8%A1%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[加载模块前，请记得升级 transformers的版本到最新版 pip install -U transformers，开始没升级，用的 4.27 各种报错，尤其是刚接触一度怀疑本地包的参数有问题浪费了不少时间，其实发现升级到新版就都可以正常使用了。 另外由于每个模型在训练的时候，会有固定对应的分词器，因此在导入模型的时候，我们同样需要同步导入对应的分词器。 自动加载如果网络好，模型加载可以直接自动联网下载并完成模型的加载.1234from transformers import AutoModelForSeq2SeqLM # import LLMmodel_name='google/flan-t5-base'model = AutoModelForSeq2SeqLM.from_pretrained(model_name) # 导入模型tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True) #导入分词器 指定的模型如果存在，会自动从huggingface上下载对应的模型，并完成模型的加载， 加载本地模型12345678910from transformers import AutoModel #自动判断模型，如果指导具体模型也可以使用其他的模块，比如AutoModelForSeq2SeqLM、LlamaForCausalLMfrom transformers import AutoTokenizer #分词器from transformers import AutoModelForSeq2SeqLM from transformers import LlamaForCausalLMmodel_name = "/data1/liubo_data/llm_model/Meta-Llama-3-8B"model = AutoModel.from_pretrained(model_name) tokenizer = AutoTokenizer.from_pretrained(model_name,use_fast=True)Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████| 4/4 [02:07&lt;00:00, 31.90s/it] 可以看到transformers本身存在很多模型类，不同的模型类对应不同的属性功能，所以在使用前一定要确认要使用到模型，和要解决的问题。具体模型分类可以查看transformers官方文档。 模型的使用完成模型的加载后，我们就可以使用模型进行一些数据的处理。12345678910111213# 获得要处理的文本# dialogue=“who are you” # 可以是我们直接提供的文本，也可以从测试数据集中获取。dialogue = dataset['test'][index]['dialogue']# 对待处理的文本进行编码，基于模型中提供的分词器，将文本转换成数字编码inputs = tokenizer(dialogue, return_tensors='pt')# 生成模型结果，并对结果进行解码output = tokenizer.decode( # 对模型生成的数字编码结果进行解码，生成具备可读性的文本。 model.generate( # 根据模型和编码后的输入文件，获取输出数字编码的结果 inputs["input_ids"], #输入结果编码后的数组 max_new_tokens=50, # 模型参数，控制输出文本的长度。 )[0], skip_special_tokens=True) 微调模型我们前面已经介绍了，如何加载一个模型，其实加载完模型，我们就可以通过模型获得我们想要的结果，但是有些时候，原始获取的模型获得的结果并不是很符合我们的预期。这时候，我们就需要对模型进行一些微调，这里我们会开始介绍一些模型进行本地化微调的方法。 数据和模型的加载既然是微调，前提其实就是我们已经有一套预训练的参数，同时我们有一套数据集，可以用于进行参数的进一步调整，所以在开始预训练前，首先我们需要做的就是加载数据集和模型。12345678 # 加载数据集dataset_name = "knkarthick/dialogsum"dataset = load_dataset(dataset_name)# 加载模型model_name='google/flan-t5-base'original_model = AutoModelForSeq2SeqLM.from_pretrained(model_name, torch_dtype=torch.bfloat16)tokenizer = AutoTokenizer.from_pretrained(model_name) 提前了解模型的情况模型的可调节参数其实所谓的模型微调，就是通过训练数据，对神经网络中的参数进行不断的修正调整，所以在动手调节前，我们肯定需要先了解， 我们使用的模型，一共有多少参数。可以通过下面的方法了解我们的参数情况。123456789101112def print_number_of_trainable_model_parameters(model): trainable_model_params = 0 all_model_params = 0 for _, param in model.named_parameters(): all_model_params += param.numel() if param.requires_grad: trainable_model_params += param.numel() return f"trainable model parameters: &#123;trainable_model_params&#125;\n"+ f"all model parameters: &#123;all_model_params&#125;\n"+ f"percentage of trainable model parameters: &#123;100 * trainable_model_params / all_model_params:.2f&#125;%"print(print_number_of_trainable_model_parameters(model)) 我们以 Meta-Llama-3-8B 为例，可以看到该模型共计 8.03B 的参数，可以训练的模型参数占比 100%。123trainable model parameters: 8030261248all model parameters: 8030261248percentage of trainable model parameters: 100.00% 模型的性能表现不同模型的预期用途不一样，所以这部分可能会在不同用途的模型上不具备通用型，但是如果刚接触，可以以此为例。简单讲，就是直接使用模型对手上测试数据的输入进行处理，然后比较生成的结果和测试数据的预期结果之间有多大差别。 总结性模型12345678910111213141516171819202122232425262728index = 200dialogue = dataset['test'][index]['dialogue']summary = dataset['test'][index]['summary']prompt = f"""Summarize the following conversation.&#123;dialogue&#125;Summary:"""inputs = tokenizer(prompt, return_tensors='pt')output = tokenizer.decode( original_model.generate( inputs["input_ids"], max_new_tokens=200, )[0], skip_special_tokens=True)dash_line = '-'.join('' for x in range(100))print(dash_line)print(f'INPUT PROMPT:\n&#123;prompt&#125;')print(dash_line)print(f'BASELINE HUMAN SUMMARY:\n&#123;summary&#125;\n')print(dash_line)print(f'MODEL GENERATION - ZERO SHOT:\n&#123;output&#125;')]]></content>
      <categories>
        <category>LLM</category>
      </categories>
      <tags>
        <tag>LLM</tag>
        <tag>transformers</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[5003.大模型-基础架构-transformer]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-25.LLM-%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86%2F5003.%E5%A4%A7%E6%A8%A1%E5%9E%8B-%E6%9E%B6%E6%9E%84-transformer-0.%E6%A6%82%E8%BF%B0%2F</url>
    <content type="text"><![CDATA[Transformer是一种用于自然语言处理（NLP）和其他序列到序列（sequence-to-sequence）任务的深度学习模型架构，它在2017年由Vaswani等人在首次提出。Transformer架构引入了自注意力机制（self-attention mechanism），这是一个关键的创新，使其在处理序列数据时表现出色。 TransformerTransformer是一种用于自然语言处理（NLP）和其他序列到序列（sequence-to-sequence）任务的深度学习模型架构，它在2017年由Vaswani等人首次提出。Transformer架构引入了自注意力机制（self-attention mechanism），这是一个关键的创新，使其在处理序列数据时表现出色。以下是Transformer的一些重要组成部分和特点： 自注意力机制（Self-Attention）：这是Transformer的核心概念之一，它使模型能够同时考虑输入序列中的所有位置，而不是像循环神经网络（RNN）或卷积神经网络（CNN）一样逐步处理。自注意力机制允许模型根据输入序列中的不同部分来赋予不同的注意权重，从而更好地捕捉语义关系。 多头注意力（Multi-Head Attention）：Transformer中的自注意力机制被扩展为多个注意力头，每个头可以学习不同的注意权重，以更好地捕捉不同类型的关系。多头注意力允许模型并行处理不同的信息子空间。 堆叠层（Stacked Layers）：Transformer通常由多个相同的编码器和解码器层堆叠而成。这些堆叠的层有助于模型学习复杂的特征表示和语义。 位置编码（Positional Encoding）：由于Transformer没有内置的序列位置信息，它需要额外的位置编码来表达输入序列中单词的位置顺序。 残差连接和层归一化（Residual Connections and Layer Normalization）：这些技术有助于减轻训练过程中的梯度消失和爆炸问题，使模型更容易训练。 编码器和解码器：Transformer通常包括一个编码器用于处理输入序列和一个解码器用于生成输出序列，这使其适用于序列到序列的任务，如机器翻译。 1. Transformer 整体结构首先介绍 Transformer 的整体结构，下图是 Transformer 用于中英文翻译的整体结构： 可以看到 Transformer 由 Encoder 和 Decoder 两个部分组成，Encoder 和 Decoder 都包含 6 个 block。Transformer 的工作流程大体如下：第一步：获取输入句子的每一个单词的表示向量 X，X由单词的 Embedding（Embedding就是从原始数据提取出来的Feature） 和单词位置的 Embedding 相加得到。 第二步：将得到的单词表示向量矩阵 (如上图所示，每一行是一个单词的表示 x) 传入 Encoder 中，经过 6 个 Encoder block 后可以得到句子所有单词的编码信息矩阵 C，如下图。单词向量矩阵用 表示， n 是句子中单词个数，d 是表示向量的维度 (论文中 d=512)。每一个 Encoder block 输出的矩阵维度与输入完全一致。 第三步：将 Encoder 输出的编码信息矩阵 C传递到 Decoder 中，Decoder 依次会根据当前翻译过的单词 1~ i 翻译下一个单词 i+1，如下图所示。在使用的过程中，翻译到单词 i+1 的时候需要通过 Mask (掩盖) 操作遮盖住 i+1 之后的单词。上图 Decoder 接收了 Encoder 的编码矩阵 C，然后首先输入一个翻译开始符 ““，预测第一个单词 “I”；然后输入翻译开始符 ““ 和单词 “I”，预测单词 “have”，以此类推。这是 Transformer 使用时候的大致流程，接下来是里面各个部分的细节。 2. Transformer 的输入Transformer 中单词的输入表示 x由单词 Embedding 和位置 Embedding （Positional Encoding）相加得到。 2.1 单词 Embedding单词的 Embedding 有很多种方式可以获取，例如可以采用 Word2Vec、Glove 等算法预训练得到，也可以在 Transformer 中训练得到。 2.2 位置 EmbeddingTransformer 中除了单词的 Embedding，还需要使用位置 Embedding 表示单词出现在句子中的位置。因为Transformer 不采用 RNN 的结构，而是使用全局信息，不能利用单词的顺序信息，而这部分信息对于 NLP 来说非常重要。所以 Transformer 中使用位置 Embedding 保存单词在序列中的相对或绝对位置。位置 Embedding 用 PE表示，PE 的维度与单词 Embedding 是一样的。PE 可以通过训练得到，也可以使用某种公式计算得到。在 Transformer 中采用了后者，计算公式如下：其中，pos 表示单词在句子中的位置，d 表示 PE的维度 (与词 Embedding 一样)，2i 表示偶数的维度，2i+1 表示奇数维度 (即 2i≤d, 2i+1≤d)。使用这种公式计算 PE 有以下的好处： 使 PE 能够适应比训练集里面所有句子更长的句子，假设训练集里面最长的句子是有 20 个单词，突然来了一个长度为 21 的句子，则使用公式计算的方法可以计算出第 21 位的 Embedding。 可以让模型容易地计算出相对位置，对于固定长度的间距 k，PE(pos+k) 可以用 PE(pos) 计算得到。因为 Sin(A+B) = Sin(A)Cos(B) + Cos(A)Sin(B), Cos(A+B) = Cos(A)Cos(B) - Sin(A)Sin(B)。 将单词的词 Embedding 和位置 Embedding 相加，就可以得到单词的表示向量 x，x 就是 Transformer 的输入。 3. Self-Attention（自注意力机制）上图是论文中 Transformer 的内部结构图，左侧为 Encoder block，右侧为 Decoder block。红色圈中的部分为 Multi-Head Attention，是由多个 Self-Attention组成的，可以看到 Encoder block 包含一个 Multi-Head Attention，而 Decoder block 包含两个 Multi-Head Attention (其中有一个用到 Masked)。Multi-Head Attention 上方还包括一个 Add &amp; Norm 层，Add 表示残差连接 (Residual Connection) 用于防止网络退化，Norm 表示 Layer Normalization，用于对每一层的激活值进行归一化。 因为 Self-Attention是 Transformer 的重点，所以我们重点关注 Multi-Head Attention 以及 Self-Attention，首先详细了解一下 Self-Attention 的内部逻辑。 3.1 Self-Attention 结构上图是 Self-Attention 的结构，在计算的时候需要用到矩阵Q(查询),K(键值),V(值)。在实际中，Self-Attention 接收的是输入(单词的表示向量x组成的矩阵X) 或者上一个 Encoder block 的输出。而Q,K,V正是通过 Self-Attention 的输入进行线性变换得到的。 3.2 Q, K, V 的计算Self-Attention 的输入用矩阵X进行表示，则可以使用线性变阵矩阵WQ,WK,WV计算得到Q,K,V。计算如下图所示，注意 X, Q, K, V 的每一行都表示一个单词。 3.3 Self-Attention 的输出得到矩阵 Q, K, V之后就可以计算出 Self-Attention 的输出了，计算的公式如下：公式中计算矩阵Q和K每一行向量的内积，为了防止内积过大，因此除以 $d_k$ 的平方根。Q乘以K的转置后，得到的矩阵行列数都为 n，n 为句子单词数，这个矩阵可以表示单词之间的 attention 强度。下图为Q乘以$K^T$ ，1234 表示的是句子中的单词。得到 $QK^T$ 之后，使用 Softmax 计算每一个单词对于其他单词的 attention 系数，公式中的 Softmax 是对矩阵的每一行进行 Softmax，即每一行的和都变为 1. 得到 Softmax 矩阵之后可以和V相乘，得到最终的输出Z。 上图中 Softmax 矩阵的第 1 行表示单词 1 与其他所有单词的 attention 系数，最终单词 1 的输出 $Z_1$ 等于所有单词 i 的值 $V_i$ 根据 attention 系数的比例加在一起得到，如下图所示： 3.4 Multi-Head Attention在上一步，我们已经知道怎么通过 Self-Attention 计算得到输出矩阵 Z，而 Multi-Head Attention 是由多个 Self-Attention 组合形成的，下图是论文中 Multi-Head Attention 的结构图。从上图可以看到 Multi-Head Attention 包含多个 Self-Attention 层，首先将输入X分别传递到 h 个不同的 Self-Attention 中，计算得到 h 个输出矩阵Z。下图是 h=8 时候的情况，此时会得到 8 个输出矩阵Z。得到 8 个输出矩阵 $Z_1$ 到 $Z_8$ 之后，Multi-Head Attention 将它们拼接在一起 (Concat)，然后传入一个Linear层，得到 Multi-Head Attention 最终的输出Z。可以看到 Multi-Head Attention 输出的矩阵Z与其输入的矩阵X的维度是一样的。 4. Encoder 结构上图红色部分是 Transformer 的 Encoder block 结构，可以看到是由 Multi-Head Attention, Add &amp; Norm, Feed Forward, Add &amp; Norm 组成的。刚刚已经了解了 Multi-Head Attention 的计算过程，现在了解一下 Add &amp; Norm 和 Feed Forward 部分。 4.1 Add &amp; NormAdd &amp; Norm 层由 Add 和 Norm 两部分组成，其计算公式如下：其中 X表示 Multi-Head Attention 或者 Feed Forward 的输入，MultiHeadAttention(X) 和 FeedForward(X) 表示输出 (输出与输入 X 维度是一样的，所以可以相加)。 Add指 X+MultiHeadAttention(X)，是一种残差连接，通常用于解决多层网络训练的问题，可以让网络只关注当前差异的部分，在 ResNet 中经常用到：Norm指 Layer Normalization，通常用于 RNN 结构，Layer Normalization 会将每一层神经元的输入都转成均值方差都一样的，这样可以加快收敛。 4.2 Feed ForwardFeed Forward 层比较简单，是一个两层的全连接层，第一层的激活函数为 Relu，第二层不使用激活函数，对应的公式如下。X是输入，Feed Forward 最终得到的输出矩阵的维度与X一致。 4.3 组成 Encoder通过上面描述的 Multi-Head Attention, Feed Forward, Add &amp; Norm 就可以构造出一个 Encoder block，Encoder block 接收输入矩阵$X_{(nd)}$ ，并输出一个矩阵$O_{(nd)}$。通过多个 Encoder block 叠加就可以组成 Encoder。 第一个 Encoder block 的输入为句子单词的表示向量矩阵，后续 Encoder block 的输入是前一个 Encoder block 的输出，最后一个 Encoder block 输出的矩阵就是编码信息矩阵 C，这一矩阵后续会用到 Decoder 中。 5. Decoder 结构上图红色部分为 Transformer 的 Decoder block 结构，与 Encoder block 相似，但是存在一些区别： 包含两个 Multi-Head Attention 层。 第一个 Multi-Head Attention 层采用了 Masked 操作。 第二个 Multi-Head Attention 层的K, V矩阵使用 Encoder 的编码信息矩阵C进行计算，而Q使用上一个 Decoder block 的输出计算。 最后有一个 Softmax 层计算下一个翻译单词的概率。 5.1 第一个 Multi-Head AttentionDecoder block 的第一个 Multi-Head Attention 采用了 Masked 操作，因为在翻译的过程中是顺序翻译的，即翻译完第 i 个单词，才可以翻译第 i+1 个单词。通过 Masked 操作可以防止第 i 个单词知道 i+1 个单词之后的信息。下面以 “我有一只猫” 翻译成 “I have a cat” 为例，了解一下 Masked 操作。 下面的描述中使用了类似 Teacher Forcing 的概念，不熟悉 Teacher Forcing 的童鞋可以参考以下上一篇文章Seq2Seq 模型详解。在 Decoder 的时候，是需要根据之前的翻译，求解当前最有可能的翻译，如下图所示。首先根据输入 “\“ 预测出第一个单词为 “I”，然后根据输入 “\&lt;Begin> I” 预测下一个单词 “have”。Decoder 可以在训练的过程中使用 Teacher Forcing 并且并行化 训练，即将正确的单词序列 ( I have a cat) 和对应输出 (I have a cat ) 传递到 Decoder。那么在预测第 i 个输出时，就要将第 i+1 之后的单词掩盖住，注意 Mask 操作是在 Self-Attention 的 Softmax 之前使用的，下面用 0 1 2 3 4 5 分别表示 “ I have a cat “。 第一步：是 Decoder 的输入矩阵和 Mask 矩阵，输入矩阵包含 “ I have a cat” (0, 1, 2, 3, 4) 五个单词的表示向量，Mask 是一个 5×5 的矩阵。在 Mask 可以发现单词 0 只能使用单词 0 的信息，而单词 1 可以使用单词 0, 1 的信息，即只能使用之前的信息。 第二步：接下来的操作和之前的 Self-Attention 一样，通过输入矩阵X计算得到Q,K,V矩阵。然后计算Q和 $K^T$ 的乘积 $QK^T$ 。 第三步：在得到 $QK^T$ 之后需要进行 Softmax，计算 attention score，我们在 Softmax 之前需要使用Mask矩阵遮挡住每一个单词之后的信息，遮挡操作如下： 得到 Mask $QK^T$之后在 Mask $QK^T$上进行 Softmax，每一行的和都为 1。但是单词 0 在单词 1, 2, 3, 4 上的 attention score 都为 0。 第四步：使用 Mask $QK^T$与矩阵 V相乘，得到输出 Z，则单词 1 的输出向量 是只包含单词 1 信息的。 第五步：通过上述步骤就可以得到一个 Mask Self-Attention 的输出矩阵$Z_i$ ，然后和 Encoder 类似，通过 Multi-Head Attention 拼接多个输出$Z_i$ 然后计算得到第一个 Multi-Head Attention 的输出Z，Z与输入X维度一样。 5.2 第二个 Multi-Head AttentionDecoder block 第二个 Multi-Head Attention 变化不大， 主要的区别在于其中 Self-Attention 的 K, V矩阵不是使用 上一个 Decoder block 的输出计算的，而是使用 Encoder 的编码信息矩阵 C 计算的。 根据 Encoder 的输出 C计算得到 K, V，根据上一个 Decoder block 的输出 Z 计算 Q (如果是第一个 Decoder block 则使用输入矩阵 X 进行计算)，后续的计算方法与之前描述的一致。 这样做的好处是在 Decoder 的时候，每一位单词都可以利用到 Encoder 所有单词的信息 (这些信息无需 Mask)。 5.3 Softmax 预测输出单词Decoder block 最后的部分是利用 Softmax 预测下一个单词，在之前的网络层我们可以得到一个最终的输出 Z，因为 Mask 的存在，使得单词 0 的输出 Z0 只包含单词 0 的信息，如下：Decoder Softmax 之前的 Z Softmax 根据输出矩阵的每一行预测下一个单词：这就是 Decoder block 的定义，与 Encoder 一样，Decoder 是由多个 Decoder block 组合而成。 Tips训练时：第i个decoder的输入 = encoder输出 + ground truth embeding。因为知道ground truth embeding，相当于知道正确答案，网络可以一次训练完成。 预测时：第i个decoder的输入 = encoder输出 + 第(i-1)个decoder输出。首先输入start，输出预测的第一个单词 然后start和新单词组成新的query，再输入decoder来预测下一个单词，循环往复 直至end 参考资料： transformers快速入门 github仓库 transformer github 原文 Transformers: State-of-the-Art Natural Language Processing Attention Is All You Need 一文了解Transformer全貌（图解Transformer） Transformer模型详解（图解最完整版）]]></content>
      <categories>
        <category>LLM</category>
      </categories>
      <tags>
        <tag>LLM</tag>
        <tag>transformer</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[5003.大模型-基础架构-hyena鬣狗]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-25.LLM-%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86%2F5003.%E5%A4%A7%E6%A8%A1%E5%9E%8B-%E5%9F%BA%E7%A1%80%E6%9E%B6%E6%9E%84-hyena%E9%AC%A3%E7%8B%97%2F</url>
    <content type="text"><![CDATA[https://ermongroup.github.io/blog/hyena/https://arxiv.org/pdf/2302.10866https://colab.research.google.com/github/expz/annotated-hyena/blob/master/annotated_hyena.ipynb#:~:text=The%20Hyena%20architecture%20is%20an%20exciting%20development%20that,layers%20in%20transformer%20models%20enabling%20long-range%20sequence%20modeling.]]></content>
      <categories>
        <category>LLM</category>
      </categories>
      <tags>
        <tag>LLM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Detection of germline CNVs from gene panel data- benchmarking the state of the art]]></title>
    <url>%2F01.%E7%9F%A5%E8%AF%86-05.Paper%2FPMC11637760-Detection_of_germline_CNVs_from_gene_panel_data%2F</url>
    <content type="text"><![CDATA[原文链接:Detection of germline CNVs from gene panel data: benchmarking the state of the art 胚系CNV检测是遗传性疾病检测的重要手段，在遗传性疾病的检测中发挥着重要的作用。但是在NGS检测中，准确检测CNV仍然是一项非常有挑战性的任务。本文章使用4个经过验证的基因数据集合，对12中公开发软件（Atlas-CNV、ClearCNV、ClinCNV、CNVkit、Cobalt、CODEX2、CoNVaDING、DECoN、ExomeDepth、GATK-gCNV、panelcn.MOPS、VisCap）使用了他们的默认参数进行了记住测试。然后评估修改了107个参数对结果的影响。确定了13个可以提高F1分数的参数。整体而言，ClinCNV和GATK-gCNV的表现最好；GATK-gCNV在灵敏度上表现也非常好。]]></content>
      <categories>
        <category>NGS</category>
        <category>文献</category>
        <category>software</category>
      </categories>
      <tags>
        <tag>CNV</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[shell-变量设置]]></title>
    <url>%2F03.%E5%BA%94%E7%94%A8-02.Linux%E5%92%8Cshell%2Fshell-%E5%8F%98%E9%87%8F%E8%AE%BE%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[资料https://c.biancheng.net/view/773.html set设置了当前shell进程的本地变量，本地变量只在当前shell的进程内有效，不会被子进程继承和传递。 env仅为将要执行的子进程设置环境变量。 export将一个shell本地变量提升为当前shell进程的环境变量，从而被子进程自动继承，但是export的变量无法改变父进程的环境变量。 source运行脚本的时候，不会启用一个新的shell进程，而是在当前shell进程环境中运行脚本。 exec运行脚本或命令的时候，不会启用一个新的shell进程，并且exec后续的脚本内容不会得到执行，即当前shell进程结束了。]]></content>
      <categories>
        <category>Tobeinsert</category>
      </categories>
      <tags>
        <tag>Tobeinsert</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[shell-编程规范:google_shell]]></title>
    <url>%2F03.%E5%BA%94%E7%94%A8-02.Linux%E5%92%8Cshell%2Fshell-%E7%BC%96%E7%A8%8B%E8%A7%84%E8%8C%83-google_shell%2F</url>
    <content type="text"><![CDATA[官方资料 : Shell Style Guide 背景shell选择bash作为唯一允许执行可执行文件的shell脚本语言。所有的shell都应该以 #!/bin/bash 作为开头]]></content>
      <categories>
        <category>Tobeinsert</category>
      </categories>
      <tags>
        <tag>Tobeinsert</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[生育-胚胎生长发育过程]]></title>
    <url>%2F01.%E7%9F%A5%E8%AF%86-11.%E9%81%97%E4%BC%A0%E7%97%85%2F%E7%94%9F%E8%82%B2-%E8%83%9A%E8%83%8E%E7%94%9F%E9%95%BF%E5%8F%91%E8%82%B2%E8%BF%87%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[人类胚胎发育过程 一个人出生前九个月的历史，可能比之后的七十年更有趣。 ——塞缪尔·泰勒·柯尔律治英国浪漫主义诗人柯勒律治的这句诗表达了每个人都曾有的疑问：一个全新的人是如何从无到有出现的？ 一般我们把胚胎发育过程，按时间分为三个时期（早期：1~3；中期：4~7；晚期8~10）。当然还有一种情况是按胎儿发育情况分为三个阶段：胚芽期、胚胎期和胎儿期。一般我们不这么说，但是这可以帮我们更好的理解整个过程。分别是 胚芽期 胚芽期是胎儿发育最短的阶段。当精子和卵子进入输卵管时，它就开始于受孕。精子使卵子受精并产生受精卵。受精卵在大约一周的时间内开始进入子宫。在此过程中，受精卵多次分裂，最终形成两个独立的结构。一种结构最终成为胚胎（后来成为胎儿），另一种结构成为胎盘。细胞分裂继续快速进行。最终，受精卵变成囊胚。囊胚到达您的子宫并植入您的子宫内膜。如果着床成功，您的身体会立即开始产生激素来支持怀孕。这也会导致你的月经停止 胚胎期 胚胎阶段大约从怀孕第三周持续到怀孕第八周结束。囊胚开始呈现出独特的人类特征。现在它被称为胚胎。神经管（后来成为大脑和脊髓）、头部、眼睛、嘴巴和四肢等结构和器官形成。形成胎儿心脏的细胞在五到六周左右开始聚集并可以搏动。第六周左右也会形成手臂和腿的芽。到第八周结束时，胚胎的大部分器官和系统已经形成。对于很多人来说，这是怀孕期间孕吐开始的时刻。 胎儿期 胎儿发育阶段从第九周左右开始，一直持续到出生。这是胚胎正式变成胎儿的时候。胎儿在​​怀孕九周左右就会获得指定的性别，尽管您的医疗保健提供者还无法通过超声波检测到它。胎儿的主要器官和身体系统继续生长和成熟。指甲、睫毛和头发等也会生长。胎儿能够移动四肢，尽管您可能要到怀孕 20 周才感觉到。体重和长度的大部分增长发生在胎儿阶段。 发育前期第 1 个月（第 1 周至第 4 周）虽然很奇怪，但怀孕的前两周是一个“准备”期。您的身体会慢慢释放更多的激素，您的子宫会为潜在的怀孕做好准备。在第二周结束时，您的卵巢会释放一个卵子（排卵）。如果精子在排卵后立即遇到卵子，怀孕的过程就会继续。 第三周受精发生在第三周。精子和卵子结合并形成受精卵。 第 4 周微小的细胞束变成囊胚并植入子宫内膜。胎盘开始形成。胚泡周围形成不透水的囊。这是羊膜囊，它在怀孕期间为胎儿提供缓冲。 第 2 个月（第 5 周至第 8 周）第五周神经管（脑、脊髓和中枢神经系统的其他神经组织）形成。到第五周结束时，微小的“心脏”管将每分钟脉冲 110 次。 第六周变成手臂和腿的小芽也开始发育。血细胞正在形成，循环将开始。耳朵、眼睛和嘴巴的结构已经形成。您的医疗保健提供者可能可以通过阴道超声检测到形成心脏的细胞簇中的脉冲。 第 7 周骨骼开始取代软软骨，生殖器开始形成。胚胎的头部与身体其他部分的比例较大。有些人认为胚胎像小蝌蚪或海马，因为它有突出的尾巴（后退）和大头。 第 8 周所有主要器官和身体系统都在发育。胚胎有网状的手和脚。眼睛变得可见，耳朵开始形成。脐带已完全发育，有助于将氧气和血液输送到胚胎。第八周后，医疗保健提供者将胚胎称为胎儿。在出生之前它将一直是胎儿。到第二个月末，胎儿的长度约为 0.5 至 1 英寸（英寸），相当于黑豆大小。 第 3 个月（第 9 周至第 12 周）怀孕的第三个月是胚胎变成胎儿的时期。这是一个快速成长和发展的时期。胎儿发育出独特的面部特征、四肢、器官、骨骼和肌肉。到第 12 周结束时，胎儿已经确定了性别，但在接下来的几周内超声波中都看不到它。 第 9 周牙齿和味蕾开始形成。它的肌肉正在形成，它的身体形状更加接近人类的外观。但是，它的头部仍然是其长度的 50%。您的提供者也许能够通过多普勒超声波听到它的心跳。 第 10 周手臂、手、手指、脚和脚趾完全成形（不再有蹼状手指）。手指甲和脚趾甲开始发育，外耳开始形成。外生殖器也开始形成，但现在在超声波上看到它们还为时过早。 第 11 周胎儿开始进行一些探索，例如张开和闭合拳头和嘴巴。它的膝盖、肘部和脚踝都可以活动，但现在感觉不到任何踢动还为时过早。骨头已经硬化，但皮肤依然透明。面部特征更加突出。 第 12 周所有器官、四肢、骨骼和肌肉均已形成，并将继续发育以发挥全部功能。循环系统、消化系统和泌尿系统也在工作，肝脏产生胆汁。胎儿正在喝羊水和撒尿。由于最关键的发育已经发生，因此 12 周（妊娠前三个月结束）后流产的几率会大大下降。大多数人现在也开始感到孕吐有所缓解。第三个月末，胎儿长约 2.5 至 3 英寸，大约有李子大小。 发育中期怀孕中期通常被认为是这段经历中最好的部分。到这个时候，孕吐可能已经消失，怀孕初期的不适也已经消失。当胎儿在子宫内翻转时，您也可能开始感觉到运动。在这三个月期间，许多人会了解胎儿的指定性别。这通常是在 20 周左右的解剖扫描（检查身体发育的超声波）过程中完成的。 第 4 个月（第 13 至 16 周）许多人在怀孕的这个阶段开始表现出怀孕的迹象，尤其是如果您以前怀孕过。您的怀孕护理人员可以通过多普勒超声听到响亮而清晰的胎儿心跳。胎儿甚至可以吮吸拇指、打哈欠、伸展身体和做鬼脸。 第 13 周声带形成，胎儿的大头开始与身体成比例地生长。 第14周胎儿的皮肤开始变厚，细毛开始生长。它可以开始将手指放到嘴里并转动头部。外生殖器已完全发育，指纹开始形成。 第 15 周一些器官，如肠和耳朵，正在移动到永久位置。胎儿仍然利用羊水练习呼吸，但肺部已经开始发育。胎儿开始做出更有目的的动作，例如吮吸拇指或微笑。 第16周胎儿有嘴唇，耳朵也足够发育，可以听到你说话。尽管胎儿的眼睛是闭着的，但它仍然可以通过避开光线来对光线做出反应。到第四个月末，胎儿长约 5 英寸，重约 4 盎司。作为参考，它大约和鳄梨一样大。 第 5 个月（第 17 周至第 20 周） 到怀孕第五个月末，大多数人开始感觉到胎儿在动。最初的动作称为加速，感觉就像扑动。如果您的怀孕到目前为止一直很健康，您最终将接受第一次超声波检查。您甚至可能会发现胎儿的指定性别。 第17周胎儿的皮肤仍然很薄，但会开始增加脂肪。它的皮肤上覆盖着一层称为胎脂的白色涂层。这种“干酪”物质被认为可以保护胎儿皮肤免受长期暴露于羊水的影响。 第 18 周胎儿身上覆盖着胎毛，即桃子绒毛状的毛发。它有助于保持胎儿温暖并提供另一层保护。胎儿可能有睡眠-觉醒周期，如果胎儿正在睡觉，大声的噪音可能会吵醒胎儿。 第19周胎儿变得越来越强壮，大多数人开始感觉到拳打脚踢。胎儿也有自己独特的指纹，并且会打嗝。 第 20 周胎儿的指甲向手指末端生长。负责五种感觉的大脑区域开始发育。 到第五个月末，胎儿长约 9 至 10 英寸，重约 1 磅。 第 6 个月（第 21 至 24 周）如果你现在可以观察子宫内部，你会发现胎儿的皮肤呈淡红色，有皱纹，透过半透明的皮肤可以看到静脉。在怀孕的第六个月，眼睑开始分开，您可能会注意到有规律的、急促的运动。胎儿通过移动或增加脉搏来对声音作出反应。 第21周肢体运动协调且频繁。胎儿的骨髓可以帮助其产生血细胞。 第22周胎儿的抓握力越来越强，可以触及耳朵和脐带。它可以听到你的心跳、胃部隆隆声和呼吸声。 第 23 周如果早产，胎儿可能在第 23 周后在重症监护下存活。它会开始迅速向体内添加脂肪。 第 24 周胎儿的肺部已完全发育，但还不足以在子宫外工作。到第六个月末，胎儿长约 12 英寸，重约 2 磅。 第 7 个月（第 25 至 28 周）胎儿继续成熟并形成体内脂肪储备。胎儿经常改变位置并对刺激做出反应，包括声音、疼痛和光。羊水开始减少。 第 25 周更多的身体脂肪使胎儿的皮肤皱纹减少，更加丰满。它的神经系统正在迅速成熟。 第 26 周胎儿产生黑色素，这种物质赋予皮肤和眼睛颜色。胎儿的肺部开始产生表面活性剂，这是一种帮助胎儿出生后呼吸的物质。 第27周胎儿可以睁开眼睛并眨眼。它还有睫毛。 第 28 周胎儿在子宫内可能会开始头朝下，准备分娩。第七个月末，胎儿长约 14 至 15 英寸，重 2 至 3 磅。 妊娠晚期这是怀孕的最后阶段。您可能会想开始倒计时距离预产期还有几天，并希望预产期早点到来，但这个最后发育阶段的每一周都有助于胎儿为出生做好准备。在整个妊娠晚期，胎儿体重迅速增加，增加体内脂肪，这对出生后有帮助。 当您接近预产期时，您的医疗保健提供者将密切监视您。您将每两周拜访您的提供商一次，然后每周拜访一次。如果您对分娩有任何疑问，请务必询问您的提供者。 第 8 个月（第 29 周至第 32 周）胎儿继续成熟并形成体内脂肪储备。这段时间大脑发育最快。胎儿可以看到和听到大多数刺激。大多数内部系统都发育良好，但肺部可能仍不成熟。 第 29 周您可能会注意到，由于胎儿在羊膜囊中变得狭窄，踢和刺感觉更像是戳。 第30周胎儿可以控制自己的体温。它的大脑正在成熟并迅速成长。 第31周胎儿可以处理更多信息和刺激。您可能会注意到它醒着和睡着时有更明显的模式。 第32周胎儿的皮肤不再透明。除了肺和大脑之外，大多数其他器官都已发育良好并已准备好出生。 胎儿长约 17 至 18 英寸，重达 5 磅。 第 9 个月（第 33 至 36 周）在这个阶段，胎儿继续生长和成熟。在怀孕的这个阶段，肺部已接近完全发育。第九个月主要是为成长和大脑发育做最后的准备。 第 33 周胎儿的骨骼正在硬化，但大脑周围的颅骨除外，颅骨需要变软才能顺利通过产道。 第34周保护胎儿皮肤的胎脂开始变厚。 第 35 周胎儿的大脑继续生长，但重量仍仅为出生时的三分之二。 第36周胎儿失去胎毛，头上长出毛发。 胎儿长约 17 至 19 英寸，重 6 至 7 磅。 第 10 个月（第 37 至 40 周）在这最后一个月里，你随时都可能分娩。此时，胎儿的位置可能已经发生变化，为分娩做准备。理想情况下，它头朝下位于子宫内。当胎儿落入您的骨盆并准备出生时，您可能会在这最后一段时间感到非常不舒服。您的提供者可能会鼓励您进行踢动计数，这是跟踪胎儿移动量的一种方法。 第37周胎儿的脚趾甲到达脚趾末端。您可能会开始感觉到胎儿落入您的骨盆。 第 38 周胎儿每周体重增加 0.5 磅，以达到最终尺寸。 第 39 周胎儿足月，准备好迎接世界了！ 第 40 周这是您的预产期周。如果您发现任何临产迹象，请致电您的怀孕护理人员。 The fetus is about 18 to 20 inches long and weighs about 7 to 9 pounds.胎儿长约 18 至 20 英寸，重约 7 至 9 磅。 参考资料卵细胞到胚胎到胎儿：生殖和发育过程Fetal Development,胎儿发育高清图解人类胎儿发育过程Drama of Life Before Birth’: Lennart Nilsson’s Landmark 1965 Photo Essay]]></content>
      <categories>
        <category>Tobeinsert</category>
      </categories>
      <tags>
        <tag>Tobeinsert</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ip和端口可用性查询]]></title>
    <url>%2F01.%E7%9F%A5%E8%AF%86-08.IT%E5%90%91%E7%9F%A5%E8%AF%86%E6%9D%82%E8%AE%B0%2Fip%E5%92%8C%E7%AB%AF%E5%8F%A3%E5%8F%AF%E7%94%A8%E6%80%A7%E6%9F%A5%E8%AF%A2%2F</url>
    <content type="text"><![CDATA[Telnettelnet IP 端口 例如：telnet 192.168.1.1 22 ncnc -z ip 端口号 例如：nc -z 192.168.1.1 22 nc -uz 192.168.1.1 53 #参数u是UTP协议 curl -v IP 端口号例如：curl -v 192.168.1.1:22 sshssh -v -p 端口号 用户@IP 例如：ssh -v -p 22 root@192.168.1.1 wgetwget IP:端口号 例如：wget 192.168.1.1:21]]></content>
      <categories>
        <category>知识沉淀</category>
        <category>IT</category>
      </categories>
      <tags>
        <tag>Tobeinsert</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[dify-构建个人知识库助理]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-20.MachineLearn_%E5%AE%9E%E8%B7%B5%E9%A1%B9%E7%9B%AE%2Fdify-%E6%9E%84%E5%BB%BA%E4%B8%AA%E4%BA%BA%E7%9F%A5%E8%AF%86%E5%BA%93%E5%8A%A9%E7%90%86%2F</url>
    <content type="text"></content>
      <categories>
        <category>LLM</category>
      </categories>
      <tags>
        <tag>LLM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python-性能调优-并行-多进程-multiprocessing]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-02.python%2FPython-%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%98-%E5%B9%B6%E8%A1%8C-%E5%A4%9A%E8%BF%9B%E7%A8%8B-multiprocessing%2F</url>
    <content type="text"><![CDATA[官方文档 multiprocessing 是一个使用类似于 threading 模块的 API 支持生成进程的包。 multiprocessing 包提供本地和远程并发，有效地避免了通过使用子进程而不是线程来实现全局解释器锁。因此， multiprocessing模块允许程序员充分利用给定机器上的多个处理器。 Pool 并行池实现一个典型的例子是 Pool 对象提供了一种方便的方法，可以跨多个输入值并行执行函数，跨进程分配输入数据（数据并行性）。以下示例演示了在模块中定义此类函数以便子进程可以成功导入该模块的常见做法。这是使用 Pool 数据并行性的基本示例， 1234567891011from multiprocessing import Pooldef f(x): return x*xif __name__ == '__main__': with Pool(5) as p: # 设置启动5个进程 # 提供一个要并行的函数，和对应需要进行并行处理的数据（list）的格式 print(p.map(f, [1, 2, 3])) # [1, 4, 9] # 返回的是一个list，和输入的list一一对应，对应输入list中每个元素经过所提供函数处理后的返回值 process]]></content>
      <categories>
        <category>编程拾慧</category>
        <category>python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python-性能调优-并行-多线程-threading]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-02.python%2FPython-%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%98-%E5%B9%B6%E8%A1%8C-%E5%A4%9A%E7%BA%BF%E7%A8%8B-threading%2F</url>
    <content type="text"><![CDATA[threading 用于提供线程相关的操作，线程是应用程序中工作的最小单元。python当前版本的多线程库没有实现优先级、线程组，线程也不能被停止、暂停、恢复、中断。 threading模块提供的类：Thread, Lock, Rlock, Condition, [Bounded]Semaphore, Event, Timer, local。 threading 模块提供的常用方法： threading.currentThread(): 返回当前的线程变量。 threading.enumerate(): 返回一个包含正在运行的线程的list。正在运行指线程启动后、结束前，不包括启动前和终止后的线程。 threading.activeCount(): 返回正在运行的线程数量，与len(threading.enumerate())有相同的结果。 threading 模块提供的常量：threading.TIMEOUT_MAX 设置threading全局超时时间。 代码实例：123456789101112131415161718192021222324# coding:utf-8import threadingimport timeISOTIMEFORMAT='%Y-%m-%d %X'def action(arg): print 'sub thread start!the thread name is:%s\r' % threading.currentThread().getName() , print time.strftime( ISOTIMEFORMAT, time.localtime( time.time() ) ) time.sleep(5) print 'the arg is:%s\r' %arg , print time.strftime( ISOTIMEFORMAT, time.localtime( time.time() ) )for i in xrange(1000): t =threading.Thread(target=action,args=(i,)) t.setDaemon(True) # 是否设置守护线程# print time.strftime( ISOTIMEFORMAT, time.localtime( time.time() ) ) t.start() while True: #判断正在运行的线程数量,如果小于5则退出while循环, #进入for循环启动新的进程.否则就一直在while循环进入死循环 if(len(threading.enumerate()) &lt; 20): breakt.join()print 'main_thread end! :',print time.strftime( ISOTIMEFORMAT, time.localtime( time.time() ) ) 线程 多线程模块参考资料]]></content>
      <categories>
        <category>编程拾慧</category>
        <category>python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python-关键字-0.不常用关键字汇总]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-02.python%2FPython-%E5%85%B3%E9%94%AE%E5%AD%97-0.%E4%B8%8D%E5%B8%B8%E7%94%A8%E5%85%B3%E9%94%AE%E5%AD%97%E6%B1%87%E6%80%BB%2F</url>
    <content type="text"><![CDATA[yieldYield 语句暂停函数的执行并将一个值发送回调用者，但保留足够的状态以使函数能够从中断处恢复。当函数恢复时，它会在上次yield 运行后立即继续执行。这使得它的代码随着时间的推移产生一系列值，而不是立即计算它们并像列表一样将它们发送回来。12345678910111213# A Simple Python program to demonstrate working of yield# A generator function that yields 1 for the first time,# 2 second time and 3 third timedef simpleGeneratorFun(): yield 1 yield 2 yield 3# Driver code to check above generator functionfor value in simpleGeneratorFun(): print(value) 123123 assert验证某个条件是否成立，不成立就退出任务。 12345678x = "hello"#if condition returns True, then nothing happens:assert x == "hello"x = "welcome"#if condition returns False, AssertionError is raised:assert x != "hello", "x should be 'hello'" nolocal12345678def myfunc1(): x = "John" def myfunc2(): x = "hello" myfunc2() return xprint(myfunc1())]]></content>
      <categories>
        <category>编程拾慧</category>
        <category>python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[行标-肿瘤体细胞变异解读规范和数据库建立的技术指南(征求意见稿)]]></title>
    <url>%2F01.%E7%9F%A5%E8%AF%86-03.standard%2F%E8%A1%8C%E6%A0%87-%E8%82%BF%E7%98%A4%E4%BD%93%E7%BB%86%E8%83%9E%E5%8F%98%E5%BC%82%E8%A7%A3%E8%AF%BB%E8%A7%84%E8%8C%83%E5%92%8C%E6%95%B0%E6%8D%AE%E5%BA%93%E5%BB%BA%E7%AB%8B%E7%9A%84%E6%8A%80%E6%9C%AF%E6%8C%87%E5%8D%97%2F</url>
    <content type="text"><![CDATA[本文件适用的体细胞变异类型为单核苷酸变异（SNV）和短片段的插入/缺失变异（Indels），不适用于胚系变异，不适用于融合、拷贝数变异（除了限制在单一基因内的缺失）或其他染色体重排。 本文件适用于对肿瘤基因体细胞序列变异致癌性的判断，是指导肿瘤精准用药的重要环节。肿瘤精准用药全流程包括以下5个步骤： 体细胞突变鉴定:体细胞突变鉴定识别癌症患者个体基因组在体细胞层面发生的变异，如单核苷酸变异（SNV）和短片段的插入/缺失变异（Indels）等。 突变注释:包括确定突变的类型、 位置以及可能影响的基因或蛋白质等，以及通过比对公共数据库，如 ClinVar、 dbSNP 或 HGMD，以了解突变的已知特性和相关研究。 突变评级:突变评级遵循 ACMG/AMP 等相关指南，将突变分为五类：致病性（P）、可能致病性（LP）、意义不明（VUS）、可能良性（ LB）和良性（B）。评级基于多条证据，包括突变的健康人群频率、蛋白功能改变等。 临床意义:评估体细胞突变对患者疾病的具体影响，包括评估突变与疾病的相关性，与特定药物的敏感性或耐药性等。 临床决策:临床医生综合以上信息做出选择用药方案、监测疾病进展、预防性干预或遗传咨询等决策行为。 本文件目标在于指导检测机构对肿瘤体细胞突变的评级，规范体细胞评级中公共数据库或内部数据库的使用，提高肿瘤体细胞评级和精准用药的准确性及有效性性。 证据等级 证据项致癌证据项人群频率数据OP4: gnomAD[3] 数据库中正常对照人群中未发现的变异（或频率极低）。 功能数据OS2: 充分验证的体外或体内的功能研究支持该变异具有致癌效应。注1： 功能实验需要具有重复性与稳定性；注2： 若OS1适用，则只有在功能研究基于特定核苷酸变化时才能使用该证据。 预测证据OVS1: 抑癌基因发生功能失活的变异， 如无义变异、移码变异、经典±1或±2剪接突变，起始密码子变异等。注1： 具体操作可参考ClinGen工作组提出的PVS1决策树，来对变异的致癌性强弱进行升降级调整[5]。 OS1: 具有与先前已确认的致癌变异相同的氨基酸变化，无论核苷酸的改变如何。需注意剪接影响的改变。 OM1: 位于关键和/或被充分认可的基因功能域，如酶的活性位点。 OM2: 已知的癌基因或抑癌基因中发生编码框内的缺失/插入或在抑癌基因中发生终止密码子丧失，引起的蛋白质长度变化。 OM4: 新的错义变异导致氨基酸变化，同一位置，另一种氨基酸变化已确认是致癌的，并且新的变异在氨基酸改变方面应该与先前被确认为致癌的变异相当或更为显著。注2： 建议采用广受认可的氨基酸差异度量标准，如Grantham&#39;s距离、 Epstein&#39;s差异系数或Miyata&#39;s距离 热点变异证据OS3: 变异位于热点区域，相同氨基酸位置上的体细胞变异在至少50个样本中观察到，并且相同氨基酸变化的样本数大于等于10。 OM3: 变异位于热点区域，相同氨基酸位置上的体细胞变异在少于50个样本中观察到，并且相同氨基酸变化的样本数大于等于10。 OP3: 变异位于热点区域，并且特定氨基酸变化的样本数少于10个。 计算证据OP1: 所有使用的生物信息学计算工具均支持变异的致癌作用（保守性/进化性、剪接效应等）。 其他证据OP2: 为恶性肿瘤已知的单一遗传病因的特定基因的体细胞变异。 良性证据项人群频率数据SBVS1: 在人群数据库中任意人群中（如东亚人、非洲人、拉丁美洲人、南亚人、欧洲人（非芬兰人）等），次等位基因的频率≥5%。 SBS1: 在人群数据库中任意人群中（如东亚人、非洲人、拉丁美洲人、南亚人、欧洲人（非芬兰人）等），次等位基因的频率≥1%。 功能数据SBS2：充分验证的体外或体内功能研究未显示出致癌效应。 预测证据SBP2: 同义变异，剪切预测算法预测它对剪切保守序列没有影响，也不会产生新的剪切位点，并且该核苷酸并不高度保守。 计算证据SBP1：所有使用的计算工具均表明该变异对基因或基因产物没有影响（保守性/进化、剪切效应等）。 证据加权规则各证据强度得分 证据强度 致癌性 良性 非常强 +8 -8 强 +4 -4 中等 +2 / 支持 +1 -1 致癌性分类的得分范围 得分范围 分类 ≥10 致癌 6-9 可能致癌 0-5 意义不明 (-6)-(-1) 可能良性 ≤-7 良性 证据不共用 证据1 证据2 不能共用的原因 OM1 OS1 都涉及从氨基酸水平评估变异的致癌性 OM1 OS3 都涉及从氨基酸水平评估变异的致癌性 OM3 OM1 都涉及从氨基酸水平评估变异的致癌性 OM3 OM4 都涉及从氨基酸水平评估变异的致癌性 OM4 OS1 都涉及从氨基酸水平评估变异的致癌性 OM4 OS3 都涉及从氨基酸水平评估变异的致癌性 OM4 OM1 都涉及从氨基酸水平评估变异的致癌性 OM2 OVS1 适用的变异类型不同 12注1： 如果变异适用OS1证据项，只有功能研究是基于特定核苷酸改变时，才能同时用OS2证据项。注2： 如果变异适用OS1证据项，只有当基于特定核苷酸改变可以观察到热点变异时，才能同时用OS3证据项。 知识库为了提高解读的准确性和解读效率，构建内部知识库已经成为行业内的实际执行标准，该方案也对知识库构建提供了一套宏观的规范和标准。 自行搭建数据库应至少考虑以下几个方面： 主要功能或目的（如变异致癌性解读数据库、人群数据库、热点突变数据库、功能学验证数据库登）， 参考来源， 数据采集方式， 存储内容与形式， 更新维护的方式和频率。 数据库的评价应考虑以下几个方面： 完整性（包括1，数据库覆盖的基因范围； 2，数据库是否能覆盖所有证据类型）， 准确性（确保收录的信息准确无误）， 时效性（确保收录的信息是最新的）为了提升体细胞变异致癌性解读的准确性，我们引入了多角度的证据类型。不同变异匹配对应的证据需要有一个可靠的知识库支持。 在知识库的搭建过程中，为了不断完善知识库，需要选择合适的变异进行多轮测试，帮助修正知识库中的问题。 结果数据构成 序号 名称 信息内容 1 肿瘤类型(Tumor Type) 肿瘤的命名应符合取得国际公认的国际疾病肿瘤学分类( International Classification of Diseases for Oncology, ICD-O)、国家癌症研究所辞典(National Cancer Institute Thesaurus, NCIt)、统一医学语言系统(Unified Medical Language System, UMLS)或同类型肿瘤学分类标准和命名规则 2 基因(Gene) 基因应以 HGNC approved symbol 应为标准名 3 变异类型(Mutation Type) 应明确突变类型为单核苷酸变异（SNV）、短片的段插入/缺失突变(Indels)，具体详见附录 A 4 变异命名描述(Mutation Description) 对于单核苷酸变异（SNV）、短片的段插入/缺失突变(Indels)，应按照人类基因组协会(Human Genome Variation Society, HGVS)命名规范进行统一的命名，包括碱基改变和氨基酸改变。 5 变异来源(Mutation Origin) 应明确证据适用胚系突变和/或体细胞突变 6 证据类型(Evidence type) 分为致癌性证据和良性证据 7 证据强度(Evidence strength) 分为非常强、强、中等和支持 8 证据等级(Evidence level) 分为非常强致癌证据（Oncogenic Very Strong， OVS）、强致癌证据（Oncogenic Strong， OS）、中等致癌证据（Oncogenic Moderate， OM）、支持致癌证据（Oncogenic Supporting， OP）、非常强良性证据（Somatic Benign Very Strong， SBVS）、强良性支持证据（Somatic Benign Strong， SBS）和支持良性证据（Somatic Benign Supporting， SBP） 9 证据来源(Source of evidence) 分为人群频率数据、功能数据、预测数据、热点变异数据、计算数据和其他数据 10 致癌性(Oncogenicity) 分为致癌、可能致癌、意义不明、可能良性、良性 11 提交者(Created user) 应记录提交人姓名或机构名称 12 提交时间(Created time) 应记录提交证据记录的日期和时间 13 审核者(Review user) 应记录审核人姓名或机构名称 14 审核时间(Review time) 应记录审核证据记录的日期和时间 测试数据集进行这种大范围开发，测试可能是我们最头疼的部分，毕竟如何用最少的数据集合，来最大范围的对我们的体系和方法进行测试验收，是开发阶段更古不变的难题。指南推荐的是ClinGen提供的10各基因的94个突变作为评测标准。 附录变异类型本来不想写这部分的，但是有个指标终于算是在一个相对官方的文章中出现了，那就是InDel和CNV到底是怎么界定，之前虽然也一直是按50bp处理（wiki建议）但是总归有些师出无名（或者这个名认可度不够~~）,也有一些方案是按30bp处理，类似变异检测软件可能直接整出来上百bp的InDel。本指南对变异类型进行了长度的明确。 变异类型 变异类型说明 WT 基因表达正常或基因为野生型 MUT 涵盖基因下所有变异类型。当证据适用于某基因或某外显子/内含子内全部变异时采用。 SNV+INDEL 涵盖基因下 SNV、 INDEL 及其子类的所有突变类型 INDEL 小片段插入或缺失突变（≤50bp） DEL 小片段缺失突变（≤50bp） DUP 小片段重复突变（≤50bp） DELINS 小片段插入缺失突变（≤50bp） INS 小片段插入突变（≤50bp） SNV 单个碱基突变 ExonDEL 基因内一个或多个外显子缺失 体细胞评级常用数据库 体细胞评级常用预测软件]]></content>
      <categories>
        <category>NGS</category>
        <category>肿瘤检测</category>
        <category>行标</category>
      </categories>
      <tags>
        <tag>解读</tag>
        <tag>循证医学</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[共识-2020-二代测序临床报告解读指引]]></title>
    <url>%2F01.%E7%9F%A5%E8%AF%86-03.standard%2F%E5%85%B1%E8%AF%86-2020-%E4%BA%8C%E4%BB%A3%E6%B5%8B%E5%BA%8F%E4%B8%B4%E5%BA%8A%E6%8A%A5%E5%91%8A%E8%A7%A3%E8%AF%BB%E6%8C%87%E5%BC%95%2F</url>
    <content type="text"><![CDATA[二代测序临床报告解读指引.pdf]]></content>
      <categories>
        <category>NGS</category>
        <category>肿瘤检测</category>
        <category>共识</category>
      </categories>
      <tags>
        <tag>解读</tag>
        <tag>循证医学</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[0009.概念-矩阵相关概念]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-20.MachineLearn%2F0009.%E6%A6%82%E5%BF%B5-%E7%9F%A9%E9%98%B5%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5%2F</url>
    <content type="text"><![CDATA[相似矩阵在线性代数中，相似矩阵是指存在相似关系的矩阵。设A，B为n阶矩阵，如果有n阶可逆矩阵P存在，使得P-1AP=B，则称矩阵A与B相似，记为A~B。对角矩阵 对角矩阵(diagonal matrix)对角矩阵(diagonal matrix)是一个主对角线之外的元素皆为0的矩阵，常写为$diag(a1，a2,…,an)$ 。对角矩阵可以认为是矩阵中最简单的一种，值得一提的是：对角线上的元素可以为 0 或其他值，对角线上元素相等的对角矩阵称为数量矩阵；对角线上元素全为1的对角矩阵称为单位矩阵。对角矩阵的运算包括和、差运算、数乘运算、同阶对角阵的乘积运算，且结果仍为对角阵。 可对角化矩阵可对角化矩阵是线性代数和矩阵论中重要的一类矩阵。如果一个方块矩阵 A 相似于对角矩阵，也就是说，如果存在一个可逆矩阵 P 使得 $P^−1AP$ 是对角矩阵，则它就被称为可对角化的。 特征值&amp;特征向量特征值 (λ)：方阵$A$ 的特征值是一个标量（单个数字）$λ$，使得存在一个非零向量 $v$（特征向量），其中以下等式成立：$$AV = λv$$ 换句话说，当您将矩阵 $A$ 乘以特征向量 $v$ 时，您会得到一个新向量，它只是 $v$ 的缩放版本（按特征值 $λ$ 缩放）。则其中国 向量$v$称为特征值$λ$对应的特征向量。特征向量在乘以矩阵 $A$ 时仅改变尺度（特征值 $λ$ ）方向保持不变。 计算方式从数学上讲，要找到特征值和特征向量，您通常可以求解以下方程来得到 $λ$ 和 $v$： $(A — λI)v = 0$其中： A 是您要查找特征值和特征向量的方阵。 λ 是您要查找的特征值。 I 是单位矩阵（对角线上有 1，其他地方有 0 的对角矩阵）。 v 是您要查找的特征向量。求解该方程涉及找到使矩阵 (A — λI) 奇异（即其行列式为零）的 λ 值，然后找到相应的 v 向量。 几何意义一个矩阵乘以一个列向量相当于矩阵的列向量的线性组合。一个行向量乘以矩阵，相当于矩阵的行向量的线性组合。 所以向量乘以矩阵之后，相当于将这个向量进行了几何变换。 之前讲了 Λ 是对角矩阵，其对角线上的元素为对应的特征值，也即$\lambda_{ii}=λ_i$。 也就是$$\left( \begin{matrix} \lambda_1 &amp; 0 &amp; \cdots &amp; 0\ 0 &amp; \lambda_2 &amp; \cdots &amp; 0\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \ 0 &amp; 0 &amp;\cdots &amp; \lambda_m \end{matrix} \right) \tag{1}$$ 这些特征值表示的是对向量做线性变换时候，各个变换方向的变换幅度。]]></content>
      <categories>
        <category>machine_learning</category>
      </categories>
      <tags>
        <tag>machine_learning</tag>
      </tags>
  </entry>
  <entry>
    <title></title>
    <url>%2F01.%E7%9F%A5%E8%AF%86-12.ACMG%E6%89%A7%E8%A1%8C%E8%AF%A6%E8%A7%A3%2F04.%E7%9B%B8%E5%85%B3%E6%95%B0%E6%8D%AE%E5%BA%93-GnomAD%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[markdown-表格技巧]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-06.markdown%2Fmarkdown-%E8%A1%A8%E6%A0%BC%E6%8A%80%E5%B7%A7%2F</url>
    <content type="text"><![CDATA[markdown 几乎已经成为工作中最常用的语言，快捷的编辑，简单的排版。但是再使用中，总会有一些数据，表格，我们需要放到文档中，其实经常使用，我们会发现其实markdown本身对于表格内的排版支持是比较差的，所以在这里记录一些在表格中进行排版的方式。 依赖于html的css进行实现假设我们有一个原本的初始表格 令牌类型 优势 ABC 1、XXXX；2、YYYY；3、ZZZZZ； 表格内换行 &lt;br&gt;表格中有时候也不是纯数字，我们需要对一些内容条目进行罗列，添加换行可以让我们的文档看起来更加清晰明了。 可以使用 实现换行。 令牌类型 优势 劣势 ABC 1、XXXX；2、YYYY；3、ZZZZZ； 表格内斜体 &lt;i&gt;&lt;/i&gt;我们使用 &lt;i&gt;3、ZZZZZ；&lt;/i&gt; 对第三条进行了斜体表示。| 令牌类型 | 优势 | 劣势 || ——– | ———————————————— | —- || ABC | 1、XXXX；2、YYYY；3、ZZZZZ； | 表格内加粗 我们使用 &lt;b&gt;3、ZZZZZ；&lt;/b&gt; 对第三条进行了斜体表示。| 令牌类型 | 优势 | 劣势 || ——– | ———————————————— | —- || ABC | 1、XXXX；2、YYYY；3、ZZZZZ； |]]></content>
      <categories>
        <category>小技巧</category>
      </categories>
      <tags>
        <tag>markdown</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[双因素的技术原理]]></title>
    <url>%2F03.%E5%BA%94%E7%94%A8-99.other%2F%E5%8F%8C%E5%9B%A0%E7%B4%A0%E7%9A%84%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%2F</url>
    <content type="text"><![CDATA[“双因素”顾名思义就是在原来“用户名+静态秘密”的基础上增加一种因素，用以判断用户身份，以确保是用户本人登录的技术。双因素认证技术如今已经广泛应用于C端用户和B端用户，双因素认证类型包含动态口令（动态密码、消息推送、邮件认证）、生物识别（指纹、人脸、虹膜、声音）、U盘证书等，由于动态口令的全场景兼容性和使用的便捷性，应用最为广泛，最常见的就是短信验证码，短信验证码就是动态口令的一种。 令牌类型动态口令除了有短信验证码，还有硬件令牌、软件令牌（APP令牌、微信/企微小程序令牌、钉钉小程序令牌、飞书小程序令牌、H5令牌）等，其中硬件令牌和软件令牌技术原理一样，短信验证码属于消息令牌，消息令牌的技术原理会有所不同，接下来就详细剖析下这两类令牌的技术原理。 总体来讲，消息令牌和软硬件令牌的主要区别是： 消息令牌是先向服务端申请动态口令，然后服务端生成后通过第三方服务发给用户端，最后再发给服务端申请认证！ 硬件/软件令牌是用户端自己生成动态口令，直接发给服务端认证，这是两者核心认证逻辑的区别； 硬件令牌和软件令牌这类令牌通常每60s自动变换1次，令牌端根据加密算法、唯一识别号和当前时间三种要素自动生成6位随机数字（也可以是4位），在服务端，也有同样的加密算法、唯一识别号和时间，以此生成同样的动态口令，验证通过； 消息令牌因为消息令牌需要短信网关，认证逻辑和原理有所不同，比如再下述情境中实现与公司现有虚拟化云桌面集成，当公司员工登录现有云桌面系统时，在原有账号认证基础上，通过手机短信验证码的方式， 增加动态密码实现双因素验证，从而有效保护用户的账号安全。我们实现消息令牌的执行路径如下： 用户输入用户名、静态密码访问Netscaler； Netscaler将用户名、静态密码转发至CKEY做认证； CKEY将用户名、静态密码转发至AD做认证； AD返回认证结果，同时CKEY生成动态码并向短信网关发送一条指令：请将该动态码发送至指定手机号（此处指定手机号是指该账号对应的手机号，AD同步获得）； 短信网关收到指令并执行发送； 用户的手机号收到短信验证码，输入登录； Netscaler将短信验证码转发至CKEY； CKEY验证动态码正确与否，然后将验证结果和AD的验证结果反馈至Netscaler； Netscaler根据反馈结果允许/禁止用户登录，当且仅当静态密码和动态口令都验证正确的情况下才允许用户登录，否则登录失败； 令牌的优势 通常情况下动态口令是6位数，每1位由“0-9”10个数字组成，所以每个动态口令有100万种变化可能； 系统后台可以设置错误尝试次数，比如3次，当错误超过3次，就会锁定当前账号（默认3分钟后自动解锁），此时服务端拒绝验证动态口令，此时被暴力破解的几率是3/100万； 正常动态码1分钟变换1次，所以等3分钟自动解锁后，动态口令已经变成了新的，前面的3次尝试无效，需要从头开始尝试，所以整体被暴力破解几率维持在3/100万； 另外默认情况下同一个动态口令只能使用1次，再次使用是无效的，比如我要登录一个系统，拿出或收到动态口令输入系统登录，此时却不小心被其他人看到了动态口令，他拿着我的动态口令去登录我的账号是无法登录的，这就有效杜绝被偷窥风险； 不同的令牌类型由于使用情境的不同本身也存在一些各自独有的优劣势。 令牌类型 优势 劣势 硬件令牌 1、完全独立存在，安全性最高；2、防爆防拆防破解，结实耐用；3、可随身携带，适用于任何场景（部分严格的机房不让带手机，也没有信号，只有硬件令牌适合）； 硬件成本 软件令牌 1、移动端软件，随身携带；2、动态码实时离线生成，不受网络信号限制，随时用随时看；3、纯软件，性价比高！ 1、受手机本身的影响，如果没电就无法使用；2、部分不让带手机的场合无法使用； 消息令牌 1、用户无感知使用，使用上最为方便； 1、同样受手机本身的影响，如果没电无法使用；2、受手机信号的影响，信号不好无法使用；3、安全性相对硬件/软件令牌低一些，因为有密码传输，有被拦截的风险；]]></content>
      <categories>
        <category>Tobeinsert</category>
      </categories>
      <tags>
        <tag>Tobeinsert</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[winAuth-双因素验证码]]></title>
    <url>%2F03.%E5%BA%94%E7%94%A8-03.windows%2FwinAuth-%E5%8F%8C%E5%9B%A0%E7%B4%A0%E9%AA%8C%E8%AF%81%E7%A0%81%2F</url>
    <content type="text"><![CDATA[官网 通过网盘分享的文件：WinAuth.exe链接: https://pan.baidu.com/s/1FPrgcQIe6YSRjvkbQJ5gIA?pwd=hdcn 提取码: hdcn]]></content>
      <categories>
        <category>Tobeinsert</category>
      </categories>
      <tags>
        <tag>Tobeinsert</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[桌面配置]]></title>
    <url>%2F03.%E5%BA%94%E7%94%A8-03.windows%2F%E6%A1%8C%E9%9D%A2%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[工作中，我们肯定会处理很多形形色色不同的文件，有时候为了高效的处理不同文件，我们会构建不同的文件夹，进行分类管理，但是这又导致我们在使用的时候，需要点击好几次进入到非常深的文件目录中。所以有一种方式就是，我们通过桌面背景对屏幕进行分区，然后对文件进行归类，这样简介的解决了上述的需求，当然桌面文件，我们可以使用快捷方式，这样可以进一步提高数据的归档效率。比如这是我自己最近再用的一张桌面背景图，当然样式不是重点，有这个思路以后，我们可以根据需要自己构建自己的分类图]]></content>
      <categories>
        <category>环境配置</category>
      </categories>
      <tags>
        <tag>Windows</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[实践说明-OSS]]></title>
    <url>%2F03.%E5%BA%94%E7%94%A8-05.%E9%98%BF%E9%87%8C%E4%BA%91%2F%E5%AE%9E%E8%B7%B5%E8%AF%B4%E6%98%8E-OSS%2F</url>
    <content type="text"><![CDATA[OSS是阿里云提供的云存储服务，简单理解就是一个网盘，可以为我们提供很多基础的远程数据存储服务，同时如果我们使用阿里云的计算服务，OSS也可以作为存储数据直接挂载到服务器上，极大的方便了我们进行数据的使用。本文主要介绍一些OSS常见使用方法/操作。官方文档OSS 和常见的网盘使用类似的，其实我们使用的主要功能就是数据的下载，上传，浏览工作。同时OSS由于其按量收费的特性，还涉及多种不同的数据类型，对应不同的数据使用情况和各自的费用标准。针对不同的平台，OSS提供了不同的工具接口： Linux ： 命令行工具：ossutil 1.0 、ossutil 2.0（预览版） 挂载工具:ossfs Windows ： 图形工具： ossbrowser 、 ossbrowser 2.0 windows环境下的图形工具使用比较简单，类似本地文件管理，本文主要介绍 Linux 的命令行工具。 Linxuossutil作为一个网盘，我们其实需要使用到的最基本的功能就是上传/下载文件，查看oss上有什么数据。 文件传输文件传输，我们可以通过两种方式实现，拷贝和同步 上传文件-cp当您需要将本地文件、图片、视频等资源上传到OSS，或者需要上传大文件至OSS时，可以使用ossutil的cp命令。您也可以指定–include和–exclude选项，批量上传符合指定条件的文件。简单命令示例：1234567891011121314151617# 上传单个文件ossutil cp examplefile.txt oss://examplebucket/desfolder/# 上传目录ossutil cp -r localfolder/ oss://examplebucket/desfolder/# 上传文件并指定时间戳ossutil cp -r localfolder/ oss://examplebucket/desfolder/ --start-time 1698718158 --end-time 1698728158# 上传并设置对象标签ossutil cp examplefile.txt oss://examplebucket/desfolder/ --tagging "abc=1&amp;bcd=2&amp;……"# 上传单个文件并指定存储类型为低频访问类型 ossutil cp examplefile.txt oss://examplebucket/desfolder/ --meta X-oss-Storage-Class:IA# 上传单个文件并指定ACL为私有ossutil cp examplefile.txt oss://examplebucket/desfolder/ --acl private meta选项设置文件存储类型。存储类型可选值为： Standard：标准存储 IA：低频访问 Archive：归档存储 ColdArchive：冷归档存储 DeepColdArchive：深度冷归档存储 acl选项设置文件的ACL。文件ACL可选值为： default：继承Bucket（默认） private：私有 public-read：公共读 public-read-write：公共读写 下载文件-cp12345678910111213141516# 下载到本地并重命名ossutil cp oss://examplebucket/destfolder/examplefile.txt localfolder/example.txt# 下载某一个文件夹（包含子目录）下所有文件的示例如下：ossutil cp -r oss://examplebucket/destfolder/ localfolder/# --exclude /--include 指定下载未见格式，下载所有格式不为 jpg 格式的文件ossutil cp oss://examplebucket/destfolder/ localfolder/ --exclude "*.jpg" -r# 下载所有文件名包含abc且不是JPG和TXT格式的文件 ossutil cp oss://examplebucket/destfolder/ localfolder/ --include "*abc*" --exclude "*.jpg" --exclude "*.txt" -r# 当批量下载失败或者需要增量下载文件时，可以通过指定--update（可缩写为-u）选项选择跳过已经成功下载的文件。如果本地与OSS没有同名文件，或本地同名文件的最后修改时间早于OSS文件，ossutil会下载该文件。如果本地已有同名文件，且文件的最后修改时间晚于OSS内文件时，ossutil会跳过该文件。示例如下： ossutil cp -r oss://examplebucket/destfolder/ localfolder/ --update# --maxdownspeed选项来限制下载的最大速度，单位为KB/s。示例如下：ossutil cp oss://examplebucket/destfolder/examplefile.txt localfolder/ --maxdownspeed 1024 同步]]></content>
      <categories>
        <category>云计算</category>
      </categories>
      <tags>
        <tag>SOP</tag>
        <tag>OSS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[测序仪发展史]]></title>
    <url>%2F01.%E7%9F%A5%E8%AF%86-01.NGS%2F%E6%B5%8B%E5%BA%8F%E4%BB%AA%E5%8F%91%E5%B1%95%E5%8F%B2%2F</url>
    <content type="text"><![CDATA[知来路，方晓归途]]></content>
      <categories>
        <category>NGS</category>
        <category>肿瘤检测</category>
      </categories>
      <tags>
        <tag>发展史</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[指南-Current_Best_Practices_for_Training_LLMs_from_Scratch]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-20.MachineLearn_%E5%AE%9E%E8%B7%B5%E9%A1%B9%E7%9B%AE%2F%E6%8C%87%E5%8D%97-Current_Best_Practices_for_Training_LLMs_from_Scratch%2F</url>
    <content type="text"><![CDATA[原文材料Current Best Practices for Training LLMs from Scratch-中译：本地文档-pdf ； 在线-pdfCurrent Best Practices for Training LLMs from Scratch 在线-pdf #]]></content>
      <categories>
        <category>LLM</category>
      </categories>
      <tags>
        <tag>LLM</tag>
        <tag>Best_Practices</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[特征工程-数据预处理-滤波算法]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-20.MachineLearn%2F1002.%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B-%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86-%E6%BB%A4%E6%B3%A2%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[滤波是一种信号处理技术。在机器学习中，滤波通常指的是对输入信号进行加工，以消除噪声、平滑信号或突出特定频率范围的信号。简单理解，就是对输入的信号进行过滤，减少数据中的噪声/波动，只留下真实/平稳/符合预期的信号，从而让我们的训练模型获得更好的表现数据预处理过程。 滤波器类型针对不同类型的数据，我们往往也需要使用不同的手段对数据进行降噪，常见的滤波器有以下几种： 线性滤波器：线性滤波器遵循可加性和比例性，通过卷积运算处理信号。 时域滤波：作用于时间序列信号。比如我们观测某只股票的历史走势，我们会在处理时使用时域滤波，计算某个时间点的股价时，可以结合时间点前后多少时/天的均值，从而可以让数据更加平缓。 频域滤波涉及傅立叶变换，允许选择性增强或抑制特定频率。低通滤波保留低频，高通滤波强调高频，带通滤波则针对特定频率范围。 线性滤波器线性滤波器的输出是输入信号与滤波器冲击响应之间的线性组合。 假设我们有一个线性滤波器，其冲击响应为 h(t)。如果我们将输入信号 x(t) 与该冲击响应进行卷积，得到输出信号 y(t)，即：$$y(t) = x(t) h(t)$$`其中， 表示卷积运算。这个公式描述了线性滤波器的输出是输入信号与滤波器冲击响应之间的线性组合。` 简单来说线性滤波器就是 输入信号跟滤波器做卷积。 2.输入信号与滤波器的冲击响应进行卷积操作。线性滤波具有可加性和比例性的特点 可加性: 即 $f(x+y) = f(x) + f(y)$假设我们有一台收音机，可以接收两个不同的广播电台发出的信号。我们将这两个信号输入到同一个滤波器中进行处理，那么输出信号就等于每个信号单独经过滤波器处理后的输出信号的总和。这意味着，如果我们想要听到两个电台发出的信号，我们可以将它们分别输入到同一个滤波器中进行处理，然后将滤波器的输出信号相加即可得到最终的信号。 比例性: 即 $f(cx) = cf(x)$假设我们有一台音响系统，音量控制旋钮可以调节输出信号的幅度。如果我们将输入信号输入到滤波器中进行处理，那么输出信号的幅度也会受到音量控制旋钮的影响。这意味着，如果我们将旋钮向右旋转，增加音量，那么输出信号的幅度也会相应地增加；如果我们将旋钮向左旋转，减小音量，那么输出信号的幅度也会相应地减小。 时域滤波时域滤波直接在时间域（针对某个点进行过滤时，会结合这个点的前或后时间点数据）中对信号进行滤波处理。例如一段音频，录音时有一些杂音干扰进入了录音中。如果你想去除这些杂音，你可以使用一个时域滤波器，例如移动平均滤波器。该滤波器通过在每个时间点上计算当前时刻和前几个时刻的平均值，对音频信号进行平滑处理，从而减少杂音的影响。这种滤波过程直接在时间域中对信号进行操作，从而改变信号的特性。 频域滤波频域滤波借助频率（机器学习中，可以是任意我们可以界定的数据强度标准）。数据处理过程只和节点数值有关，但是对数值不是简单的线性变换，而是会有非线性的变化。频率滤波基于不同的作用方式，又分为： 低通滤波 (Low-Pass Filtering)：低通滤波器允许通过低频信号，并削弱或阻止高频信号。它的原理是在频域上去除高频分量，只保留低频分量。低通滤波常用于去除高频噪声或平滑信号。例如，可以使用低通滤波器来平滑图像并去除图像中的细节或噪声。 高通滤波 (High-Pass Filtering)：高通滤波器允许通过高频信号，并削弱或阻止低频信号。它的原理是在频域上去除低频分量，只保留高频分量。高通滤波常用于强调图像或信号中的边缘、纹理或其他高频特征。例如，可以使用高通滤波器增强图像的边缘。 带通滤波 (Band-Pass Filtering)：带通滤波器允许通过特定的频率范围内的信号，并削弱或阻止其他频率范围内的信号。它的原理是在频域上选择一个频率范围，去除其他频率范围的分量。带通滤波常用于信号处理中需要提取特定频率范围内的信号或去除其他频率范围的干扰。 上面是滤波的类型，可以看到，滤波的类型，其实主要基于我们要处理的数据情况和整体策略进行划分的，接下来我们就看看常见的几种具体滤波算法。 滤波算法1. 均值滤波算术平均滤波是一种最简单常用的数字滤波算法之一，也是一种基于时间域的滤波方法。其原理是将连续采集到的一组数据进行加和，并求出其平均值，以此作为滤波后的输出值。这种方法能够有效平滑信号，去除噪声干扰，同时保留信号的趋势和主要特征。算术平均滤波的基本原理是取一段时间内的信号值的平均值作为输出。这段时间可以是固定的，也可以是动态调整的，取决于具体的实现。 初始化： 设置一个窗口大小（表示取平均的时间范围）和一个缓存区，用于存储窗口内的信号值。 输入信号： 在每个时刻，输入一个新的信号值。 更新缓存区： 将新的信号值加入缓存区，并去除窗口之外的最早的信号值，以保持窗口大小不变。 计算平均值： 对缓存区内的所有信号值取平均值。 输出： 将平均值作为输出信号。 算术平均滤波对于平稳的信号非常有效，因为它能够消除瞬时噪声。然而，它对于快速变化的信号响应较慢，因为窗口内的信号值需要时间来适应变化。 2. 一阶滞后滤波一阶滞后滤波法是一种常见的滤波方法，也被称为指数加权平均滤波。它基于一个简单的思想，即当前的输出值是前一次输出值和当前输入值的加权平均值。这种加权平均值的计算方法使得前一次的输出值在当前输出值中占有一定的比重，从而可以平滑信号，并减小由于突然变化引起的干扰。 一阶滞后滤波的基本原理是通过引入一个时间常数（time constant）来调节滞后程度。时间常数决定了滤波器对信号变化的响应速度。较大的时间常数会导致较慢的响应，更强的平滑效果，但可能较慢地跟踪信号的快速变化；较小的时间常数则会导致较快的响应，但可能对噪声更敏感。 一阶滞后滤波的差分方程可以表示为：$y[n] = (1-a)\cdot y[n-1]+a\cdot x[n]$y[n] 是当前时刻的输出。 y[n-1] 是上一时刻的输出。 x[n] 是当前时刻的输入。 \alpha 是介于 0 和 1 之间的常数，表示时间常数。在这个方程中，\alpha 越接近 1，滤波器的响应就越慢，平滑效果越强。 3.限幅消抖滤波限幅消抖滤波法是一种简单有效的数字滤波算法，常用于对采集到的离散信号进行去抖动处理。它可以去除信号中的瞬时噪声和突发干扰，同时保留信号的主要特征。 限幅消抖滤波法的原理是通过设置一个合适的阈值，将信号限制在一个固定的范围内，并消除信号中的抖动。当信号的变化速度超过阈值时，限制信号的变化幅度，以消除抖动；当信号变化速度较缓时，允许信号在一定范围内波动，以保留信号的主要特征。在实际应用中，通常将限幅消抖滤波法与其他滤波算法结合使用，以进一步提高滤波效果。 限幅（Clipping）： 首先，将输入信号限制在一个预定的范围内。这个范围由上下限值确定。如果输入信号超出这个范围，它将被截断或限制在范围的边界上。 消抖（Debouncing）： 消抖是指去除输入信号中的瞬时干扰或噪声，以获得更加平滑的输出。这可以通过在一定时间内对输入信号进行平均、滤波或延时来实现。 限幅消抖滤波的应用场景主要涉及那些由于环境原因或传感器特性而引入干扰的系统。例如，当使用传感器测量某个物理量时，由于传感器的特性或环境噪声，可能会产生一些突发的异常值。通过限制幅度和消除瞬时干扰，可以得到更加稳定和可靠的测量结果。在一些要求实时性较高的应用中，可能会选择简单的限幅操作。在对信号平滑性要求较高的场合，可能需要采用更复杂的滤波算法。 4.递推平均滤波递推平均滤波法，又称为滑动平均滤波法，是一种对于输入信号进行平滑处理的算法。该算法采用一定的方式对一定数量的输入信号进行加权平均，得到一个平滑的输出信号。具体地，递推平均滤波法使用一个固定长度的窗口，每当有新的输入信号到来时，就将窗口内的旧的信号淘汰掉，并将新的信号加入到窗口中，然后重新计算窗口内所有信号的平均值作为当前的输出信号。因此，随着新的信号不断到来，窗口内的信号会不断滑动，而输出信号也会不断变化，从而实现对输入信号的平滑处理。 递推平均滤波法的优点是简单、实时性好，对于周期性的噪声有一定的抑制效果。其缺点是在处理突变的输入信号时，输出信号会有一定的延迟，且在窗口大小不够大的情况下，噪声的抑制效果会比较有限。 下面是递推平均滤波法的算法步骤： 定义一个固定长度为 N 的窗口，并初始化窗口内的所有数据为 0。 当有新的输入信号 x_i 到来时，将窗口内的第一个信号 x_{i-N} 移除，并将新的信号 x_i 加入到窗口中。 计算窗口内所有信号的平均值，作为当前的输出信号 y_i 。 返回输出信号 y_i ，并等待下一次输入信号到来。 5.加权递推平均滤波加权递推平均滤波是一种滤波技术，它对信号的当前值和过去的滤波结果进行加权组合，以获得平滑的输出。与一阶滞后滤波不同，加权递推平均滤波使用多个权重来对不同时刻的输入信号进行加权，以更灵活地调整对历史数据的依赖程度。$y[n] = \sum _{i=0}^{N}w_i\cdot x[n-1]$ 123y[n] 是当前时刻的输出。x[n-1] 是过去的输入信号值，i 表示过去的时间步。w_i 是权重系数，表示对应时刻的权重。 权重系数的选择是加权递推平均滤波的关键，它决定了滤波器对历史数据的关注程度。通常情况下，权重系数越大，对应时刻的输入信号在输出中的影响就越大。这样设计的目的是在保持对当前值敏感的同时，平滑掉噪声或瞬时变化。 6.中值滤波中值滤波是一种非线性滤波方法，常用于去除信号中的脉冲噪声或椒盐噪声。它的原理是将一组数据排序，然后选择中间位置的值作为滤波结果。中值滤波对于去除离群值（异常值）非常有效，因为它不受异常值的影响。 下面是中值滤波的算法步骤： 选择窗口大小： 窗口大小决定了用于计算中值的数据点数量。通常，窗口大小是一个奇数，以确保存在中间位置的值。 将窗口内的数据排序： 对窗口内的数据进行排序，确定中间位置的值。 选择中值： 选择排序后的数据集的中间位置的值作为输出。 中值滤波的优势在于它能够有效地保留信号的边缘信息，而不像线性滤波那样引入过多的平滑。因此，中值滤波在一些对信号细节要求较高的应用中比较有用。需要注意的是，中值滤波可能会导致信号的某些特征被模糊化，因此在应用中需要谨慎选择邻域大小和形状，以及滤波器的使用场景。 7.中位值平均滤波法中位值平均滤波法（Median Filtering with Averaging）是一种结合了中值滤波和平均滤波的方法。在这种滤波法中，首先通过中值滤波获得当前时刻的输出，然后将该输出与一定时期内的平均值进行加权平均。这种方法的目的是综合利用中值滤波和平均滤波的优势，对信号进行平滑处理，并能够有效地去除脉冲噪声或其他突发性噪声。 中位值平均滤波法的基本步骤如下： 中值滤波： 使用中值滤波器，通过排序窗口内的数据并选择中间值，得到一个初步的滤波结果。 平均滤波： 将中值滤波得到的结果与一定时期内的平均值进行加权平均。通常，这里的平均值是使用递推平均滤波法得到的。 输出： 输出平均滤波的结果作为最终的滤波输出。 这种滤波法的优势在于中值滤波能够有效去除离群值和脉冲噪声，而平均滤波则能够对信号进行较好的平滑。通过结合这两种方法，中位值平均滤波法在一些特定噪声环境下表现得比单一滤波方法更为鲁棒。 8.一阶低通滤波低通滤波（Low Pass Filter）用于从一个信号中去除高于某个频率的成分。它的基本原理是，信号中高于某个频率的成分在信号传输或接收过程中会发生衰减，而低于该频率的成分则不受影响。因此，通过将信号通过一个低通滤波器，可以去除高频噪声，保留信号中的低频成分。 一阶低通滤波器是低通滤波的一阶离散形式，用于滤除输入信号中的高频分量，只保留低频分量。它通过减弱高频部分的幅度，从而实现对信号的平滑处理。一阶低通滤波器的基本原理涉及限制信号的变化速率，对快速变化的信号进行衰减，而对缓慢变化的信号保留。 一阶低通滤波的形式与一阶滞后滤波完全相同。倒不如说一阶滞后滤波其实就是一阶低通滤波，只不过当该滤波器用于不同的作用时，我们将其冠以了不同的称呼。 $ y[n] = (1-a)\cdot y[n-1]+a\cdot x[n]$ 其中：1234y[n] 是当前时刻的输出。y[n-1] 是上一时刻的输出。x[n] 是当前时刻的输入。\alpha 是介于 0 和 1 之间的常数，表示时间常数。 在这个方程中，\alpha 越小，时间常数越大，低通滤波器的截止频率就越低，对高频部分的抑制效果就越强。 一阶低通滤波器常用于需要平滑信号或去除高频噪声的应用场景。它们在信号处理、通信系统、控制系统等领域都有广泛的应用。 9.二阶低通滤波二阶低通滤波器是低通滤波器的二阶差分形式，用于滤除输入信号中的高频分量，同时保留低频分量。相比于一阶低通滤波器，二阶低通滤波器在频域上更为复杂，具有更陡的滤波特性。它可以更有效地抑制高频噪声，同时允许低频信号通过。 一个常见的二阶低通滤波器可以通过差分方程表示为： $y[n] = (1-\alpha -\beta)\cdot y[n-2]+2\cdot \alpha \cdot y[n-1]+\beta \cdot x[n]$ 其中： 1234y[n] 是当前时刻的输出。y[n-1] 和 y[n-2] 是上两个时刻的输出。x[n] 是当前时刻的输入。\alpha 和 \beta 是介于 0 和 1 之间的常数，分别表示两个不同的时间常数，决定了滤波器的特性。 在这个方程中，\alpha 和 \beta 的选择影响了滤波器的频率响应和阶数。较小的时间常数表示较大的截止频率，而较大的时间常数表示较小的截止频率。这样，可以根据具体需求调整时间常数来达到期望的滤波效果。 二阶低通滤波器通常应用于需要更复杂频率响应和更好滤波性能的情况，如音频处理、图像处理和控制系统等领域。 10.一阶高通滤波高通滤波（High Pass Filter）可以滤除信号中的低频部分，保留高频部分。高通滤波器的应用非常广泛，例如在音频处理中可以用来去除低频噪声、在图像处理中可以用来增强图像的边缘等。 高通滤波算法的基本思想是：将信号分解成高频和低频两部分，去掉低频部分，只保留高频部分。高通滤波的实现可以通过频域方法和时域方法两种方式实现。 频域方法是将信号转换到频域进行处理，常用的有傅里叶变换和小波变换等。通过滤波器在频域中滤除低频成分，然后再将信号转换回时域。 时域方法则是通过差分等方式，直接在时域中滤除低频部分。 一阶高通滤波器是高通滤波的一阶差分形式，用于滤除输入信号中的低频分量，同时保留高频分量。高通滤波器的作用是弱化或消除信号中的低频成分，从而突出高频变化或忽略缓慢变化的部分。一阶高通滤波器的设计原理涉及对低频分量进行衰减，保留高频部分。 一阶高通滤波器的差分方程一般表示为： $y[n]=\alpha \cdot y[n-1]+\alpha \cdot(x[n] - x[n-1])$ 1234y[n] 是当前时刻的输出。y[n-1] 是上一时刻的输出。x[n] 是当前时刻的输入。\alpha 是介于 0 和 1 之间的常数，表示时间常数，决定了滤波器的截止频率。 在这个方程中，\alpha 越小，时间常数越大，高通滤波器的截止频率就越低，对低频部分的抑制效果就越弱。 一阶高通滤波器通常应用于需要突出信号中快速变化或高频成分的应用场景。在图像处理、音频处理、传感器信号处理等领域，高通滤波器被广泛用于去除低频噪声或趋势成分。 11.二阶高通滤波二阶高通滤波器是高通滤波的二阶差分形式，用于滤除输入信号中的低频分量，同时保留高频分量。相较于一阶高通滤波器，二阶高通滤波器在频域上更为复杂，具有更陡的滤波特性。它可以更有效地抑制低频噪声，同时允许高频信号通过。 一个常见的二阶高通滤波器可以通过差分方程表示为： $y[n] = \alpha \cdot y[n-2] - \beta \cdot x[n-2] + 2\cdot \beta \cdot x[n-1] - \alpha \cdot y[n-1]$ 1234y[n] 是当前时刻的输出。y[n-1] 和 y[n-2] 是上两个时刻的输出。x[n-1] 和 x[n-2] 是上两个时刻的输入。\alpha 和 \beta 是介于 0 和 1 之间的常数，分别表示两个不同的时间常数，决定了滤波器的特性。 在这个方程中，\alpha 和 \beta 的选择影响了滤波器的频率响应和阶数。较小的时间常数表示较大的截止频率，而较大的时间常数表示较小的截止频率。这样，可以根据具体需求调整时间常数来达到期望的滤波效果。 二阶高通滤波器通常应用于需要更复杂频率响应和更好滤波性能的情况，如音频处理、图像处理和控制系统等领域。]]></content>
      <categories>
        <category>machine_learning</category>
      </categories>
      <tags>
        <tag>machine_learning</tag>
        <tag>特征工程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[0001.遗传检测常见概念]]></title>
    <url>%2F01.%E7%9F%A5%E8%AF%86-11.%E9%81%97%E4%BC%A0%E7%97%85%2F0001.%E9%81%97%E4%BC%A0%E6%A3%80%E6%B5%8B%E5%B8%B8%E8%A7%81%E6%A6%82%E5%BF%B5%2F</url>
    <content type="text"><![CDATA[每个行业方向都有其约定俗成的常用术语，这些属于对于从业者而言，简洁明了的表述了特定概念，但是对于跨行业者，可能会带来一定的理解门槛，尤其是想当然的情况下，甚至可能带来不可知的应用风险。所以后续针对每个方向，整理汇总行业内的一些术语（也许不是这个方向的转悠，而是我自己了解一个行业方向时，初见曾生迷茫的描述性话语）]]></content>
      <categories>
        <category>遗传病</category>
        <category>概念</category>
      </categories>
      <tags>
        <tag>遗传病</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[指南共识-针对生育人群的携带者筛查实验室和临床实践专家共识]]></title>
    <url>%2F01.%E7%9F%A5%E8%AF%86-11.%E9%81%97%E4%BC%A0%E7%97%85%2F%E6%8C%87%E5%8D%97%E5%85%B1%E8%AF%86-%E9%92%88%E5%AF%B9%E7%94%9F%E8%82%B2%E4%BA%BA%E7%BE%A4%E7%9A%84%E6%90%BA%E5%B8%A6%E8%80%85%E7%AD%9B%E6%9F%A5%E5%AE%9E%E9%AA%8C%E5%AE%A4%E5%92%8C%E4%B8%B4%E5%BA%8A%E5%AE%9E%E8%B7%B5%E4%B8%93%E5%AE%B6%E5%85%B1%E8%AF%86%2F</url>
    <content type="text"><![CDATA[共识原文：《针对生育人群的携带者筛查实验室和临床实践专家共识》 pdf 携带者筛查作为一项孕前/孕早期筛查技术，已经由最初仅针对特定人群、特定疾病发展为现如今的针对一般人群、多种疾病的筛查，在防控常染色体隐性或 X 连锁遗传病引起的出生缺陷方面表现出了显著的临床效用，增加了受检夫妇的生育自主权。为规范携带者筛查的实验室检测和临床实施，经中国妇幼保健协会生育保健分会专家讨论，结合我国实际情况，制定本专家共识，供临床应用参考。 于 2024.02 发表于 中国妇幼保健协会生育保健分会。该指南就适用人群、疾病选择、筛查策略、实验室检测流程、报告解读、遗传咨询和伦理问题进行了介绍。 适用人群有生育意愿，希望评估生育隐性遗传病患儿风险的夫妇。 如果家族存在先证者，应先对先证者进行诊断性检测（筛查不能代替诊断）。 疾病的选择针对的疾病应仅限于常染色体隐性或者 X 连锁遗传病，本共识建议暂不考虑常染色体显性遗传病、线粒体遗传病、成年期发病的单基因遗传病等类型疾病。 基因的选择基因的临床有效性：由于携带者筛查预期用途属于预测性检测，原则上只能纳入有充分遗传学和实验证据证明与疾病有明确相关性的基因。建议参考美国临床基因组资源组织（Clinical Validityof Gene-Disease Associations，ClinGen）发布的基于客观的遗传学和实验证据的基因-疾病临床有效性评估标准［15］，确定基因-疾病的临床有效性。选择其中的 “强”和“确定” 两个级别的基因-疾病进行筛查。 疾病的选择疾病严重程度是携带者筛查疾病选择的重要标准。普遍标准是将严重影响生存质量、影响认知能力、缺乏有效治疗方案、严重程度足以影响夫妻双方改变生育决策的早发性疾病作为携带者筛查疾病选择的原则性标准。推荐参考《用于评估扩展携带者筛查小组的疾病严重程度的系统分类》所述属评估方式，根据疾病的外显率、表现都、临床表现、可治性、发病年龄等参数，将疾病氛围极重度、重度、中度、轻度4个等级，建议仅纳入中度以上的疾病。 携带者的频率携带者频率是指特定人群中，携带指定变异个体的比例。大多数常染色体隐性遗传和 X 连锁遗传病缺乏携带者频率及发病率的相关信息。鉴于此，对于缺少中国人群相应信息的疾病，推荐两种基于人群基因型数据的疾病携带者频率及发病率的评估方法 ：1.Estimating yields of prenatal carrier screening and implications for design of expanded carrier screening panels05013-9/fulltext) 的估算方法 ；2.The prevalence, genetic complexity and population-specific founder effects of human autosomal recessive disorders的估算方法。两种方法不同之处是变异数据集来源不同，方法 2 除了收集 ClinVar 数据库中致病性或可能致病性变异进入变异数据集的同时，还纳入了 ClinVar 数据库未收录的有可能导致疾病的变异。本共识推荐，对于入选的常染色体隐性遗传病，其携带者频率应大于等于 1/200，对于 X 连锁遗传病，其发病率阈值应为大于等于 1/40000。 筛查疾病数量筛选的目标疾病并不是越多越好，由于我国各地的经济发展、医疗基础、教育水平等存在显著的差异，不建议全国一套标准。应该集合地区、人群、经济水平、医疗水平个性化考虑，推荐。 在经济或医疗条件有限的地区，可以实施单病种或少数常见病种的携带者筛查； 经济或医疗条件允许的地区，备孕或早孕期夫妻可以考虑携带者频率大于等于 1/200 的常染色体隐性遗传病 和 发病率大于等于1/40000 的 X 连锁遗传病，并根据具有地域特点高发的遗传病进行适当调整； 对于近亲结婚的夫妻，可以考虑携带者频率小于 1/200 的常染色体隐性遗传病； 对于因其他原因已准备接受辅助生殖的受检者，经评估可以从更大范围的筛查中获益，且有相关意愿，可以考虑纳入更多的筛查疾病，比如：疾病携带者频率可以小于1/200（大于等于1/500）的常染色体隐性遗传病或发病率小于1/40000 的 X 连锁遗传病；可以考虑纳入表现度差异较大的疾病；可以考虑纳入临床治疗效果较好或症状较轻的疾病，但足够影响患者生活质量，预计会影响受检者辅助生殖选择的疾病。 一、筛查策略筛查策略分三种： 序贯筛查：受检者夫妻中，女方先接受筛查，如果为常染色体隐性遗传病的携带者，再召回其配偶进行筛查。需要注意的是，夫妻双方前后接受的筛查内容应保持一致。 同步筛查：受检者夫妻同时接受筛查，尤其建议妊娠早期夫妇优先考虑该策略。 同时采样-序贯筛查：同时采集受检者夫妻双方的样本，仅在女方为常染色体隐性遗传病的携带者的情况下，才对配偶进行携带者筛查。 3 种筛查策略在受检者依从性、检测周期、非必要的检测及高风险夫妻检出率 4 个方面有明显差异。检测前，受检者夫妻可以根据自身的具体情况，从检测周期、费用等角度考虑不同的筛查策略。建议同步筛查，不建议同时采样-序贯筛查 二、检测流程高通量目标区域捕获测序是当前携带者筛查的主流技术，可用于多种遗传病的联合筛查。对于有特殊变异筛查需求的实验室，如 F8 基因倒位、脆性 X 综合征相关的动态突变等，可补充使用其他技术。对于开展单一病种筛查的实验室，建议根据所筛查的具体疾病、疾病变异类型和数量以及实验室条件等合理选择检测方法，并做充分的性能确认，以保障临床报告的准确性。本章节以主流的采用高通量目标区域测序技术开展多疾病联合筛查为代表，对实验室检测流程和报告解读部分，制定了如下共识，以期为单基因遗传病携带者筛查的临床应用提供指导： （一）检测流程的建立1. 样品收集：在样本采集前，实验室应向受检者提供检测前的遗传咨询与知情同意，并填写送检单。样本采集过程应严格按照卫生部 WS 233-2002《微生物和生物医学实验室生物安全通用准则》要求，做好防护，避免样品污染和保护实验人员的操作安全等。需根据不同样品类型匹配采集容器，并根据实验室的要求采集足够量的样品，建议优先使用外周血进行检测。 2. 数据产出（1）DNA 提取：建议 DNA 提取量大于两次建库需 求 ，以 备 建 库 失 败 重 复 实 验 及 后 续 二 代 测 序（next generation sequencing，NGS）结果验证的需求量。需对所得 DNA 进行质控，存在严重降解的需要重新提取或重新采样。（2）文库制备：可采用物理打断或 DNA 片段化酶酶切对 DNA 进行打断，需使用和测序仪匹配的成品试剂盒进行文库制备，对构建文库浓度、片段长度与分布进行质控。（3）杂交捕获：采用满足性能要求的探针与所制备的混合文库进行杂交，对目标区域进行富集后用于 DNA 测序。（4）测序：需根据测序仪厂商提供的标准测序流程进行测序操作。 3. 生物信息学分析：实验室可根据检测基因突变类型和范围，参考《遗传病二代测序临床检测全流程规范化共识探讨（3）》，搭建相应的分析流程。需使用已知阴阳样本或质控品对所搭建流程的分析能力进行测试与确认，并确定质控参数。 4. 解读报告：变异致病性可根据指南分为五类 ，致病的（pathogenic，P）、可能致病的（likely pathogenic，LP）、临床意义未明的（variants ofuncertain significance，VUS）、可能良性的（likely benign，LB）、良性的（benign，B）。建议报告致病变异和可能致病变异，一般不建议报告 VUS 变异，但在特殊情况下，实验室可以考虑在取得受检者及其配偶知情同意后，对有可能重分析后定级为P/LP 的 VUS 变异（如根据 ClinGen 的贝叶斯分类框架评分为 4~5 分）进行报告：①夫妇一方已检出常染色体隐性致病基因 P/LP 的变异，另外一方检出的同一基因的 VUS 变异的情况；②女方检出 X 连锁遗传病致病基因的 VUS 变异的情况。报告内容：参考《临床基因检测报告规范与基因检测行业共识探讨》，单基因遗传病携带者筛查基因检测报告可分为正文和附录两部分。报告正文需至少包含样本信息、检测项目及方法、检测结果、结果说明、疾病简介与变异详情、参考文献及必要的其他说明。附录内容需至少对检测方法、检测范围、检测局限性进行介绍。 5. 检测结果的验证：检出的阳性变异报告前需进行核验，对于单核苷酸变异，如果实验室已经基于该检测体系足量的既往数据建立了成熟的质控标准，对满足质控标准的变异可不进行 Sanger 验证，如没有建立成熟的质控标准，则需对解读为致病和疑似致病的变异进行 Sanger 验证；对于复杂的变异类型，如小的插入缺失，拷贝数变异，则需进行验证。在实验室没有建立成熟完善的质控体系之前，建议对所有阳性变异进行验证。 （二）性能确认实验室应优先选择国家药品监督管理局批准的试剂和仪器，如暂时无已获批的试剂盒，需按照实验室自建检测方法试剂的要求进行管理。所有相关的仪器、试剂、检测流程等均需进行性能确认。 1. 测序平台：测序仪安装时，需由厂方工程师按照说明书所注明的仪器性能指标逐项验证，达到厂方声称的指标并满足临床预期用途为合格。 2. 生物信息分析平台的性能确认： 实验室可以选择测序平台配套的分析系统，也可以选择合适的算法和软件搭建本实验室的生物信息学分析流程，但均需要进行必要的性能确认。 3. 分析性能确认： 分析性能确认应包括从核酸提取到生物信息学分析的全过程，建议实验室至少对精密度、准确度、分析特异度、检测限等指标进行确认。 （三）质量控制1. 检测前：实验室需对样本采集流程、采集管、采集量、样本转运、样本接收与录入、保存条件等制定相应的标准作业程序（standard operation procedure，SOP）文件。制定样本接收与拒收的标准，对于拒收的样本应制定相应的处理措施。 2. 检测中：实验室应根据性能确认结果建立变异检测的标准作业程序，制定各个环节的质量控制参数，并从批次质控、样本质控、变异质控等维度进行质量控制。对整个操作流程中的关键数据进行记录。 整个检测流程中，应设置质控品，至少包括阳性对照和阴性对照。质控品应首选标准品，在无标准品或标准品不易获得的情况下，阴性质控品也可以选用经过确认的不含目标变异的已知阴性标本样本；阳性质控品也可以选用经过确认的已知阳性标本样本，或者经过验证的相关细胞系等。本共识仅推荐基于高通量测序方法的单基因遗传病携带者筛查技术流程中的质控环节和参数（外周血样本类型）。对于批次质控，可通过批次测序数据的 Q30、阴阳质控品检测结果的一致性 进行质量控制；对于样本质控，可通过单样本测序数据的Q30、有效测序深度、性别一致性等进行质量控制；对于变异质控，根据在生物信息学分析阶段针对不同变异类型建立的分析质控参数进行质量控制，如对单核苷酸变异/插入和缺失变异，可采用阳性变异的突变频率（突变所在染色体的有效reads数/突变所在位置的所有有效reads数）、有效深度进行指控 3. 检测后：实验室应参考指南、共识或文献建立变异致病性分类 SOP，并定期对从事变异分类的人员进行相关的培训。每份报告均要求有经验和一定资质的人员审核，对有异议的报告要结合临床正确分析原因，重新核对标本，重新检测复核，把好质量关。需要对报告周转时间进行统计和质控，及时分析超期样品原因进行整改。此外，开展遗传病携带者筛查的实验室，每年应参加国家卫生健康委临床检验中心组织的高通量应用于遗传病检测相关的室间质评，成绩合格。对于国家卫生健康委临床检验中心暂未开设本项目的室间质评时，应开展对实验室间能力验证，制订年度能力验证计划、标准作业程序等 三、携带者筛查的遗传咨询携带者筛查的遗传咨询包括检测前咨询和检测后咨询，由受过专业培训的临床医生或遗传咨询师提供，其目的是提供风险评估、支持、教育和资源，以促进咨询者做出最符合家庭需求和价值观的决策。遗传咨询应遵循知情同意、非指令性、信任与保护隐私、平等与信息公开及咨询教育与持续支持的基本原则。 （一）检测前的遗传咨询要点作为一项遗传检测项目，检测前建议相关专业人员向受检者夫妇提供携带者筛查的目的、意义、检测方法、检测范围及局限性等方面的遗传咨询，同时还应告知可能的检测结果，并向他们解释可用的生殖选择。 携带者筛查的目的和意义：携带者筛查的目的是通过基因检测及时获得受检者有关隐性遗传病的携带状态。须向受检者解释携带者通常不会表现出任何疾病症状，当夫妻携带同一基因的致病变异时，每次妊娠他们的孩子都有四分之一的风险遗传到双方的致病变异并罹患该种疾病；当女性是X 连锁遗传病的携带者时，其男性后代患病的风险为二分之一。大部分单基因遗传病危害大（致死、致残或致畸），且缺乏有效的治疗手段或者治疗费用昂贵，通过携带者筛查可以及时获得有关其生殖风险的信息，促进生殖决策，以避免生育单基因隐性遗传病患儿。 携带者筛查的范围和检测的局限性：检测前需向检测者详细解释携带者筛查的优势和局限性，使其在充分的知情同意下自主选择并取得书面的知情同意。携带者筛查遵循单基因遗传病的诊断流程，包括病史的采集（包括基本信息、生育史、家族史等）、临床诊断。对于有家族史或者致病基因明确的个人，应进行特定的基因突变检测；对于正常的无相关遗传病史的受检人群，应客观主动地介绍该项目，并充分尊重咨询者的选择。筛查范围的告知是检测前遗传咨询的重点，解释的内容包括筛查所包含的病种、选择依据、可检测的 DNA 变异类型、报告范围和技术局限性。应告知受检者携带者筛查无法检出后代的新发变异，仅报告致病或疑似致病变异，由于技术的局限性，即使检测结果阴性，受检者仍可能存在因携带其他检测范围外的基因或致病变异位点导致检测范围内疾病的剩余风险。提供 VUS 报告选项的实验室，应在检测前遗传咨询过程中解释 VUS 变异的临床含义及报告或不报告的情况下所面临的风险，并获得受检者夫妇的知情同意。 （二）检测后的遗传咨询要点根据 ACMG 指南，建议对受检者进行检测后遗传咨询，检测后遗传咨询应告知检测结果、描述相关疾病的临床性质、建议生殖伴侣的检测、计算妊娠后生育的患儿风险。当发现一人是某遗传病的携带者时，应鼓励患者告知其亲属风险和筛查的必要性，尤其是发现女性为 X 连锁遗传病的携带者时。值得注意的是，筛查为 X 连锁遗传病携带者的女性可能由于 X 染色体失活偏好而表现出该病的症状，比如部分杜氏肌营养不良症女性携带者会出现心肌病。 1. 序贯筛查模式中一方结果阳性：当夫妇一方为某一常染色体隐性遗传病致病基因的携带者，应对配偶进行后续筛查，以明确夫妇双方是否携带同一基因的致病变异；配偶阴性结果能降低疾病风险，但不能完全消除后代的患病风险。若女方为X 连锁致病变异的携带者，则遵循高风险夫妇的遗传咨询要点。 2. 高风险夫妇的遗传咨询要点：高风险夫妇是指夫妇双方在同一基因中携带 P 或 LP 变异，或者女性携带 X 连锁 P 或 LP 变异。针对高风险夫妇对其进行生育风险评估和生育指导应遵循自主和无恶意的伦理原则。高风险夫妇在其生育上可以选择的方式有：①自然妊娠后尽早进行产前诊断；②采用辅助生殖和胚胎植入前遗传学检测（preimplantation genetic testing，PGT）的方式生育，目前已在临床上使用；③夫妇双方采用供卵或供精及领养的方式避免遗传病患儿的出生，但该种方式需要遵循相关的法律法规并在严格的伦理监管下进行。 3. 低风险夫妇的咨询要点：低风险夫妇即双方并未携带同一基因的致病变异或女方并未携带 X 连锁致病变异，即使检测结果阴性，也并不能完全排除生育遗传病患儿的可能，应再次告知受检者筛查后仍存在残余风险，此外携带者筛查不能取代新生儿筛查。需要注意的是，携带者筛查报告中出现的 VUS 变异，本身不是进行产前诊断或 PGT 的适应证。提供遗传咨询的专家需要结合咨询过程中获取的最新家族史和疾病史、最新研究进展、功能研究及实验室之间的共享数据等更新信息，对该变异进行进一步的评估。根据最终的判断结果，结合产前诊断或 PGT 的相关规定进行咨询与建议。 （三）辅助生殖人群的携带者筛查遗传咨询相对于普通人群，有不良孕产史且需进行辅助生殖的夫妇在孕前常规需进行遗传咨询，他们更关注生育子代的出生缺陷风险，因此也更有可能接受携带者筛查。在辅助生殖治疗前已确定携带单基因遗传性疾病变异位点的夫妇可以选择进行单基因疾病胚胎植入前遗传学检测（PGT for monogenic，PGT-M），以避免遗传学异常胚胎移植，预防严重遗传疾病患儿的出生。因此，参与辅助生殖治疗的医务人员不仅有责任协助夫妇妊娠，还应告知辅助生殖人群接受携带者筛查的临床价值。由于辅助生殖人群染色体异常发生率及部分致病变异的携带率高于普通人群（如因出生缺陷家族史、近亲结婚或疑似遗传病病史寻求辅助生殖的人群），因此除了进行常规的携带者检测前咨询和检测后咨询外，还需考虑以下咨询要点： 由于许多遗传疾病都可能直接或间接影响生育能力，因此建议对有不孕不育家族史、配子成熟障碍或反复胚胎发育异常等疑似遗传性疾病可能的患者进行额外基因检测，或者携带者筛查。应告知基因检测与携带者筛查在目的、意义及检测内容上的区别，使其在充分的知情同意下自主选择。 由于 NGS 的技术限制，一些辅助生殖人群中相对高发染色体异常。因此，有必要确定受检者的染色体状态，并应在携带者筛查报告中添加适当的免责声明。 随着 PGT 技术的发展和广泛应用，PGT 的适应证已经从最初针对严重、儿童期发病的疾病，扩展到包括成人发病、人类白细胞抗原检测、肿瘤易感、非严重性疾病。因此虽然目前一般不建议筛查发病较晚、不完全外显率或可变表型的疾病，但已有学者建议在辅助生殖人群中纳入更多的筛查病种，包括临床表型相对较轻和表现度差异较大的疾病（如 BRCA1 相关的遗传性乳腺癌），以保护和尊重患者最大的生殖自主权。虽然此类疾病不在本文的讨论范围内，但由于这类疾病的筛查会增加阳性检出夫妇生殖决策时的复杂性和情绪困扰。因此这种情况下，需要在检测前进行额外的咨询，全面告知利弊，协助夫妇以结合自身情况，自主选择是否接受检测。 当检出的高风险夫妇选择 PGT 时，检测后需要针对体外受精-卵胞质内单精子注射/胚胎植入前 遗 传 学 检 测（in vitro fertilization-intracytoplasmic sperm injection/PGT，IVF-ICSI/PGT）治 疗 进 行 额 外的遗传咨询，需要告知 PGT 的检测目的、预期结果、技术局限性以及可能的风险。对于有 PGT 原始指征的夫妇，告知夫妇携带者筛查阳性可让 PGT 的检测范围扩大，但 PGT 范围越广，检测结果正常的胚胎越少，夫妇没有可移植胚胎的风险就越高。 辅助生殖人群在特定的情况下，需要使用捐赠的配子受孕。辅助生殖机构除了需要收集捐赠者的疾病家族史并随时间变化更新相关家族史外，有条件的机构可以考虑对捐赠者进行携带者筛查。接受配子捐赠前，受捐者在了解捐赠者的完整家族史信息和携带者筛查的结果后，需考虑接受相同范围的携带者筛查。若捐赠者与受捐者双方的携带者筛查结果提示后代面临常染色体隐性或 X 连锁遗传病的高风险，受捐者应接受相关疾病的遗传咨询；其次即使筛查结果为阴性也应告知受捐者筛查结果的局限性。即使进行完整的筛查后，仍存在一定的残余风险。 （四）相关的伦理问题携带者筛查旨在通过携带者状态的检查，筛选出人群中可能生育常染色体隐性或 X 连锁疾病患儿的高风险夫妻。该技术作为一项基因检测项目，开展时需要遵循医学伦理的基本原则。携带者筛查一般不会对受检者身体造成明显的伤害，但因为阳性结果可能会提示夫妻双方未来将面临生育隐性遗传病患儿的高风险，而阴性结果也不能完全排除夫妻双方未来生育隐性遗传病患儿的风险（残余风险的问题），从而不可避免的对受检者的心理产生负面影响。因此，在检测前应取得受检者的知情同意方可实施。需要注意的是，携带者筛查不应变相地作为胎儿选择的工具，而应强调该技术在提供自主生殖选择的价值和作用。]]></content>
      <categories>
        <category>共识</category>
        <category>指南</category>
        <category>遗传病</category>
      </categories>
      <tags>
        <tag>遗传病</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大模型-搭建个人知识库]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-20.MachineLearn_%E5%AE%9E%E8%B7%B5%E9%A1%B9%E7%9B%AE%2F%E5%A4%A7%E6%A8%A1%E5%9E%8B-%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E7%9F%A5%E8%AF%86%E5%BA%93%2F</url>
    <content type="text"><![CDATA[https://blog.csdn.net/luomao2012/article/details/139457751]]></content>
      <categories>
        <category>LLM</category>
      </categories>
      <tags>
        <tag>LLM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Database-Human Phenotype Ontology (HPO)]]></title>
    <url>%2F01.%E7%9F%A5%E8%AF%86-04.Database%2FDatabase-HPO%2F</url>
    <content type="text"><![CDATA[人类表型本体论 （HPO，Human Phenotype Ontology） 提供了人类疾病中遇到的表型异常的标准化词汇表。 HPO 中的每个术语都描述一种表型异常，例如房间隔缺损。 HPO 目前正在使用医学文献、Orphanet、DECIPHER 和 OMIM 进行开发。 HPO 目前包含超过 18,000 个遗传性疾病术语和超过 156,000 个注释。 HPO 项目和其他项目开发了用于表型驱动的差异诊断、基因组诊断和转化研究的软件。 HPO 是Monarch Initiative的旗舰产品，Monarch Initiative 是 NIH 支持的国际联盟，致力于生物医学和模式生物数据的语义集成，最终目标是改进生物医学研究。 HPO 作为 Monarch Initiative 的一部分，是全球基因组学与健康联盟(GA4GH)战略路线图13 个驱动项目之一的核心组成部分。 HPO的基本结构 术语（Term）：每个表型特征都有一个唯一的ID（如HP:0000118）和一个描述（如“Abnormality of the genital system”）。 层次结构（Hierarchy）：HPO术语按层次组织，从广泛的类别到具体的表型。了解层次结构有助于定位感兴趣的术语。 关联基因和疾病：HPO术语通常与特定的基因和疾病相关联，这对于基因组分析和疾病诊断非常有用。 使用HPO官网和工具 HPO浏览器：访问HPO的官网https://hpo.jax.org/app/可以方便地浏览和搜索术语。 快速搜索：利用搜索栏输入关键字、HPO ID或基因，快速找到相关表型术语和信息。 HPO提供的API：如果需要进行大规模数据分析或集成到应用程序中，可以使用HPO提供的API，支持自动化检索和批量查询。 结合基因组数据 注释基因变异：利用HPO术语注释基因变异，帮助理解这些变异可能导致的表型。 患者表型匹配：将患者的临床表型与HPO数据库中的表型进行匹配，辅助诊断罕见疾病。 利用第三方工具和数据库 Phenomizer：一个基于HPO的工具，通过输入患者的表型特征，可以预测可能的疾病。 Exomiser：用于分析外显子组数据的工具，通过HPO表型信息提高变异筛选的精确度。 Phenotype Ontology Lookup Service (OLS)：可以通过OLS快速查找HPO术语，并与其他本体数据集进行比较。 定期更新和社区参与 定期检查更新：HPO数据库经常更新，定期检查更新日志确保你使用的术语和信息是最新的。 社区贡献：如果发现错误或有改进建议，可以参与HPO的社区贡献，帮助改进数据库的准确性。]]></content>
      <categories>
        <category>知识沉淀</category>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>数据库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[常用信息及对应参考数据库来源]]></title>
    <url>%2F01.%E7%9F%A5%E8%AF%86-04.Database%2F%E5%B8%B8%E7%94%A8%E4%BF%A1%E6%81%AF%E5%8F%8A%E5%AF%B9%E5%BA%94%E5%8F%82%E8%80%83%E6%95%B0%E6%8D%AE%E5%BA%93%E6%9D%A5%E6%BA%90%2F</url>
    <content type="text"></content>
      <categories>
        <category>知识沉淀</category>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>数据库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[02.证据项-默认规则说明]]></title>
    <url>%2F01.%E7%9F%A5%E8%AF%86-12.ACMG%E6%89%A7%E8%A1%8C%E8%AF%A6%E8%A7%A3%2F02.%E8%AF%81%E6%8D%AE%E9%A1%B9-%E9%BB%98%E8%AE%A4%E8%A7%84%E5%88%99%E8%AF%B4%E6%98%8E%2F</url>
    <content type="text"><![CDATA[证据项标签 2015 年共识证据项每条证据项的等级和判定标准会在临床实践的过程中不断吧 分局具体标准 最终变异等级确定规则 证据项表示随着逐步的扩展，证据项内容会发生变动，证据项强度也会发生变化。例如PM2 从中等证据降级为支持性证据使用 PM2_P ，属于从致病性中等证据变更为致病性的支持证据。]]></content>
      <categories>
        <category>NGS</category>
        <category>标准化</category>
        <category>遗传分析</category>
        <category>证据项</category>
      </categories>
      <tags>
        <tag>证据项</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[遗传分析证据项 - PP3,PP4]]></title>
    <url>%2F01.%E7%9F%A5%E8%AF%86-12.ACMG%E6%89%A7%E8%A1%8C%E8%AF%A6%E8%A7%A3%2F02.%E8%AF%81%E6%8D%AE%E9%A1%B9-PP3-PP4%2F</url>
    <content type="text"><![CDATA[美国医学遗传学和基因组学学院以及分子病理学协会 (ACMG/AMP) 关于解释序列变异的建议指定使用计算预测因子分别使用标准 PP3 和 BP4 作为致病性或良性的“支持”证据水平。然而，工具开发人员定义的评分区间以及需要多个预测变量达成共识的 ACMG/AMP 建议缺乏定量支持。之前，我们描述了一个概率框架，该框架量化了 ACMG/AMP 建议中证据的强度（支持、中等、强、非常强）。我们已将此框架扩展到计算预测器，并引入了一个新标准，可将工具的分数转换为 PP3 和 BP4 证据强度。我们的方法基于估计局部阳性预测值，并且可以校准任何计算工具或任何变异类型的其他连续规模证据。我们使用精心组装的独立数据集，估计了与十三个错义变异解释工具的致病性和良性证据强度相对应的阈值（评分区间）。大多数工具使用新建立的阈值实现了致病性和良性分类的支持证据水平。多种工具达到了中等程度的分数阈值，并且有几种工具达到了强有力的证据水平。一种工具对于某些变异的良性分类达到了非常有力的证据水平。基于这些发现，我们使用单独的工具和未来对临床解释计算方法的评估，为 PP3 和 BP4 ACMG/AMP 标准的循证修订提供建议。 2015 版 ACMG/AMP 计算分类规则规定，如果“多行计算证据”支持致病性或良性分类，则可以为它们分配最低级别的证据：PP3 支持致病性或 BP4 支持良性。然后，支持证据必须与大量其他证据相结合，以将变异分类为致病性、良性或意义不确定。 参考依据Calibration of computational tools for missense variant pathogenicity classification and ClinGen recommendations for PP3/BP4 criteria ##]]></content>
      <categories>
        <category>NGS</category>
        <category>标准化</category>
        <category>遗传分析</category>
        <category>证据项</category>
      </categories>
      <tags>
        <tag>证据项</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[遗传分析证据项 - PP2]]></title>
    <url>%2F01.%E7%9F%A5%E8%AF%86-12.ACMG%E6%89%A7%E8%A1%8C%E8%AF%A6%E8%A7%A3%2F02.%E8%AF%81%E6%8D%AE%E9%A1%B9-PP2%2F</url>
    <content type="text"><![CDATA[许多基因具有明确的致病变异和良性变异谱.在某些基因中, 错义突变是导致疾病的常见原因, 且该基因上的良性突变非常少, 那么这种基因上的新发错义突变可作为致病变异的支持证据(PP2). 自动化判定流程方案PP2通用判定规则：Z scores ≥3.09 适用PP2（适用变异类型：Missense variant ）gnomAD(gnomAD v2.1.1) Z-score 信息源。 注意：PP2优先级低于PM1，当PM1存在时候，不考虑基于 Z-scores判定的PP2证据项。]]></content>
      <categories>
        <category>NGS</category>
        <category>标准化</category>
        <category>遗传分析</category>
        <category>证据项</category>
      </categories>
      <tags>
        <tag>证据项</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python-包-运行日志-logging]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-02.python%2FPython-%E5%8C%85-%E8%BF%90%E8%A1%8C%E6%97%A5%E5%BF%97-%E5%88%B7%E6%96%B0%E6%89%93%E5%8D%B0%2F</url>
    <content type="text"><![CDATA[有时候，我们想知道任务运行的阶段，或者当前正在处理的数据情况。有时候我们会输出一些任务运行阶段的信息，但是直接使用print或者logging 可能会输出太多，可能会导致刷屏甚至卡顿，这样的输出并不美观，这时候其实我们希望在终端上实时刷新当前进度，而不是每次输出新的一行。这时候，我们可以通过 \r 符号实现，\r会回到行首，然后输出新的内容，覆盖之前的内容。借助这个功能我们可以实现输出的文本内容的刷新（而不是堆叠）python 实现原地刷新的方式，例如实时进度百分比 借用sys.stdout1234import syssys.stdout.write(&apos;\r&apos; + &apos;你的输出详情&apos;)sys.stdout.flush() print1print(&apos;\r&apos; + &apos;你的输出详情&apos;, end=&apos;&apos;, flush=True)]]></content>
      <categories>
        <category>编程拾慧</category>
        <category>python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python-包-运行日志-logging]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-02.python%2FPython-%E5%8C%85-%E8%BF%90%E8%A1%8C%E6%97%A5%E5%BF%97-logging%2F</url>
    <content type="text"><![CDATA[程序开发完成之后，我们会将它部署到生产环境中去，这时候代码相当于是在一个黑盒环境下运行的，我们只能看到其运行的效果，是不能直接看到代码运行过程中每一步的状态的。运行过程中难免会在某个地方出现问题，甚至这个问题可能是我们开发过程中未曾遇到的问题，碰到这种情况应该怎么办？ 如果我们现在只能得知当前问题的现象，没有其他任何信息的话，如果我们想要解决掉这个问题的话，那么只能根据问题的现象来试图复现一下，然后再一步步去调试。 Debug 的过程也会耗费巨多的时间，这样一旦生产环境上出现了问题，修复就会变得非常棘手。但这如果我们当时有做日志记录的话，不论是正常运行还是出现报错，都有相关的时间记录，状态记录，错误记录等，那么这样我们就可以方便地追踪到在当时的运行过程中出现了怎样的状况，从而可以快速排查问题。 日志记录的流程框架在 Python 中有一个标准的 logging 模块，我们可以使用它来进行标注的日志记录，利用它我们可以更方便地进行日志记录，同时还可以做更方便的级别区分以及一些额外日志信息的记录，如时间、运行模块信息等。 接下来我们先了解一下日志记录流程的整体框架。 如图所示，整个日志记录的框架可以分为这么几个部分： Logger：即 Logger Main Class，是我们进行日志记录时创建的对象，我们可以调用它的方法传入日志模板和信息，来生成一条条日志记录，称作 Log Record。 Log Record：就代指生成的一条条日志记录。 Handler：即用来处理日志记录的类，它可以将 Log Record 输出到我们指定的日志位置和存储形式等，如我们可以指定将日志通过 FTP 协议记录到远程的服务器上，Handler 就会帮我们完成这些事情。 Formatter：实际上生成的 Log Record 也是一个个对象，那么我们想要把它们保存成一条条我们想要的日志文本的话，就需要有一个格式化的过程，那么这个过程就由 Formatter 来完成，返回的就是日志字符串，然后传回给 Handler 来处理。 Filter：另外保存日志的时候我们可能不需要全部保存，我们可能只需要保存我们想要的部分就可以了，所以保存前还需要进行一下过滤，留下我们想要的日志，如只保存某个级别的日志，或只保存包含某个关键字的日志等，那么这个过滤过程就交给 Filter 来完成。 Parent Handler：Handler 之间可以存在分层关系，以使得不同 Handler 之间共享相同功能的代码。 以上就是整个 logging 模块的基本架构和对象功能，了解了之后我们详细来了解一下 logging 模块的用法。 用法说明总的来说 logging 模块相比 print 有这么几个优点： 可以在 logging 模块中设置日志等级，在不同的版本（如开发环境、生产环境）上通过设置不同的输出等级来记录对应的日志，非常灵活。 print 的输出信息都会输出到标准输出流中，而 logging 模块就更加灵活，可以设置输出到任意位置，如写入文件、写入远程服务器等。 logging 模块具有灵活的配置和格式化功能，如配置输出当前模块信息、运行时间等，相比 print 的字符串格式化更加方便易用。 下面我们初步来了解下 logging 模块的基本用法，先用一个实例来感受一下： 123456789import logginglogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')logger = logging.getLogger(__name__)logger.info('This is a log info')logger.debug('Debugging')logger.warning('Warning exists')logger.info('Finish') 首先引入了 logging 模块，然后进行了一下基本的配置，这里通过 basicConfig 配置了 level 信息和 format 信息，这里 level 配置为 INFO 信息，即只输出 INFO 级别的信息，另外这里指定了 format 格式的字符串，包括 asctime、name、levelname、message 四个内容，分别代表运行时间、模块名称、日志级别、日志内容，这样输出内容便是这四者组合而成的内容了，这就是 logging 的全局配置。 接下来声明了一个 Logger 对象，它就是日志输出的主类，调用对象的 info () 方法就可以输出 INFO 级别的日志信息，调用 debug () 方法就可以输出 DEBUG 级别的日志信息，非常方便。在初始化的时候我们传入了模块的名称，这里直接使用 name 来代替了，就是模块的名称，如果直接运行这个脚本的话就是 main，如果是 import 的模块的话就是被引入模块的名称，这个变量在不同的模块中的名字是不同的，所以一般使用 name 来表示就好了，再接下来输出了四条日志信息，其中有两条 INFO、一条 WARNING、一条 DEBUG 信息，我们看下输出结果：1232018-06-03 13:42:43,526 - __main__ - INFO - This is a log info2018-06-03 13:42:43,526 - __main__ - WARNING - Warning exists2018-06-03 13:42:43,526 - __main__ - INFO - Finish 通过调整输出日志的级别，我们可以输出更多的内容，比如我们的输出级别调整为DEBUG，可以输出DEBUG级别的日志。1logging.basicConfig(level=logging.DEBUG, format=&apos;%(asctime)s - %(name)s - %(levelname)s - %(message)s&apos;) 输出示例12342018-06-03 13:49:22,770 - __main__ - INFO - This is a log info2018-06-03 13:49:22,770 - __main__ - DEBUG - Debugging2018-06-03 13:49:22,770 - __main__ - WARNING - Warning exists2018-06-03 13:49:22,770 - __main__ - INFO - Finish 由此可见，相比 print 来说，通过刚才的代码，我们既可以输出时间、模块名称，又可以输出不同级别的日志信息作区分并加以过滤，是不是灵活多了？ 当然这只是 logging 模块的一小部分功能，接下来我们首先来全面了解一下 basicConfig 的参数都有哪些： filename：即日志输出的文件名，如果指定了这个信息之后，实际上会启用 FileHandler，而不再是 StreamHandler，这样日志信息便会输出到文件中了。 filemode：这个是指定日志文件的写入方式，有两种形式，一种是 w，一种是 a，分别代表清除后写入和追加写入。 format：指定日志信息的输出格式，即上文示例所示的参数，详细参数可以参考：docs.python.org/3/library/l…，部分参数如下所示： %(levelno) s：打印日志级别的数值。 %(levelname) s：打印日志级别的名称。 %(pathname) s：打印当前执行程序的路径，其实就是 sys.argv [0]。 %(filename) s：打印当前执行程序名。 %(funcName) s：打印日志的当前函数。 %(lineno) d：打印日志的当前行号。 %(asctime) s：打印日志的时间。 %(thread) d：打印线程 ID。 %(threadName) s：打印线程名称。 %(process) d：打印进程 ID。 %(processName) s：打印线程名称。 %(module) s：打印模块名称。 %(message) s：打印日志信息。 datefmt：指定时间的输出格式。 style：如果 format 参数指定了，这个参数就可以指定格式化时的占位符风格，如 %、{、$ 等。 level：指定日志输出的类别，程序会输出大于等于此级别的信息。 stream：在没有指定 filename 的时候会默认使用 StreamHandler，这时 stream 可以指定初始化的文件流。 handlers：可以指定日志处理时所使用的 Handlers，必须是可迭代的。 一个配置的示例如下：123456789101112import logginglogging.basicConfig(level=logging.DEBUG, filename='output.log', datefmt='%Y/%m/%d %H:%M:%S', format='%(asctime)s - %(name)s - %(levelname)s - %(lineno)d - %(module)s - %(message)s')logger = logging.getLogger(__name__)logger.info('This is a log info')logger.debug('Debugging')logger.warning('Warning exists')logger.info('Finish') 这里我们指定了输出文件的名称为 output.log，另外指定了日期的输出格式，其中年月日的格式变成了 % Y/% m/% d，另外输出的 format 格式增加了 lineno、module 这两个信息，运行之后便会生成一个 output.log 的文件，内容如下：12342018/06/03 14:43:26 - __main__ - INFO - 9 - demo3 - This is a log info2018/06/03 14:43:26 - __main__ - DEBUG - 10 - demo3 - Debugging2018/06/03 14:43:26 - __main__ - WARNING - 11 - demo3 - Warning exists2018/06/03 14:43:26 - __main__ - INFO - 12 - demo3 - Finish 可以看到日志便会输出到文件中，同时输出了行号、模块名称等信息。 以上我们通过 basicConfig 来进行了一些全局的配置，我们同样可以使用 Formatter、Handler 进行更灵活的处理，下面我们来了解一下。 Level首先我们来了解一下输出日志的等级信息，logging 模块共提供了如下等级，每个等级其实都对应了一个数值，列表如下： 等级 数值 CRITICAL 50 FATAL 50 ERROR 40 WARNING 30 WARN 30 INFO 20 DEBUG 10 NOTSET 0 这里最高的等级是 CRITICAL 和 FATAL，两个对应的数值都是 50，另外对于 WARNING 还提供了简写形式 WARN，两个对应的数值都是 30。 我们设置了输出 level，系统便只会输出 level 数值大于或等于该 level 的的日志结果，例如我们设置了输出日志 level 为 INFO，那么输出级别大于等于 INFO 的日志，如 WARNING、ERROR 等，DEBUG 和 NOSET 级别的不会输出。 Handler下面我们先来了解一下 Handler 的用法，看下面的实例：12345678910111213import logginglogger = logging.getLogger(__name__)logger.setLevel(level=logging.INFO)handler = logging.FileHandler('output.log')formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')handler.setFormatter(formatter)logger.addHandler(handler)logger.info('This is a log info')logger.debug('Debugging')logger.warning('Warning exists')logger.info('Finish') 另外我们还可以使用其他的 Handler 进行日志的输出，logging 模块提供的 Handler 有： StreamHandler：logging.StreamHandler；日志输出到流，可以是 sys.stderr，sys.stdout 或者文件。 FileHandler：logging.FileHandler；日志输出到文件。 BaseRotatingHandler：logging.handlers.BaseRotatingHandler；基本的日志回滚方式。 RotatingHandler：logging.handlers.RotatingHandler；日志回滚方式，支持日志文件最大数量和日志文件回滚。 TimeRotatingHandler：logging.handlers.TimeRotatingHandler；日志回滚方式，在一定时间区域内回滚日志文件。 SocketHandler：logging.handlers.SocketHandler；远程输出日志到 TCP/IP sockets。 DatagramHandler：logging.handlers.DatagramHandler；远程输出日志到 UDP sockets。 SMTPHandler：logging.handlers.SMTPHandler；远程输出日志到邮件地址。 SysLogHandler：logging.handlers.SysLogHandler；日志输出到 syslog。 NTEventLogHandler：logging.handlers.NTEventLogHandler；远程输出日志到 Windows NT/2000/XP 的事件日志。 MemoryHandler：logging.handlers.MemoryHandler；日志输出到内存中的指定 buffer。 HTTPHandler：logging.handlers.HTTPHandler；通过”GET” 或者”POST” 远程输出到 HTTP 服务器。 下面我们使用三个 Handler 来实现日志同时输出到控制台、文件、HTTP 服务器：]]></content>
      <categories>
        <category>编程拾慧</category>
        <category>python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Git-7.添加执行管道CI-CD]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-09.%E7%89%88%E6%9C%AC%E6%8E%A7%E5%88%B6%2FGit-7.%E9%9B%86%E6%88%90%E5%B7%A5%E5%85%B7CI-CD%2F</url>
    <content type="text"><![CDATA[持续集成是一种软件开发实践，即团队开发成员经常集成他们的工作，通常每个成员每天至少集成一次，也就意味着每天可能会发生多次集成。每次集成都通过自动化的构建（包括编译，发布，自动化测试)来验证，从而尽快地发现集成错误。 几个概念 持续集成(Continuous Integration)：频繁地(一天多次)将代码集成到主干。让产品可以快速迭代，同时还能保持高质量。它的核心措施是，代码集成到主干之前，必须通过自动化测试。只要有一个测试用例失败，就不能集成。“持续集成并不能消除 Bug，而是让它们非常容易发现和改正。” 持续交付(Continuous Delivery)：频繁地将软件的新版本，交付给质量团队或者用户，以供评审。如果评审通过，代码就进入生产阶段。持续交付可以看作持续集成的下一步。它强调的是，不管怎么更新，软件是随时随地可以交付的。 持续部署(continuous Deployment)：代码通过评审以后，自动部署到生产环境。是持续部署是持续交付的下一步，持续部署的目标是，代码在任何时刻都是可部署的，可以进入生产阶段。 持续集成工具[Jenkins](https://www.jenkins.io/) [GitLab CI](https://docs.gitlab.com/ee/topics/build_your_application.html) TeamCity Travis CI Bamboo CircleCI Gitlab CI 工作逻辑框架GitLab CI/CD 是GitLab Continuous Integration（Gitlab持续集成）的简称。GitLab 自GitLab 8.0开始提供了持续集成的功能，且对所有项目默认开启。只要在项目仓库的根目录添加.gitlab-ci.yml文件，并且配置了Runner（运行器），那么每一次push或者合并请求（Merge Request）都会触发CI Pipeline。 GitLab RunnerGitLab Runner 是一个开源项目，可以运行在 GNU / Linux，macOS 和 Windows 操作系统上。每次push的时候 GitLab CI 会根据.gitlab-ci.yml配置文件运行你流水线（Pipeline）中各个阶段的任务（Job），并将结果发送回 GitLab。GitLab Runner 是基于 Gitlab CI 的 API 进行构建的相互隔离的机器（或虚拟机）。GitLab Runner 不需要和 Gitlab 安装在同一台机器上，且考虑到 GitLab Runner 的资源消耗问题和安全问题，也不建议这两者安装在同一台机器上。 Gitlab Runner 分为三种： 共享Runner(Shared runners) 专享Runner(Specific runners) 分组Runner(Group Runners) PipelinesPipelines 是分阶段（stage）执行的构建任务。如：安装依赖、运行测试、打包、部署开发服务器、部署生产服务器等流程。每一次push或者Merge Request都会触发生成一条新的Pipeline。若一次推送包含了多个提交，则管道与最后那个提交相关联，管道(pipeline)就是一个分成不同阶段(stage)的作业(job)的集合。 下面是流水线示例图： StagesStages 表示构建阶段，可以理解为上面所说“安装依赖”、“运行测试”等环节的流程。我们可以在一次 Pipeline 中定义多个 Stages，这些 Stages 会有以下特点： 所有 Stages 会按照顺序运行，即当一个 Stage 完成后，下一个 Stage 才会开始（当然可以在.gitlab-ci.yml文件中配置上一阶段失败时下一阶段也执行） 只有当所有 Stages 完成后，该构建任务 (Pipeline) 才会成功 如果任何一个 Stage 失败，那么后面的 Stages 不会执行，该构建任务 (Pipeline) 失败 下面是一个流水线内的阶段任务示例图： JobsJobs 表示构建的作业（或称之为任务），表示某个 Stage 里面执行的具体任务。我们可以在 Stages 里面定义多个 Jobs，这些 Jobs 会有以下特点： 相同 Stage 中的 Jobs 无执行顺序要求，会并行执行 相同 Stage 中的 Jobs 都执行成功时，该 Stage 才会成功 如果任何一个 Job 失败，那么该 Stage 失败，即该构建任务 (Pipeline) 也失败（可以在.gitlab-ci.yml文件中配置允许某 Job 可以失败，也算该 Stage 成功） .gitlab-ci.ymlGitLab 中默认开启了 Gitlab CI/CD 的支持，但是要是用这个功能，我们需要告诉系统，我们需要执行什么样的pipeline，在gitlab里面，这通过 .gitlab-ci.yml 来实现。且使用YAML文件.gitlab-ci.yml来管理项目构建配置。该文件需要存放于项目仓库的根目录（默认路径，可在 GitLab 中修改），它定义该项目的 CI/CD 如何配置。所以，我们只需要在.gitlab-ci.yml配置文件中定义流水线的各个阶段，以及各个阶段中的若干作业（任务）即可。 下面是.gitlab-ci.yml文件的一个简单的Hello World示例：123456789101112131415161718# 定义 test 和 package 两个 Stagesstages: - test - package# 定义 package 阶段的一个 jobpackage-job: stage: package script: - echo "Hello, package-job" - echo "I am in package stage"# 定义 test 阶段的一个 jobtest-job: stage: test script: - echo "Hello, test-job" - echo "I am in test stage" 以上配置中，用 stages 关键字来定义 Pipeline 中的各个构建阶段，然后用一些非关键字来定义 jobs。每个 job 中可以可以再用 stage 关键字来指定该 job 对应哪个 stage。job 里面的script关键字是每个 job 中必须要包含的，它表示每个 job 要执行的命令。 BadgesBadges 即：徽章，当 Pipelines 执行过程中或者执行完成时会生成徽章，你可以将这些徽章加入到你的README.md文件中，便于从仓库主页看到最新的构建状态。 Gitlab CI 实践如上所述，我们的所有测试都需要提供硬件的基础环境（Runner）来为自动化的集成测试/部署提供底层基建，所以我们要做的第一件事情就是安装gitlab Runner gitlab Runner配置安装gitlab Runners在此以CentOS为例进行说明，其他平台参考 官方指南。12345678# 添加官方的repo.curl -L https://packages.gitlab.com/install/repositories/runner/gitlab-runner/script.rpm.sh | sudo bash# yum 安装Gtilab Runner.sudo yum install gitlab-runner# rpm离线安装事先下载好的 Gitlab Runner rpm包。前置需要完成git的安装。rpm -ivh gitlab-runner-10.5.0-1.x86_64.rpm 注册 Gitlab安装了 GitLab Runner 之后,就可以为 GitLab 中的仓库注册一个 Runner，注册的交互式命令如下：12345678910111213141516171819202122# 输入注册命令sudo gitlab-runner register# 输入公司的 GitLab 网站地址Please enter the gitlab-ci coordinator URL (e.g. https://gitlab.com )http://gitlab.xxxx.com/# 你项目仓库的token，token可以在 Settings -&gt; CI/CD -&gt; Runners settings 中找到.Please enter the gitlab-ci token for this runnerxxx# 输入描述这个 runner 的名称Please enter the gitlab-ci description for this runner[hostame] my-runner# 输入 runner 的标签Please enter the gitlab-ci tags for this runner (comma separated):my-tag,another-tag# 输入 runner 的执行器.Please enter the executor: ssh, docker+machine, docker-ssh+machine, kubernetes, docker, parallels, virtualbox, docker-ssh, shell:shell Gitlab Runner 常用命令汇总面的表格中列出了一些常用的Gitlab Runner命令，以供参考： 命令 描述 gitlab-runner run 运行一个runner服务 gitlab-runner register 注册一个新的runner gitlab-runner start 启动服务 gitlab-runner stop 关闭服务 gitlab-runner restart 重启服务 gitlab-runner status 查看各个runner的状态 gitlab-runner unregister 注销掉某个runner gitlab-runner list 显示所有运行着的runner gitlab-runner verify 检查已注册的运行程序是否可以连接到GitLab，但它不验证GitLab Runner服务是否正在使用运行程序。 任务配置任务配置的一个示例接下来，用一个实际项目来演示 GitLab CI/CD 的配置和使用，其中主要包括：编译测试、项目打包、部署服务、Sonar手动检查、Sonar定时检查五个阶段。 下面用一个传统的 Java web 项目(这里称之为cidemo)和Tomcat来作为示例，并用来展示常用配置的使用。当我每次push代码或者Merge Request时，都会生成一条流水线，且会自动执行我们上面所说的一些阶段，而Sonar手动检查我们设置为手动操作，且再额外配置Sonar定时检查的任务。12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667# 定义stagesstages: - test - install - run - sonar# 定义安装包的存放位置和Tomcat服务器的地址的变量，便于后续部署使用.variables: CIDEMO_PACKAGE_DIR: '/home/gitlab-runner/packages/cidemo/' SERVER_HOME_DIR: '/home/gitlab-runner/tomcat/cidemo-tomcat/'###################### 构建编译和单元测试的job. #######################编译测试任务: stage: test only: - branches script: - mvn clean test###################### Maven安装得到war包的job. #######################打包任务: stage: install only: - develop script: - mvn install - echo '准备将最新的war包复制、保存到某个目录里面供后续使用.' - rm -rf $CIDEMO_PACKAGE_DIR/*.war - cp target/*.war $CIDEMO_PACKAGE_DIR/cidemo.war####################### 部署运行war包的job. #######################部署运行任务: stage: run only: - develop script: - echo '准备部署和运行war包！(为了方便部署到了Tomcat中运行)' - cd $SERVER_HOME_DIR - sh bin/shutdown.sh - rm -rf webapps/cidemo.war - cp $CIDEMO_PACKAGE_DIR/cidemo.war $SERVER_HOME_DIR/webapps/cidemo.war - nohup sh ./bin/startup.sh &gt; logs/cidemo_nohup.log 2&gt;&amp;1 &amp;###################### Sonar手动构建的job. #######################Sonar手动检查: stage: sonar when: manual only: - develop script: - echo '准备对项目代码做sonar的质量检查！' - mvn compile &amp;&amp; mvn sonar:sonar -Dsonar.host.url=http://172.16.34.102:9000 -Dsonar.login=497a0e0e2fc07f64c4b54edc17bb47dfa251ba34###################### Sonar每晚定时构建的job. #######################Sonar定时检查: stage: sonar only: - schedules script: - echo '开始定时对项目代码做sonar的质量检查！' - mvn compile &amp;&amp; mvn sonar:sonar -Dsonar.host.url=http://172.16.34.102:9000 -Dsonar.login=497a0e0e2fc07f64c4b54edc17bb47dfa251ba34 Gitlab CI/CD yaml 常用配置介绍开始构建之前.gitlab-ci.yml文件定义了一系列带有约束说明的任务。这些任务都是以任务名开始并且至少要包含script部分，.gitlab-ci.yml允许指定无限量 jobs。每个 jobs 必须有一个唯一的名字，且名字不能是下面列出的保留字段：| Keyword | Required | Description || ————- | ——– | ———————————————————————————- || script | yes | Runner执行的命令或脚本 || extends | no | 定义此作业将继承的父作业（GitLab 11.3 引入），重定义参数覆盖，未定义参数继承父任务 || image | no | 所使用的docker镜像，查阅使用docker镜像 || services | no | 所使用的docker服务，查阅使用docker镜像 || stage | no | 定义job stage（默认：test） || type | no | stage的别名（已弃用） || variables | no | 定义job级别的变量 || only | no | 定义一列git分支，并为其创建job || except | no | 定义一列git分支，不创建job || tags | no | 定义一列tags，用来指定选择哪个Runner（同时Runner也要设置tags） || allow_failure | no | 允许job失败。失败的job不影响commit状态 || when | no | 定义何时开始job。可以是on_success，on_failure，always或者manual || dependencies | no | 定义job依赖关系，这样他们就可以互相传递artifacts || cache | no | 定义应在后续运行之间缓存的文件列表 || before_script | no | 重写一组在作业前执行的命令 || after_script | no | 重写一组在作业后执行的命令 || environment | no | 定义此作业完成部署的环境名称 || coverage | no | 定义给定作业的代码覆盖率设置 || etry | no | 定义在发生故障时可以自动重试作业的时间和次数 || parallel | no | 定义应并行运行的作业实例数 | 接下来对其中的一些重要管检测进行补充说明 stagesstages 用来定义可以被 job 调用的 stages。stages 的规范允许有灵活的多级 pipelines。stages中的元素顺序决定了对应job的执行顺序： 相同 stage 的 job 可以平行执行。 下一个 stage 的 job 会在前一个 stage 的 job 成功后开始执行。 接下仔细看看这个例子，它包含了3个 stage：1234stages: - build - test - deploy 首先，所有 build 的 jobs 都是并行执行的。 所有 build 的 jobs 执行成功后，test 的 jobs 才会开始并行执行。 所有 test 的 jobs 执行成功，deploy 的 jobs 才会开始并行执行。 所有的 deploy 的 jobs 执行成功，commit才会标记为success。 任何一个前置的 jobs 失败了，commit会标记为failed并且下一个 stages 的 jobs 都不会执行。 这有两个特殊的例子值得一提： 如果.gitlab-ci.yml中没有定义stages，那么 job’s stages 会默认定义为build，test和deploy。 如果一个 job 没有指定 stage，那么这个任务会分配到 test stage。 参考信息]]></content>
      <categories>
        <category>编程拾慧</category>
        <category>Git</category>
      </categories>
      <tags>
        <tag>培训</tag>
        <tag>Git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python-代码加密-pyarmor]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-02.python%2FPython-%E4%BB%A3%E7%A0%81%E5%8A%A0%E5%AF%86-pyarmor%2F</url>
    <content type="text"><![CDATA[https://pyarmor.readthedocs.io/zh/latest/part-1.html，pyarmor的说明文档]]></content>
      <categories>
        <category>Tobeinsert</category>
      </categories>
      <tags>
        <tag>Tobeinsert</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[perl-加密-pp]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-01.perl%2Fperl-%E5%8A%A0%E5%AF%86-pp%2F</url>
    <content type="text"></content>
      <categories>
        <category>Tobeinsert</category>
      </categories>
      <tags>
        <tag>Tobeinsert</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[相关分析]]></title>
    <url>%2F01.%E7%9F%A5%E8%AF%86-02.Math%2F%E7%9B%B8%E5%85%B3%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[相关分析是研究两个或两个以上处于同等地位的随机变量间的相关关系的统计分析方法。可以帮助我们测量多个变量变量之间线性关系的强度和方向。 在进行生信方法开发过程中，我们会经常用到，比如开发一个biomarker时，如何进行特征的选择，如何确定我们的选择的特征是有益于判断的，包括在生物统计方向的各种大规模人群研究探索。当然不只生物信息领域，数据科学都无法脱离相关性分析。例如，我们的BMI是基于身高和体重之间相关性；天气预报是基于空气中的相对湿度、温度、气流和天气变化的相关分析研究。在这里补充一点，相关分析与回归分析之间的区别：回归分析侧重于研究随机变量间的依赖关系，以便用一个变量去预测另一个变量;相关分析侧重于发现随机变量间的种种相关特性。当然很多时候，进行回归前，我们可能也需要基于数据的相关性进行特征的选择，以便获得更好的回归结果，这也是目前一系列机器学习、 神经网络乃至于大模型的底层基础。 相关分析的方法 皮尔逊相关系数（Pearson Correlation Coefficient）： 皮尔逊相关系数是一种用于衡量两个连续变量之间线性关系的方法。它的取值范围在-1到1之间，其中1表示完全正相关，-1表示完全负相关，0表示无相关性。皮尔逊相关系数基于协方差和标准差计算，适用于连续型数据且假定数据呈正态分布。 斯皮尔曼相关系数（Spearman Rank Correlation Coefficient）： 斯皮尔曼相关系数是一种非参数的相关性分析方法，它基于变量的等级顺序而不是原始数值。这使得它更适用于有序数据、序数数据或偏态数据。斯皮尔曼相关系数可以用于测量变量之间的单调关系，不要求数据满足正态分布假设。 肯德尔相关系数（Kendall’s Tau Correlation Coefficient）： 肯德尔相关系数也是一种非参数的相关性分析方法，用于测量两个变量之间的排序关系。它基于排列对的数量，可以度量变量的等级之间的一致性程度。肯德尔相关系数对于小样本数据和存在重复值的情况更稳健。 点二列相关系数（Point-Biserial Correlation Coefficient）： 点二列相关系数用于衡量一个二元变量与一个连续变量之间的关系。它类似于皮尔逊相关系数，但适用于包含一个二元变量的情况，其中0和1表示两种不同的状态。 双变量相关性分析（Bivariate Correlation Analysis）： 这种方法用于衡量两个连续变量之间的关系。它包括散点图、回归分析和相关系数等技术，可用于可视化和量化两个变量之间的线性或非线性关系。 多变量相关性分析（Multivariate Correlation Analysis）： 多变量相关性分析用于研究多个变量之间的关系。主成分分析（PCA）和因子分析是常见的多变量相关性分析方法，用于降维和识别主要相关性模式。 假设检验： 假设检验方法用于验证两个或多个变量之间是否存在显著的关系。例如，t检验和方差分析可以用于比较组之间的均值差异，从而确定它们是否相关。 交叉表和卡方检验： 交叉表用于分析两个或多个分类变量之间的关系。卡方检验可用于确定观察到的频数是否与预期频数有显著差异，从而评估两个变量之间的相关性。]]></content>
      <categories>
        <category>知识沉淀</category>
      </categories>
      <tags>
        <tag>统计学</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[相关系数的介绍与特征选择]]></title>
    <url>%2F01.%E7%9F%A5%E8%AF%86-02.Math%2F%E7%9B%B8%E5%85%B3%E5%88%86%E6%9E%90-%E7%9B%B8%E5%85%B3%E7%B3%BB%E6%95%B0%2F</url>
    <content type="text"><![CDATA[在进行数据分析的时候，相关性相关系数和特征选择之前，先来区别两个概念，一个是属性，一个是特征。一般，把数据集中的各列称为属性，而对算法模型表现有益的属性称为特征。举个例子，在预测泰坦尼克乘客的存活情况时，乘客姓名这个属性对我们的预测可能没有帮助，甚至会干扰模型表现；而乘客年龄、性别或许与存活情况有很强的关系，这时，可以称它们为特征。 三种主要的相关系数计算方式皮尔逊相关系数使用前提：皮尔逊相关系数适用场景是呈正态分布的连续变量，当数据集的数量超过 500 时，可以近似认为数据呈正态分布，因为按照中心极限定理，当数据量足够大时,可以认为数据是近似正态分布的。设(X,Y)是一个二维随机变量，且Var(X)&gt;0,Var(Y)&gt;0，则称 $Corr(X,Y)=\frac{Cov(X,Y)}{\sqrt{Var(X)*Var(Y)}}$ 为相关系数。 $Cov(X,Y)$为协方差，是数据集中每组种两个位点和对应均值差值的乘积 $Cov(X,Y)=\frac{\sum_{i=1}^n(x_i-\bar{x})(y_i-\bar{y})}{N-1}$。 示例12345678910#使用Python scipyfrom scipy import statsa = np.array([0, 0, 0, 1, 1, 1, 1])b = np.arange(7)c,p = stats.pearsonr(a, b)print('Pearson correlation：%s'%c)print('p-value：%s'%p) 斯皮尔曼相关系数使用前提：皮尔逊Pearson相关系数使用前提条件中，任何一个条件不满足时可以考虑使用该系数；又称斯皮尔曼秩相关系数，是秩相关系数的一种。“秩”，即秩序，可以理解为一种顺序或排序，根据变量在数据内的位置进行计算。斯皮尔曼更关注的是变量相关顺序的比较（高低顺序），而不是绝对值的相关性。$p=1-\frac{6\sum_{i=1}^{n-1}d_i^2}{n^3-n}$ 示例1234567#使用Python scipyfrom scipy import statss,p1 = stats.spearmanr([1,2,3,4,5], [5,6,7,8,7])print('Spearman correlation：%s'%s)print('p-value：%s'%p1) 肯德尔相关系数使用前提：肯德尔相关系数，又称肯德尔秩相关系数，它也是一种秩相关系数，不过，它的目标对象是有序的类别变量，比如名次、年龄段、肥胖等级(重度肥胖，中度肥胖、轻度肥胖、不肥胖)等。它可以度量两个有序变量之间单调关系强弱。肯德尔相关系数使用了“成对“这一概念来决定相关系数的强弱。成对可以分为一致对(Concordant)和分歧对(Discordant)。一致对是指两个变量取值的相对关系一致，可以理解为X2-X1与Y2-Y1有相同的符号；分歧对则是指它们的相对关系不一致，X2-X1与Y2-Y1有着相反的符号。 肯德尔系数有两个计算公式，一个是Tau-a，另一个是Tau-b。两者的区别是Tau-b可以处理有相同值的情况，即并列(tied ranks)。下面依次来介绍这两个公式。 Tau-a的计算公式为： $\tau_a = \frac{c - d}{\frac{1}{2}n(n-1)}$, c 表示一致对数，d 表示一致对数， 表示所有样本两两组合的数量，当没有重复值时，组合数量等于c+d。 Tau-b的计算公式为：$\tau_b = \frac{c - d}{\sqrt{(c+d+t_z)(c+d+t_y)}}$ 下面通过例子来理解这个公式。 对上面图片中的数据进行离散化后，得到的数据如下： 示例12345678from scipy import statsx1 = [12, 2, 1, 12, 2]x2 = [1, 4, 7, 1, 0]k, p2 = stats.kendalltau(x1, x2)print('Kendallta correlatio：%s'%k)print('p-value：%s'%p2)]]></content>
      <categories>
        <category>知识沉淀</category>
      </categories>
      <tags>
        <tag>统计学</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[1201.机器学习-算法-Logistic 回归]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-20.MachineLearn%2F1201.%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%AE%97%E6%B3%95-Logistic%20%E5%9B%9E%E5%BD%92%2F</url>
    <content type="text"><![CDATA[Logistic模型，又称为Logistic回归模型，是一种广义线性模型（GLM），主要用于处理二分类问题。它通过使用Logistic函数（或称为Sigmoid函数）来估计概率，从而预测一个事件的发生与否。尽管它涉及到一些高等数学的概念，但是其基本原理和公式是可以通过初高中数学基础来理解的。 公式和概念 Logistic函数（Sigmoid函数）：Logistic模型的核心是Logistic函数，也称为Sigmoid函数，其数学表达式为：$$f(x) = \frac{1}{1+e^{-x}}$$这个函数的输出值介于0和1之间，非常适合用来表示某个事件发生的概率。其中，$e$是自然对数的底数，约等于2.71828。 Logistic回归模型：对于二分类问题，我们有一个因变量$y$，它只能取两个值，比如0和1，分别代表阴性和阳性结果。我们还有一系列的自变量$x_1,x_2,…,x_m$，它们是影响因变量的因素。Logistic回归模型试图找到这些自变量和因变量之间的关系。模型的公式为：$$P(y=1|x_1,x_2,…,x_m) = \frac{1}{1+e^{-(\beta_0+\beta_1x_1+\beta_2x_2+…+\beta_mx_m)}}$$这里， $P(y=1|x_1,x_2,…,x_m)$ 表示在给定自变量的条件下，因变量取值为1的概率。而 $\beta_0,\beta_1,\beta_2,…,β_m$ 是模型参数，它们需要通过数据来估计。 Logit变换： 为了将概率值转换为可以进行线性回归分析的形式，我们进行Logit变换，即：$$logit(p)=ln(\frac{p}{1-p})$$其中，ln表示自然对数。这样，我们就可以将非线性的概率模型转换为线性模型，以便使用线性回归的方法来估计参数。 模型参数的估计： Logistic回归模型的参数通常通过极大似然估计法来估计。这个方法的基本思想是找到一组参数，使得观测到的数据在这个模型下出现的概率（似然）最大。在实际操作中，通常使用优化算法（如梯度下降）来求解参数。 我们可以看到Logistic模型涉及到概率、指数函数、对数等初高中数学概念，这些概念在高等数学中会有更深入的探讨，理解这些概念有助于把握Logistic回归模型的工作原理和应用场景。 Sigmoid函数介绍让我们通过一个简单的例子来理解Logistic回归中的Sigmoid函数： 假设我们有一个问题，需要根据学生的考试成绩（$x$）来预测他们是否能够通过考试（$y$）。这里，$y$是一个二分类变量，如果学生通过考试则$y=1$，否则$y=0$。我们的任务是建立一个模型来预测给定考试成绩x的学生通过考试的概率。 为了解决这个问题，我们可以使用Logistic回归模型。在这个模型中，Sigmoid函数将发挥重要作用。Sigmoid函数的公式如下：$$f(x) = \frac{1}{1+e^{-x}}$$ 这个函数的输出值介于0和1之间，非常适合用来表示某个事件发生的概率。在这个例子中，事件就是学生通过考试。 现在，让我们假设通过分析历史数据，我们得出了一个简单的Logistic回归模型，其中只有一个自变量$x$（考试成绩），模型参数（$β0和β1$）已经通过极大似然估计法求得。模型可能如下所示：$$f(y=1|x) = \frac{1}{1+e^{-(\beta_0+\beta_1x))}}$$ 假设我们得到的参数估计值为$β_0=−2$和$β_1=0.5$，那么模型变为：$$P(y=1|x) = \frac{1}{1+e^{-(-2+0.5x)}}$$ 现在，我们可以用这个模型来预测不同考试成绩x的学生通过考试的概率。例如： 如果一个学生的考试成绩是60分（$x=60$），那么通过考试的概率$P(y=1|x)$可以这样计算：$$P(y=1|x) = \frac{1}{1+e^{-(-2+0.5*60)}}~0.99$$ 计算这个表达式，我们可以得到该学生通过考试的概率。 通过这个例子，我们可以看到Sigmoid函数如何将考试成绩这个连续变量转换成一个介于0和1之间的概率值，从而帮助我们预测二分类结果（通过或不通过考试）的概率。这就是Logistic回归中Sigmoid函数的实际应用。]]></content>
      <categories>
        <category>machine_learning</category>
        <category>算法</category>
      </categories>
      <tags>
        <tag>machine_learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[1201.机器学习-算法-K-Means聚类]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-20.MachineLearn%2F1201.%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%AE%97%E6%B3%95-K-Means%E8%81%9A%E7%B1%BB%2F</url>
    <content type="text"><![CDATA[K-means 算法是一种经典的无监督学习方法，用于对未标记的数据集进行分群，即将数据集中相似的对象划分为不同的簇。 基本原理： 初始化： 设定簇的数量（K）：由用户预先指定，表示希望得到的簇的数量。 选择初始聚类中心（Centroids）：通常随机从数据集中选取 K 个对象作为初始的聚类中心。 分配对象到簇： 计算距离：对于数据集中每一个对象，计算其与 K 个聚类中心之间的距离（通常使用欧氏距离）。 分配归属：将每个对象分配到与其最近的聚类中心对应的簇中。 更新聚类中心： 计算簇内平均值：对于每个簇，计算其包含的所有对象的特征均值，得到新的聚类中心。 移动中心：将簇的聚类中心更新为这个新的计算出的均值位置。 判断收敛与迭代： 检查终止条件：比较当前迭代前后聚类中心的变化情况，如果变化小于某个预定阈值或达到最大迭代次数，则算法结束；否则，返回步骤2，继续进行新一轮的分配和更新。 上述过程反复进行，直到聚类中心的位置不再显著变化或达到预设的迭代次数上限。最终得到的簇即为数据集中的自然结构划分，每个簇内的对象在特征空间中较为接近，而不同簇之间的对象相对较远。 K-means 优缺点优点： 算法简单，易于理解和实现。 在处理大数据集时，计算效率较高。 可以用于发现任意形状的簇。 缺点： 需要预先指定k值，而k值的选择可能依赖于领域知识或试错。 对初始簇中心的选择敏感，可能导致局部最优解。 对噪声和异常点敏感，可能影响簇中心的计算。 只能发现数值型特征的簇，不适合文本数据等非数值型数据。 K-means 聚类算法的优化与改进尽管 K-means 算法简单易用，但在实际应用中可能会遇到一些挑战，为此研究人员提出了多种优化与改进策略： 初始聚类中心的选择： K-means++：通过概率方法选择初始聚类中心，确保它们尽可能分散且能代表数据的整体分布，从而提高算法的稳定性和收敛速度。 其他策略：如基于密度的方法、基于层次的方法或使用智能优化算法（如遗传算法、模拟退火等）来确定初始聚类中心。 距离度量与标准化： 非欧氏距离：根据数据特性选择更适合的距离度量，如曼哈顿距离、余弦相似度、马氏距离等(更多可以参考文章”机器学习中的距离计算”)。 特征缩放与标准化：对数据进行预处理，如归一化、标准化等，以消除特征间尺度差异对聚类结果的影响。 处理不同类型数据与噪声： 模糊 C 均值（FCM）：允许对象属于多个簇，适用于边界模糊或含有噪声的数据。 DBSCAN 或 OPTICS：针对具有不同密度区域的数据，发现任意形状的簇，并能较好地处理噪声点和离群值。 动态调整簇数量 K： 肘部法则：通过观察轮廓系数、 inertia（簇内平方和）等指标随 K 值变化的趋势，选择“肘部”处的 K 值作为最优簇数。 交叉验证或贝叶斯信息准则（BIC）等统计方法：用于评估不同 K 值下的聚类质量，选择最优 K。 并行与分布式计算： MapReduce 或 Spark 等框架：对大规模数据集进行分布式 K-means 聚类，利用多核处理器或集群的并行计算能力加速算法执行。 异质聚类： 混合高斯模型（GMM）：将数据视为由多个高斯分布生成，每个高斯分布对应一个簇，适用于数据内部存在异质性的场景。GMM 通过 EM 算法进行参数估计和聚类。 概率潜在语义分析（PLSA）：适用于处理文本数据，假设每个文档是若干隐含主题的混合，每个主题对应一个簇，通过最大化似然函数进行参数估计和聚类。 高维数据聚类： 子空间聚类（如 CLIQUE、SPEC、PROCLUS 等）：寻找数据中具有聚类结构的低维子空间，降低维度以改善 K-means 在高维空间中的性能。 稀疏编码或深度学习预处理：通过学习数据的潜在表示（如自编码器、深度神经网络等），将原始高维数据映射到低维、更利于聚类的特征空间。 时间序列与流数据聚类： 在线 K-means 或 增量 K-means：适应数据流的实时更新，仅对新加入的数据点或发生变化的簇进行重新分配和中心更新，无需每次都遍历整个数据集。 动态聚类（如 DenStream、CluStream 等）：适用于数据分布随时间变化的场景，能够持续监控数据流，发现并跟踪动态出现和消失的簇。 加权 K-means 聚类： 加权 K-means：为数据点赋予权重，反映其在聚类中的相对重要性，适用于处理带有不确定性的数据或含有噪声的数据集。 约束 K-means：引入先验知识或用户指定的约束条件（如必须将某些对象分到同一簇、某些对象不能分到同一簇等），引导聚类过程，提高结果的实用价值。 聚类后处理与评估：后处理方法：如对小簇合并、大簇分裂、边界对象重新分配等操作，以改善聚类的直观解释性和用户接受度。聚类评估指标：如轮廓系数、Calinski-Harabasz 指数、Davies-Bouldin 指数等，定量评价聚类结果的质量，为算法选择和参数调优提供依据。 综上所述，通过对 K-means 聚类算法进行适当的优化与改进，我们可以应对更广泛的数据类型、规模、特性和应用场景，提高聚类的准确性和效率，使其在实际问题中发挥更大的作用。同时，结合领域知识和具体需求，灵活运用各种策略和方法，有助于获得更为满意的聚类结果。]]></content>
      <categories>
        <category>machine_learning</category>
        <category>算法</category>
      </categories>
      <tags>
        <tag>machine_learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[1201.机器学习-算法-支持向量机]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-20.MachineLearn%2F1201.%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%AE%97%E6%B3%95-%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%2F</url>
    <content type="text"><![CDATA[初识SVM算法支持向量机（Support Vector Machine，SVM）是一种经典的监督学习算法，用于解决二分类和多分类问题。其核心思想是通过在特征空间中找到一个最优的超平面来进行分类，并且间隔最大。它的基本模型是定义在特征空间上的间隔最大的线性分类器，间隔最大使它有别于感知机；SVM还包括核技巧，这使它成为实质上的非线性分类器。SVM的的学习策略就是间隔最大化，可形式化为一个求解凸二次规划的问题，也等价于正则化的合页损失函数的最小化问题。SVM的的学习算法就是求解凸二次规划的最优化算法。 SVM能够执行线性或非线性分类、回归，甚至是异常值检测任务。它是机器学习领域最受欢迎的模型之一。SVM特别适用于中小型复杂数据集的分类 SVM算法原理SVM通过优化一个凸二次规划问题来求解最佳的超平面，其中包括最小化模型的复杂度（即最小化权重的平方和），同时限制训练样本的误分类情况。这个优化问题可以使用拉格朗日乘子法来求解。对于非线性可分的情况，SVM可以通过核函数（Kernel Function）将输入特征映射到高维空间，使得原本线性不可分的数据在高维空间中变得线性可分。常用的核函数包括线性核、多项式核、高斯核等。 硬间隔和软间隔：硬间隔分类：在上面我们使用超平面进行分割数据的过程中，如果我们严格地让所有实例都不在最大间隔之间，并且位于正确的一边，这就是硬间隔分类。 硬间隔分类有两个问题，首先，它只在数据是线性可分离的时候才有效；其次，它对异常值非常敏感。 当有一个额外异常值的鸢尾花数据：左图的数据根本找不出硬间隔，而右图最终显示的决策边界与我们之前所看到的无异常值时的决策边界也大不相同，可能无法很好地泛化。 软间隔分类：要避免这些问题，最好使用更灵活的模型。目标是尽可能在保持最大间隔宽阔和限制间隔违例（即位于最大间隔之上，甚至在错误的一边的实例）之间找到良好的平衡，这就是软间隔分类。 在Scikit-Learn的SVM类中，可以通过超参数C来控制这个平衡：C值越小，则间隔越宽，但是间隔违例也会越多。上图显示了在一个非线性可分离数据集上，两个软间隔SVM分类器各自的决策边界和间隔。 左边使用了高C值，分类器的错误样本（间隔违例）较少，但是间隔也较小。 右边使用了低C值，间隔大了很多，但是位于间隔上的实例也更多。看起来第二个分类器的泛化效果更好，因为大多数间隔违例实际上都位于决策边界正确的一边，所以即便是在该训练集上，它做出的错误预测也会更少。 SVM算法的优点：1）SVM方法既可以用于分类（二/多分类），也可用于回归和异常值检测。2）SVM具有良好的鲁棒性，对未知数据拥有很强的泛化能力，特别是在数据量较少的情况下，相较其他传统机器学习算法具有更优的性能。使用SVM作为模型时，通常采用如下流程： 1）对样本数据进行归一化 2）应用核函数对样本进行映射（最常采用和核函数是RBF和Linear，在样本线性可分时，Linear效果要比RBF好) 3）用cross-validation和grid-search对超参数进行优选 4）用最优参数调练得到模型 5）测试 reference Scikit-Learn SVM]]></content>
      <categories>
        <category>machine_learning</category>
        <category>算法</category>
      </categories>
      <tags>
        <tag>machine_learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[1201.机器学习-算法-线性回归]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-20.MachineLearn%2F1201.%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%AE%97%E6%B3%95-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%2F</url>
    <content type="text"><![CDATA[线性回归模型简介线性与非线性 线性：两个变量之间的关系是一次函数关系的——图象是直线，叫做线性。注意：线性是指广义的线性，也就是数据与数据之间的关系。 非线性：两个变量之间的关系不是一次函数关系的——图象不是直线，叫做非线性。 线性回归是统计学中一种常用的预测分析方法，它通过最小化误差的平方和来研究变量之间的线性关系。线性回归模型可以是简单的一元线性回归，也可以是包含多个自变量的多元线性回归。 线性回归的基本概念线性回归模型的基本形式可以表示为 $y = wx + e$，其中 $y$ 是因变量，$x$是自变量，$w$ 是回归系数，$e$ 是误差项，通常假设 $e$ 服从均值为 $0$ 的正态分布。在一元线性回归中，只包含一个自变量和一个因变量，而在多元线性回归中，包含两个或多个自变量。 线性回归的应用线性回归广泛应用于各个领域，包括经济学、金融、医学和工程等。在经济学中，线性回归用于预测消费支出、投资支出等经济指标。在金融领域，它用于分析和计算投资的系统风险。在医学研究中，线性回归可以帮助分析吸烟对死亡率和发病率的影响。 线性回归的假设从这四个数据集的分布可以看出，并不是所有的数据集都可以用一元线性回归来建模。现实世界中的问题往往更复杂，变量几乎不可能非常理想化地符合线性模型的要求。 为了有效地使用线性回归模型，需要满足几个基本假设： 自变量和因变量之间存在线性关系。 误差项应服从均值为零的正态分布。 自变量之间应相互独立，避免多重共线性问题。 线性回归的实现线性回归模型可以通过最小二乘法来求解，得到回归系数和截距。在 Python 中，可以使用 sklearn.linear_model 中的 LinearRegression 类来实现线性回归模型。以下是一个简单的线性回归模型的实现示例：1234567891011121314import numpy as npfrom sklearn.linear_model import LinearRegression# 创建数据集X = np.array([[1], [2], [3], [4], [5]])y = np.array([1, 3, 2, 3, 5])# 创建线性回归模型model = LinearRegression()model.fit(X, y)# 打印模型系数和截距print('系数:', model.coef_)print('截距:', model.intercept_) 线性回归的优缺点 线性回归模型的优点包括模型简单、易于理解和实现，结果具有良好的可解释性。然而，它的缺点是对非线性数据建模能力有限，且当自变量之间存在相关性时，模型的预测性能会下降。 线性回归是一种强大且实用的工具，但在应用时需要仔细考虑其假设条件和适用范围。通过对数据进行适当的预处理和选择合适的模型，线性回归可以在许多领域提供有价值的洞察。]]></content>
      <categories>
        <category>machine_learning</category>
        <category>算法</category>
      </categories>
      <tags>
        <tag>machine_learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[1201.机器学习-算法-朴素贝叶斯]]></title>
    <url>%2F01.%E7%9F%A5%E8%AF%86-14.%E7%AE%97%E6%B3%95%2F01.%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86-%E5%8C%88%E7%89%99%E5%88%A9%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[匈牙利算法(Hungarian Algorithm) 是一种在多项式时间内求解的分配问题的组合优化算法, 由 Harold Kuhn 在1955年完善并发表. 算法的命名是因为该算法很大程度上是基于两位匈牙利数学家 Dénes Kőnig and Jenő Egerváry 的工作而来的. James Munkres 在 1957 年证明了该算法的复杂度是(强)多项式时间的, 此后该算法也被称为 Kuhn-Munkres 算法或 Munkres 分配算法. 原始算法的时间复杂度为$O(n^4)$ , Edmonds 和 Karp, 以及 Tomizawa 发现该算法可以优化到$O(n^3)$ . 匈牙利算法是一种在图论中寻找最优匹配的算法。它的目标是在给定的二分图中，找到最大的边数的匹配，使得每个顶点都与一条边相连，且每条边都只被一个顶点连接。什么是二分图?二分图又称作二部图，是图论中的一种特殊模型。 设G=(V,E)是一个无向图，如果顶点V可分割为两个互不相交的子集(A,B)，并且图中的每条边（i，j）所关联的两个顶点i和j分别属于这两个不同的顶点集(i in A,j in B)，则称图G为一个二分图。简而言之，就是顶点集V可分割为两个互不相交的子集，并且图中每条边依附的两个顶点都分属于这两个互不相交的子集，且任意两条边不共用一个顶点。 应用场景例子A例子B一个典型场景是任务分配. 举个例子: 现在有三名工人张三, 李四和王五以及三份工作扫地, 浇花和擦窗户. 每名工人做不同工作要求的工资是不同的, 如下表所示| 姓名 | 扫地 | 浇花 | 擦窗户 || :—: | :—: | :—: | :—-: || 张三 | 2 | 1 | 3 || 李四 | 3 | 3 | 4 || 王五 | 3 | 3 | 2 | 每名工人分配一份工作, 问如何分配可以使得所付工资总和最少?在上面这个例子中我们很容易观察出 张三-浇花, 李四-扫地, 王五-擦窗户 的成本最低. 但当工人数量和工作数量较大时, 寻找最优分配就变得不这么直观了. 匈牙利算法我们发现这类分配问题可以用一个矩阵表示(称为效益矩阵), 记为 $C$ 算法如下: 效益矩阵初始化(1.1) 将矩阵 $C$ 每行减去该行的最小元素, 得新矩阵 $C’$;(1.2) 将矩阵 $C’$ 每列减去该列的最小元素, 得新矩阵 $C’’$, 以下操作对 $C’’$ 进行; 对当前效益矩阵求可行解(2.1) 从含零最少的行(或列)开始, 圈出矩阵中的一个 “0”, 并划去其所在的行和列;(2.2) 重复步骤 (2.1) 直至矩阵中所有的 “0” 均被划去或圈出为止;(2.3) 若已圈出 n 个 “0”, 它们即对应一最优解, 如不然转步骤 3. 求覆盖当前效益矩阵中所有 “0” 的最少条数直线(3.1) 对没有圈出 “0” 的行打 √, 对打√ 行上所有 “0” 所在的列打√ , 再对打√ 的列上所有 “0” 被圈出的行打√ , 直至打不出√为止;(3.2) 将没有打 √的行和打√ 的列划上直线, 这些直线即为所求. 调整效益矩阵(4.1) 找出没有被直线覆盖的所有元素中最小的数 $\overline{c}$;(4.2) 将打√ 的行上每个元素减去 $\overline{c}$ , 打√ 的列上每个元素加上 $\overline{c}$ , 得到新的效益矩阵. 返回步骤 2.]]></content>
      <categories>
        <category>machine_learning</category>
        <category>算法</category>
      </categories>
      <tags>
        <tag>machine_learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[1201.机器学习-算法-朴素贝叶斯]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-20.MachineLearn%2F1201.%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%AE%97%E6%B3%95-%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%2F</url>
    <content type="text"><![CDATA[18世纪英国业余(一点都不业余好吗)数学家托马斯·贝叶斯(Thomas Bayes，1702～1761)提出过一种看似显而易见的观点：用客观的新信息更新我们最初关于某个事物的信念后，我们就会得到一个新的、改进了的信念。这个研究成果由于简单显得平淡无奇，直至他死后两年才于1763年由他的朋友理查德·普莱斯帮助发表。它的数学原理很容易理解，简单说就是，如果你看到一个人总是做一些好事，则会推断那个人多半会是一个好人。这就是说，当你不能准确知悉一个事物的本质时，你可以依靠与事物特定本质相关的事件出现的多少去判断其本质属性的概率。用数学语言表达就是：支持某项属性的事件发生得愈多，则该属性成立的可能性就愈大。 贝叶斯定理拉普拉斯不断地搜集新增的出生记录，并用之推断原有的概率是否准确。每一个新的记录都减少了不确定性的范围。拉普拉斯给出了我们现在所用的贝叶斯公式的表达：$P(A|B)=\frac{P(B|A)P(A)}{P(B)}$该公式表示在B事件发生的条件下A事件发生的条件概率，等于A事件发生条件下B事件发生的条件概率乘以A事件的概率，再除以B事件发生的概率。公式中，P(A)也叫做先验概率，P(A/B)叫做后验概率。严格地讲，贝叶斯公式至少应被称为“贝叶斯-拉普拉斯公式”。深刻理解上述公式，理解了贝叶斯定理，明白条件概率，也就理解了朴素贝叶斯的算法基础。 几个扩展的公式： 条件概率：$P(A|B)$就是事件 $A$ 在另外一个事件 $B$ 已经发生条件下的发生概率。条件概率表示为$ P ( A ∣ B )$ ，读作“在 $B$ 发生的条件下 $A$ 发生的概率”。 联合概率：表示两个事件共同发生（数学概念上的交集）的概率。$A$ 与 $B$ 的联合概率表示为联合概率。 $p(AB) = P(A∣B)P(B)=P(B∣A)P(A)$ ，若 $A、B$ 相互独立，$P(AB) = P(A)P(B)$， 全概率公式：$P(X)=\sum_k{ P(X∣Y=Y_k)P(Y_k)}$ ，其中$\sum_k{P(Y_k​)}=1$ 朴素贝叶斯算法朴素贝叶斯算法（Naive Bayesian algorithm） 是应用最为广泛的分类算法之一。朴素贝叶斯方法是在贝叶斯算法的基础上进行了相应的简化，即假定给定目标值时属性之间相互条件独立。也就是说没有哪个属性变量对于决策结果来说占有着较大的比重，也没有哪个属性变量对于决策结果占有着较小的比重。虽然这个简化方式在一定程度上降低了贝叶斯分类算法的分类效果，但是在实际的应用场景中，极大地简化了贝叶斯方法的复杂性。当然这是一种取舍，实际应用中，我们面对的问题，会有诸多的属性，这些属性之间也很难完全独立，但是通过把这些属性视作独立的特征可以极大的降低复杂性，同时结果上也基本可以满足我们的应用需求。 示例贝叶斯示例1分别有 A、B 两个容器， A容器：有 7 个红球和 3 个白球; B容器：有 1 个红球和 9 个白球。 现已知从这两个容器里任意抽出了一个球，且是红球，问这个红球是来自容器 A 的概率是多少? 123456789# X事件为选中了红球p(X) = 8/20# Y事件代表选中A容器p(Y) = 1/2# A容器中红球的概率p(X|Y) = 7/10# 选中一个红球，该球来自A容器的概率为p(Y|X) = &#123;p(X|Y)*p(Y)&#125; / p(X)p(Y|X) = 7/10 * 1/2 * 20/8 = 7/8 当然我们也可以换个角度，我们所有的已知信息中，一共有8个红球，其中7个在容器A中，1个在容器B中，所以我们抽中红球是来自容器A 的概率是 $7/8$ 示例2当然示例1的问题比较简单，我们只有一个特征信息（球的颜色），但是我们要解决的实际问题往往复杂的多，比如预测一场比赛的胜负，那么我们就需要用到多个特征信息，比如：球员的技术水平，球员的身体状态，是否伤病等多个因素相关，但是显而易见的其中伤病和身体状体其实会存在一定的关联性，这个时候，如果我们需要在计算概率时，由于特征之间的各种相关性会非常复杂，所以有时候为了，简化计算过程，我们会在应用过程中，对所有特征进行简化，认为所有特征之间都是互相独立的，虽然会损失一些准确性，但是可以极大的降低计算的复杂性。 原理的扩展我们了解了贝叶斯公式：$P(A|B)=\frac{P(B|A)P(A)}{P(B)}$。对整个公式进行拓展，在我们进行应用过程中，我们所知道的每一个已知信息，都是先验信息，我们计算的其实就是在确定的先验信息下，数据/样本归类到某个类别的概率 P(C|F1,….,F2)。]]></content>
      <categories>
        <category>machine_learning</category>
        <category>算法</category>
      </categories>
      <tags>
        <tag>machine_learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python-包-可视化-matplotlib]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-02.python%2FPython-%E5%8C%85-%E5%8F%AF%E8%A7%86%E5%8C%96-matplotlib%2F</url>
    <content type="text"><![CDATA[绘图标记通过marker可以指定绘制图形中所用标记的形状，比如fmt 参数定义了基本格式，如标记、线条样式和颜色。markersize，简写为 ms：定义标记的大小。markerfacecolor，简写为 mfc：定义标记内部的颜色。markeredgecolor，简写为 mec：定义标记边框的颜色。 1234567891011import matplotlib.pyplot as pltimport numpy as npypoints = np.array([1,3,4,5,8,9,6,1,3,4,5,2,4])plt.plot(ypoints, marker = 'o')plt.plot(ypoints, fmt = 'o:r')plt.plot(ypoints, marker = 'o', ms = 20, mfc = 'r')plt.plot(ypoints, marker = 'o', ms = 20, mec = '#4CAF50', mfc = '#4CAF50')plt.show() 点类型符号如下：通过 marker 或 fmt(同时指定线条样式和颜色) 指定：12plt.plot(ypoints, marker = &apos;o&apos;)plt.plot(ypoints, fmt = &apos;o:r&apos;) 标记 描述 “.” 点 “,” 像素点 “o” 实心圆 “v” 下三角 “^” 上三角 “&lt;” 左三角 “&gt;” 右三角 “1” 下三叉 “2” 上三叉 “3” 左三叉 “4” 右三叉 “8” 八角形 “s” 正方形 “p” 五边形 “P” 加号（填充） “*” 星号 “h” 六边形 1 “H” 六边形 2 “+” 加号 “x” 乘号 x “X” 乘号 x (填充) “D” 菱形 “d” 瘦菱形 “ “ 竖线 “_” 横线 0 (TICKLEFT) 左横线 1 (TICKRIGHT) 右横线 2 (TICKUP) 上竖线 3 (TICKDOWN) 下竖线 4 (CARETLEFT) 左箭头 5 (CARETRIGHT) 右箭头 6 (CARETUP) 上箭头 7 (CARETDOWN) 下箭头 8 (CARETLEFTBASE) 左箭头 (中间点为基准) 9 (CARETRIGHTBASE) 右箭头 (中间点为基准) 10 (CARETUPBASE) 上箭头 (中间点为基准) 11 (CARETDOWNBASE) 下箭头 (中间点为基准) “None”, “ “ or “” 没有任何标记 ‘$…$’ 渲染指定的字符。例如 “$f$” 以字母 f 为标记。 线类型如下线的类型可以使用 linestyle 参数来定义，简写为 ls。12plt.plot(ypoints, linestyle = 'dotted')plt.plot(ypoints, ls = '-.') 线类型标记 简写 描述 ‘solid’ (默认) ‘-‘ 实线 ‘dotted’ ‘:’ 虚线 ‘dashed’ ‘–’ 破折线 ‘dashdot’ ‘-.’ 点划线 ‘None’ ‘’ 不画线 线条的粗细线条的粗细直接使用 linewidth 定义。1plt.plot(ypoints, linewidth = '12.5') 颜色类型通过 color 或 r指定12plt.plot(ypoints, color = 'r')plt.plot(ypoints, c = '#8FBC8F') 颜色标记 描述 ‘r’ 红色 ‘g’ 绿色 ‘b’ 蓝色 ‘c’ 青色 ‘m’ 品红 ‘y’ 黄色 ‘k’ 黑色 ‘w’ 白色 指定图片的标题和标签标题我们可以使用 title() 方法来设置标题。title() 方法提供了 loc 参数来设置标题显示的位置，可以设置为: ‘left’, ‘right’, 和 ‘center’， 默认值为 ‘center’。 1plt.title(&quot;TEST TITLE&quot;,loc=&quot;left&quot;) 坐标轴我们可以使用 xlabel() 和 ylabel() 方法来设置 x 轴和 y 轴的标签。xlabel() 方法提供了 loc 参数来设置 x 轴显示的位置，可以设置为: ‘left’, ‘right’, 和 ‘center’， 默认值为 ‘center’。ylabel() 方法提供了 loc 参数来设置 y 轴显示的位置，可以设置为: ‘bottom’, ‘top’, 和 ‘center’， 默认值为 ‘center’。12plt.xlabel(&quot;x - label&quot;,loc=&quot;left&quot;)plt.ylabel(&quot;y - label&quot;,loc=&quot;left&quot;) 中文支持默认绘图时不支持中文的，如果要在绘图中显示中文，需要先导入字体，并设置字体。123456789from matplotlib import pyplot as pltimport matplotliba=sorted([f.name for f in matplotlib.font_manager.fontManager.ttflist])# 打印系统中的所有字体for i in a: print(i)# 挑选要给中文字体加入绘图库plt.rcParams['font.family']=['STFangsong'] 网格线一张图片绘制多图的排版matplotlib 支持使用两种方式进行多子图的排版，分别是 pyplot 中的 subplot() 和 subplots() 方法来绘制多个子图。 subplot() 方法在绘图时需要指定位置； subplots() 方法可以一次生成多个，在调用时只需要调用生成对象的 ax 即可。 subplot()示例12345subplot(nrows, ncols, index, **kwargs) # 前面两个参数是进行分区，index负责某一个子图的定位## subplots() 示例subplot(pos, **kwargs)subplot(**kwargs)subplot(ax) 以上函数将整个绘图区域分成 nrows 行和 ncols 列，然后从左到右，从上到下的顺序对每个子区域进行编号 1…N ，左上的子区域的编号为 1、右下的区域编号为 N，编号可以通过参数 index 来设置。 设置 numRows ＝ 1，numCols ＝ 2，就是将图表绘制成 1x2 的图片区域, 对应的坐标为：(1, 1), (1, 2) 根据位置分别绘图123456789101112131415161718192021import matplotlib.pyplot as pltimport numpy as np#plot 1:xpoints = np.array([0, 6])ypoints = np.array([0, 100])plt.subplot(1, 2, 1)plt.plot(xpoints,ypoints)plt.title("plot 1")#plot 2:x = np.array([1, 2, 3, 4])y = np.array([1, 4, 9, 16])plt.subplot(1, 2, 2)plt.plot(x,y)plt.title("plot 2")plt.suptitle("RUNOOB subplot Test")plt.show() subplots() 示例1matplotlib.pyplot.subplots(nrows=1, ncols=1, *, sharex=False, sharey=False, squeeze=True, subplot_kw=None, gridspec_kw=None, **fig_kw) nrows：默认为 1，设置图表的行数。 ncols：默认为 1，设置图表的列数。 sharex、sharey：设置 x、y 轴是否共享属性，默认为 false，可设置为 ‘none’、’all’、’row’ 或 ‘col’。 False 或 none 每个子图的 x 轴或 y 轴都是独立的，True 或 ‘all’：所有子图共享 x 轴或 y 轴，’row’ 设置每个子图行共享一个 x 轴或 y 轴，’col’：设置每个子图列共享一个 x 轴或 y 轴。 squeeze：布尔值，默认为 True，表示额外的维度从返回的 Axes(轴)对象中挤出，对于 N1 或 1N 个子图，返回一个 1 维数组，对于 N*M，N&gt;1 和 M&gt;1 返回一个 2 维数组。如果设置为 False，则不进行挤压操作，返回一个元素为 Axes 实例的2维数组，即使它最终是1x1。 subplot_kw：可选，字典类型。把字典的关键字传递给 add_subplot() 来创建每个子图。 gridspec_kw：可选，字典类型。把字典的关键字传递给 GridSpec 构造函数创建子图放在网格里(grid)。 **fig_kw：把详细的关键字参数传给 figure() 函数。 123456789fig, axes = plt.subplots(2, 2, sharex=True, sharey=True, figsize=(14, 8))# 通过axes的索引定位绘制图片axes[0, 0].plot(data)axes[0, 1].bar(range(len(data)), data, color='k', alpha=0.5)axes[1, 0].scatter(range(len(data)), data)axes[1, 1].plot(data, 'c&gt;--')# 调节子图直接的距离plt.subplots_adjust(wspace=0, hspace=0)plt.show() 各类图标命令简介折线图1234567891011#merged_df=pd.read_csv("./data/merged_df.csv")plt.figure(figsize=[12,5])color=['#A51C36','#84BA42','#7ABBDB','#682487','#9E9E9E','#DBB428']for i in range(0,6): plt.plot(merged_df.index,merged_df[merged_df.columns[i]],label=merged_df.columns[i],color=color[i])plt.legend()plt.xticks(rotation=45)plt.xlabel("STRC")plt.ylabel("Correlation(.corr())")plt.show() 箱线图12 1234567891011121314151617181920plt.boxplot(x, # x：指定要绘制箱图的数据 notch=None, # notch：是否是凹口的形式展现箱线图，默认非凹口 sym=None, # sym：指定异常点的形状，默认为+号显示 vert=None, # vert：是否需要将箱线图垂直摆放，默认垂直摆放 whis=None, # whis：指定上下须与上下四分位的距离，默认为1.5倍的四分位差 positions=None, # positions：指定箱线图的位置，默认为[0,1,2…] widths=None, # widths：指定箱线图的宽度，默认为0.5 patch_artist=None, # patch_artist：是否填充箱体的颜色 meanline=None, # meanline：是否用线的形式表示均值，默认用点来表示 showmeans=None, # showmeans：是否显示均值，默认不显示 showcaps=None, # showcaps：是否显示箱线图顶端和末端的两条线，默认显示 showbox=None, # showbox：是否显示箱线图的箱体，默认显示 showfliers=None, # showfliers：是否显示异常值，默认显示 boxprops=None, # boxprops：设置箱体的属性，如边框色，填充色等 labels=None, # labels：为箱线图添加标签，类似于图例的作用 flierprops=None, # filerprops：设置异常值的属性，如异常点的形状、大小、填充色等 medianprops=None, # medianprops：设置中位数的属性，如线的类型、粗细等 meanprops=None, # meanprops：设置均值的属性，如点的大小、颜色等 capprops=None, # capprops：设置箱线图顶端和末端线条的属性，如颜色、粗细等 whiskerprops=None) # whiskerprops：设置须的属性，如颜色、粗细、线的类型等 参考资料 https://www.runoob.com/matplotlib/matplotlib-grid.html]]></content>
      <categories>
        <category>编程拾慧</category>
        <category>python</category>
      </categories>
      <tags>
        <tag>可视化</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习-算法-决策树]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-20.MachineLearn%2F1201.%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%AE%97%E6%B3%95-%E5%86%B3%E7%AD%96%E6%A0%91%2F</url>
    <content type="text"><![CDATA[决策树(decision tree)是一种基本的分类与回归方法。依托于策略抉择而建立起来的树。机器学习中，决策树是一个预测模型；他代表的是对象属性与对象值之间的一种映射关系。 树中每个节点表示某个对象，而每个分叉路径则代表的某个可能的属性值，从根节点到叶节点所经历的路径对应一个判定测试序列。决策树可以是二叉树或非二叉树，也可以把他看作是 if-else 规则的集合，也可以认为是在特征空间上的条件概率分布。决策树在机器学习模型领域的特殊之处，在于其信息表示的清晰度。决策树通过训练获得的 “知识”，直接形成层次结构。这种结构以这样的方式保存和展示知识，即使是非专家也可以很容易地理解。 决策树的优点： 决策树算法中学习简单的决策规则建立决策树模型的过程非常容易理解， 决策树模型可以可视化，非常直观 应用范围广，可用于分类和回归，而且非常容易做多类别的分类 能够处理数值型和连续的样本特征 决策树的缺点： 很容易在训练数据中生成复杂的树结构，造成过拟合（overfitting）。剪枝可以缓解过拟合的负作用，常用方法是限制树的高度、叶子节点中的最少样本数量。 学习一棵最优的决策树被认为是NP-Complete问题。实际中的决策树是基于启发式的贪心算法建立的，这种算法不能保证建立全局最优的决策树（Random Forest 引入随机能缓解这个问题）。 决策树，其实算是在所有学习模型里面比较符合人类思考理解事件的过程，就是我们根据特征值进行一系列判断决策的过程，每个节点对应一次独立的决策，每次决策往往也只考虑单一的特征，随着特征的增加，整个决策过程枝繁叶茂起来，最终形成一棵决策树。在根节点，我们收集到一次实践的所有特征，然后不断在决策路径上取出某一个特征并进行判断。举个例子： 现在有人给你介绍对象，你打听到对方的特点：白不白，富不富，美不美，然后决定去不去相亲。根据以往经验，我们给出所有可能性：所以每当出现一个新的对象时，我们就要一个个特点去判断，于是这种判断的过程就可以画成一棵树，例如根据特点依次判断：这就是决策树，每一层我们都提出一个问题，根据问题的回答来走向不同的子树，最终到达叶子节点时，做出决策（去还是不去）。可以看到，决策树没有任何特别的地方。其本质就是根据数据的某个维度进行切分，并不断重复这个过程，直至到达叶子节点（做出决策）。当然，如果切分的顺序不同，会得到不同的树。如果我们树足够大，那么所有的数据都可以实现进行精确的区分，但是这可能会直接导致泛化能力变差，模型过拟合。所以决策树的训练过程就是一个剪枝的过程。就是筛选重要特征，优先根据这些重要特征进行决策识别来提高处理效率，然后通过一定成都的剪枝提升模型的泛化能力。上述例子中，如果仔细观察，我们发现决策树中有一些叶子节点是可以合并的，合并之后，到达某个节点时就不需要进行额外的决策，例如切分顺序“白，富，美”得到的决策树合并后如下：模型变得更简洁明了，同时我们也可以更容易发现那些特征发挥着更重要的作用。 举个通俗易懂的例子，如下图所示的流程图就是一个决策树，长方形代表判断模块(decision block)，椭圆形成代表终止模块。https://www.cnblogs.com/JetpropelledSnake/p/14513544.html]]></content>
      <categories>
        <category>machine_learning</category>
        <category>算法</category>
      </categories>
      <tags>
        <tag>machine_learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[5023.大模型-模型优化-微调工具-Unsloth]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-25.LLM-%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86%2F5023.%E5%A4%A7%E6%A8%A1%E5%9E%8B-%E6%A8%A1%E5%9E%8B%E4%BC%98%E5%8C%96-%E5%BE%AE%E8%B0%83%E5%B7%A5%E5%85%B7-Unsloth%2F</url>
    <content type="text"><![CDATA[Unsloth简介2.1 主要特性 (1) 所有的内核均以OpenAI的Triton语言实现，并且手动实现反向传播引擎。Triton语言是面向LLM训练加速。 (2) 准确率0损失，没有近似方法，方法完全一致。 (3) 硬件层面无需变动。支持18年之后的Nvidia GPU(V100, T4, Titan V, RTX20,30,40x, A100, H100, L40等，GTX1070,1080也支撑，但比较慢)，Cuda最低兼容版本是7.0 (4) 通过WSL适用于Linux和Windows (5) 基于bisandbytes包，支持4bit和16bit的 QLoRA/LoRA微调 (6) 开源代码有5倍的训练效率提升， Unsloth Pro可以提升至30倍]]></content>
      <categories>
        <category>LLM</category>
      </categories>
      <tags>
        <tag>LLM</tag>
        <tag>fit</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[5023.大模型-模型优化-微调进程可视化工具-Unsloth]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-25.LLM-%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86%2F5023.%E5%A4%A7%E6%A8%A1%E5%9E%8B-%E6%A8%A1%E5%9E%8B%E4%BC%98%E5%8C%96-%E5%BE%AE%E8%B0%83%E8%BF%9B%E7%A8%8B%E5%8F%AF%E8%A7%86%E5%8C%96%E5%B7%A5%E5%85%B7-swanlab%2F</url>
    <content type="text"><![CDATA[SwanLab 是一个跟踪实验、可视化结果、发现新改进点等的在线平台。简而言之，SwanLab使你能够更快地构建更好的模型，并轻松与同事分享发现。 使用说明这是一个已经集成到python 的可视化库，使用非常简单，通过pip安装 swanlab,然后通过key(SwanLab 注册后获取) 登录 swanlab 账号。123pip install swanlabswanlab login 登录后，在代码中添加以下代码即可使用swanlab可视化工具。可以跟踪 系统指标 和 控制台日志。运行此示例代码，以查看新的运行在SwanLab中出现：1234567891011121314151617181920212223242526272829import swanlabimport random# 初始化一个新的swanlab run类来跟踪这个脚本swanlab.init( # 设置将记录此次运行的项目信息，会在web上显示项目名称，所以这需要时一个可以辨别的名称。 project="my-awesome-project", # 跟踪超参数和运行元数据，在页面都是可视的。 config=&#123; "learning_rate": 0.02, "architecture": "CNN", "dataset": "CIFAR-100", "epochs": 10 &#125;)# 模拟训练epochs = 10offset = random.random() / 5for epoch in range(2, epochs): acc = 1 - 2 ** -epoch - random.random() / epoch - offset loss = 2 ** -epoch + random.random() / epoch + offset # 向swanlab上传训练指标，这些指标会在网页端以折线图的形式展示出来。 swanlab.log(&#123;"acc": acc, "loss": loss&#125;)# [可选] 完成训练，这在notebook环境中是必要的swanlab.finish() 通过在我们的项目中插入代码，整个训练过程中的信息都会被记录下来，并在SwanLab中可视化。 包括config配置的内容， 每次迭代过程中通过 swanlab.log 存储的指标信息，同时swanlab还会自动的获取项目的仓库、版本、系统信息（cpu、内存、显卡、显存）等。 一个示例结果 exp_Aug22_15-13-31]]></content>
      <categories>
        <category>LLM</category>
      </categories>
      <tags>
        <tag>LLM</tag>
      </tags>
  </entry>
  <entry>
    <title></title>
    <url>%2F05.%E5%A4%87%E8%80%83-03.%E9%9B%85%E6%80%9D%2F%E9%9B%85%E6%80%9D-01%2F</url>
    <content type="text"><![CDATA[第1-2月：基础建设阶段 了解考试结构：熟悉雅思的四个部分：听力、阅读、写作和口语。 评估当前水平：通过官方雅思练习测试或在线模拟考试来评估你当前的水平。 制定学习计划：根据评估结果，确定哪些部分需要重点提升，并制定具体的学习计划。 基础知识学习：开始逐步建立词汇量，学习基础的语法知识。 日常学习习惯：每天至少安排一到两个小时的学习时间，并逐渐增加。 初步应用：开始做一些基础的听力和阅读练习，记录新词汇和短语。 第3-4月：技能强化阶段 深入学习：针对每个部分进行更加深入的学习。对于弱项特别加强。 模拟测试：开始做完整的雅思模拟测试，至少每周一次，模拟真实考试的环境。 逐步提高：通过练习和复习错误逐步提高听力和阅读速度和准确度。 写作练习：每周至少练习两次写作，包括Task 1和Task 2，并寻求反馈。 口语练习：开始与英语母语者练习对话，或者参加语言交换活动，提高口语流利度。 第5月：综合提升阶段 考试技巧：学习各种应试技巧，例如时间管理、快速寻找关键信息等。 强化模拟：加大模拟测试的频率，每周至少两次，重点是时间管理和应试策略。 弱点攻克：集中攻克自己的弱点领域，使用额外的材料和练习加以强化。 反馈和调整：根据模拟测试和实践中的反馈进行调整，提升效率。 第6月：冲刺复习阶段 全面复习：回顾所有的学习材料，确保你对每个部分都有深入的理解和准备。 真实模拟：进行更多的全真模拟测试，模拟考试的压力和流程。 焦点整合：聚焦于整合语言技能，进行综合性的练习。 放松调节：考前保持良好的休息，避免过度疲劳。 考前准备：熟悉考试当天的流程，理清考试策略。]]></content>
  </entry>
  <entry>
    <title><![CDATA[4031.大模型-编程基础-使用GPU]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-25.LLM-%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86%2F4031.%E5%A4%A7%E6%A8%A1%E5%9E%8B-%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%80-%E4%BD%BF%E7%94%A8GPU%2F</url>
    <content type="text"><![CDATA[使用说明因为CPU和GPU本身是不同的硬件单元，而我们所有正常的编码操作，默认都是在cpu上运行的，缓存参数也默认是存储在内存（对应cpu）上。要使用GPU进行相关操作，起始就是要我们把对应的模型部署到GPU上，也就是把模型和参数指标存储到显存（显卡存储）上，这时候在进行相关操作就是在gpu进行的操作。具体要迁移的模型和参数指标如下： 将模型部署到GPU上，需要在导入模型时，设置 .from_pretrained(model_name,device_map=&quot;cuda&quot;, torch_dtype=torch.float16)。8B模型需要的显存在16G左右，因为使用的是 3090&amp;24G显存 的卡，只能运行低精度的模型，所以在部署阶段指定了低精度方案 (torch_dtype=torch.float16) 。 除了模型，我们最终得到的输入到模型的张量也需要从内存（cpu）拷贝到显存（gpu）上。tokenizer(prompt, return_tensors=&quot;pt&quot;).to(&quot;cuda&quot;) 123456789101112131415161718192021222324252627282930313233343536from transformers import AutoTokenizer, AutoModelForCausalLMimport torch# 加载模型model_name="/data1/tf_data/model/llama3/Meta-Llama-3-8B-Instruct"## device_map="cuda" 指定模型部署到GPU上model = AutoModelForCausalLM.from_pretrained(model_name,device_map="cuda", torch_dtype=torch.float16) tokenizer = AutoTokenizer.from_pretrained(model_name)# 文本生成函数def generate_response(prompt): # 对输入进行分词， 将分词张量拷贝到gpu上 inputs = tokenizer(prompt, return_tensors="pt").to("cuda") # 生成响应，相应的token数目和答复的数目。 outputs = model.generate(**inputs, max_length=200, num_return_sequences=1) # 解码输出 response = tokenizer.decode(outputs[0], skip_special_tokens=True) return response# 输入处理包装函数def preprocess_input(input): messages = [ &#123;"role": "system", "content": "hello,You are a helpful human assistant!"&#125;, &#123;"role": "user", "content": input&#125;, ] text = tokenizer.apply_chat_template(messages, tokenize=False, # tokenize需要设置 False，否则反馈会和预期有很大出入。测试的时候，反馈的是拼音和翻译成英文的句子┑(￣Д ￣)┍。 add_generation_prompt=True) return text# 示例对话user_input = "介绍一下中国"response = generate_response(preprocess_input(user_input))print("模型回答:", response) 使用GPU以后，模型加载的时间会略有增长（cpu：2s vs gpu: 6s），但是推理时间会大幅减少（cpu：200s vs gpu: 5s）。通过使用GPU 相同任务，推理时间从原来的运行时间从原来的5min 减少到5.1s。（含模型部署加载）]]></content>
      <categories>
        <category>LLM</category>
      </categories>
      <tags>
        <tag>LLM</tag>
        <tag>gpu</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大模型-部署工具-transformer]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-25.LLM-%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86%2F5003.%E5%A4%A7%E6%A8%A1%E5%9E%8B-%E6%9E%B6%E6%9E%84-transformer-1.pipeline%2F</url>
    <content type="text"><![CDATA[conda 环境配置1conda create -n transformer python=3.8 下载模型1git clone https://github.com/meta-llama/llama3.git 示例代码基于pipeline 调用使用pipeline的方式，可以很好的帮助我们快速实现模型的调用。尤其是其中 prompt 的构建。 123456789101112131415161718192021222324252627282930313233343536import transformersimport torchmodel_name="/data1/tf_data/model/llama3/Meta-Llama-3-8B-Instruct"pipeline = transformers.pipeline( "text-generation", model=model_name, model_kwargs=&#123;"torch_dtype": torch.float16&#125;, device="cuda",)messages = [ &#123;"role": "system", "content": "hello,You are a helpful human assistant!"&#125;, &#123;"role": "user", "content": "介绍一下中国,请用中文回答"&#125;,]prompt = pipeline.tokenizer.apply_chat_template( messages, tokenize=False, add_generation_prompt=True)terminators = [ pipeline.tokenizer.eos_token_id, pipeline.tokenizer.convert_tokens_to_ids("&lt;|eot_id|&gt;")]outputs = pipeline( prompt, max_new_tokens=256, eos_token_id=terminators, do_sample=True, temperature=0.6, top_p=0.9,) 直接调用12345678910111213141516171819202122232425262728293031323334from transformers import AutoTokenizer, AutoModelForCausalLM# 加载模型model_name="/data1/tf_data/model/llama3/Meta-Llama-3-8B-Instruct"model = AutoModelForCausalLM.from_pretrained(model_name)tokenizer = AutoTokenizer.from_pretrained(model_name)# 文本生成函数def generate_response(prompt): # 对输入进行分词 inputs = tokenizer(prompt, return_tensors="pt") # 生成响应，相应的token数目和答复的数目。 outputs = model.generate(**inputs, max_length=200, num_return_sequences=1) # 解码输出 response = tokenizer.decode(outputs[0], skip_special_tokens=True) return response# 输入处理包装函数def preprocess_input(input): messages = [ &#123;"role": "system", "content": "hello,You are a helpful human assistant!"&#125;, &#123;"role": "user", "content": input&#125;, ] tokenizer_input = tokenizer.apply_chat_template(messages, tokenize=False, # tokenize需要设置 False，否则反馈会和预期有很大出入。测试的时候，反馈的是拼音和翻译成英文的句子┑(￣Д ￣)┍。 add_generation_prompt=True) return tokenizer_input# 示例对话user_input = "介绍一下中国"response = generate_response(preprocess_input(user_input))print("模型回答:", response) 更多调用示例，参考开箱即用的-pipelines 背后原理这些简单易用的 pipeline 模型实际上封装了许多操作，下面我们就来了解一下它们背后究竟做了啥。以情感分析 pipeline 为例，我们运行下面的代码12345from transformers import pipelineclassifier = pipeline("sentiment-analysis")result = classifier("I've been waiting for a HuggingFace course my whole life.")print(result) 就会得到结果：1[&#123;'label': 'POSITIVE', 'score': 0.9598048329353333&#125;] 实际上它的背后经过了三个步骤： 预处理 (preprocessing)，将原始文本转换为模型可以接受的输入格式； 将处理好的输入送入模型； 对模型的输出进行后处理 (postprocessing)，将其转换为人类方便阅读的格式。 使用分词器进行预处理因为神经网络模型无法直接处理文本，因此首先需要通过预处理环节将文本转换为模型可以理解的数字。具体地，我们会使用每个模型对应的分词器 (tokenizer) 来进行： 将输入切分为词语、子词或者符号（例如标点符号），统称为 tokens； 根据模型的词表将每个 token 映射到对应的 token 编号（就是一个数字）； 根据模型的需要，添加一些额外的输入。 我们对输入文本的预处理需要与模型自身预训练时的操作完全一致，只有这样模型才可以正常地工作。注意，每个模型都有特定的预处理操作，如果对要使用的模型不熟悉，可以通过 Model Hub 查询。这里我们使用 AutoTokenizer 类和它的 from_pretrained() 函数，它可以自动根据模型 checkpoint 名称来获取对应的分词器。 情感分析 pipeline 的默认 checkpoint 是 distilbert-base-uncased-finetuned-sst-2-english，下面我们手工下载并调用其分词器：1234567891011from transformers import AutoTokenizercheckpoint = "distilbert-base-uncased-finetuned-sst-2-english"tokenizer = AutoTokenizer.from_pretrained(checkpoint)raw_inputs = [ "I've been waiting for a HuggingFace course my whole life.", "I hate this so much!",]inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors="pt")print(inputs) 1234567891011&#123; &apos;input_ids&apos;: tensor([ [ 101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012, 102], [ 101, 1045, 5223, 2023, 2061, 2172, 999, 102, 0, 0, 0, 0, 0, 0, 0, 0] ]), &apos;attention_mask&apos;: tensor([ [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0] ])&#125; 可以看到，输出中包含两个键 input_ids 和 attention_mask，其中 input_ids 对应分词之后的 tokens 映射到的数字编号列表，而 attention_mask 则是用来标记哪些 tokens 是被填充的（这里“1”表示是原文，“0”表示是填充字符）。 先不要关注 padding、truncation 这些参数，以及 attention_mask 项，后面我们会详细介绍:)。 将预处理好的输入送入模型预训练模型的下载方式和分词器 tokenizer 类似，Transformers 包提供了一个 AutoModel 类和对应的 from_pretrained() 函数。下面我们手工下载这个 distilbert-base 模型：1234from transformers import AutoModelcheckpoint = "distilbert-base-uncased-finetuned-sst-2-english"model = AutoModel.from_pretrained(checkpoint) 预训练模型的本体只包含基础的 Transformer 模块，对于给定的输入，它会输出一些神经元的值，称为 hidden states 或者特征 (features)。对于 NLP 模型来说，可以理解为是文本的高维语义表示。这些 hidden states 通常会被输入到其他的模型部分（称为 head），以完成特定的任务，例如送入到分类头中完成文本分类任务。 其实前面我们举例的所有 pipelines 都具有类似的模型结构，只是模型的最后一部分会使用不同的 head 以完成对应的任务。 transformer_and_head Transformers 库封装了很多不同的结构，常见的有： Model （返回 hidden states） ForCausalLM （用于条件语言模型） ForMaskedLM （用于遮盖语言模型） ForMultipleChoice （用于多选任务） ForQuestionAnswering （用于自动问答任务） ForSequenceClassification （用于文本分类任务） *ForTokenClassification （用于 token 分类任务，例如 NER） Transformer 模块的输出是一个维度为 (Batch size, Sequence length, Hidden size) 的三维张量，其中 Batch size 表示每次输入的样本（文本序列）数量，即每次输入多少个句子，上例中为 2；Sequence length 表示文本序列的长度，即每个句子被分为多少个 token，上例中为 16；Hidden size 表示每一个 token 经过模型编码后的输出向量（语义表示）的维度。 预训练模型编码后的输出向量的维度通常都很大，例如 Bert 模型 base 版本的输出为 768 维，一些大模型的输出维度为 3072 甚至更高。 我们可以打印出这里使用的 distilbert-base 模型的输出维度：123456789101112131415from transformers import AutoTokenizer, AutoModelcheckpoint = "distilbert-base-uncased-finetuned-sst-2-english"tokenizer = AutoTokenizer.from_pretrained(checkpoint)model = AutoModel.from_pretrained(checkpoint)raw_inputs = [ "I've been waiting for a HuggingFace course my whole life.", "I hate this so much!",]inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors="pt")outputs = model(**inputs)print(outputs.last_hidden_state.shape)torch.Size([2, 16, 768]) Transformers 模型的输出格式类似 namedtuple 或字典，可以像上面那样通过属性访问，也可以通过键（outputs[“last_hidden_state”]），甚至索引访问（outputs[0]）。 对于情感分析任务，很明显我们最后需要使用的是一个文本分类 head。因此，实际上我们不会使用 AutoModel 类，而是使用 AutoModelForSequenceClassification：12345678910111213141516from transformers import AutoTokenizerfrom transformers import AutoModelForSequenceClassificationcheckpoint = "distilbert-base-uncased-finetuned-sst-2-english"tokenizer = AutoTokenizer.from_pretrained(checkpoint)model = AutoModelForSequenceClassification.from_pretrained(checkpoint)raw_inputs = [ "I've been waiting for a HuggingFace course my whole life.", "I hate this so much!",]inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors="pt")outputs = model(**inputs)print(outputs.logits.shape)torch.Size([2, 2]) 可以看到，对于 batch 中的每一个样本，模型都会输出一个两维的向量（每一维对应一个标签，positive 或 negative）。对模型输出进行后处理 由于模型的输出只是一些数值，因此并不适合人类阅读。例如我们打印出上面例子的输出：1234567891011121314151617from transformers import AutoTokenizerfrom transformers import AutoModelForSequenceClassificationcheckpoint = "distilbert-base-uncased-finetuned-sst-2-english"tokenizer = AutoTokenizer.from_pretrained(checkpoint)model = AutoModelForSequenceClassification.from_pretrained(checkpoint)raw_inputs = [ "I've been waiting for a HuggingFace course my whole life.", "I hate this so much!",]inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors="pt")outputs = model(**inputs)print(outputs.logits)tensor([[-1.5607, 1.6123], [ 4.1692, -3.3464]], grad_fn=&lt;AddmmBackward0&gt;) 模型对第一个句子输出 $[-1.5607, 1.6123]$，对第二个句子输出 $[ 4.1692, -3.3464]$，它们并不是概率值，而是模型最后一层输出的 logits 值。要将他们转换为概率值，还需要让它们经过一个 SoftMax 层，例如：123import torchpredictions = torch.nn.functional.softmax(outputs.logits, dim=-1)print(predictions) 12tensor([[4.0195e-02, 9.5980e-01], [9.9946e-01, 5.4418e-04]], grad_fn=&lt;SoftmaxBackward0&gt;) 所有 Transformers 模型都会输出 logits 值，因为训练时的损失函数通常会自动结合激活函数（例如 SoftMax）与实际的损失函数（例如交叉熵 cross entropy）。 这样模型的预测结果就是容易理解的概率值：第一个句子 $[0.0402, 0.9598]$，第二个句子 $[0.9995, 0.0005]$。最后，为了得到对应的标签，可以读取模型 config 中提供的 id2label 属性：123print(model.config.id2label)&#123;0: 'NEGATIVE', 1: 'POSITIVE'&#125; 于是我们可以得到最终的预测结果： 第一个句子: NEGATIVE: 0.0402, POSITIVE: 0.9598 第二个句子: NEGATIVE: 0.9995, POSITIVE: 0.0005]]></content>
      <categories>
        <category>LLM</category>
      </categories>
      <tags>
        <tag>LLM</tag>
        <tag>transformer</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大模型-数据集-数据格式]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-25.LLM-%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86%2F5030.%E5%A4%A7%E6%A8%A1%E5%9E%8B-%E6%95%B0%E6%8D%AE%E9%9B%86-%E6%95%B0%E6%8D%AE%E6%A0%BC%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[sharegpt]]></content>
      <categories>
        <category>LLM</category>
      </categories>
      <tags>
        <tag>LLM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux-常用shell命令-ssh]]></title>
    <url>%2F03.%E5%BA%94%E7%94%A8-02.Linux%E5%92%8Cshell%2FLinux-%E5%B8%B8%E7%94%A8shell%E5%91%BD%E4%BB%A4-ssh%2F</url>
    <content type="text"><![CDATA[中间主机中转链接-J 选项的语法为 -J [user@]host1[:port] ，通过使用 -J 选项，可以在不直接连接目标主机的情况下，通过中间主机进行连接，从而实现网络访问的中转和穿透。 user 是可选参数，表示中间主机上的用户名， host1 是中间主机的主机名或IP地址， port 是可选参数，表示SSH端口号。]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Shell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[集成学习-Stacking]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-20.MachineLearn%2F1204.%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0-3.Stacking%2F</url>
    <content type="text"><![CDATA[代码示例使用 Python 中 Scikit-learn 库中的 StackingClassifier 实现堆叠的简单示例：123456789101112131415161718192021222324252627282930313233343536# Importing necessary librariesfrom sklearn.datasets import load_irisfrom sklearn.model_selection import train_test_splitfrom sklearn.ensemble import RandomForestClassifierfrom sklearn.linear_model import LogisticRegressionfrom sklearn.ensemble import StackingClassifierfrom sklearn.metrics import accuracy_score# 加载 Iris 数据集。iris = load_iris()X, y = iris.data, iris.target# 数据集分为训练集和测试集。X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)# 定义基分类器，包括 RandomForestClassifier 和 LogisticRegression。base_classifiers = [ ('rf', RandomForestClassifier(n_estimators=10, random_state=42)), ('lr', LogisticRegression(random_state=42))]# 定义一个元分类器，它是另一个 LogisticRegression 分类器。meta_classifier = LogisticRegression(random_state=42)# 使用基分类器和元分类器定义 StackingClassifier。stacking_classifier = StackingClassifier(estimators=base_classifiers, final_estimator=meta_classifier)# 在训练数据上训练 StackingClassifier。基分类器的预测用作元分类器的输入特征。stacking_classifier.fit(X_train, y_train)# 预测测试数据的标签。y_pred = stacking_classifier.predict(X_test)# 计算了 StackingClassifier 在测试集上的准确率。accuracy = accuracy_score(y_test, y_pred)print("Accuracy:", accuracy)]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>Machine-Learning</tag>
        <tag>集成学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[集成学习-GBTD]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-20.MachineLearn%2F1204.%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0-2.Boosting-2.GBTD%2F</url>
    <content type="text"><![CDATA[GBDT基本概念和简介GBDT (Gradient Boosting Decision Tree) 是另一种基于 Boosting 思想的集成算法，除此之外 GBDT 还有很多其他的叫法，例如：GBM (Gradient Boosting Machine)，GBRT (Gradient Boosting Regression Tree)，MART (Multiple Additive Regression Tree) 等等。GBDT 算法由 3 个主要概念构成：Gradient Boosting (GB)，Regression Decision Tree (DT 或 RT) 和 Shrinkage。 梯度提升决策树（GBDT）是一种特殊的提升算法，它使用决策树作为基学习器，并通过梯度下降方法优化损失函数。 从 GBDT 的众多别名中可以看出，GBDT 中使用的决策树并非我们最常用的分类树，而是回归树。分类树主要用于处理响应变量为因子型的数据，例如天气 (可以为晴，阴或下雨等)。回归树主要用于处理响应变量为数值型的数据，例如商品的价格。当然回归树也可以用于二分类问题，对于回归树预测出的数值结果，通过设置一个阈值即可以将数值型的预测结果映射到二分类问题标签上，即 $Y={-1,1}$。 对于 Gradient Boosting 而言，首先，Boosting 并不是 Adaboost 中 Boost 的概念，也不是 Random Forest 中的重抽样。在 Adaboost 中，Boost 是指在生成每个新的基学习器时，根据上一轮基学习器分类对错对训练集设置不同的权重，使得在上一轮中分类错误的样本在生成新的基学习器时更被重视。GBDT 中在应用 Boost 概念时，每一轮所使用的数据集没有经过重抽样，也没有更新样本的权重，而是每一轮选择了不用的回归目标，即上一轮计算得到的残差 (Residual)。其次，Gradient 是指在新一轮中在残差减少的梯度 (Gradient) 上建立新的基学习器。 所以在GBTD中，我们的目标其实已经发生了变化，实际上我们的建模目标是降低特定损失函数计算得到的残差（但是显而易见的残差越低，模型准确性越高）。 概念说明残差：残差是实际值与预测值的差。针对预测问题，我们有预测函数$F(x)$，则第 $t$ 轮的预测值为 $F_t(x)$，第 $t$ 轮的残差为 $r_{t,i} = y_i - F_t(x_i)$。 GBDT的基本流程：1. 初始化模型，通常是一个常数值 2. 计算当前模型的残差（实际值与预测值的差） 3. 训练一个新的决策树来拟合这些残差 4. 将新树添加到现有模型中 5. 重复步骤2-4直到满足停止条件 示例下面我们通过一个年龄预测的 示例 简单介绍 GBDT 的工作流程。 假设存在 4 个人 (P = {p_1, p_2, p_3, p_4})，他们的年龄分别为 (14, 16, 24, 26)。其中 (p_1, p_2) 分别是高一和高三学生，(p_3, p_4) 分别是应届毕业生和工作两年的员工。利用原始的决策树模型进行训练可以得到如下图所示的结果：利用 GBDT 训练模型，由于数据量少，在此我们限定每个基学习器中的叶子节点最多为 2 个，即树的深度最大为 1 层。训练得到的结果如下图所示： 在训练第一棵树过程中，利用年龄作为预测值，根据计算可得由于 (p_1, p_2) 年龄相近，(p_3, p_4) 年龄相近被划分为两组。通过计算两组中真实年龄和预测的年龄的差值，可以得到第一棵树的残差 (R = {-1, 1, -1, 1})。因此在训练第二棵树的过程中，利用第一棵树的残差作为标签值，最终所有人的年龄均正确被预测，即最终的残差均为 (0)。 则对于训练集中的 4 个人，利用训练得到的 GBDT 模型进行预测，结果如下： (p_1) ：14 岁高一学生。购物较少，经常问学长问题，预测年龄 (Age = 15 - 1 = 14)。 (p_2) ：16 岁高三学生。购物较少，经常回答学弟问题，预测年龄 (Age = 15 + 1 = 16)。 (p_3) ：24 岁应届毕业生。购物较多，经常问别人问题，预测年龄 (Age = 25 - 1 = 24)。 (p_4) ：26 岁 2 年工作经验员工。购物较多，经常回答别人问题，预测年龄 (Age = 25 + 1 = 26)。 整个 GBDT 算法流程如下所示： GBDT 中也应用到了 Shrinkage 的思想，其基本思想可以理解为每一轮利用残差学习得到的回归树仅学习到了一部分知识，因此我们无法完全信任一棵树的结果。Shrinkage 思想认为在新的一轮学习中，不能利用全部残差训练模型，而是仅利用其中一部分，即： $$ r_m = y - s F_m \left(x\right), 0 \leq s \leq 1 $$ 注意，这里的 Shrinkage 和学习算法中 Gradient 的步长是两个不一样的概念。Shrinkage 设置小一些可以避免发生过拟合现象；而 Gradient 中的步长如果设置太小则会陷入局部最优，如果设置过大又容易结果不收敛。 代表框架LightGBMreference精通梯度提升算法]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>Machine-Learning</tag>
        <tag>集成学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[集成学习-Adaboost]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-05.MachineLearn%2F1204.%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0-2.Boosting-Adaboost%2F</url>
    <content type="text"><![CDATA[Adaboost 是 Boosting 算法中有代表性的一个。原始的 Adaboost 算法用于解决二分类问题，因此对于一个训练集 $$ T = {\left(x_1, y_1\right), \left(x_2, y_2\right), …, \left(x_n, y_n\right)} $$ 其中 (x_i \in \mathcal{X} \subseteq \mathbb{R}^n, y_i \in \mathcal{Y} = {-1, +1})，首先初始化训练集的权重 $$ \begin{equation} \begin{split} D_1 =&amp; \left(w_{11}, w_{12}, …, w_{1n}\right) \ w_{1i} =&amp; \dfrac{1}{n}, i = 1, 2, …, n \end{split} \end{equation} $$ 根据每一轮训练集的权重 (D_m)，对训练集数据进行抽样得到 (T_m)，再根据 (T_m) 训练得到每一轮的基学习器 (h_m)。通过计算可以得出基学习器 (h_m) 的误差为 (\epsilon_m)，根据基学习器的误差计算得出该基学习器在最终学习器中的权重系数 $$ \alpha_m = \dfrac{1}{2} \ln \dfrac{1 - \epsilon_m}{\epsilon_m} $$ 更新训练集的权重 $$ \begin{equation} \begin{split} D_{m+1} =&amp; \left(w_{m+1, 1}, w_{m+1, 2}, …, w_{m+1, n}\right) \ w_{m+1, i} =&amp; \dfrac{w_{m, i}}{Z_m} \exp \left(-\alpha_m y_i h_m\left(x_i\right)\right) \end{split} \end{equation} $$ 其中 (Z_m) 为规范化因子 $$ Z_m = \sum_{i = 1}^{n} w_{m, i} \exp \left(-\alpha_m y_i h_m \left(x_i\right)\right) $$ 从而保证 (D_{m+1}) 为一个概率分布。最终根据构建的 (M) 个基学习器得到最终的学习器： $$ h_f\left(x\right) = \text{sign} \left(\sum_{m=1}^{M} \alpha_m h_m\left(x\right)\right) $$ 代码实现123456789101112131415161718192021222324252627282930# Importing necessary librariesfrom sklearn.datasets import load_irisfrom sklearn.model_selection import train_test_splitfrom sklearn.ensemble import AdaBoostClassifierfrom sklearn.tree import DecisionTreeClassifierfrom sklearn.metrics import accuracy_score# 加载 Iris 数据集。iris = load_iris()X, y = iris.data, iris.target# 将数据集分为训练集和测试集。X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)# 定义 max_depth=1 的基础决策树分类器来创建弱学习器。base_classifier = DecisionTreeClassifier(max_depth=1)# 使用基分类器（决策树）定义 AdaBoost 分类器，并指定要使用的估计器（增强轮数）的数量adaboost_classifier = AdaBoostClassifier(estimator=base_classifier, n_estimators=50, random_state=42)# 在训练数据上训练 AdaBoost 分类器。adaboost_classifier.fit(X_train, y_train)# 预测测试数据的标签。y_pred = adaboost_classifier.predict(X_test)# 最后，我们计算 AdaBoost 分类器在测试集上的准确率。accuracy = accuracy_score(y_test, y_pred)print("Accuracy:", accuracy)]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>Machine-Learning</tag>
        <tag>集成学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[集成学习-Adaboost]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-20.MachineLearn%2F1204.%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0-2.Boosting-1.Adaboost%20%2F</url>
    <content type="text"><![CDATA[Boosting算法特征如下：通过将一些表现效果一般（可能仅仅优于随机猜测）的模型通过特定方法进行组合来获得一个表现效果较好的模型。从抽象的角度来看，Boosting算法是借助convex loss function在函数空间进行梯度下降的一类算法。Gradient Boost和Adaboost就是其中比较常见的两种。 Adaboost 是 Boosting 算法中有代表性的一个。由 Yoav Freund 和 Robert Schapire 提出，两人因此获得了哥德尔奖。 示例二元分类问题，例如如何划分红球和篮球。显然这个问题用一个线性分类器的话很难取得最好的效果。有没有办法通过组合一系列和正方形平行的线（每条线都相当于一个线性分类器）来获得一个比较好的分类效果呢？ 第一步：先矮子里拔将军，选择一条平行于四边且最不坏的线段。下图第一排中间的小图里，直线把图分为左边（红点）和右边（蓝点）两类，被错分的点只有3个，这似乎是能得到的最好的结果了。然后我们想去找第二条线（第二个线性分类器）。如果只是基于现有的这些点的话那么说不定得到的线段还是和之前那条差不多，那多个线段（线性分类器）就没有意义了。所以我们要稍微调整一下某些点在分类结果里的重要性，提升他们的权重。我们在这里提升了那三个被错分的点的权重。 第二步：我们找到了一条新的线段，因为之前提升了把蓝点和蓝点分在一起的重要性，所以这条线段没有选择平行于上下两边把上面4个点（1红3蓝）和下面7个点（5红2蓝）分开，而是选择尽可能多地把蓝点归在一起。然而这次不同的是我们分错的是右边那4个红点，于是我们放大那4个红点，提升他们的权重。 不断重复这一过程。最终我们得到了多个线性分类器，把这些线性分类器的结果做一个线性组合，我们就得到了整个集成模型的结果。每个线性分类器的结果的系数（权重）取决于它的表现，表现越好，权重越高。比如第一条线段的分类错误就优于第二条线段，那么它获得的权重也就会更大。集成模型的效果非常好。资料来源：Mehryar Mohris Foundations of Machine Learning的上课讲义 顺带一提，第一条线段的分类正确率是8/11，第二条线段的分类正确率是7/11，第三条线段的分类正确率是8/11，确实要好于随机猜测（以1/2为界）。 把 AdaBoost 和一开始对 Boosting 算法的定义比较看看，我们确实通过组合一系列表现一般的模型获得了一个表现优秀的模型。另外值得注意的是在训练过程中，每个新的模型都会基于前一个模型的表现结果进行调整（调整过程是和bagging类算法的主要区别），这也就是为什么 AdaBoost 是自适应（adaptive）的原因。 算法数学原理原始的 Adaboost 算法用于解决二分类问题，因此对于一个训练集 $$ T = {\left(x_1, y_1\right), \left(x_2, y_2\right), …, \left(x_n, y_n\right)} $$ 其中 (x_i \in \mathcal{X} \subseteq \mathbb{R}^n, y_i \in \mathcal{Y} = {-1, +1})，首先初始化训练集的权重 $$ \begin{equation} \begin{split} D_1 =&amp; \left(w_{11}, w_{12}, …, w_{1n}\right) \ w_{1i} =&amp; \dfrac{1}{n}, i = 1, 2, …, n \end{split} \end{equation} $$ 根据每一轮训练集的权重 (D_m)，对训练集数据进行抽样得到 (T_m)，再根据 (T_m) 训练得到每一轮的基学习器 (h_m)。通过计算可以得出基学习器 (h_m) 的误差为 (\epsilon_m)，根据基学习器的误差计算得出该基学习器在最终学习器中的权重系数 $$ \alpha_m = \dfrac{1}{2} \ln \dfrac{1 - \epsilon_m}{\epsilon_m} $$ 更新训练集的权重 $$ \begin{equation} \begin{split} D_{m+1} =&amp; \left(w_{m+1, 1}, w_{m+1, 2}, …, w_{m+1, n}\right) \ w_{m+1, i} =&amp; \dfrac{w_{m, i}}{Z_m} \exp \left(-\alpha_m y_i h_m\left(x_i\right)\right) \end{split} \end{equation} $$ 其中 (Z_m) 为规范化因子 $$ Z_m = \sum_{i = 1}^{n} w_{m, i} \exp \left(-\alpha_m y_i h_m \left(x_i\right)\right) $$ 从而保证 (D_{m+1}) 为一个概率分布。最终根据构建的 (M) 个基学习器得到最终的学习器： $$ h_f\left(x\right) = \text{sign} \left(\sum_{m=1}^{M} \alpha_m h_m\left(x\right)\right) $$ 代码实现123456789101112131415161718192021222324252627282930# Importing necessary librariesfrom sklearn.datasets import load_irisfrom sklearn.model_selection import train_test_splitfrom sklearn.ensemble import AdaBoostClassifierfrom sklearn.tree import DecisionTreeClassifierfrom sklearn.metrics import accuracy_score# 加载 Iris 数据集。iris = load_iris()X, y = iris.data, iris.target# 将数据集分为训练集和测试集。X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)# 定义 max_depth=1 的基础决策树分类器来创建弱学习器。base_classifier = DecisionTreeClassifier(max_depth=1)# 使用基分类器（决策树）定义 AdaBoost 分类器，并指定要使用的估计器（增强轮数）的数量adaboost_classifier = AdaBoostClassifier(estimator=base_classifier, n_estimators=50, random_state=42)# 在训练数据上训练 AdaBoost 分类器。adaboost_classifier.fit(X_train, y_train)# 预测测试数据的标签。y_pred = adaboost_classifier.predict(X_test)# 最后，我们计算 AdaBoost 分类器在测试集上的准确率。accuracy = accuracy_score(y_test, y_pred)print("Accuracy:", accuracy) reference精通梯度提升算法]]></content>
      <categories>
        <category>机器学习</category>
        <category>集成学习</category>
      </categories>
      <tags>
        <tag>Machine-Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[集成学习-XGBoost]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-20.MachineLearn%2F1204.%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0-2.Boosting-3.XGBoost%2F</url>
    <content type="text"><![CDATA[基本概念与原理XGBoost （eXtreme Gradient Boosting）的概念 “Extreme”表示其极致的性能优化 “Gradient”指其使用梯度下降算法优化损失函数 “Boosting”表示其采用提升（Boosting）集成学习方法 XGBoost 是一种高效的机器学习算法，由华盛顿大学的Tianqi Chen开发。它是梯度提升决策树（GBDT）的一种改进实现，通过集成多个弱学习器（通常是决策树）来构建强大的预测模型。 XGBoost 相较GBDT的优势XGBoost在传统GBDT的基础上引入了多项创新： 目标函数优化：XGBoost的目标函数由两部分组成： • 损失函数（Loss）：衡量模型预测与实际值的差距 • 正则化项（Regularization）：控制模型复杂度，防止过拟合 目标函数可表示为：Obj = L + Ω 二阶泰勒展开 XGBoost使用损失函数的二阶泰勒展开进行近似，这使得优化过程更加高效。 树结构学习：XGBoost采用贪心算法来确定最佳的树结构，通过评估分裂增益来决定是否进行节点分裂。 XGBoost 的实现原理XGBoost 的基本思想同 GBDT 一样，对于一个包含 $n$ 个样本和 $m$ 个特征的数据集 (\mathcal{D} = \left{\left(\mathbf{x}_i, y_i\right)\right})，其中 (\left|\mathcal{D}\right| = n, \mathbf{x}_i \in \mathbb{R}^m, y_i \in \mathbb{R})，一个集成树模型可以用 (K) 个加法函数(经过K轮迭代训练）预测输出： $$ \hat{y}_i^K = \phi \left(\mathbf{x}i\right) = \sum{k=1}^{K}{f_k \left(\mathbf{x}_i\right)} =\hat{y}_i^{K-1} + f_K \left(\mathbf{x}_i\right) , f_k \in \mathcal{F} $$ 其中，(\mathcal{F} = \left{f \left(\mathbf{x}\right) = w_{q}{\left(\mathbf{x}\right)}\right} \left(q: \mathbb{R}^m \to T, w \in \mathbb{R}^T\right)) 为回归树 (CART)，(q) 表示每棵树的结构，其将一个样本映射到最终的叶子节点，(T) 为叶子节点的数量，每个 (f_w) 单独的对应一棵结构为 (q) 和权重为 (w) 的树。不同于决策树，每棵回归树的每个叶子节点上包含了一个连续的分值，我们用 (w_i) 表示第 (i) 个叶子节点上的分值。 XGBoost 首先对损失函数进行了改进，添加了 L2 正则项，同时进行了二阶泰勒展开。损失函数表示为： $$ \begin{equation} \begin{split} \mathcal{L} \left(\phi\right) = \sum_{i}{l \left(\hat{y}_i, y_i\right)} + \sum_{k}{\Omega \left(f_k\right)} \ \text{where} \ \Omega \left(f\right) = \gamma T + \dfrac{1}{2} \lambda \left| w \right|^2 \end{split} \end{equation} $$ 其中，(l) 为衡量预测值 (\hat{y}_i) 和真实值 (y_i) 之间差异的函数，(\Omega) 为惩罚项，(\gamma) 和 (\lambda) 为惩罚项系数。 我们用 (\hat{y}_i^{\left(t\right)}) 表示第 (t) 次迭代的第 (i) 个实例，我们需要增加 (f_t) 来最小化如下的损失函数： $$ \mathcal{L}^{\left(t\right)} = \sum_{i=1}^{n}{l \left(y_i, \hat{y}_i^{\left(t-1\right)} + f_t \left(\mathbf{x}_i\right)\right)} + \Omega \left(f_t\right) $$ 对上式进行二阶泰勒展开有： $$ \mathcal{L}^{\left(t\right)} \simeq \sum_{i=1}^{n}{\left[l \left(y_i, \hat{y}_i^{\left(t-1\right)}\right) + g_i f_t \left(\mathbf{x}_i\right) + \dfrac{1}{2} h_i f_t^2 \left(\mathbf{x}_i\right)\right]} + \Omega \left(f_t\right) $$ 其中，(g_i = \partial_{\hat{y}^{\left(t-1\right)}} l \left(y_i, \hat{y}^{\left(t-1\right)}\right), h_i = \partial_{\hat{y}^{\left(t-1\right)}}^{2} l \left(y_i, \hat{y}^{\left(t-1\right)}\right)) 分别为损失函数的一阶梯度和二阶梯度。去掉常数项，第 (t) 步的损失函数可以简化为： $$ \tilde{\mathcal{L}}^{\left(t\right)} = \sum_{i=1}^{n}{\left[ g_i f_t \left(\mathbf{x}_i\right) + \dfrac{1}{2} h_i f_t^2 \left(\mathbf{x}_i\right)\right]} + \Omega \left(f_t\right) $$ 令 (I_j = \left{i \ | \ q \left(\mathbf{x}_i\right) = j\right}) 表示叶子节点 (j) 的实例集合，上式可重写为： $$ \begin{equation} \begin{split} \tilde{\mathcal{L}}^{\left(t\right)} &amp;= \sum_{i=1}^{n}{\left[ g_i f_t \left(\mathbf{x}_i\right) + \dfrac{1}{2} h_i f_t^2 \left(\mathbf{x}i\right)\right]} + \gamma T + \dfrac{1}{2} \lambda \sum{j=1}^{T}{w_j^2} \ &amp;= \sum_{j=1}^{T}{\left[\left(\sum_{i \in I_j}{g_i}\right) w_j + \dfrac{1}{2} \left(\sum_{i \in I_j}{h_i + \lambda}\right) w_j^2\right]} + \gamma T \end{split} \end{equation} $$ 对于一个固定的结构 (q \left(\mathbf{x}\right))，可以通过下式计算叶子节点 (j) 的最优权重 (w_j^*)： $$ w_j^* = - \dfrac{\sum_{i \in I_j}{g_i}}{\sum_{i \in I_j}{h_i} + \lambda} $$ 进而计算对应的最优值： $$ \tilde{\mathcal{L}}^{\left(t\right)} \left(q\right) = - \dfrac{1}{2} \sum_{j=1}^{T}{\dfrac{\left(\sum_{i \in I_j}{g_i}\right)^2}{\sum_{i \in I_j}{h_i} + \lambda}} + \gamma T $$ 上式可以作为评价树的结构 (q) 的评分函数。通常情况下很难枚举所有可能的树结构，一个贪心的算法是从一个节点出发，逐层的选择最佳的分裂节点。令 (I_L) 和 (I_R) 分别表示分裂后左侧和右侧的节点集合，令 (I = I_L \cup I_R)，则分裂后损失的减少量为： $$ \mathcal{L}{\text{split}} = \dfrac{1}{2} \left[\dfrac{\left(\sum{i \in I_L}{g_i}\right)^2}{\sum_{i \in I_L}{h_i} + \lambda} + \dfrac{\left(\sum_{i \in I_R}{g_i}\right)^2}{\sum_{i \in I_R}{h_i} + \lambda} - \dfrac{\left(\sum_{i \in I}{g_i}\right)^2}{\sum_{i \in I}{h_i} + \lambda}\right] - \gamma $$ XGBoost 也采用了 Shrinkage 的思想减少每棵树的影响，为后续树模型留下更多的改进空间。同时 XGBoost 也采用了随机森林中的特征下采样 (列采样) 方法用于避免过拟合，同时 XGBoost 也支持样本下采样 (行采样)。XGBoost 在分裂点的查找上也进行了优化，使之能够处理无法将全部数据读入内存的情况，同时能够更好的应对一些由于数据缺失，大量零值和 One-Hot 编码导致的特征稀疏问题。除此之外，XGBoost 在系统实现，包括：并行化，Cache-Aware 加速和数据的核外计算 (Out-of-Core Computation) 等方面也进行了大量优化，相关具体实现请参见论文和 文档。 运行示例简单示例代码如下，涵盖数据加载，模型训练，模型预测，性能评估。1234567891011121314151617181920212223242526272829303132333435363738394041424344# loadDatairis = load_iris()X, Y = iris.data, iris.targetfeature_names = iris.feature_namestarget_names = iris.target_namesX_train, X_test, Y_train, Y_test = train_test_split( X, Y, test_size=0.2, random_state=30, stratify='y' )# 将数据转换为DMatrix格式dtrain = xgb.DMatrix(X_train, label=y_train, feature_names=feature_names)dtest = xgb.DMatrix(X_test, label=y_test, feature_names=feature_names)# 训练参数params = &#123; 'objective': 'multi:softmax', # 多分类问题 'num_class': 3, # 类别数量 'max_depth': 3, # 树的最大深度 'learning_rate': 0.1, # 学习率 'eval_metric': 'mlogloss', # 评估指标 'seed': 42 # 随机种子 &#125;# 训练模型num_rounds = 100 # 迭代次数model = xgb.train( params, dtrain, num_rounds, evals=[(dtrain, 'train'), (dtest, 'test')], early_stopping_rounds=10, verbose_eval=20)# 模型预测y_pred = model.predict(dtest)# 准确率评估accuracy = accuracy_score(Y_test, y_pred)# 混淆矩阵cm = confusion_matrix(Y_test, y_pred) reference精通梯度提升算法]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>Machine-Learning</tag>
        <tag>集成学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux-端口管理-iptables]]></title>
    <url>%2F03.%E5%BA%94%E7%94%A8-02.Linux%E5%92%8Cshell%2FLinux-%E7%AB%AF%E5%8F%A3%E7%AE%A1%E7%90%86-iptables%2F</url>
    <content type="text"><![CDATA[iptables]]></content>
      <categories>
        <category>Tobeinsert</category>
      </categories>
      <tags>
        <tag>Tobeinsert</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[5012.大模型-部署-构建本地知识库实践]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-25.LLM-%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86%2F5012.%E5%A4%A7%E6%A8%A1%E5%9E%8B-%E9%83%A8%E7%BD%B2-%E6%9E%84%E5%BB%BA%E6%9C%AC%E5%9C%B0%E7%9F%A5%E8%AF%86%E5%BA%93%E5%AE%9E%E8%B7%B5%2F</url>
    <content type="text"><![CDATA[referenceDify]]></content>
      <categories>
        <category>LLM</category>
      </categories>
      <tags>
        <tag>LLM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Software-MSI-软件测评]]></title>
    <url>%2F03.%E5%BA%94%E7%94%A8-01.software_in_NGS%2FSoftware-MSI-%E8%BD%AF%E4%BB%B6%E6%B5%8B%E8%AF%84%2F</url>
    <content type="text"><![CDATA[微卫星不稳定性 (MSI) 是多种癌症类型中常见的一种现象，其特征是在整个基因组中发现的微卫星区域中插入和缺失（indels）的积累，具有高微卫星不稳定性 (MSI-H) 的癌症可能是免疫检查点抑制剂治疗的良好候选者。所以MSI自1993年首次在结直肠癌中被提出后，不断发展。目前作为帮助指导免疫检查点抑制剂治疗的生物标志物，已经进入数十种癌症的指南/共识之中。详细信息可以参考我们之前的合集文章 。 虽然尚未在实验室环境中确切证明 MSI 是如何产生的，但主要的假设是 DNA 错配修复途径的缺陷可能导致微卫星位点插入/缺失数量的增加。这种插入缺失率的显着增加是 MSI 的主要特征及其主要识别方式。目前已经有很多基于不同原理的检测方法包括IHC、PCR、NGS。之前的文章也有介绍，在这里我们也不进行赘述，今天介绍的是最近刚发表在发表在《Briefings in Bioinformatics》期刊上的一篇对现有公开的基于NGS数据进行MSI检测软件进行的评估文章。 因为MSI-L不是在所有软件都支持，所以所有测评都是进行的二分类：MSI-H 和 MSS（MSI-L视为MSS）。 测试数据集数据分为两个部分：TCGA数据 和 其他测试数据。 TCGA的测试数据都是从GDC下载的，数据格式都是Bam文件格式，数据类型包括 WXS（852）、WGS（321）、RNA（825） 的测序数据。具体清单如下： | Project ID | Cancer | Sequencing | Number of Samples | Number of MSS | Number of MSI-H | | ———- | ——————- | ———- | —————– | ————- | ————— | | COAD | Colon | WGS | 56 | 46 | 10 | | ESCA | Esophageal | WGS | 2 | 0 | 2 | | STAD | Stomach | WGS | 136 | 107 | 29 | | UCEC | Uterine/endometrial | WGS | 145 | 102 | 43 | | COAD | Colon | WXS | 284 | 232 | 52 | | ESCA | Esophageal | WXS | 3 | 0 | 3 | | READ | Rectum | WXS | 3 | 0 | 3 | | STAD | Stomach | WXS | 292 | 228 | 64 | | UCEC | Uterine/endometrial | WXS | 268 | 196 | 72 | | COAD | Colon | RNA | 280 | 230 | 50 | | ESCA | Esophageal | RNA | 3 | 0 | 3 | | READ | Rectum | RNA | 3 | 0 | 3 | | STAD | Stomach | RNA | 272 | 213 | 59 | | UCEC | Uterine/Endometrial | RNA | 268 | 196 | 72 | 非TCGA数据都是从SRA数据库数据库中提取出来的，然后参考TCGA的处理流程（BWA比对到GRCh38.p14）进行的相关处理。 | Project ID | Cancer | Sequencing | Number of Samples | Number of MSS | Number of MSI-H | | ———– | ———– | —————- | —————– | ————- | ————— | | PRJNA629785 | Colorectal | End-seq | 34 | 7 | 27 | | PRJNA810563 | Pan | 6 Marker Panel | 178 | 166 | 12 | | SRP008162 | Prostate | T/O WXS | 21 | 16 | 5 | | PRJNA727917 | Colorectal | P/N WXS | 21 | 0 | 21 | | PRJNA256024 | Prostate | 53 Marker Panel | 43 | 30 | 13 | | PRJNA701182 | Pan | 161 Marker Panel | 191 | 185 | 6 | | PRJNA841034 | Gastric | TSO500 | 36 | 34 | 2 | | PRJEB57620 | Male Breast | TSO500 | 14 | 14 | 0 | | PRJNA843231 | Pan | TSO500 | 14 | 11 | 3 | | PRJNA748264 | Colon | RNA | 143 | 122 | 21 | 测评的软件文章总共评估了8款软件，分别是：MSIsensor , MSIsensor2 , MSIsensor-pro , mSINGS, MANTIS , MSINGB , PreMSIm , and MSIsensor-RNA。 当然所有的相关处理基本（除了 MANTIS 调整了质量阈值，否则找不到可用位点无法分析）都是按着研发作者的推荐设置进行的配置。 Tool Original evaluation data Algorithm used for MSI detection Output (MSI score) Recommended threshold Requires paired normal MSIsensor 242 endometrial TCGA WXS samples χ2 test between tumor and normal read counts Percent of unstable microsatellites 3.5 Yes MSIsensor-pro 1532 pan-cancer TCGA WXS samples Multinomial distribution model distinguishes MSI sites by comparing probability of polymerase slippage Percent of unstable microsatellites None No MSIsensor2 117 EGA samples and 10 TSO500 samples (TCGA also used but not numerically described) Machine learning based (specifics not given) Percent of unstable microsatellites 20 No mSINGS 26 TCGA pan-cancer WXS and 298 pan-cancer gene panel samples Read count differences between tumor sample and baseline normal Fraction of unstable microsatellites 0.2 No MANTIS 387 pan-cancer TCGA WXS samples Absolute stepwise difference between tumor and normal read counts Average aggregate instability 0.4 Yes MSINGB 1432 pan-cancer TCGA WXS samples and 1055 pan-cancer non-TCGA WXS samples NGBoost machine learning model based on somatic mutations MSI status and probability of the classification N/A (No score output) No PreMSIm 1383 pan-cancer TCGA RNA samples and 2006 gastric/colorectal microarray samples K-nearest neighbors machine learning model based on gene expression MSI status and probability of the classification N/A (No score output) No MSIsensor-RNA 1428 pan-cancer TCGA RNA samples, 247 non-TCGA RNA samples, 1468 gastric/colorectal microarray samples, and 133 SC-RNA colorectal samples Support vector machine learning classifier based on gene expression MSI status and probability of the classification N/A (No score output, but there are recommendations for feature selection thresholds) No 测评结果MSI 工具在 WXS 样本上的表现优于 WGS 样本大多数 MSI 工具在 WXS 数据上的表现优于在 WGS 数据上的表现。两个例外是 mSINGS 和 MSINGB，它们在额外的配对正常和仅肿瘤 WXS 数据集上的性能指标较低。所有 MSI 工具都对 TCGA WXS 数据表现出良好的性能，但 mSINGS 除外，它的召回率和 F1 分数较低。在所有 MSI 工具中，只有 MSIsensor2 在 WGS 数据的所有性能指标上都具有较高的值 所有 MSI 工具及其在创建混淆矩阵的所有数据集上的性能的热图（具体数据不影响我们整体理解软件性能，有需要可以在文章原文中查看）。黑色图块是 NA 值，黑白条纹图块是无法计算指标的实例。 P/N WXS 是额外的配对正常全外显子组测序数据集， T/O WXS 是额外的仅肿瘤全外显子组测序数据集。 6 Marker是 6 个单核苷酸芯片。 TCGA WGS 是由来自TCGA的全外显子组 WXS 是由来自TCGA的 WGS 数据。 all 是每个工具的合并结果。 MSIsensor、MSIsensor-pro、MSIsensor2 和 MANTIS 的 ROC 和 PR 曲线均具有较高的曲线下面积 (AUC)。除 MSIsensor2 和 MANTIS 之外，所有工具的 WGS 数据的 ROC 和 PR AUC 值也显着低于 WXS 数据（A、B、C、D )。在 ROC 空间与 PR 空间中测量时，AUC 也出现大幅下降，这意味着工具可能会遗漏更多真实的阳性结果（2C、D )。 ROC 和 PR AUC 最显着的差异体现在 WXS 数据上的 mSINGS 以及 WGS 数据上的 MSIsensor 和 MSIsensor-pro 所有 TCGA 样本的 ROC 和 PR 曲线。 TCGA WXS（A、B）和 WGS（C、D）样本的所有 ROC 曲线和 PR 曲线。 MSI 工具的性能指标因测序类型而异，并且在多种测序类型上缺乏一致性]]></content>
      <categories>
        <category>Tobeinsert</category>
      </categories>
      <tags>
        <tag>Tobeinsert</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Standard-ACGS_Best_Practice_Guidelines_for_Variant_Classification_in_Rare_Disease_2024]]></title>
    <url>%2F01.%E7%9F%A5%E8%AF%86-12.ACMG%E6%89%A7%E8%A1%8C%E8%AF%A6%E8%A7%A3%2F01.%E6%A6%82%E8%BF%B0-%E9%81%97%E4%BC%A0%E5%88%86%E6%9E%90%E7%B3%BB%E5%88%97%E6%8C%87%E5%8D%97%2F</url>
    <content type="text"><![CDATA[20152015年，ACMG &amp; AMP 联合发表了standards and guidelines for the interpretation of sequence variants (Richards et al Genetics in Medicine 2015)30223-9/fulltext) 具体参考我们之前的文章 指南描述了一种根据一系列标准和证据水平（非常强、强、中等或支持性）对变异进行分类的方法，包括“致病性”、“可能致病性”、“意义不明确”、“可能良性”或“良性”。 20172017年1月，AMP &amp; ASCO &amp; CAP 针对癌症基因组解读发表了Standards and Guidelines for the Interpretation and Reporting of Sequence Variants in Cancer 具体参考我们之前的文章 和2015年发表的文章提供了一套独立不同级别体系，判定的关注点不是致病性，而是临床意义。 tier I，具有强烈临床意义的变体（A和B级证据）； tier II，具有潜在临床意义的变体（C或D级证据）； tier III，临床意义未知的变体； tier IV，良性或可能良性的变体 2017年12月，AGCS 发表了 ACGS Best Practice Guidelines for Variant Classification 2017因为2015年的指南对所有变异都根据疾病和遗传模式进行分类。随着认知的增加，考虑到特定标准的适用性和权重可能因疾病和基因而异，需要对特定基因的变异分类提供更有针对性的指导。 美国ClinGen项目进一步制定这些指南，该项目包括特定疾病工作组和序列变异解释(SVI)工作组，该工作组为调整指南提供建议和协调方法，而本次发表的最佳实践，是2015年指南的第一次更新。 相关历史工作可以在ClinGen 项目组的历史记录 查看（不止于2017年）。在本次更新中，主要涉及两个部分。 证据标准的判定需要结合患者的临床表型进行评估，仅当患者的表型与疾病基因关联一致时才应使用这些标准，涉及 PS2 、PS3 和 PM6 。 增加了使用功能实验 能/否 作为 PS3 证据项的说明。 针对2015的所有证据项，提供了更详细的补充说明。这里列出部分，详细内容可以参考上述链接原文获取。 20182018年8月，AGCS 发表了 ACGS Best Practice Guidelines for Variant Classification 2018 大概看了下，开始我都恍惚以为是完全一样的内容，后来细看了一下，还是有些区别的发现相比2017年的版本，在2018年对证据项进行了进一步的补充说明。 20192019年5月，AGCS 发表了 ACGS Best Practice Guidelines for Variant Classification 2019 补充了使用型别作为PP4证据项的示例。 对证据项的补充说明进行了进一步的补充完善。 20202020年2月，AGCS 发表了 ACGS Best Practice Guidelines for Variant Classification 2020 更新了根据证据项判定最终致病和疑似致病的的标准 Classification Combining rules Pathogenic (a) 1 Very strong AND ≥1 Strong OR ≥1 Moderate OR ≥2 Supporting Pathogenic (b) ≥3 Strong Pathogenic (c) 2 Strong AND ≥1 Moderate OR ≥2 Supporting Pathogenic (d) 1 Strong AND ≥3 Moderate OR ≥2 Moderate AND ≥2 Supporting OR ≥1 Moderate AND ≥4 Supporting Likely pathogenic (a) ≥2 Strong Likely pathogenic (b) 1 Strong AND 1-2 Moderate OR ≥2 Supporting Likely pathogenic (c) ≥3 Moderate OR 2 Moderate AND ≥2 Supporting OR 1 Moderate AND ≥4 Supporting 对stop loss类型的变异，如何区别PVS1和PM4两个证据项进行了具体描述。 针对检测实验室如何出局报告，及如何在报告中展示证据项信息进行了详细的示例说明。 然后惯例性的，又对证据项的补充说明进行了进一步的补充完善。 20242024年2月，AGCS 发表了 ACGS Best Practice Guidelines for Variant Classification 2024重大的更新是在指南中首次纳入了CNV的解读（ 2020年，ACMG联合CLinGen，曾单独发表过一篇CNV解读的技术标准: guidelines specific for the interpretation of copy number variants (CNVs)(Riggs et al., 2020) 在之前的stop-loss的基础上，针对start-loss 和frameshift类型的变异，增加了PVS1和PM4两个证据项进行了具体描述。因为2024版本是目前的最新版本，同时新增 CNV的解读，内容较多，具体内容单独介绍。参考文章]]></content>
      <categories>
        <category>NGS</category>
        <category>标准化</category>
        <category>遗传分析</category>
      </categories>
      <tags>
        <tag>Standards and guidelines</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大模型-检索增强生成(RAG)]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-25.LLM-%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86%2F5022.%E5%A4%A7%E6%A8%A1%E5%9E%8B-%E6%A8%A1%E5%9E%8B%E4%BC%98%E5%8C%96-RAG-00.%E6%A6%82%E8%BF%B0%2F</url>
    <content type="text"><![CDATA[检索增强生成（RAG）是指对大型语言模型输出进行优化，使其能够在生成响应之前引用训练数据来源之外的权威知识库。大型语言模型（LLM）用海量数据进行训练，使用数十亿个参数为回答问题、翻译语言和完成句子等任务生成原始输出。在 LLM 本就强大的功能基础上，RAG 将其扩展为能访问特定领域或组织的内部知识库，所有这些都无需重新训练模型。这是一种经济高效地改进 LLM 输出的方法，让它在各种情境下都能保持相关性、准确性和实用性。检索增强生成技术（Retrieval-Augmented Generation）受到了广泛关注，并被应用于各种场景，如知识库问答、法律顾问、学习助手、网站机器人等。 RAG的优势大模型面临的挑战尽管ChatGPT和Claude等LLM展现了令人印象深刻的能力，但它们也面临着一些挑战： 产生臆想的答案（幻觉）：在没有确切答案的情况下，LLM可能会产生误导性信息。 知识更新慢：当用户需要基于最新情况的具体响应时，LLM可能提供过时或不具体的信息。 知识来源缺乏引用：LLM可能引用非权威来源的信息，影响回答的准确性。 术语混淆：不同训练数据中相同的术语可能指向不同概念，导致LLM产生混淆。 领域专业知识不足： 尽管LLM拥有广泛的知识基础，但它们并不了解特定业务的细节，如公司的私有数据。 RAG 解决的问题: 避免“幻觉”问题：RAG 通过检索外部信息作为输入，获取领域特定的知识，辅助大型模型回答问题，这种方法能显著减少生成信息不准确的问题，。 信息的实时性： RAG 允许从外部数据源实时检索信息，RAG允许LLM访问最新的客户记录、产品规格和实时库存信息，解决知识时效性问题。 解决黑匣子问题：RAG技术使GenAI应用程序能够提供其使用的数据来源，增加透明度，类似于学术论文中的引用，增加回答的可追溯性。 数据隐私和安全： RAG 可以将知识库作为外部附件管理企业或机构的私有数据，避免数据在模型学习后以不可控的方式泄露。 降低应用成本：RAG提供了一种经济高效的方法，使得组织能够在不重新训练模型的情况下，提升LLM的输出质量。 RAG的关键优势： 白盒模型的透明度和可解释性：RAG的工作流程相对透明，模块之间的关系清晰，这为模型的效果调优和可解释性提供了优势。在检索召回的内容质量不高或置信度不足时，RAG系统有能力避免生成误导性信息，选择不生成回答而非提供错误的信息。 成本效益和响应速度：与微调模型相比，RAG的训练周期更短，成本更低。与长文本处理相比，RAG能够提供更快速的响应和更低的推理成本。在工业界和产业应用中，成本是一个至关重要的因素，而RAG在这一点上具有明显的优势。 私有数据管理与安全性：RAG通过将知识库与大型模型分离，为私有数据的管理提供了一个安全的实践基础。这种方法有助于企业更好地控制和管理其知识资产，同时解决了知识依赖问题。此外，RAG底座数据库的访问权限控制和数据管理相对容易实现，而这对于大模型来说则较为困难。 灵活性和适应性：RAG能够适应不同的数据源和检索需求，为特定领域或任务提供定制化的解决方案。这种灵活性使RAG在多种应用场景中都能找到其适用之处。 模块化和可扩展性：RAG的模块化设计允许它轻松集成新模块或调整现有模块之间的交互流程，以适应不断变化的任务需求和数据环境。 RAG和微调方法的异同 特征比较 RAG 微调 知识更新 直接更新检索知识库，无需重新训练。信息更新成本低，适合动态变化的数据。 通常需要重新训练来保持知识和数据的更新。更新成本高，适合静态数据。 外部知识 擅长利用外部资源，特别适合处理文档或其他结构化/非结构化数据库。 将外部知识学习到 LLM 内部。 数据处理 对数据的处理和操作要求极低。 依赖于构建高质量的数据集，有限的数据集可能无法显著提高性能。 模型定制 侧重于信息检索和融合外部知识，但可能无法充分定制模型行为或写作风格。 可以根据特定风格或术语调整 LLM 行为、写作风格或特定领域知识。 可解释性 可以追溯到具体的数据来源，有较好的可解释性和可追踪性。 黑盒子，可解释性相对较低。 计算资源 需要额外的资源来支持检索机制和数据库的维护。 依赖高质量的训练数据集和微调目标，对计算资源的要求较高。 推理延迟 增加了检索步骤的耗时 单纯 LLM 生成的耗时 降低幻觉 通过检索到的真实信息生成回答，降低了产生幻觉的概率。 模型学习特定领域的数据有助于减少幻觉，但面对未见过的输入时仍可能出现幻觉。 伦理隐私 检索和使用外部数据可能引发伦理和隐私方面的问题。 训练数据中的敏感信息需要妥善处理，以防泄露。 RAG 技术原理幻觉现象在大型语言模型（LLM）中主要因其无法访问最新信息而产生，这一问题根源于模型对其训练数据集的严重依赖。为了解决这一局限，提出了一种名为RAG（Retrieval-Augmented Generation）的方法，该方法允许LLM通过外部信息源动态地补充训练数据，从而提高回答的准确性。RAG通过结合传统的检索方法和预训练的语言模型，实现了对LLM输入信息的实时更新，避免了对模型进行昂贵且耗时的微调和再训练。这种方法增强了模型的灵活性和扩展性，使其可以轻松地应用于不同的LLM，满足多样化的需求。在实际应用中，RAG通过引入人类编写的现实世界数据作为信息源，不仅简化了回答生成过程，还大幅提高了生成响应的可靠性。随着技术的进步，RAG不断发展，已经能够支持检索与生成组件之间的多轮交互，通过多次迭代检索提高信息准确性，同时逐步优化生成的输出质量。平台如langchain和llamaindex已经对RAG方法进行了模块化处理，这些平台通过实现多轮搜索迭代到连续生成的不同策略，提高了方法的适应性，并扩展了其在实际应用中的范围。这些创新表明，尽管各平台对RAG的具体实施方法各不相同，但它们都遵循了基本的RAG工作流程，展示了这一技术在现代AI应用中的广泛适用性和有效性。 一个典型的RAG应用，主要包含两个部分： 1.索引 ； 2. 检索与生成。 索引从源数据中加载数据并进行索引，通常离线进行，并且支持动态更新，分为： 加载：根据不同的数据源选择合适的加载器，加载数据得到文档。 切片：使用文本切分器将文档切分成更小的片段，使用小片段一方面可以更好地匹配用户问题，同时也可以适应模型的有限上下文窗口。 存储：存储和索引切片，以便在检索时能够快速找到相关的数据，通常使用 Embeddings 模型和向量数据库（VectorStore）来完成。 检索与生成实际的 RAG 链，接收用户问题，从索引中检索相关数据，基于问题和这些数据生成结果，分为： 检索：给定用户输入，使用检索器从存储中检索相关的切片。 生成：使用包括问题和检索到的数据的提示调用 LLM 来生成答案。 RAG系统的主要组件和工作流程RAG（Retrieval-Augmented Generation，检索增强生成）系统是一个将生成式AI的优势与搜索引擎的功能相结合的复杂系统。要深入理解RAG，关键在于剖析其核心组件以及这些组件是如何协同工作的，以提供无缝的AI体验。 检索引擎：数据的精准定位RAG过程的首步是检索引擎的介入。这一环节涉及对庞大的信息库进行搜索，以定位与输入查询相匹配的相关数据。检索引擎运用精细的算法，确保所检索到的信息不仅相关性强，而且保持最新，为生成准确的响应打下坚实基础。 增强引擎：上下文的深度融合检索得到的相关信息随后被送入增强引擎。这一引擎的职责是将检索到的数据与原始查询紧密结合，从而扩充上下文的深度，为生成过程奠定一个更加明智的基础。增强引擎的介入，使得生成的响应更加精准和全面。 生成引擎：智能响应的构建最终，经过增强的输入信息被送入生成引擎。这里，通常是利用复杂的语言模型，基于扩充后的上下文创建出既连贯又紧密相关的响应。这种响应的生成，不仅依托于模型内部的先验知识，还通过检索引擎提供的外部数据得到了显著增强。 RAG工作流程的全景视图RAG的基本工作流程始于创建一个包含外部信息源的索引库。这个索引库是检索器模型的基石，它根据特定查询检索出相关的信息。在这一过程的最后阶段，生成器模型将检索到的信息与原始查询融合，形成最终的输出结果。RAG（Retrieval-Augmented Generation，检索增强生成）应用程序的成功依赖于一系列精心设计的步骤，确保了输出的准确性和相关性。以下是RAG系统关键步骤的深入分析。 第一步，数据索引：构建检索的基石在RAG系统中，数据索引是基础。它涉及将文档分割成块，编码为向量，并存储于向量数据库中，为检索引擎提供参考点。文本规范化，包括标记化、词干提取和停用词去除，是增强文本适用性的重要步骤。深度学习技术的应用，特别是预训练的语言模型（LM），为文本生成语义向量表示，极大地提升了索引的效率和检索的精确度。 第二步，输入查询处理：理解并转化用户需求用户的输入是RAG过程的起点。系统需要准确处理和理解这些输入，以形成检索引擎的搜索查询。这一步骤是确保检索结果与用户需求高度相关的关键。 第三步，搜索和排名：找到最相关的信息检索引擎根据输入查询的相关性对索引数据进行搜索和排名。利用语义相似性，检索与问题最相关的前k个块，这一步骤是RAG系统的核心，它决定了后续生成响应的质量和相关性。 第四步，提示增强：丰富输入，提升输出将检索到的最相关信息与原始查询结合，形成增强的提示。这一步骤为生成引擎提供了更丰富的信息源，有助于生成更准确和相关的响应。 第五步，响应生成：构建最终答案生成引擎结合原始问题和检索到的信息，输入LLM生成最终答案。这一步骤需要在保持与检索内容一致性和准确性的同时，引入创造性，以生成既准确又具有洞察力的文本。 第六步，评估：持续优化的关键评估是确保RAG应用成功的最后一步。它提供了关于系统输出准确性、忠实性和响应速度的客观衡量，是持续优化RAG策略的关键环节。我们仔细看看RAG内部从索引，检索，增强到最后生成每一步都是怎么运行的，以及使用RAG和不使用RAG对返回结果的效果影响如何。 RAG范式RAG研究范式在不断发展，我们将其分为三个阶段：Naive RAG、Advanced RAG和Modular RAG。尽管 RAG 方法具有成本效益并且超越了LLM的性能，但它们也表现出一些局限性。 Advanced RAG 和 Modular RAG 的发展正是针对 Naive RAG 的这些具体缺点的回应。 第一阶段，朴素RAGNaive RAG 研究范式代表了最早的方法，在 ChatGPT 广泛采用后不久就得到了重视。Naive RAG 遵循传统的过程，包括索引、检索和生成，也被称为“检索-生成”框架，前面讲的RAG是朴素RAG，它是最基础的，最核心的架构。随着大模型落地不断深化，朴素RAG也有一些缺点： 检索挑战——检索阶段经常在【精确度】和【召回率】方面遇到困难，导致选择错位或不相关的块，并丢失关键信息。 生成困难——在生成响应时，模型可能会面临幻觉问题，即生成【检索到的上下文不支持的内容】。此阶段还可能会受到输出的【不相关性、毒性或偏差】的影响，从而降低响应的质量和可靠性。 增强障碍——将检索到的信息与不同的任务集成可能具有挑战性，有时会导致【输出脱节或不连贯】。当从多个来源检索类似信息时，该过程还可能遇到【冗余，从而导致重复响应】。确定各个段落的重要性和相关性并确保风格和语气的一致性进一步增加了复杂性。面对复杂的问题，基于原始查询的【单一检索可能不足以获取足够的上下文信息】。 第二阶段，高级RAGAdvanced RAG 引入了特定的改进来克服 Naive RAG 的局限性。它着眼于提高检索质量，采用检索前和检索后策略。为了解决索引问题，Advanced RAG 通过使用【滑动窗口方法】、【细粒度分段】和【合并元数据】来改进其索引技术。此外，它还结合了多种优化方法来简化检索过程。 预检索过程——这一阶段的主要重点是【优化索引结构和原始查询】。优化索引的目标是提高索引内容的质量。这涉及到策略：增强数据粒度、优化索引结构、添加元数据、对齐优化、混合检索。而查询优化的目标是让用户的原始问题更清晰、更适合检索任务。常见的方法包括查询重写、查询变换、查询扩展等技术。检索后过程——一旦检索到相关上下文，将其与查询有效集成就至关重要。检索后过程中的主要方法包括【重新排序块和上下文压缩】。重新排列检索到的信息以将最相关的内容重新定位到提示的上下文中是一个关键策略。 第三阶段，模块化RAG模块化 RAG 架构超越了前两种 RAG 范例，提供了增强的适应性和多功能性。它采用了多种策略来改进其组件，例如添加用于相似性搜索的搜索模块以及通过微调来改进检索器。重组 RAG 模块等创新并重新排列 RAG 管道的引入是为了应对特定的挑战。向模块化 RAG 方法的转变正在变得普遍，支持跨其组件的顺序处理和集成的端到端训练。尽管具有独特性，模块化 RAG 仍建立在高级 RAG 和朴素 RAG 的基本原则之上，展示了 RAG 系列的进步和完善。 新模块模块化 RAG 框架引入了额外的专用组件来增强检索和处理能力。搜索模块适应特定场景，使用LLM生成的代码和查询语言，可以跨搜索引擎、数据库和知识图谱等各种数据源直接搜索。RAG-Fusion 通过采用多查询策略将用户查询扩展到不同的视角，利用并行向量搜索和智能重新排序来揭示显式和变革性知识，从而解决了传统搜索的局限性 。内存模块利用LLM的内存来指导检索，创建一个无限的内存池，通过迭代的自我增强使文本与数据分布更紧密地对齐。RAG系统中的路由可导航不同的数据源，为查询选择最佳路径，无论是涉及汇总、特定数据库搜索还是合并不同的信息流 。Predict模块旨在通过LLM直接生成上下文来减少冗余和噪音，确保相关性和准确性。任务适配器模块根据各种下游任务定制 RAG，自动提示检索零样本输入，并通过少数样本查询生成创建特定于任务的检索器 。这种综合方法不仅简化了检索过程，而且显着提高了检索信息的质量和相关性，以更高的精度和灵活性满足了广泛的任务和查询。 模块化 RAG 的优势模块化 RAG 通过允许模块替换或重新配置来解决特定挑战，从而提供卓越的适应性。这超越了 Naive 和 Advanced RAG 的固定结构，其特点是简单的“检索”和“读取”机制。 此外，模块化 RAG 通过集成新模块或调整现有模块之间的交互流程来扩展这种灵活性，从而增强其在不同任务中的适用性。重写-检索-读取等创新模型利用LLM的能力通过重写模块和LM反馈机制来细化检索查询来更新重写模型，从而提高任务性能。类似地，像Generate-Read 用LLM生成的内容取代传统检索，而背诵阅读强调从模型权重中检索，增强模型处理知识密集型任务的能力。混合检索策略集成了关键字、语义和向量搜索来满足不同的查询。此外，采用子查询和假设文档嵌入（HyDE）旨在通过关注生成的答案和真实文档之间嵌入相似性来提高检索相关性。模块布局和交互的调整，例如演示-搜索-预测（DSP）框架和 ITER-RETGEN 的迭代检索-读取-检索-读取流程 ，展示了模块输出的动态使用来支持另一个模块的功能，说明了对增强模块协同作用的复杂理解。Modular RAG Flow 的灵活编排展示了通过 FLARE 等技术进行自适应检索的优势 和自我 RAG 。该方法超越了固定的RAG检索过程，根据不同场景评估检索的必要性。灵活架构的另一个好处是RAG系统可以更轻松地与其他技术（例如微调或强化学习）集成 。 模块化RAG的核心在于将各种功能解耦，将其作为独立的模块进行处理。具体来说，模块化RAG包括了「搜索」、「预测」、「记忆」、「评估」、「验证」和「对齐」等外层模块，以及内层的「检索」、「重排序」、「重写」和「阅读」等RAG核心过程。在处理信息和响应用户查询的过程中，模块化RAG采用了多种信息处理流程。例如，传统的Navie RAG模式仅包括基本的「检索」和「阅读」步骤。而在更复杂的Advanced RAG模式中，包括了「重写」→「检索」→「重排序」→「阅读」的路径，这在提高检索和生成内容的质量方面尤为有效。DSP（Demonstrate-Search-Predict）模式则专注于验证、搜索和预测阶段，这些模块和模式的组合赋予了模块化RAG极大的灵活性和适应性，使其成为一种强大且可扩展的工具，能够有效地应对各种信息处理挑战，并在多样化的应用场景中提供高质量的回答。 RAG 场景对向量数据库的需求而检索系统对向量数据库的需求可以抽象描述为： 高精度的召回：向量数据库需要能够准确召回与查询语义最相关的文档或信息片段。这要求数据库能够理解和处理高维向量空间中的复杂语义关系，确保召回内容与查询的高度相关性。这里的效果既包括向量检索的数学召回精度也包括嵌入模型的语义精度。 快速响应：为了不影响用户体验，召回操作需要在极短的时间内完成，通常是毫秒级别。这要求向量数据库具备高效的查询处理能力，以快速从大规模数据集中检索和召回信息。此外，随着数据量的增长和查询需求的变化，向量数据库需要能够灵活扩展，以支持更多的数据和更复杂的查询，同时保持召回效果的稳定性和可靠性。 处理多模态数据的能力：随着应用场景的多样化，向量数据库可能需要处理不仅仅是文本，还有图像、视频等多模态数据。这要求数据库能够支持不同种类数据的嵌入，并能根据不同模态的数据查询进行有效的召回。 可解释性和可调试性：在召回效果不理想时，能够提供足够的信息帮助开发者诊断和优化是非常有价值的。因此，向量数据库在设计时也应考虑到系统的可解释性和可调试性。可选的向量数据库Pinecone WeaviateQdrantMilvus/ZillizChromaLanceDBVespa什么是Embedding矢量数据库不仅存储原始数据（可以是图像、音频或文本），还存储其编码形式：Embedding。这些 Embedding 本质上是存储数据上下文表示的数字（即 vector ）列表。直观上，当我们提到 Embedding 时，我们谈论的是实际存在于更高维度的数据（图像、文本、音频）的压缩、低维表示。 Embedding基于一个技巧：获取一段内容（文字，图片，视频…..）并将该内容转换为浮点数数组。 Embedding的重要性它建立了一座桥梁，连接了人类语言的丰富多彩与算法的冷冰冰的计算效率。算法擅长数字游戏，却不通人情，而通过文本向量化，它们仿佛获得了解读和处理语言的新技能。其应用范围广泛，从推荐触动人心的内容，到让聊天机器人更具人情味，再到在浩瀚的文本海洋中寻找微妙的规律，文本向量化无处不在。文本向量化 让机器能够进行情感分析、语言转换等看似高深的任务，以一种越来越接近人类的方式来理解和处理语言。这个图的左边，我们看的每一列（维)的数字代表一种特征，比如有代表是否是人类，年龄，性别等。 在二维平面里用图形化表示，我们可以理解Embeddings就是在x和y上的坐标，相同的类会聚集在一起，但是为什么又叫做向量或者矢量，矢量是代表有方向的。我们看的男人和女，与国王和女王线是平行的。说明沿着这条线的方向就代表性别的强度。越往右上角越代表越女性化。当维度越多，表征就更多，代表的语义就更加丰富。演示地址：https://projector.tensorflow.org/?ref=mlq.ai 向量之间的距离向量可能非常长且复杂。例如，OpenAI 的向量大小通常为 1536，这意味着每个Embeddings都是 1536 个浮点数的数组。就其本身而言，这些数据并没有多大意义：它只是为了找到其他接近的Embeddings。当我们将图像或文本片段表示为向量嵌入时，它们的语义相似性由它们的向量在向量空间中的接近程度来表示。 因此，我们想要查看的是对象向量之间的距离。这些向量表示（嵌入）通常是根据输入数据和任务通过训练模型创建的。 Word2Vec、GLoVE、USE 等是从文本数据生成嵌入的流行模型，而像 VGG 这样的 CNN 模型通常用于创建图像嵌入。 我们之前提到，我们通过计算对象向量之间的距离来发现对象之间的相似性。 我们可以根据最适合我们问题的距离度量来计算向量空间中这些向量之间的距离。相关距离计算方式可以参考我们之前的文章《机器学习中的距离计算》，里面介绍了欧几里德距离度量、曼哈顿距离度量、余弦距离度量、切比雪夫距离度量等常用的距离计算方式。 如何选择嵌入模型 常见的类Embbedding模型 检索用Embbedding 重排序用Embbedding Embbedding模型在RAG种的应用场景 知识库存入向量数据库之前，需要使用Embbedding模型 用户提问时的问题，需要使用使用Embbedding模型 检索完成之后重排序的时候，需要 Rank Embbedding模型 在哪里找到合适Embbedding模型？MTEB 被公认为是目前业界最全面、最权威的中文语义向量评测基准之一，涵盖了分类、聚类、检索、排序、文本相似度、STS等6个经典任务，共计35个数据集，为深度测试中文语义向量的全面性和可靠性提供了可靠的实验平台。通过这个网站可以看到所有开源的语义向量模型： Embbedding模型选型说了那么多的模型，怎么选择一个好的Embbedding模型，它是由很多个维度可以选择的，首先要考虑几个公共的维度，然后还需要考虑场景，因为不同的Embbedding模型训练的语料不一样，业务数据与Embbedding模型训练的语料匹配度越高，效果越佳。 Retrieval Average： NDCG是衡量检索系统性能的常用指标。 NDCG 较高表示模型能够更好地在检索结果列表中将相关项目排名靠前。 模型大小：模型的大小（以 GB 为单位）。它给出了运行模型所需的计算资源的概念。虽然检索性能随模型大小而变化，但值得注意的是，模型大小也会对延迟产生直接影响。在生产设置中，延迟与性能的权衡变得尤为重要。 最大令牌数：可以压缩为单个嵌入的令牌数。您通常不想放置超过一个文本段落（~100 个代币) 到单个嵌入中。因此，即使模型的最大令牌数为 512，也应该绰绰有余。 Embbedding维度：嵌入向量的长度。较小的嵌入可提供更快的推理，并且存储效率更高，而更多的维度可以捕获数据中的细微细节和关系。最终，我们希望在捕获数据的复杂性和运营效率之间取得良好的权衡。 支持的语言 (Languages): 中文 (zh)，英文 (en) 等 嵌入是如何生成的方式一：模型托管方式生成 嵌入：比如一些MaaS(模型即服务)服务厂商会提供嵌入模型的API，比如OpenAI的text-embedding-3-large 方式二：自己部署模型生成 嵌入：另外一种是使用开源的嵌入模型，然后通过使用GPU服务器运行起来，自己封装接口。 referencesLangChain+RAG—构建知识库（一）LangChain+RAG—构建知识库（二）]]></content>
      <categories>
        <category>LLM</category>
        <category>RAG</category>
      </categories>
      <tags>
        <tag>LLM</tag>
        <tag>RAG</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大模型-检索增强生成(RAG)]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-25.LLM-%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86%2F5022.%E5%A4%A7%E6%A8%A1%E5%9E%8B-%E6%A8%A1%E5%9E%8B%E4%BC%98%E5%8C%96-RAG-01.%E6%9C%80%E6%96%B0%E7%A0%94%E7%A9%B6%E8%BF%9B%E5%B1%95%2F</url>
    <content type="text"><![CDATA[Paper: 2025.7.25 A Systematic Review of Key Retrieval-Augmented Generation (RAG) Systems: Progress, Gaps, and Future Directions 2024.12.31 Retrieval-Augmented Generation with Graphs (GraphRAG)]]></content>
      <categories>
        <category>LLM</category>
      </categories>
      <tags>
        <tag>LLM</tag>
        <tag>RAG</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[4000.大模型-数学-基础知识]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-25.LLM-%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86%2F4000.%E5%A4%A7%E6%A8%A1%E5%9E%8B-%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80%2F</url>
    <content type="text"></content>
      <categories>
        <category>LLM</category>
      </categories>
      <tags>
        <tag>LLM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大模型-编程基础 pytorch进行张量运算]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-25.LLM-%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86%2F4011.%E5%A4%A7%E6%A8%A1%E5%9E%8B-%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%80-pytorch-%E5%BC%A0%E9%87%8F%E8%BF%90%E7%AE%97%2F</url>
    <content type="text"><![CDATA[PyTorch是一个开源的深度学习框架，由Facebook的AI研究团队于2016年发布。它提供了丰富的工具和库，用于构建、训练和部署深度神经网络模型。PyTorch采用动态计算图的设计，使得构建和调试模型变得直观而灵活。它的自动求导功能也使得实现反向传播算法变得简单，是训练深度学习模型的关键组件。接下来，让我们一起了解一下Pytorch的基础语法，为后续Pytorch实战打好基础，本节主要分为如下两个主要内容： 创建张量：学会如何创建空张量、随机张量、全零张量以及使用已有数据创建张量。 张量的操作：学会如何对张量进行操作，包括张量的加减乘除运算、shape变换、转置、索引与切片以及数组、列表与张量之间的互转。 自动求导原理与实现：自动求导是一个关键的功能，它允许我们自动计算梯度，从而进行反向传播和优化。 反向传播：反向传播是深度学习中的关键概念之一，它是训练神经网络模型的基础。 一、张量的概念和表示在PyTorch中，张量（tensor）是最基本的数据结构，它类似于多维数组。张量在深度学习中扮演着核心的角色，用于表示和处理数据以及进行数值计算。 1. 张量的维度和形状张量可以具有不同的维度和形状。在PyTorch中，我们可以使用torch.Tensor类来创建张量。以下是一些常见的张量及其对应的维度和形状的示例： 标量（0维张量）：一个单独的数值。例如，tensor = torch.tensor(5)表示一个标量，其维度为0，形状为空。 向量（1维张量）：具有一个轴的张量。例如，tensor = torch.tensor([1, 2, 3, 4, 5])表示一个向量，其维度为1，形状为(5,)。 矩阵（2维张量）：具有两个轴的张量。例如，tensor = torch.tensor([[1, 2, 3], [4, 5, 6]])表示一个矩阵，其维度为2，形状为(2, 3)。 3维张量：具有三个轴的张量。例如，tensor = torch.tensor([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])表示一个3维张量，其维度为3，形状为(2, 2, 2)。 您可以使用torch.Tensor类的构造函数以及其他一些创建张量的函数来创建具有不同维度和形状的张量。下面使用代码进行介绍123456789# 导入PyTorch库import torch # 创建一个PyTorch张量（tensor），包含数字1到5tensor = torch.tensor([1, 2, 3, 4, 5])# 打印张量的形状（shape），即张量中元素的维度信息print(tensor.shape)# torch.Size([5]) 2. 张量的操作PyTorch提供了许多用于操作张量的函数和方法。以下是一些常用的张量操作： 创建张量：可以使用torch.tensor函数从Python列表或NumPy数组创建张量，也可以使用其他创建函数，如torch.zeros、torch.ones、torch.rand等创建具有特定形状和初始值的张量。 12345# 创建一个形状为(2, 3)的零张量zeros_tensor = torch.zeros((2, 3))# 创建一个形状为(3, 3)的随机张量rand_tensor = torch.rand((3, 3)) 张量操作：可以对张量执行各种数学运算和操作，如加法、减法、乘法、除法、矩阵乘法等。这些操作可以使用算术运算符或对应的函数来执行。 12345678910tensor1 = torch.tensor([1, 2, 3])tensor2 = torch.tensor([4, 5, 6])# 加法操作result = tensor1 + tensor2print(result) # 输出: tensor([5, 7, 9])# 乘法操作result = tensor1 * tensor2print(result) # 输出: tensor([4, 10, 18]) 张量索引和切片：可以使用索引和切片操作来访问张量中的特定元素或子张量。 12345678910import torch# 创建一个形状为(3, 3)的张量tensor = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])# 访问第一个元素print(tensor[0, 0]) # 输出: tensor(1)# 切片操作print(tensor[:, 1]) # 输出: tensor([2, 5, 8]) 3. 张量的GPU加速在PyTorch中，可以将张量存储在CPU或GPU上。GPU加速可以显著提高深度学习模型的计算性能。要将张量移动到GPU上，可以使用.to方法。当然我们也可以在创建张量时，通过 .cuda(), 或 device 参数直接指定将张量存储在GPU上 123456789import torch# 创建一个形状为(2, 2)的张量并将其移动到GPU上tensor = torch.tensor([[1, 2], [3, 4]])tensor = tensor.to('cuda')# 直接在 GPU上创建张量torch.rand(2, 3).cuda() torch.rand(2, 3, device="cuda") 如果您的系统具有可用的GPU，并且已正确配置PyTorch以使用GPU加速，那么上述代码将使张量存储在GPU上。否则，它将继续在CPU上运行。 下面以计算三个矩阵相乘的结果为例，我们分别通过 CPU 和 NVIDIA Tesla V100 GPU 来直观感受一下使用GPU的带来的速度提升。12345678import torchimport timeitM = torch.rand(1000, 1000)print(timeit.timeit(lambda: M.mm(M).mm(M), number=5000))N = torch.rand(1000, 1000).cuda()print(timeit.timeit(lambda: N.mm(N).mm(N), number=5000)) 4. 将张量转移到CPU123tensor = tensor.to('cpu')tensor 这是关于”张量的概念和表示”部分的一个简要介绍。在接下来的教程中，我们将深入探讨更多有关PyTorch的功能和操作。 二、创建张量在PyTorch中，张量（Tensor）是一种多维数组，类似于NumPy的ndarray对象。张量可以用来表示数据和进行各种数学运算。本节将介绍如何创建不同类型的张量。 1. 创建空张量要创建一个空张量，可以使用torch.empty()函数。空张量的元素值将不会被初始化，它们的内容是未知的。12# 创建一个未初始化的 5x3 张量x = torch.empty(5, 3) 2. 创建随机张量要创建一个随机初始化的张量，可以使用torch.rand()函数。这将生成一个在[0, 1)范围内均匀分布的随机张量。12# 创建一个形状为 2x2 的随机张量x = torch.rand(2, 2) 3. 创建全零张量要创建一个全零的张量，可以使用torch.zeros()函数。12# 创建一个形状为 3x3 的全零张量x = torch.zeros(3, 3) 4. 创建全1张量12# 创建一个形状为 3x3 的全零张量x = torch.ones(3, 3) 5. 从数据创建张量你还可以从现有的数据创建张量。可以使用torch.tensor()函数从Python列表或NumPy数组创建张量。12345678910# 从Python列表创建张量data_list = [1, 2, 3, 4, 5]x = torch.tensor(data_list)print(x)# 从NumPy数组创建张量data_array = np.array([6, 7, 8, 9, 10])x = torch.from_numpy(data_array) # from_numpy 方法x = torch.tensor(data_array) # 张量转换print(x) 6. 创建具有特定数据类型的张量在PyTorch中，张量可以具有不同的数据类型，如浮点型、整型等。可以使用dtype参数指定张量的数据类型。123456# 创建一个具有特定数据类型的张量x = torch.tensor([1, 2, 3], dtype=torch.float32)print(x.dtype)# 创建一个具有特定数据类型的全零张量x = torch.zeros(3, 3, dtype=torch.int32) 三、张量操作和运算1. 张量运算PyTorch提供了许多张量运算操作，例如加法、减法、乘法和除法。这些运算可以应用于张量之间，也可以应用于张量与标量之间。下面是一些常用的张量运算示例：123456789101112131415161718192021222324252627282930# 张量加法a = torch.tensor([1, 2, 3])b = torch.tensor([4, 5, 6])c = a + b# 张量减法c = a - b# 张量乘法c = a * b# 张量除法c = a / b# 张量与标量相加c = 2d = a + c# 张量与标量相乘d = a * c# 计算点积a.dot(b)tensor(32., dtype=torch.float64)# &gt;&gt;&gt; a.sin()tensor([0.8415, 0.9093, 0.1411], dtype=torch.float64)&gt;&gt;&gt; a.exp()tensor([ 2.7183, 7.3891, 20.0855], dtype=torch.float64) 指定维度的张量运算，对张量进行聚合（如求平均、求和、最大值和最小值等）或拼接操作时，可以指定进行操作的维度 (dim)。12345678910a = torch.tensor([[1, 2, 3], [4, 5, 6]], dtype=torch.double)# 计算张量的平均值，在默认情况下会计算所有元素的平均值。：a.mean()# 设定计算的维度，可以实现分别对第 0 维和第 1 维计算平均值：x.mean(dim=0) # tensor([2.5000, 3.5000, 4.5000], dtype=torch.float64)x.mean(dim=1) # tensor([2., 5.], dtype=torch.float64)# 注意，上面的计算自动去除了多余的维度，因此结果从矩阵变成了向量，如果要保持维度不变，可以设置 keepdim=True：x.mean(dim=0, keepdim=True) # tensor([[2.5000, 3.5000, 4.5000]], dtype=torch.float64) 拼接 cat拼接 torch.cat 操作类似，通过指定拼接维度，可以获得不同的拼接结果：12345678910&gt;&gt;&gt; x = torch.tensor([[1, 2, 3], [ 4, 5, 6]], dtype=torch.double)&gt;&gt;&gt; y = torch.tensor([[7, 8, 9], [10, 11, 12]], dtype=torch.double)&gt;&gt;&gt; torch.cat((x, y), dim=0)tensor([[ 1., 2., 3.], [ 4., 5., 6.], [ 7., 8., 9.], [10., 11., 12.]], dtype=torch.float64)&gt;&gt;&gt; torch.cat((x, y), dim=1)tensor([[ 1., 2., 3., 7., 8., 9.], [ 4., 5., 6., 10., 11., 12.]], dtype=torch.float64) 2. 张量形状变换在实际的深度学习任务中，经常需要对张量进行形状变换。PyTorch提供了一系列函数来改变张量的形状，例如reshape()、view()和transpose()等。下面是一些常用的张量形状变换的示例： 形状转换 view， 将张量转换为新的形状，需要保证总的元素个数不变。进行 view 操作的张量必须是连续的 (contiguous)，可以调用 is_conuous 来判断张量是否连续；如果非连续，需要先通过 contiguous 函数将其变为连续的。也可以直接调用 Pytorch 新提供的 reshape 函数，它与 view 功能几乎一致，并且能够自动处理非连续张量。 1234567891011121314151617&gt;&gt;&gt; x = torch.tensor([1, 2, 3, 4, 5, 6])&gt;&gt;&gt; print(x, x.shape)tensor([1, 2, 3, 4, 5, 6]) torch.Size([6])&gt;&gt;&gt; x.view(2, 3) # shape adjusted to (2, 3)tensor([[1, 2, 3], [4, 5, 6]])&gt;&gt;&gt; x.view(3, 2) # shape adjusted to (3, 2)tensor([[1, 2], [3, 4], [5, 6]])&gt;&gt;&gt; x.view(-1, 3) # -1 means automatic inferencetensor([[1, 2, 3], [4, 5, 6]])&gt;&gt;&gt; x.reshape(3, 2) # 改变为3x2的张量tensor([[1, 2, 3], [4, 5, 6]]) 转置 transpose， 交换张量中的两个维度，参数为相应的维度： 12345678&gt;&gt;&gt; x = torch.tensor([[1, 2, 3], [4, 5, 6]])&gt;&gt;&gt; xtensor([[1, 2, 3], [4, 5, 6]])&gt;&gt;&gt; x.transpose(0, 1)tensor([[1, 4], [2, 5], [3, 6]]) 交换维度 permute 与 transpose 函数每次只能交换两个维度不同，permute 可以直接设置新的维度排列方式： 1234567891011&gt;&gt;&gt; x = torch.tensor([[[1, 2, 3], [4, 5, 6]]])&gt;&gt;&gt; print(x, x.shape)tensor([[[1, 2, 3], [4, 5, 6]]]) torch.Size([1, 2, 3])&gt;&gt;&gt; x = x.permute(2, 0, 1)&gt;&gt;&gt; print(x, x.shape)tensor([[[1, 4]], [[2, 5]], [[3, 6]]]) torch.Size([3, 1, 2]) 3. 降维与升维有时为了计算需要对一个张量进行降维或升维。例如神经网络通常只接受一个批次 (batch) 的样例作为输入，如果只有 1 个输入样例，就需要手工添加一个 batch 维度。具体地： 升维 torch.unsqueeze(input, dim, out=None) 在输入张量的 dim 位置插入一维，与索引一样，dim 值也可以为负数； 降维 torch.squeeze(input, dim=None, out=None) 在不指定 dim 时，张量中所有形状为 1 的维度都会被删除，例如$[A,1,B,1,C]$ 会变成 $[A,B,C]$； 当给定 dim 时，只会删除给定的维度（形状必须为 1），例如 $[A,1,B]$ squeeze(input, dim=0) 会保持张量不变，只有 squeeze(input, dim=1) 形状才会变成 $[A,B]$. 。 下面是一些示例：123456789101112&gt;&gt;&gt; a = torch.tensor([1, 2, 3, 4])&gt;&gt;&gt; a.shapetorch.Size([4])&gt;&gt;&gt; b = torch.unsqueeze(a, dim=0)&gt;&gt;&gt; print(b, b.shape)tensor([[1, 2, 3, 4]]) torch.Size([1, 4])&gt;&gt;&gt; b = a.unsqueeze(dim=0) # another way to unsqueeze tensor&gt;&gt;&gt; print(b, b.shape)tensor([[1, 2, 3, 4]]) torch.Size([1, 4])&gt;&gt;&gt; c = b.squeeze()&gt;&gt;&gt; print(c, c.shape)tensor([1, 2, 3, 4]) torch.Size([4]) 3. 张量索引和切片对于张量中的元素，可以使用索引和切片来访问和操作它们。下面是一些常用的张量索引和切片的示例：1234# 张量索引a = torch.tensor([[1, 2, 3], [4, 5, 6]])element = a[0, 1] # 获取第一行第二列的元素slice = a[:, 1:3] # 获取所有行的第二列到第三列的切片 4. 获取张量中的数值 如果 tensor 仅有一个元素，可以采用 .item() 来获取数值： 1234a = torch.randn(1)print(a)print(a.item()) 如果张量中含有多个元素 1234a = torch.tensor([[1, 2, 3], [4, 5, 6]])print(a[1:2,1])print(a[1:2,1].item()) 5. 张量转化为数组123456789# 创建一个PyTorch张量tensor = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])# 将张量转换为NumPy数组array = tensor.numpy()# 现在，`array`就是一个NumPy数组，你可以使用NumPy的功能来操作它 6. 张量转化为列表123456# 创建一个PyTorch张量tensor = torch.tensor([[1, 2, 3], [4, 5, 6]])# 将张量转换为Python列表tensor_list = tensor.tolist() 请注意，.tolist()方法只能用于包含标量元素的张量，例如整数或浮点数。如果张量包含其他张量或复杂数据类型，则不能直接使用.tolist()。在这种情况下，你需要先将内部的张量或复杂数据类型转换为Python列表。 四、自动求导原理1. 梯度在深度学习中，梯度是一个非常重要的概念，它表示了函数在某一点的变化率。在PyTorch中，计算梯度是一项关键操作，它允许我们通过反向传播算法有效地更新模型参数。本节将详细介绍如何在PyTorch中计算梯度，并提供一些示例来帮助你更好地理解这个过程。 梯度是一个向量，其方向指向函数值增长最快的方向，而其大小表示函数值的变化率。在深度学习中，我们通常希望最小化损失函数，因此我们要沿着梯度的反方向更新模型参数，以逐步降低损失值。 PyTorch中的torch.Tensor类是PyTorch的核心数据结构，同时也是计算梯度的关键。每个张量都有一个属性requires_grad，默认为False。如果我们希望计算某个张量的梯度，需要将requires_grad设置为True，那么就会开始追踪在该变量上的所有操作，而完成计算后，可以调用 .backward() 并自动计算所有的梯度，得到的梯度都保存在属性 .grad 中。 调用 .detach() 方法分离出计算的历史，可以停止一个 tensor 变量继续追踪其历史信息 ，同时也防止未来的计算会被追踪。而如果是希望防止跟踪历史（以及使用内存），可以将代码块放在 with torch.no_grad(): 内，这个做法在对模型进行评估的时候非常有用（节约算力、不会发生模型参数变化）。 对于 autograd 的实现，还有一个类也是非常重要–Function 。Tensor 和 Function 两个类是有关联并建立了一个非循环的图，可以编码一个完整的计算记录。每个 tensor 变量都带有属性 .grad_fn ，该属性引用了创建了这个变量的 Function （除了由用户创建的 Tensors，它们的 grad_fn二小节介绍梯度的内容。 在PyTorch中，自动求导是一个关键的功能，它允许我们自动计算梯度，从而进行反向传播和优化。 2. 梯度计算过程在PyTorch中，计算梯度的过程主要分为以下几个步骤： 12345678910# 1. 创建张量并设置`requires_grad=True`：# 首先，我们需要创建一个张量，并将`requires_grad`属性设置为`True`，以便PyTorch跟踪其梯度。x = torch.tensor([2.0, 3.0], requires_grad=True)y = x**2 + 3*x + 1# 2. 计算梯度：使用`backward()`方法自动计算梯度。y.sum().backward()# 3. 获取梯度值：通过访问张量的`grad`属性，我们可以获得计算得到的梯度值。print(x.grad) 在上面的示例中，我们创建了一个包含两个元素的张量x，并将requires_grad设置为True。使用backward()方法计算梯度，并通过访问x.grad获取了梯度值。 问题一：为什么要先执行sum再求导？ 上面的代码中，先执行y.sum()再求导的目的是为了计算一个标量值（scalar）的梯度。PyTorch的自动微分机制是基于标量值的，因此我们需要确保我们要计算的梯度是一个标量。具体解释如下： y是一个张量，它可能包含多个元素，例如[y1, y2, ...]，每个元素都与x相关。如果我们直接调用y.backward()，PyTorch会尝试计算y中每个元素对应的梯度，这将得到一个与x具有相同形状的梯度张量。这在某些情况下可能是有用的，但通常我们更关心的是一个标量目标函数的梯度。 通过执行y.sum()，我们将y中的所有元素相加，得到一个标量值（单个数字）。然后，我们对这个标量值执行反向传播，计算相对于x的梯度。这确保了我们计算的是整个目标函数相对于x的梯度，而不是每个元素的梯度。 如果去掉上面代码中的.sum()，将会提示你 RuntimeError: grad can be implicitly created only for scalar outputs 3. 梯度计算示例下面是一些更复杂的示例，以帮助你更好地理解计算梯度的过程。 示例 1：线性回归 考虑一个简单的线性回归模型，我们的目标是找到一条直线，以最小化预测值与真实值之间的平方误差。我们可以使用梯度下降算法来更新直线的参数。12345678910111213141516171819202122232425262728293031323334353637import torch# 创建训练数据x_train = torch.tensor([[1.0], [2.0], [3.0]])y_train = torch.tensor([[2.0], [4.0], [6.0]])# 定义模型参数w = torch.tensor([[0.0]], requires_grad=True)b = torch.tensor([[0.0]], requires_grad=True)# 定义模型def linear_regression(x): return torch.matmul(x, w) + b# 定义损失函数def loss_fn(y_pred, y): return torch.mean((y_pred - y)**2)# 定义优化器optimizer = torch.optim.SGD([w, b], lr=0.01)# 训练模型for epoch in range(100): # 前向传播 y_pred = linear_regression(x_train) # 计算损失 loss = loss_fn(y_pred, y_train) # 反向传播 loss.backward() # 更新参数 optimizer.step() # 清零梯度 optimizer.zero_grad() 五、反向传播1. 反向传播原理反向传播（Backpropagation，缩写为BP）是“误差反向传播”的简称，该方法对网络中所有权重计算损失函数的梯度。 这个梯度会反馈给梯度下降法，用来更新权重值以最小化损失函数，从而训练神经网络模型。 而反向传播是计算损失函数对模型参数梯度的一种有效方法。通过计算参数梯度，我们可以根据梯度的反方向更新参数，使得模型的预测结果逐渐接近真实标签。反向传播的过程可以概括为以下几个步骤： 前向传播：将输入样本通过神经网络的前向计算过程，计算出预测结果。 计算损失：将预测结果与真实标签进行比较，并计算损失函数的值。 反向传播梯度：根据损失函数的值，计算损失函数对模型参数的梯度。 参数更新：根据参数的梯度和优化算法的规则，更新模型的参数。 在PyTorch中，反向传播过程是自动完成的。通过调用backward()函数，PyTorch会自动计算损失函数对所有可学习参数的梯度，并将其保存在相应的参数张量的.grad属性中。接下来，我们将通过具体示例来演示这一过程。 2. 反向传播示例假设我们有一个简单的线性回归模型，其中只有一个输入特征和一个输出标签。我们的目标是通过训练模型来找到最佳的线性关系。 首先，我们需要导入所需的库和模块、定义模型类1234567891011import torchimport torch.nn as nnimport torch.optim as optimclass LinearRegression(nn.Module): def __init__(self): super(LinearRegression, self).__init__() self.linear = nn.Linear(1, 1) # 输入维度为1，输出维度为1 def forward(self, x): return self.linear(x) 然后，我们创建模型的实例、定义损失函数和优化器： 12345model = LinearRegression()criterion = nn.MSELoss() # 均方误差损失函数# 随机梯度下降优化器optimizer = optim.SGD(model.parameters(), lr=0.01) 现在，我们生成一些样本数据，并进行训练： 12345678910111213141516171819# 生成样本数据x_train = torch.tensor([[1.0], [2.0], [3.0], [4.0]])y_train = torch.tensor([[2.0], [4.0], [6.0], [8.0]])# 训练模型for epoch in range(100): optimizer.zero_grad() # 梯度清零 # 前向传播 y_pred = model(x_train) # 计算损失 loss = criterion(y_pred, y_train) # 反向传播 loss.backward() # 参数更新 optimizer.step() 在上述代码中，我们使用了一个简单的数据集，包含了输入特征x_train和对应的真实标签y_train。在每个训练迭代中，我们首先将梯度清零，然后进行前向传播计算预测值y_pred，接着计算损失loss，然后通过调用.backward()函数执行反向传播，最后使用优化器的.step()方法来更新模型的参数。 通过上述示例，我们可以看到在PyTorch中实现反向传播是非常简单的。PyTorch会自动计算参数的梯度，并通过优化器来更新参数，从而实现模型的训练过程。 参考资料pytorch-基础]]></content>
      <categories>
        <category>LLM</category>
      </categories>
      <tags>
        <tag>LLM</tag>
        <tag>pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大模型-编程基础 常见依赖库几功能]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-25.LLM-%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86%2F4010.%E5%A4%A7%E6%A8%A1%E5%9E%8B-%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%80%2F</url>
    <content type="text"><![CDATA[知名仓库平滑量化量化技术Quantization原理学习 常用依赖包1234567891011pip install torchpip torchdata # 帮助 torch 加载数据库pip install transformers # 用于微调的依赖包pip install loralibpip install peft# 用于模型打分的依赖包pip install evaluates # 计算ROUGE 分值pip install rouge_score # 计算ROUGE 分值 torchPyTorch是一个开源的深度学习框架，由Facebook的AI研究团队于2016年发布。它提供了丰富的工具和库，用于构建、训练和部署深度神经网络模型。PyTorch采用动态计算图的设计，使得构建和调试模型变得直观而灵活。它的自动求导功能也使得实现反向传播算法变得简单，是训练深度学习模型的关键组件。后续会有相关章节，具体介绍pyTorch的相关内容。 使用pytorch 进行张量运算 使用pytorch 进行神经网络的构建 数据库相关12 数据处理相关1import 模型微调小关12pip install loralibpip install peft 性能评估相关12]]></content>
      <categories>
        <category>LLM</category>
      </categories>
      <tags>
        <tag>LLM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大模型-编程基础 pytorch 构建神经网络]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-25.LLM-%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86%2F4012.%E5%A4%A7%E6%A8%A1%E5%9E%8B-%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%80-pytorch-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%2F</url>
    <content type="text"><![CDATA[一、神经网络的组成部分在第一部分中我们了解了 Pytorch 的相关基础知识，在这一篇文章中我将使用 Pytorch 进入深度学习的学习，学习如果使用 Pytorch 搭建神经网络中的一些基础代码，具体讲包含如下内容： 神经网络的组成部分 神经元 神经网络层 如何使用 Pytorch 完成数据加载工作以及相应的数据预处理 训练神经网络模型并验证 神经网络是一种由多个神经元以一定的方式联结形成的网络结构，是一种仿照生物神经系统结构和功能的人工智能技术。神经网络通常由输入层、输出层和若干个隐藏层组成，每个层包含若干个神经元。 神经网络的基本组成单位是神经元，它模拟了生物神经元的行为特征，包括输入信号的接收、加权求和、非线性激活等过程。神经元接收来自前一层神经元的输入信号，将输入信号进行加权求和，并通过激活函数将结果转换为输出信号，并将输出信号传递给下一层神经元。如图1所示为一个经典的以全连接（Full Connected, FC）方式形成的神经网络，每个圆圈代表一个神经元，圆圈间的连线代表神经元之间的联结。 1. 神经元 神经生物学家Warren MeCulloch和数学家Walter Pitts于1943年提出了一种基于早期的神经元理论学说的人工神经网络模型，称为MP模型（McCulloch-Pitts模型）。该模型是一种具有生物神经元特征的人工神经网络模型，被认为是神经网络研究的开端。 MP模型的基本思想是将神经元视为一个二进制变量，其输出值只能为0或1。神经元接收来自其他神经元的输入信号，通过一个阈值函数对输入信号进行加权和处理，并产生一个二进制输出值。MP模型中的神经元只有两种状态，即兴奋态（输出值为1）和抑制态（输出值为0），通过神经元之间的连接，可以实现复杂的计算功能。MP模型的主要贡献在于将生物神经元的工作原理转化为数学模型，为后续的神经网络研究奠定了基础。虽然MP模型非常简单，但它的基本思想和理论对于神经网络的发展和应用具有重要意义，为人工智能和机器学习的发展奠定了基础。如图2所示为MP神经元模型。 如上图所示，$u_1, ……u_j, ……u_n$是一个$n$维向量，代表与第$i$个神经元相连接的其他神经元传递的信号；$w_{1i}, ….., w_{ji}, ……w_{ni}$分别代表其他神经元和第$i$个神经元之间连接的权重值；代表第$i$个神经元的阈值；$x_i$则称为第$i$个神经元的输入，可表示为如式1所示；$f(x_i)$是非线性函数，如式2所示。 $x_{i}=\sum_{j=1}^{n} w_{j i} u_{i}-\theta_{i}\tag1$$y_{i}=f\left(x_{i}\right)=f\left(\sum_{j=1}^{n} w_{j i} u_{j}-\theta_{i}\right)\tag2$ 神经元通常由以下几个部分组成： 输入（Inputs）：神经元接收来自其他神经元或外部环境的输入数据。 权重（Weights）：每个输入都与一个权重相关联，用于调整输入的重要性。 激活函数：激活函数将加权输入映射到输出。常用的激活函数包括Sigmoid、ReLU和Tanh等。 偏差（Bias）：偏差是一个可学习的参数，用于调整神经元输出的阈值。 1234567891011121314151617181920212223242526import torchclass Neuron(torch.nn.Module): def __init__(self, input_size): super(Neuron, self).__init__() # 定义可学习的权重参数，形状为 (input_size,)，与输入特征数量相对应 self.weights = torch.nn.Parameter(torch.randn(input_size)) # 定义可学习的偏置参数，初始化为随机值，标量 self.bias = torch.nn.Parameter(torch.randn(1)) def forward(self, inputs): # 计算加权和，点乘输入和权重，然后加上偏置 weighted_sum = torch.sum(inputs * self.weights) + self.bias # 应用 sigmoid 激活函数，将结果压缩到 [0, 1] 范围内 output = torch.sigmoid(weighted_sum) return output# 创建一个具有3个输入的神经元neuron = Neuron(3)# 输入数据inputs = torch.tensor([0.5, -0.3, 0.1])# 计算输出output = neuron(inputs)print(output) 这是一个简单的神经元模型，用 PyTorch 构建。让我解释一下这个模型的结构和功能： Neuron 类：是一个继承自 torch.nn.Module 的自定义神经元模型。继承自 torch.nn.Module 的基类允许你定义具有可学习参数的自定义神经网络模型。 init 方法：这是模型的构造函数，它接受一个参数 input_size，表示输入特征的数量。在这个方法中，模型初始化了两个可学习的参数：weights 和 bias，这两个参数都被包装成 torch.nn.Parameter 对象，以便在模型的训练过程中进行优化。 weights 是一个形状为 (input_size,) 的可学习权重向量，它与输入特征进行点乘。 bias 是一个标量值，它用于调整模型的输出。 forward 方法：这是模型的前向传播方法。在前向传播过程中，输入 inputs 与权重 weights 进行点乘，然后将点乘结果与 bias 相加，得到加权和 weighted_sum。然后，通过 sigmoid 激活函数对加权和进行激活，将结果作为模型的输出返回。 这个神经元模型可以用于二分类问题，其中 input_size 表示输入特征的数量，模型通过学习适当的权重和偏置来进行二元分类。在训练过程中，你可以使用标准的 PyTorch 优化器和损失函数来训练这个模型，以便它能够适应你的分类任务。 2. 神经网络层 神经网络由多个神经元层组成。每一层都由许多神经元组成，并且通常具有相同的结构和激活函数。以下是一些常见的神经网络层类型： 全连接层（Fully Connected Layer）：每个神经元都与前一层的所有神经元相连接。 卷积层（Convolutional Layer）：应用卷积操作来提取输入数据中的空间特征。 池化层（Pooling Layer）：通过减少特征图的大小来降低计算量，并保留重要的特征。 循环层（Recurrent Layer）：通过在神经网络中引入时间维度来处理序列数据。 以下是一个包含两个全连接层的神经网络示例代码：123456789101112131415161718192021222324252627import torch# 定义神经网络类，继承自 torch.nn.Moduleclass NeuralNetwork(torch.nn.Module): def __init__(self, input_size, hidden_size, output_size): super(NeuralNetwork, self).__init__() # 定义第一个全连接层，输入大小为 input_size，输出大小为 hidden_size self.fc1 = torch.nn.Linear(input_size, hidden_size) # 定义第二个全连接层，输入大小为 hidden_size，输出大小为 output_size self.fc2 = torch.nn.Linear(hidden_size, output_size) def forward(self, inputs): # 使用 ReLU 激活函数计算第一个全连接层的输出 hidden = torch.relu(self.fc1(inputs)) # 使用 sigmoid 激活函数计算第二个全连接层的输出，最终的模型输出 output = torch.sigmoid(self.fc2(hidden)) return output# 创建一个具有2个输入、3个隐藏神经元和1个输出的神经网络net = NeuralNetwork(2, 3, 1)# 输入数据inputs = torch.tensor([0.5, -0.3])# 计算输出output = net(inputs)print(output) 3. 损失函数 神经网络的目标是最小化预测输出与真实标签之间的差异。损失函数衡量了这种差异，并提供一个可优化的目标。常见的损失函数包括均方误差（Mean Squared Error）、交叉熵损失（Cross-Entropy Loss）等。 均方误差（Mean Squared Error，MSE）：计算预测值与目标值之间的平方差的平均值。 交叉熵损失（Cross-Entropy Loss）：在分类问题中，计算预测概率分布与真实标签之间的交叉熵。 以下是一个使用均方误差作为损失函数的示例：12345678910import torch# 随机生成一些示例数据predictions = torch.tensor([0.9, 0.2, 0.1])labels = torch.tensor([1.0, 0.0, 0.0])# 计算均方误差损失loss_function = torch.nn.MSELoss()loss = loss_function(predictions, labels)print(loss) 4. 优化器 优化器用于更新神经网络的参数以最小化损失函数。它使用梯度下降算法来调整参数的值。常见的优化器包括随机梯度下降（Stochastic Gradient Descent，SGD）、Adam等。 以下是一个使用Adam优化器进行参数更新的示例：1234567891011121314151617181920212223import torch# 创建一个神经网络和损失函数net = NeuralNetwork(2, 3, 1)loss_function = torch.nn.MSELoss()# 创建一个Adam优化器optimizer = torch.optim.Adam(net.parameters(), lr=0.01)# 输入数据和真实标签inputs = torch.tensor([0.5, -0.3])labels = torch.tensor([1.0])# 前向传播output = net(inputs)loss = loss_function(output, labels)# 反向传播和参数更新optimizer.zero_grad()loss.backward()optimizer.step()print(loss) 通过理解和应用这些神经网络的组成部分，能够构建和训练自己的深度学习模型。 二、PyTorch中的层在PyTorch中，神经网络层（Layers）是神经网络的基本组成部分，用于对输入数据进行转换和提取特征。PyTorch提供了丰富的层类型和功能，使得构建和训练深度学习模型变得更加便捷和灵活。这里将介绍PyTorch中的一些常用层，并提供示例代码来帮助读者理解和学习。 目录 全连接层（Fully Connected Layer） 卷积层（Convolutional Layer） 池化层（Pooling Layer） 循环神经网络层（Recurrent Neural Network Layer） 转置卷积层（Transpose Convolutional Layer） 归一化层（Normalization Layer） 激活函数层（Activation Function Layer） 损失函数层（Loss Function Layer） 优化器层（Optimizer Layer） 1. 全连接层 全连接层，也被称为线性层或密集层，是最简单的神经网络层之一。它将输入的每个元素与权重相乘，并加上偏置项，然后通过激活函数进行非线性变换。全连接层的输出形状由其输入形状和输出维度确定。 下面是一个创建全连接层的示例代码：12345678910111213import torchimport torch.nn as nn# 定义输入和输出维度input_size = 784output_size = 10# 创建全连接层fc_layer = nn.Linear(input_size, output_size)# 打印全连接层的权重和偏置项print("权重：", fc_layer.weight)print("偏置项：", fc_layer.bias) 2. 卷积层 卷积层是卷积神经网络中的核心层之一，用于从输入数据中提取空间特征。卷积层通过滑动窗口（卷积核）在输入上进行局部感知，并输出对应的特征图。PyTorch中的卷积层包括二维卷积层和三维卷积层，分别用于处理二维和三维数据。 下面是一个创建二维卷积层的示例代码：1234567891011121314import torchimport torch.nn as nn# 定义输入通道数、输出通道数和卷积核大小in_channels = 3out_channels = 16kernel_size = 3# 创建二维卷积层conv_layer = nn.Conv2d(in_channels, out_channels, kernel_size)# 打印二维卷积层的权重和偏置项print("权重：", conv_layer.weight)print("偏置项：", conv_layer.bias) 3. 池化层 池化层用于减小特征图的空间维度，降低模型的参数数量，并增强模型的平移不变性。最大池化和平均池化是常用的池化方式，它们分别选择局部区域中的最大值和平均值作为输出。 下面是一个创建最大池化层的示例代码：12345678910111213import torchimport torch.nn as nn# 定义池化区域大小和步幅kernel_size = 2stride = 2# 创建最大池化层pool_layer = nn.MaxPool2d(kernel_size, stride)# 打印最大池化层的参数print("池化区域大小：", pool_layer.kernel_size)print("步幅：", pool_layer.stride) 4. 循环神经网络层 循环神经网络（Recurrent Neural Network, RNN）层用于处理序列数据，具有记忆性和上下文感知能力。RNN层通过在时间步之间共享权重，实现对序列的逐步处理，并输出相应的隐藏状态。 下面是一个创建RNN层的示例代码：123456789101112131415import torchimport torch.nn as nn# 定义输入特征维度、隐藏状态维度和层数input_size = 10hidden_size = 20num_layers = 2# 创建RNN层rnn_layer = nn.RNN(input_size, hidden_size, num_layers)# 打印RNN层的参数print("输入特征维度：", rnn_layer.input_size)print("隐藏状态维度：", rnn_layer.hidden_size)print("层数：", rnn_layer.num_layers) 5. 转置卷积层 转置卷积层，也被称为反卷积层，用于实现上采样操作，将低维特征图转换为高维特征图。转置卷积层通过反向卷积操作将输入特征图映射到更大的输出特征图。 下面是一个创建转置卷积层的示例代码：1234567891011121314import torchimport torch.nn as nn# 定义输入通道数、输出通道数和卷积核大小in_channels = 3out_channels = 16kernel_size = 3# 创建转置卷积层transconv_layer = nn.ConvTranspose2d(in_channels, out_channels, kernel_size)# 打印转置卷积层的权重和偏置项print("权重：", transconv_layer.weight)print("偏置项：", transconv_layer.bias) 6. 归一化层 归一化层用于调整神经网络的激活值分布，提升模型的收敛速度和泛化能力。常用的归一化层包括批归一化（Batch Normalization）和层归一化（Layer Normalization）。 下面是一个创建批归一化层的示例代码：12345678910111213import torchimport torch.nn as nn# 定义特征维度num_features = 16# 创建批归一化层bn_layer = nn.BatchNorm2d(num_features)# 打印批归一化层的参数print("特征维度：", bn_layer.num_features)print("均值：", bn_layer.running_mean)print("方差：", bn_layer.running_var) 7. 激活函数层 激活函数层用于引入非线性变换，增加神经网络的表达能力。常用的激活函数包括ReLU、Sigmoid和Tanh等。 下面是一个使用ReLU激活函数的示例代码：1234567891011121314import torchimport torch.nn as nn# 创建激活函数层（ReLU）activation_layer = nn.ReLU()# 定义输入张量input_tensor = torch.randn(10)# 对输入张量进行激活函数变换output_tensor = activation_layer(input_tensor)# 打印输出张量print("输出张量：", output_tensor) 三、数据加载与预处理在深度学习任务中，数据的加载和预处理是非常重要的步骤。PyTorch提供了强大的数据加载和预处理工具，使得我们能够高效地处理各种类型的数据。这里将介绍PyTorch中的数据加载和预处理方法，并提供使用示例。 1. 数据加载PyTorch中的数据加载主要通过torch.utils.data模块实现。该模块提供了Dataset和DataLoader两个核心类，分别用于定义数据集和数据加载器。 🚩DatasetDataset类是一个抽象类，用于表示数据集。我们可以继承该类并实现自定义的数据集。在自定义数据集中，我们需要实现两个方法：__len__和__getitem__。__len__方法返回数据集的样本数量，__getitem__方法根据索引返回单个样本。 以下是一个自定义数据集的示例：1234567891011121314151617import torchfrom torch.utils import datafrom torch.utils.data import Datasetclass MyDataset(data.Dataset): def __init__(self, data_list): # 初始化数据集 self.data_list = data_list def __len__(self): # 返回数据集大小 return len(self.data_list) def __getitem__(self, index): # 根据索引获取样本 sample = self.data_list[index] return sample 在上述示例中，MyDataset类接受一个数据列表作为输入，并实现了__len__和__getitem__方法。 🚩DataLoadertorch.utils.data.DataLoader是PyTorch中一个重要的类，用于高效加载数据集。它可以处理数据的批次化、打乱顺序、多线程数据加载等功能。以下是一个简单的示例：12345678910import torch.utils.data as datamy_dataset = MyDataset([1, 2, 3, 4, 5])my_dataloader = data.DataLoader(my_dataset, batch_size=4, shuffle=True)for batch in my_dataloader: print(batch) 在这个示例中，我们首先创建了一个MyDataset实例my_dataset，它包含了一个整数列表。然后，我们使用DataLoader类创建了一个数据加载器my_dataloader，它将my_dataset作为输入，并将数据分成大小为4的批次，并对数据进行随机化。最后，遍历my_dataloader，并打印出每个批次的数据。 总结一下，torch.utils.data.Dataset用于构建数据集，torch.utils.data.DataLoader用于加载数据集，并对数据进行批量处理和随机化。下面是一个完整的示例，展示了如何使用这两个类来加载和处理数据： 123456789101112131415161718192021222324import torch.utils.data as dataclass MyDataset(data.Dataset): def __init__(self, data_list): # 初始化数据集 self.data_list = data_list def __len__(self): # 返回数据集大小 return len(self.data_list) def __getitem__(self, index): # 根据索引获取样本 sample = self.data_list[index] return samplemy_dataset = MyDataset([1, 2, 3, 4, 5])my_dataloader = data.DataLoader(my_dataset, batch_size=4, shuffle=True)for batch in my_dataloader: print(batch) 除了上述介绍的基本用法，torch.utils.data模块还有许多其他的功能和选项。下面介绍一些常用的选项和功能。 2. 数据预处理数据预处理是在将数据输入模型之前对数据进行的一系列操作，以提高模型的性能和准确性。PyTorch提供了多种数据预处理方法，包括常见的数据变换、标准化、图像增强等。以下是一些常见的数据预处理方法： 🚩Tensor转换将数据转换为torch.Tensor类型是数据预处理的第一步。torch.Tensor是PyTorch中表示张量的主要数据类型。1234import torch data = [1, 2, 3, 4, 5] tensor = torch.tensor(data) 🚩数据变换数据变换是对数据进行形状调整或维度变换的操作。PyTorch提供了一系列的数据变换方法，如torchvision.transforms模块中的Resize、ToTensor等。 123456789from torchvision import transforms transform = transforms.Compose([ transforms.Resize((224, 224)), transforms.ToTensor() ]) # 对数据进行变换 transformed_data = transform(data) 🚩数据标准化数据标准化是对数据进行平均值和标准差的缩放，以使得数据具有零均值和单位方差。这通常用于提高模型的收敛性和稳定性。 1234567import torchvision.transforms as transforms normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) # 对图像进行标准化 normalized_image = normalize(image) 🚩图像增强图像增强是对图像进行变换或添加噪声，以增加训练数据的多样性和鲁棒性。PyTorch提供了torchvision.transforms模块中的多种图像增强方法，如随机裁剪、翻转、旋转等。 12345678910import torchvision.transforms as transforms transform = transforms.Compose([ transforms.RandomCrop(224), transforms.RandomHorizontalFlip(), transforms.RandomRotation(30) ]) # 对图像进行增强 transformed_image = transform(image) 本节介绍了PyTorch中的数据加载和预处理方法。通过自定义数据集和数据加载器，我们可以高效地加载和处理数据。同时，PyTorch提供了多种数据预处理方法，如数据变换、标准化和图像增强，以提高模型的性能和准确性。 四、模型训练与验证1. 模型训练PyTorch中的模型训练主要涉及以下几个步骤： 准备数据：首先，我们需要准备好训练数据和对应的标签。可以使用torch.utils.data模块中的Dataset和DataLoader类来加载和批量处理数据。 定义模型：接下来，我们需要定义模型的结构。可以使用torch.nn模块中的各种层和模型来构建自己的神经网络模型。 定义损失函数：为了训练模型，我们需要定义损失函数来度量模型预测结果与真实标签之间的差异。可以使用torch.nn模块中的各种损失函数，如均方误差（MSE）、交叉熵损失等。 定义优化器：为了更新模型的参数，我们需要选择一个优化器来优化模型的损失函数。可以使用torch.optim模块中的各种优化器，如随机梯度下降（SGD）、Adam等。 训练模型：在每个训练迭代中，我们需要执行以下步骤： 前向传播：将输入数据通过模型，得到模型的输出结果。 计算损失：将模型的输出结果与真实标签计算损失函数的值。 反向传播：根据损失函数的梯度，计算模型参数的梯度。 参数更新：使用优化器根据梯度信息更新模型的参数。 以下是一个简单的模型训练示例： 12345678910111213141516171819202122232425262728293031323334353637import torch import torch.nn as nn import torch.optim as optim from torch.utils.data import DataLoader # 准备数据 train_dataset = MyDataset(train_data) train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True) # 定义模型 model = MyModel() # 定义损失函数 loss_fn = nn.CrossEntropyLoss() # 定义优化器 optimizer = optim.SGD(model.parameters(), lr=0.01) # 模型训练 for epoch in range(num_epochs): for batch in train_dataloader: inputs, labels = batch # 前向传播 outputs = model(inputs) # 计算损失 loss = loss_fn(outputs, labels) # 反向传播 optimizer.zero_grad() loss.backward() # 参数更新 optimizer.step() 在上述示例中，我们使用自定义的数据集和数据加载器准备训练数据，定义了模型、损失函数和优化器，并在每个训练迭代中执行了前向传播、计算损失、反向传播和参数更新的步骤。 2. 模型验证在模型训练之后，我们需要对模型进行验证以评估其性能和准确性。模型验证的步骤与模型训练类似，但不需要进行参数更新。 以下是一个简单的模型验证示例： 123456789101112131415# 准备验证数据 val_dataset = MyDataset(val_data) val_dataloader = DataLoader(val_dataset, batch_size=64) # 模型验证 model.eval() # 设置模型为评估模式 with torch.no_grad(): # 禁止梯度计算 for batch in val_dataloader: inputs, labels = batch # 前向传播 outputs = model(inputs) # 在这里可以对模型输出进行后处理，如计算准确率、绘制预测结果等 在上述示例中，我们使用自定义的验证数据集和数据加载器准备验证数据，并使用model.eval()将模型设置为评估模式。然后，在验证数据上进行前向传播，并根据需要对模型输出进行后处理。 介绍了PyTorch中的模型训练和验证方法。通过准备数据、定义模型、损失函数和优化器，以及执行训练和验证循环，我们可以高效地训练和评估深度学习模型。]]></content>
      <categories>
        <category>LLM</category>
      </categories>
      <tags>
        <tag>LLM</tag>
        <tag>pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[5002.大模型-经典网络架构]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-25.LLM-%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86%2F5002.%E5%A4%A7%E6%A8%A1%E5%9E%8B-%E6%A6%82%E8%BF%B0-%E7%BB%8F%E5%85%B8%E7%BD%91%E7%BB%9C%E6%9E%B6%E6%9E%84%2F</url>
    <content type="text"><![CDATA[以GPT系列中的Transformer为例，这种深度学习模型结构通过自注意力机制等技巧解决了相关问题。正是得益于Transformer架构，基于GPT的大型语言模型取得了显著的进展。Transformer模型架构包含了众多模块，而我们讨论的各种微调技术通常是对这些模块中的特定部分进行优化，以实现微调目的。 要深入理解各类微调手段，首先需要对网络架构有一个基本的认识。以下以Transformer为例，阐述各个模块的作用： 输入嵌入层（Input Embedding） 输入（Inputs）：模型的输入环节，通常为单词或符号序列。 输入嵌入（Input Embedding）：此步骤将输入序列（例如句中的每个单词）转化为嵌入表示，即能够表征单词语义信息的高维向量。 位置编码（Positional Encoding）：鉴于Transformer不依赖序列，位置编码旨在提供序列中单词位置的信息，这些编码添加到输入嵌入中，确保模型即便同时处理输入也能够利用单词的顺序信息。 编码器层（Encoder，左边） Nx：指示有N个相同的编码器层叠加而成。每个编码器层包括两个主要子层：多头自注意力机制和前馈神经网络。 多头自注意力（Multi-Head Attention）：注意力机制允许模型在处理每个单词时考虑到输入序列中的所有单词。多头部分表示模型并行学习输入数据的不同表示。 残差连接和归一化（Add &amp; Norm）：注意力层后面跟着残差连接和层归一化，有助于防止深层网络中的梯度消失问题，并稳定训练过程。 前馈神经网络（Feed Forward）：全连接神经网络处理自注意力层的输出，包含两个线性变换和一个非线性激活函数。 解码器层（Decoder，右侧） 解码器亦包含多个相同的层，每层包括三个主要子层：掩蔽的多头自注意力机制、多头自注意力机制和前馈神经网络。 掩蔽多头自注意力（Masked Multi-Head Attention）：与编码器的多头自注意力机制类似，但为确保解码顺序性，掩蔽操作确保预测仅依赖于之前的输出。 前馈神经网络（Feed Forward）：与编码器相同，每个子层之后也有加法和归一化步骤。 输出嵌入层和输出过程 解码器端的嵌入层将目标序列转换为向量形式。 线性层（Linear）和Softmax层：解码器的输出通过线性层映射到一个更大的词汇空间，Softmax函数将输出转换为概率分布。]]></content>
      <categories>
        <category>LLM</category>
      </categories>
      <tags>
        <tag>LLM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pipeline-frameworks-cromwell-03.原生调度系统]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-04.pipeline%2Fpipeline-frameworks-cromwell-03.%E4%BB%BB%E5%8A%A1%E6%8A%95%E9%80%92%2F</url>
    <content type="text"><![CDATA[需要注意几个点： cromwel在进行任务投递时，会构建一个临时的工作目录，用于存储任务的中间文件，因此，我们需要保证工作目录的权限和空间足够。 cromwell存储每个任务的状态和任务间的逻辑关系，如果没有部署数据库，这部分信息会存储到内存中，因此如果没有部署数据库，需要确保内存足够支撑任务的运行，否则可能会出现内存爆掉的问题。 Run 模式投递wdl本身有多种投递模式，其中最简单的是直接再前台进行任务的投递（run模式）123456java -Xms512m -Xmx1g \-Dconfig.file=cromwell-87.cfg \-jar cromwell-87.jar \run \pipeline.wdl \-i pipeline.input 这种模式，是直接进行单次的任务投递，执行简单无需额外的配置，但是在任务执行过程中，会占用前台的资源，如果任务复杂（task多）或并行的数据多，会导致内存爆掉。 因此，再非测试阶段，最好使用Server Mode进行任务的投递，将任务投递到后台进行执行。 Server Mode 投递进行后台投递时候，我们首先需要启动一个后台服务12345nohup java -Xmx512m \-Dconfig.file=cromwell-87.server.cfg \-jar cromwell-87.jar \server \&gt; cromwell.log 2&gt;&amp;1 &amp; 启动服务后，我们可以通过submit命令进行任务的投递123456789101112outdir="Server_test"cromwell="cromwell-87.jar"cromwell_config="cromwell-87.server.cfg"wdl="pipeline.wdl"wdl_config="pipeline.input"/share/app/java/jdk-15/bin/java -Xmx512m \-Dconfig.file=$&#123;cromwell_config&#125; \-jar $&#123;cromwell&#125; \submit $&#123;wdl&#125; \-i $&#123;wdl_config&#125; \&gt; $&#123;outdir&#125;/cromwell.log \]]></content>
      <categories>
        <category>pipeline</category>
        <category>framework</category>
        <category>编程拾慧</category>
        <category>任务调度</category>
        <category>python</category>
      </categories>
      <tags>
        <tag>流程开发</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pipeline-frameworks-cromwell-03.调度系统-oliver]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-04.pipeline%2Fpipeline-frameworks-cromwell-03.%E6%8A%95%E9%80%92%E5%B7%A5%E5%85%B7-oliver%2F</url>
    <content type="text"><![CDATA[oliver12345oliver st -g MGISEQ-2000 -rd# -r Show jobs in the 'Running' state.# /jdfstj4/B2C_RD_R1/fuxiangke/wesLite/env/miniconda3/envs/cromwell/bin/oliver status -j MGISEQ-2000A2302412010004 -d status 参数`shell -d, –detail-view Show detailed view which displays information about each individual job. -z, –steps-view Show the ‘steps’ view which displays summary information about how many jobs are at each step. -a, –aborted Show jobs in the ‘Aborted’ state. -b BATCHES_RELATIVE [BATCHES_RELATIVE …], –batches-relative BATCHES_RELATIVE [BATCHES_RELATIVE …] Starting with the most recent batch, compute batches separated by batch-interval-mins. Any batches not contained in batches are filtered. -B BATCHES_ABSOLUTE [BATCHES_ABSOLUTE …], –batches-absolute BATCHES_ABSOLUTE [BATCHES_ABSOLUTE …] Starting with the first batch in time, compute batches separated by batch-interval-mins. Any batches not contained in batches are filtered. –failed-calls FAILED_CALLS [FAILED_CALLS …] Only show calls with the given name that failed (useful in restarting failed steps). -f, –failed Show jobs in the ‘Failed’ state. -g JOB_GROUP, –job-group JOB_GROUP Specify the Oliver job group. -i CROMWELL_WORKFLOW_UUID, –cromwell-workflow-uuid CROMWELL_WORKFLOW_UUID Filter by workflow id matching argument. -j JOB_NAME, –job-name JOB_NAME Specify the Oliver job name. -n CROMWELL_WORKFLOW_NAME, –cromwell-workflow-name CROMWELL_WORKFLOW_NAME Filter by workflow name matching argument. -r, –running Show jobs in the ‘Running’ state. -t SUBMISSION_TIME, –submission-time SUBMISSION_TIME Show only jobs which were submitted at most N hours ago. -s, –succeeded Show jobs in the ‘Succeeded’ state. –grid-style GRID_STYLE Any valid tablefmt for python-tabulate. `]]></content>
      <categories>
        <category>pipeline</category>
        <category>framework</category>
        <category>编程拾慧</category>
        <category>任务调度</category>
        <category>python</category>
      </categories>
      <tags>
        <tag>流程开发</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[5020.大模型-模型优化]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-25.LLM-%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86%2F5020.%E5%A4%A7%E6%A8%A1%E5%9E%8B-%E6%A8%A1%E5%9E%8B%E4%BC%98%E5%8C%96%2F</url>
    <content type="text"><![CDATA[自从大型语言模型（LLM）和先进的聊天模型发布以来，人们已经使用了各种技术来优化这些人工智能系统的输出，使其更适配本地化的数据和应用场景。其中 检索增强生成（RAG）和微调（Fine-tuning）是提升大语言模型性能的两种常用方法，除此之外还有提示工程，但是提示工程本身是调整的输入给大模型的信息，而不是对大模型的调整。 主要方式提示工程提示是与任何大型语言模型进行交互的最基本方式。这就像给出指示。当您使用提示时，您告诉模型您希望它给您提供什么样的信息。这也被称为提示工程。这有点像学习如何提出正确的问题以获得最佳答案。但是它有一定的局限性。这是因为模型只能返回它在训练中已经了解的内容。提示工程的好处在于它非常直观。您不需要成为技术专家才能做到这一点，这对大多数人来说非常好。但是，由于它在很大程度上依赖于模型的原始学习，它可能无法始终提供您所需的最新或最具体的信息。在处理一般主题或仅需要快速答案而不涉及太多细节时，这是最好的选择。 优点： 易于使用：提示工程用户友好，不需要高级技术技能，适用于广泛的用户群体。 成本效益：由于使用预训练模型，与微调相比，计算成本较低。 灵活性：可以快速调整提示以探索不同的输出，无需重新训练模型。 缺点： 不一致性：模型的响应质量和相关性可能会根据提示的措辞而有很大差异。 定制能力有限：定制模型的响应能力受限于创造有效提示的创造力和技巧。 依赖模型的知识：输出仅限于模型在初始训练过程中学到的内容，对于高度专业化或最新信息，效果较差。 微调微调是指对语言模型进行更新和特殊训练。可以将其视为更新手机上的应用程序以获得更好的功能。但在这种情况下，应用程序（模型）需要大量的新信息和时间来正确学习所有内容。这有点像模型回到学校。由于微调需要大量的计算资源和时间，因此可能会很昂贵。但是，如果您需要语言模型非常了解特定主题，那么微调是值得的。这就像教模型成为您感兴趣的领域的专家。在微调之后，模型可以给出更准确、更接近您寻找的答案。 优点： 定制能力：允许进行广泛的定制，使模型能够生成针对特定领域或风格的响应。 提高准确性：通过在专门的数据集上进行训练，模型可以生成更准确和相关的响应。 适应性：微调的模型可以更好地处理特定领域或原始训练中未涵盖的最新信息。 缺点： 成本：微调需要大量的计算资源，比提示更昂贵。 技术技能：这种方法需要对机器学习和语言模型架构有更深入的了解。 数据要求：有效的微调需要大量且精心策划的数据集，这可能很具有挑战性。 检索增强生成（RAG）检索增强生成（RAG）将传统语言模型的能力与外部知识库的精确性相结合。当模型需要回答问题时，它首先从知识库中查找并收集相关信息，然后根据该信息（整合进模型的输入信息中）回答问题。这就像模型快速检查信息库以确保给出最佳答案。RAG在需要最新信息或涉及比模型最初学习的主题范围更广的答案时特别有用。在设置和成本方面，它介于两者之间。它非常好，因为它帮助语言模型给出新鲜且更详细的答案。但是，与微调一样，它需要额外的工具和信息才能发挥良好的效果。RAG系统的成本、速度和响应质量严重依赖于向量数据库，这使其成为RAG系统的非常重要的一部分。 优点： 动态信息：通过实时检索外部数据，确保提供的信息是最新和相关的。这对于需要最新信息的应用非常重要，例如新闻相关查询或快速发展的领域。 平衡：在定制和资源需求方面也提供了一个平衡的方法。与完全微调相比，它不需要大量的计算资源，可以进行更灵活和资源高效的操作，使更多的用户和开发人员能够使用。 上下文相关性：通过额外的上下文增强模型的响应，从而产生更具见解和细致的输出。 缺点： 复杂性：实施RAG可能很复杂，需要在语言模型和检索系统之间进行集成。 资源密集型：虽然比完全微调要求的资源少，但RAG仍需要相当大的计算能力。 数据依赖性：输出的质量严重依赖于检索到的信息的相关性和准确性。]]></content>
      <categories>
        <category>LLM</category>
        <category>微调</category>
      </categories>
      <tags>
        <tag>LLM</tag>
        <tag>微调</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[特征工程-特征选择]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-20.MachineLearn%2F1003.%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B-%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9%2F</url>
    <content type="text"><![CDATA[对于一个机器学习问题，数据和特征决定了机器学习的上限，而模型和算法只是逼近这个上限。由此可见，数据和特征在模型的整个开发过程中是比较重要。特征工程，顾名思义，是对原始数据进行一系列工程处理，将其提炼为特征，作为输入供算法和模型使用。从本质上来讲，特征工程是一个表示和展现数据的过程。在实际工作中，特征工程旨在去除原始数据中的杂质和冗余，以设计更高效的特征以刻画求解的问题与预测模型之间的关系。 虽然说理论上只要我们能掌握一类事件的全部特征，并理解特征在整个事件种发挥的作用，我们就可以预测一件事情发生的过程细节，并精准的获取事件的走向和结果。但是显然这会带来巨大的计算成本，让我们对于一些无足轻重的事情花费巨大的代价（比如预测一个某个患者的生存周期，我们不可能去考虑他出交通事故的可能性，更不可能考虑第三次世界大战的发生导致死亡）；当然除了更多的计算成本外，随着特征的增加，我们必须也要补充对应的训练样本，来捕获所有的特征数据及每个特征的作用（权重。所以在实际的模型应用中并不是特征越多越好，特征越多固然会给我们带来很多额外的信息，但是与此同时，一方面，这些额外的信息也增加实验的时间复杂度和最终模型的复杂度，造成的后果就是特征的“维度灾难”，使得计算耗时大幅度增加；另一方面，可能会导致模型的复杂程度上升而使得模型变得不通用。所以我们就要在众多的特征中选择尽可能相关的特征和有效的特征，使得计算的时间复杂度大幅度减少来简化模型，并且保证最终模型的有效性不被减弱或者减弱很少，这也就是我们特征选择的目的。 一般来说，如果特征过多，我们都会在特征工程里面减少输入的特征。其中一种方案，就是进行特征筛选，直接对输入的特征进行维度的过滤。减少模型的特征输入。 剔除方差过低的数据选择特征的最简单方法是删除方差非常小的特征。如果特征的方差非常小（即非常接近于 0），它们就接近于常量，因此根本不会给任何模型增加任何价值。最好的办法就是去掉它们，从而降低复杂度。请注意，方差也取决于数据的缩放。 Scikit-learn 的 VarianceThreshold 实现了这一点。 123456from sklearn.feature_selection import VarianceThresholddata = ...# 创建 VarianceThreshold 对象 var_thresh，指定方差阈值为 0.1var_thresh = VarianceThreshold(threshold=0.1)# 使用 var_thresh 对数据 data 进行拟合和变换，将方差低于阈值的特征移除transformed_data = var_thresh.fit_transform(data) 剔除高相关特征我们还可以删除相关性较高的特征。要计算不同数字特征之间的相关性，可以使用皮尔逊相关性。123456789101112131415import pandas as pdfrom sklearn.datasets import fetch_california_housing# 加载数据data = fetch_california_housing() # 从数据集中提取特征矩阵 XX = data["data"]# 从数据集中提取特征的列名col_names = data["feature_names"] # 从数据集中提取目标变量 yy = data["target"]df = pd.DataFrame(X, columns=col_names)# 添加 MedInc_Sqrt 列，是 MedInc 列中每个元素进行平方根运算的结果df.loc[:, "MedInc_Sqrt"] = df.MedInc.apply(np.sqrt) # 计算皮尔逊相关性矩阵df.corr() 我们可以针对所有数值型的特征得到相关性矩阵，然后针对一些强相关的特征进行剔除。我们看到， MedInc_Sqrt 与 MedInc 的相关性非常高（0.9843）。因此，我们可以删除其中一个特征。 单变量选择单变量特征选择是针对给定目标对每个特征进行评分。互信息、方差分析 F 检验和卡方检验（非负数据才能使用卡方检验） 是一些最常用的单变量特征选择方法。在 scikit- learn 中，有两种方法可以使用这些方法。 SelectKBest：保留得分最高的 k 个特征； SelectPercentile：保留用户指定百分比内的顶级特征。 在自然语言处理中，当我们有一些单词或基于 tf-idf 的特征时，这是一种特别有用的特征选择技术。我们可以为单变量特征选择创建一个包装器，从而可以用于未来的新问题进行特征选择。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566from sklearn.feature_selection import chi2from sklearn.feature_selection import f_classiffrom sklearn.feature_selection import f_regressionfrom sklearn.feature_selection import mutual_info_classiffrom sklearn.feature_selection import mutual_info_regressionfrom sklearn.feature_selection import SelectKBestfrom sklearn.feature_selection import SelectPercentileclass UnivariateFeatureSelction: def __init__(self, n_features, problem_type, scoring): # 若问题类型是分类问题 if problem_type == "classification": # 创建字典 valid_scoring ，包含各种特征重要性衡量方式 valid_scoring = &#123; "f_classif": f_classif, "chi2": chi2, "mutual_info_classif": mutual_info_classif &#125; # 若问题类型是回归问题 else: # 创建字典 valid_scoring，包含各种特征重要性衡量方式 valid_scoring = &#123; "f_regression": f_regression, "mutual_info_regression": mutual_info_regression &#125; # 检查特征重要性方式是否在字典中 if scoring not in valid_scoring: raise Exception("Invalid scoring function") # 检查 n_features 的类型，如果是整数，则使用 SelectKBest 进行特征选择 if isinstance(n_features, int): self.selection = SelectKBest( valid_scoring[scoring], k=n_features ) # 如果 n_features 是浮点数，则使用 SelectPercentile 进行特征选择 elif isinstance(n_features, float): self.selection = SelectPercentile( valid_scoring[scoring], percentile=int(n_features * 100) ) # 如果 n_features 类型无效，引发异常 else: raise Exception("Invalid type of feature") # 定义 fit 方法，用于拟合特征选择器 def fit(self, X, y): return self.selection.fit(X, y) # 定义 transform 方法，用于对数据进行特征选择转换 def transform(self, X): return self.selection.transform(X) # 定义 fit_transform 方法，用于拟合特征选择器并同时进行特征选择转换 def fit_transform(self, X, y): return self.selection.fit_transform(X, y)# 使用该类非常简单。# 实例化特征选择器，保留前10%的特征，回归问题，使用f_regression衡量特征重要性ufs = UnivariateFeatureSelction(n_features=0.1, problem_type="regression", scoring="f_regression" )# 拟合特征选择器ufs.fit(X, y)# 特征转换X_transformed = ufs.transform(X) 贪婪特征选择前面的方法起始都是从特征本身入手，筛选过程和下游方法可能关系没有那么大。而贪婪特征选择的方法是和我们要应用的模型强相关的。在贪婪特征选择中： 第一步是选择一个模型。 第二步是选择损失/评分函数。 第三步也是最后一步是反复评估每个特征，如果能提高损失/评分，就将其添加到 “好 “特征列表中。 这个逻辑本身很直接。但是需要注意这种特征选择过程在每次评估特征时都会适合给定的模型。而且相当于在遍历各种可能的情况，所以这种方法的化，计算量会非常大。12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667import pandas as pdfrom sklearn import linear_modelfrom sklearn import metricsfrom sklearn.datasets import make_classificationclass GreedyFeatureSelection: # 定义评估分数的方法，用于评估模型性能 def evaluate_score(self, X, y): # 逻辑回归模型 model = linear_model.LogisticRegression() # 训练模型 model.fit(X, y) # 预测概率值 predictions = model.predict_proba(X)[:, 1] # 计算 AUC 分数 auc = metrics.roc_auc_score(y, predictions) return auc # 特征选择函数 def _feature_selection(self, X, y): # 初始化空列表，用于存储最佳特征和最佳分数 good_features = [] best_scores = [] # 获取特征数量 num_features = X.shape[1] # 开始特征选择的循环 while True: this_feature = None best_score = 0 # 遍历每个特征 for feature in range(num_features): if feature in good_features: continue selected_features = good_features + [feature] xtrain = X[:, selected_features] score = self.evaluate_score(xtrain, y) # 如果当前特征的得分优于之前的最佳得分，则更新 if score &gt; best_score: this_feature = feature best_score = score # 若找到了新的最佳特征 if this_feature != None: # 特征添加到 good_features 列表 good_features.append(this_feature) # 得分添加到 best_scores 列表 best_scores.append(best_score) # 如果 best_scores 列表长度大于2，并且最后两个得分相比较差，则结束循环 if len(best_scores) &gt; 2: if best_scores[-1] &lt; best_scores[-2]: break # 返回最佳特征的得分列表和最佳特征列表 return best_scores[:-1], good_features[:-1] # 定义类的调用方法，用于执行特征选择 def __call__(self, X, y): scores, features = self._feature_selection(X, y) return X[:, features], scoresif __name__ == "__main__": # 生成一个示例的分类数据集 X 和标签 y X, y = make_classification(n_samples=1000, n_features=100) # 实例化 GreedyFeatureSelection 类，并使用 __call__ 方法进行特征选择 X_transformed, scores = GreedyFeatureSelection()(X, y) 贪婪特征选择方法,会不断的遍历所有的特征，每次遍历会得到剩余特征中带来帮助最大（模型预测得分提升最高）的一个特征，将最优特征放入特征列表（在下一轮会作为固定特征使用），直至特征的增加不会带来性能的帮助（或者现有的特征已经能达到我们所要求的性能需求），会返回得分和特征列表。 递归特征消除法贪婪特征选择，我们是从1个特征开始，不断的添加特征，而递归特征消除法则是从所有特征开始，剔除一个对模型帮助最小的特征（线性支持向量机（SVM）或逻辑回归等模型，我们会为每个特征得到一个系数，该系数决定了特征的重要性。而对于任何基于树的模型，我们得到的是特征重要性，而不是系数。在每次迭代中，我们都可以剔除最不重要的特征，直到达到所需的特征数量为止）。进行递归特征剔除时，在每次迭代中，我们都会剔除特征重要性较高的特征或系数接近 0 的特征。基于之前的贪婪特征选择代码我们可以很容易实现反向的特征剔除。同时 scikit-learn 也提供了 RFE。下面的示例展示了一个简单的用法。 线性回归示例123456789101112131415161718import pandas as pdfrom sklearn.feature_selection import RFEfrom sklearn.linear_model import LinearRegressionfrom sklearn.datasets import fetch_california_housingdata = fetch_california_housing() X = data["data"]col_names = data["feature_names"]y = data["target"]model = LinearRegression()# 创建 RFE（递归特征消除），指定模型为线性回归模型，要选择的特征数量为 3rfe = RFE( estimator=model, n_features_to_select=3 )# 训练模型rfe.fit(X, y)# 使用 RFE 选择的特征进行数据转换X_transformed = rfe.transform(X) 随机森林示例12345678910111213141516171819202122232425import pandas as pdfrom sklearn.datasets import load_diabetesfrom sklearn.ensemble import RandomForestRegressor data = load_diabetes() X = data["data"]col_names = data["feature_names"] y = data["target"]# 实例化随机森林模型model = RandomForestRegressor()# 拟合模型model.fit(X, y)# 获取特征重要性importances = model.feature_importances_# 降序排列idxs = np.argsort(importances)# 设定标题plt.title('Feature Importances')# 创建直方图plt.barh(range(len(idxs)), importances[idxs], align='center')# y轴标签plt.yticks(range(len(idxs)), [col_names[i] for i in idxs])# x轴标签plt.xlabel('Random Forest Feature Importance')plt.show() 我们可以看到不同特征的重要性因子如下图。 原文链接]]></content>
      <categories>
        <category>machine_learning</category>
      </categories>
      <tags>
        <tag>machine_learning</tag>
        <tag>特征工程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[特征工程-时间周期处理]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-20.MachineLearn%2F1001.%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B-%E6%95%B0%E6%8D%AE%E7%BC%96%E7%A0%81-%E6%97%B6%E9%97%B4%E5%91%A8%E6%9C%9F%E5%A4%84%E7%90%86%2F</url>
    <content type="text"><![CDATA[我们有一个带有日期类型列的 pandas 数据帧。利用这一列，我们可以创建以下特征： 年 年中的周 月 星期 周末 小时 还有更多 123456789101112# 添加'year'列，将 'datetime_column' 中的年份提取出来df.loc[:, 'year'] = df['datetime_column'].dt.year# 添加'weekofyear'列，将 'datetime_column' 中的周数提取出来df.loc[:, 'weekofyear'] = df['datetime_column'].dt.weekofyear # 添加'month'列，将 'datetime_column' 中的月份提取出来df.loc[:, 'month'] = df['datetime_column'].dt.month# 添加'dayofweek'列，将 'datetime_column' 中的星期几提取出来df.loc[:, 'dayofweek'] = df['datetime_column'].dt.dayofweek# 添加'weekend'列，判断当天是否为周末df.loc[:, 'weekend'] = (df.datetime_column.dt.weekday &gt;=5).astype(int) # 添加 'hour' 列，将 'datetime_column' 中的小时提取出来df.loc[:, 'hour'] = df['datetime_column'].dt.hour 有时在处理时间序列问题时，您可能需要的特征不是单个值，而是一系列值。 例如，客户在特定时间段内的交易。在这种情况下，我们会创建不同类型的特征，例如：使用数值特征时，在对分类列进行分组时，会得到类似于时间分布值列表的特征。在这种情况下，您可以创建一系列统计特征，例如123456789101112131415161718192021222324252627import numpy as np# 创建字典，用于存储不同的统计特征feature_dict = &#123;&#125;# 计算 x 中元素的平均值，并将结果存储在 feature_dict 中的 'mean' 键下feature_dict['mean'] = np.mean(x) # 计算 x 中元素的最大值，并将结果存储在 feature_dict 中的 'max' 键下feature_dict['max'] = np.max(x) # 计算 x 中元素的最小值，并将结果存储在 feature_dict 中的 'min' 键下feature_dict['min'] = np.min(x) # 计算 x 中元素的标准差，并将结果存储在 feature_dict 中的 'std' 键下feature_dict['std'] = np.std(x)# 计算 x 中元素的方差，并将结果存储在 feature_dict 中的 'var' 键下feature_dict['var'] = np.var(x) # 计算 x 中元素的差值，并将结果存储在 feature_dict 中的 'ptp' 键下feature_dict['ptp'] = np.ptp(x) # 计算 x 中元素的第10百分位数（即百分之10分位数），并将结果存储在 feature_dict 中的 'percentile_10' 键下feature_dict['percentile_10'] = np.percentile(x, 10)# 计算 x 中元素的第60百分位数，将结果存储在 feature_dict 中的 'percentile_60' 键下feature_dict['percentile_60'] = np.percentile(x, 60)# 计算 x 中元素的第90百分位数，将结果存储在 feature_dict 中的 'percentile_90' 键下feature_dict['percentile_90'] = np.percentile(x, 90)# 计算 x 中元素的5%分位数（即0.05分位数），将结果存储在 feature_dict 中的 'quantile_5' 键下feature_dict['quantile_5'] = np.quantile(x, 0.05)# 计算 x 中元素的95%分位数（即0.95分位数），将结果存储在 feature_dict 中的 'quantile_95' 键下feature_dict['quantile_95'] = np.quantile(x, 0.95)# 计算 x 中元素的99%分位数（即0.99分位数），将结果存储在 feature_dict 中的 'quantile_99' 键下feature_dict['quantile_99'] = np.quantile(x, 0.99) 时间序列数据（数值列表）可以转换成许多特征。在这种情况下，一个名为 tsfresh 的 python 库非常有用。 1234567891011from tsfresh.feature_extraction import feature_calculators as fc# 计算 x 数列的绝对能量（abs_energy），并将结果存储在 feature_dict 字典中的 'abs_energy' 键下feature_dict['abs_energy'] = fc.abs_energy(x)# 计算 x 数列中高于均值的数据点数量，将结果存储在 feature_dict 字典中的 'count_above_mean' 键下feature_dict['count_above_mean'] = fc.count_above_mean(x)# 计算 x 数列中低于均值的数据点数量，将结果存储在 feature_dict 字典中的 'count_below_mean' 键下feature_dict['count_below_mean'] = fc.count_below_mean(x)# 计算 x 数列的均值绝对变化（mean_abs_change），并将结果存储在 feature_dict 字典中的 'mean_abs_change' 键下feature_dict['mean_abs_change'] = fc.mean_abs_change(x)# 计算 x 数列的均值变化率（mean_change），并将结果存储在 feature_dict 字典中的 'mean_change' 键下feature_dict['mean_change'] = fc.mean_change(x)]]></content>
      <categories>
        <category>machine_learning</category>
      </categories>
      <tags>
        <tag>machine_learning</tag>
        <tag>数据预处理</tag>
        <tag>特征工程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[特征工程 - 概述]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-20.MachineLearn%2F1000.%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B-%E6%A6%82%E8%BF%B0%2F</url>
    <content type="text"><![CDATA[对于一个机器学习问题，数据和特征决定了机器学习的上限，而模型和算法只是逼近这个上限。由此可见，数据和特征在模型的整个开发过程中是比较重要。特征工程，顾名思义，是对原始数据进行一系列工程处理，将其提炼为特征，作为输入供算法和模型使用。从本质上来讲，特征工程是一个表示和展现数据的过程。在实际工作中，特征工程旨在去除原始数据中的杂质和冗余，以设计更高效的特征以刻画求解的问题与预测模型之间的关系。 虽然说理论上只要我们能掌握一类事件的全部特征，并理解特征在整个事件种发挥的作用，我们就可以预测一件事情发生的过程细节，并精准的获取事件的走向和结果。但是显然这会带来巨大的计算成本，让我们对于一些无足轻重的事情花费巨大的代价（比如预测一个某个患者的生存周期，我们不可能去考虑他出交通事故的可能性，更不可能考虑第三次世界大战的发生导致死亡）；当然除了更多的计算成本外，随着特征的增加，我们必须也要补充对应的训练样本，来捕获所有的特征数据及每个特征的作用（权重。所以在实际的模型应用中并不是特征越多越好，特征越多固然会给我们带来很多额外的信息，但是与此同时，一方面，这些额外的信息也增加实验的时间复杂度和最终模型的复杂度，造成的后果就是特征的“维度灾难”，使得计算耗时大幅度增加；另一方面，可能会导致模型的复杂程度上升而使得模型变得不通用。所以我们就要在众多的特征中选择尽可能相关的特征和有效的特征，使得计算的时间复杂度大幅度减少来简化模型，并且保证最终模型的有效性不被减弱或者减弱很少。因此在进行下游的机器学习、深度学习等模型训练签，我们需要对数据特征进行一些提前的处理工作，这些工作涵盖了多个细分领域 数据编码：对于分类变量，我们使用不同的数字标签([-1，1])，或者向量（红[1,0,0]; 黄[0,1,0] 绿[0,0,1]）等进行转码。 数据预处理：比如数据的归一化、标准化或者缩放处理；以及缺失值的处理 特征选择：筛选有效的特征用于下游模型的训练。单变量选择、多变量选择等。 特征降维：进行多为特征的整合，来降低训练模型接收的维度。]]></content>
      <categories>
        <category>machine_learning</category>
      </categories>
      <tags>
        <tag>machine_learning</tag>
        <tag>数据预处理</tag>
        <tag>特征工程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[特征工程 数据预处理-分类变量处理]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-20.MachineLearn%2F1001.%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B-%E6%95%B0%E6%8D%AE%E7%BC%96%E7%A0%81-%E5%88%86%E7%B1%BB%E5%8F%98%E9%87%8F%E5%A4%84%E7%90%86%2F</url>
    <content type="text"><![CDATA[往往我们需要进行训练时，读取的特征是多种多样的，数值类型的变量可能只是其中的一部分（比如年龄），但是还会有一些其他类型的分类变量（比如性别，比如月份）。而我们这里讨论的分类变量/特征是指任何特征类型，可分为两大类： 无序变量: 是指有两个或两个以上类别的变量，这些类别没有任何相关顺序。例如，如果将性别分为两组，即男性和女性，则可将其视为名义变量。 有序变量: 则有 “等级 “或类别，并有特定的顺序。例如，一个顺序分类变量可以是一个具有低、中、高三个不同等级的特征。顺序很重要。 计算机无法理解文本数据，因此我们需要将这些类别转换为数字。为了便于理解这里使用kaggle项目的一个数据 index id bin_0 bin_1 bin_2 bin_3 bin_4 nom_0 nom_1 nom_2 nom_3 … nom_9 ord_0 ord_1 ord_2 ord_3 ord_4 ord_5 day month target 0 0 0 0 0 T Y Green Triangle Snake Finland … 2f4cb3d51 2 Grandmaster Cold h D kr 2 2 0 1 1 0 1 0 T Y Green Trapezoid Hamster Russia … f83c56c21 1 Grandmaster Hot a A bF 7 8 0 2 2 0 0 0 F Y Blue Trapezoid Lion Russia … ae6800dd0 1 Expert Lava Hot h R Jc 7 2 0 3 3 0 1 0 F Y Red Trapezoid Snake Canada … 8270f0d71 1 Grandmaster Boiling Hot i D kW 2 1 1 4 4 0 0 0 F N Red Trapezoid Lion Canada … b164b72a7 1 Grandmaster Freezing a R qP 7 8 0 这份输入数据中有 5个二元变量 10个无序变量 6个有序变量 2个循环变量 1个目标变量编码标签编码让我们来看看数据集中的 ord_2 特征。它包括6个不同的类别： Cold、Hot、Lava Hot、Boiling Hot、Freezing 等。计算机无法理解文本数据，因此我们需要将这些类别转换为数字。一个简单的方法是创建一个字典，将这些值映射为从 0 到 N-1 的数字，其中 N 是给定特征中类别的总数。123456789# 映射字典mapping = &#123;"Freezing": 0, "Warm": 1, "Cold": 2,"Boiling Hot": 3, "Hot": 4,"Lava Hot": 5 &#125; 然后我们可以读取数据，并将这些类别转换为数字。12345import pandas as pd# 读取数据df = pd.read_csv("../input/cat_train.csv") # 取*ord_2*列，并使用映射将类别转换为数字df.loc[:, "*ord_2*"] = df.*ord_2*.map(mapping) 这种分类变量的编码方式被称为标签编码（Label Encoding）我们将每个类别编码为一个数字标签。 我们也可以使用 scikit-learn 中的 LabelEncoder 进行编码。12345678910import pandas as pdfrom sklearn import preprocessing # 读取数据df = pd.read_csv("../input/cat_train.csv") # 将缺失值填充为"NONE" scikit-learn 的 LabelEncoder 无法处理 NaN 值,所以需要提前处理。df.loc[:, "*ord_2*"] = df.*ord_2*.fillna("NONE") # LabelEncoder编码lbl_enc = preprocessing.LabelEncoder()# 转换数据df.loc[:, "*ord_2*"] = lbl_enc.fit_transform(df.*ord_2*.values) 我们可以在许多基于树的模型中直接使用它：决策树、随机森林、XGBoost 、GBM、LightGBM等。但是这种编码方式不能用于线性模型、支持向量机或神经网络，因为它们希望数据是标准化的。 二值化的稀疏矩阵为了对分类数据的处理能适用于线性模型、支持向量机或神经网络。对于这些类型的模型，我们可以对数据进行二值化（binarize）处理。二值化不是二分类，我们可以通过特征拆解，将一个特征用多个特征表示，实现多分类。 123456Freezing --&gt; 0 --&gt; 0 0 0Warm --&gt; 1 --&gt; 0 0 1 Cold --&gt; 2 --&gt; 0 1 0 Boiling Hot --&gt; 3 --&gt; 0 1 1 Hot --&gt; 4 --&gt; 1 0 0 Lava Hot --&gt; 5 --&gt; 1 0 1 这只是将类别转换为数字，然后再转换为二值化表示。这样，我们就把一个特征分成了三个（在本例中）特征（或列）。如果我们有更多的类别，最终可能会分成更多的列。对应的特征 Feature 就可以由 Feature_0、Feature_1、Feature_2 表示。| Feature | Feature_0 | Feature_1 | Feature_2 || ——– | ——— | ——— | ——— || Warm | 0 | 0 | 1 || Hot | 1 | 0 | 0 || Lava Hot | 1 | 0 | 1 | 由于我们只关注其中的非0值（对应特征为1 的值）所以将上述特征格式转换成稀疏矩阵会进一步减少内存使用。用 1 表示矩阵的一种方法是某种字典方法，其中键是行和列的索引，值是 1。12345(0, 2) 1(1, 0) 1(2, 0) 1(2, 2) 1# 占用32字节 在大数据矩阵中，稀疏矩阵可以减少内存的效果会更加明显。1234567891011121314151617181920212223import numpy as npfrom scipy import sparsen_rows = 10000n_cols = 100000# 生成符合伯努利分布的随机数组，维度为[10000, 100000]example = np.random.binomial(1, p=0.05, size=(n_rows, n_cols))print(f"Size of dense array: &#123;example.nbytes&#125;") # Size of dense array: 8000000000 # 将随机矩阵转换为稀疏矩阵sparse_example = sparse.csr_matrix(example)print(f"Size of sparse array: &#123;sparse_example.data.nbytes&#125;") # Size of sparse array: 399932496full_size = ( sparse_example.data.nbytes + sparse_example.indptr.nbytes + sparse_example.indices.nbytes )print(f"Full size of sparse array: &#123;full_size&#125;")# Full size of sparse array: 599938748 因此，密集阵列需要 ~8000MB 或大约 8GB 内存。而稀疏阵列只占用 399MB 内存。当我们的特征中有越多零时，稀疏阵列对内存的降低效果越显著。 独热编码二值化特征的稀疏表示比其密集表示所占用的内存要少得多，但对于分类变量来说，还有一种转换所占用的内存更少。这就是所谓的 “独热编码”。独热编码也是一种二值编码，因为只有 0 和 1 两个值。但必须注意的是，它并不是二值表示法。我们可以通过下面的例子来理解它的表示法。假设我们用一个向量来表示 ord_2 变量的每个类别。这个向量的大小与 ord_2 变量的类别数相同。在这种特定情况下，每个向量的大小都是 6，并且除了一个位置外，其他位置都是 0。让我们来看看这个特殊的向量表。| Category | Fea_0 | Fea_1 | Fea_2 | Fea_3 | Fea_4 | Fea_5 || ———– | :—: | :—: | :—: | :—: | :—: | :—: || Freezing | 0 | 0 | 0 | 0 | 0 | 1 || Warm | 0 | 0 | 0 | 0 | 1 | 0 || Cold | 0 | 0 | 0 | 1 | 0 | 0 || Boiling Hot | 0 | 0 | 1 | 0 | 0 | 0 || Hot | 0 | 1 | 0 | 0 | 0 | 0 || Lava Hot | 1 | 0 | 0 | 0 | 0 | 0 | 使用热独编码相比二值化编码，在进行稀疏化后，可以更加节省内存，还是以之前的例子，我们表示 Warm、Hot、Lava Hot 这三个类别的稀疏表示如下：1234(1, 4) 1(4, 1) 1(5, 0) 1# 直接少了一个非零值，仅占用24字节 二值化中示例的更大的数据，我们来看看变化1234567891011121314151617181920212223242526import numpy as npfrom sklearn import preprocessing# 生成符合均匀分布的随机整数，维度为[1000000, 10000000]example = np.random.randint(1000, size=1000000)# 独热编码，非稀疏矩阵ohe = preprocessing.OneHotEncoder(sparse=False)# 将随机数组展平ohe_example = ohe.fit_transform(example.reshape(-1, 1))print(f"Size of dense array: &#123;ohe_example.nbytes&#125;") # Size of dense array: 8000000000 # 独热编码，稀疏矩阵ohe = preprocessing.OneHotEncoder(sparse=True)# 将随机数组展平ohe_example = ohe.fit_transform(example.reshape(-1, 1))print(f"Size of sparse array: &#123;ohe_example.data.nbytes&#125;") # Size of sparse array: 8000000 full_size = ( ohe_example.data.nbytes + ohe_example.indptr.nbytes + ohe_example.indices.nbytes )print(f"Full size of sparse array: &#123;full_size&#125;")# Full size of sparse array: 16000004 这里的密集阵列大小约为 8GB，稀疏阵列在二值化时是300MB，使用热独编码的稀疏矩阵只有 8MB。 罕见类有时候罕见类是指在数据集中出现次数少于一定阈值的类别。]]></content>
      <categories>
        <category>machine_learning</category>
      </categories>
      <tags>
        <tag>machine_learning</tag>
        <tag>数据预处理</tag>
        <tag>特征工程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[1020.性能评估-]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-20.MachineLearn%2F1020.%E6%80%A7%E8%83%BD%E8%AF%84%E4%BC%B0-%2F</url>
    <content type="text"><![CDATA[针对分类 准确率（Accuracy）准确率：这是机器学习中最直接的指标之一。它定义了模型的准确度。如果你建立的模型能准确分类100条数据中的90条数据，那么你的准确率就是 90% 或 0.90。评估的就是预测结果和真实结果的一致性。 1234567891011121314151617# 从头实现def accuracy(y_true, y_pred): correct_counter = 0 # 遍历y_true，y_pred中所有元素 for yt, yp in zip(y_true, y_pred): if yt == yp: # 如果预测标签与真实标签相同，则增加计数器 correct_counter += 1 # 返回正确率，正确标签数/总标签数 return correct_counter / len(y_true)# 我们还可以使用 scikit-learn 计算准确率。In [X]: from sklearn import metrics ...: l1 = [0,1,1,1,0,0,0,1] ...: l2 = [0,1,0,1,0,1,0,0] ...: metrics.accuracy_score(l1, l2)Out[X]: 0.625 精确率（P） 召回率（R） F1 分数（F1） AUC（AUC） 对数损失（Log loss） k 精确率（P@k） k 平均精率（AP@k） k 均值平均精确率（MAP@k） 针对回归 平均绝对误差 （MAE） 均方误差 （MSE） 均方根误差 （RMSE） 均方根对数误差 （RMSLE） 平均百分比误差 （MPE） 平均绝对百分比误差 （MAPE） R2]]></content>
      <categories>
        <category>Tobeinsert</category>
      </categories>
      <tags>
        <tag>Tobeinsert</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[1010.数据分组-交叉验证]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-20.MachineLearn%2F1010.%E6%95%B0%E6%8D%AE%E5%88%86%E7%BB%84-%E4%BA%A4%E5%8F%89%E6%A3%80%E9%AA%8C%2F</url>
    <content type="text"><![CDATA[交叉检验是构建机器学习模型过程中的一个步骤，它可以帮助我们确保模型准确拟合数据，同时确保我们不会过拟合。起始随着深度学习的兴起，我们可以想想一下，只要我们的特征足够多，那么任何问题我们都可以在训练集达到 100% 的性能。甚至最极端的方法，我们的分析方法甚至于完全可以采用枚举的方法（当然这是不对的）这种随着特征的过度增加，或者方法的过度复杂，训练集的性能虽然不断提升，但是测试集却开始变差。那这时就进入了过拟合的状态。 因此除了训练数据外，我们总是需要一些测试数据来评估我们的模型，来确认我们模型的泛化能力。这时候，常采用的方法就是进行交叉检验，交叉检验是将训练数据分层几个部分，我们在其中一部分上训练模型，然后在其余部分上进行测试。常用的交叉检验方法如下： k 折交叉检验其中一种方法是将我们的数据平均分为 k 个互不关联的不同集合。这就是所谓的 k 折交叉检验。每个样本分配一个从 0 到 k-1 的值。123456789101112131415161718# 导入 pandas 和 scikit-learn 的 model_selection 模块import pandas as pdfrom sklearn import model_selectionif __name__ == "__main__": # 训练数据存储在名为 train.csv 的 CSV 文件中 df = pd.read_csv("train.csv") # 我们创建一个名为 kfold 的新列，并用 -1 填充 df["kfold"] = -1 # 接下来的步骤是随机打乱数据的行 df = df.sample(frac=1).reset_index(drop=True) # 从 model_selection 模块初始化 kfold 类 kf = model_selection.KFold(n_splits=5) # 填充新的 kfold 列（enumerate的作用是返回一个迭代器） for fold, (trn_, val_) in enumerate(kf.split(X=df)): df.loc[val_, 'kfold'] = fold # 保存带有 kfold 列的新 CSV 文件 df.to_csv("train_folds.csv", index=False) 分层 k 折交叉检验K折交叉检验，针对正常数据已经够用了，但是有时候，我们拿到的数据可能并不是均匀分布的，比如阴阳性比例悬殊，极端点，可能有 90% 的数据是阴性，只有 10% 的数据是阳性。这时候简单的k折可能会导致 10%的阳性样本基本都分到训练集，导致测试集基本都是阴性样本。这类数据使用 分层k折交叉检验 可以规避掉数据分布不均的问题。分层 k 折交叉检验可以保持每个折中标签的比例不变。因此，在每个折叠中，都会有相同的 90% 正样本和 10% 负样本。因此，无论您选择什么指标进行评估，都会在所有折叠中得到相似的结果。 1234567891011121314151617181920# 导入 pandas 和 scikit-learn 的 model_selection 模块import pandas as pdfrom sklearn import model_selectionif __name__ == "__main__": # 训练数据保存在名为 train.csv 的 CSV 文件中 df = pd.read_csv("train.csv") # 添加一个新列 kfold，并用 -1 初始化 df["kfold"] = -1 # 随机打乱数据行 df = df.sample(frac=1).reset_index(drop=True) # 获取目标变量 y = df.target.values # 初始化 StratifiedKFold 类，设置折数（folds）为 5 kf = model_selection.StratifiedKFold(n_splits=5) # 使用 StratifiedKFold 对象的 split 方法来获取训练和验证索引 for f, (t_, v_) in enumerate(kf.split(X=df, y=y)): df.loc[v_, 'kfold'] = f # 保存包含 kfold 列的新 CSV 文件 df.to_csv("train_folds.csv", index=False) 分层k折交叉检验基本上全方位优于k折交叉检验，所以起始可以默认使用分层。 暂留交叉检验上面的方法是针对比较小的数据量，我们会同时计算训练集和测试集的性能表现。但是假设我们有 100 万个样本。5 倍交叉检验意味着在 800k 个样本上进行训练，在 200k 个样本上进行验证。根据我们选择的算法，对于这样规模的数据集来说，训练甚至验证都可能非常昂贵。在这种情况下，我们可以选择暂留交叉检验。创建保持结果的过程与分层 k 折交叉检验相同。对于拥有 100 万个样本的数据集，我们可以创建 10 个折叠而不是 5 个，并保留其中一个折叠作为保留样本。这意味着，我们将有 10 万个样本被保留下来，我们将始终在这10W的保留样本集上计算损失、准确率和其他指标，并在 90 万个样本上进行训练。 留一交叉检验在很多情况下，我们必须处理小型数据集，而使用大量数据用于验证意味着模型学习过程能使用的训练数据非常有限。在这种情况下，我们可以选择留一交叉检验，相当于特殊的 k 折交叉检验其中 k=N ，N 是数据集中的样本数。这意味着在所有的训练折叠中，我们将对除 1 之外的所有数据样本进行训练。这种类型的交叉检验的折叠数与数据集中的样本数相同。只留一例样本进行验证，剩下的样本都用于训练集，只适合非常小型的数据集，基本用不到。]]></content>
      <categories>
        <category>machine_learning</category>
      </categories>
      <tags>
        <tag>machine_learning</tag>
        <tag>数据预处理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[特征工程-数据标准化-sklearn]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-20.MachineLearn%2F1002.%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B-%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86-%E6%95%B0%E6%8D%AE%E6%A0%87%E5%87%86%E5%8C%96-sklearn%2F</url>
    <content type="text"><![CDATA[数据标准化sklearn 数据标准化StandardScalerStandardScaler 通过去除均值并缩放至单位方差来标准化特征。数据粗粒逻辑为 $x_{fit} = (x_{row}-mean(x))/std(x)$12345from sklearn.preprocessing import StandardScaler scaler = StandardScaler()scaler.fit(methylation) # 计算用于以后缩放的平均值和标准差。 methylation=pd.DataFrame(scaler.transform(methylation)) # 通过居中和缩放来执行标准化。]]></content>
      <categories>
        <category>machine_learning</category>
      </categories>
      <tags>
        <tag>machine_learning</tag>
        <tag>数据预处理</tag>
        <tag>特征工程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据预处理-特征降维]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-20.MachineLearn%2F1004.%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B-%E7%89%B9%E5%BE%81%E9%99%8D%E7%BB%B4-SVD%2F</url>
    <content type="text"><![CDATA[奇异值分解(Singular Value Decomposition，以下简称SVD)是在机器学习领域广泛应用的算法，它不光可以用于降维算法中的特征分解，还可以用于推荐系统，以及自然语言处理等领域。是很多机器学习算法的基石。本文就对SVD的原理做一个总结，并讨论在在PCA降维算法中是如何运用运用SVD的。 回顾特征值和特征向量我们首先回顾下特征值和特征向量的定义如下：$$Ax=λx$$其中A是一个$n×n$的实对称矩阵，$x$是一个$n$维向量，则我们说$λ$是矩阵$A$的一个特征值，而 $x$ 是矩阵 $A$ 的特征值$λ$所对应的特征向量。求出特征值和特征向量有什么好处呢？ 就是我们可以将矩阵A特征分解。如果我们求出了矩阵A的n个特征值$λ1≤λ2≤…≤λn$,以及这n个特征值所对应的特征向量${w1,w2,…wn}$，，如果这n个特征向量线性无关，那么矩阵A就可以用下式的特征分解表示：$$A=WΣW−1$$ 其中$W$是这$n$个特征向量所张成的$n×n$维矩阵，而$Σ$为这$n$个特征值为主对角线的$n×n$维矩阵。一般我们会把 $W$ 的这 $n$ 个特征向量标准化，即满足$||w_i||_2=1$, 或者说$w^T_iw_i=1$，此时$W$的$n$个特征向量为标准正交基，满足$W^TW=I$，即$W^T=W^{−1}$, 也就是说W为酉矩阵。这样我们的特征分解表达式可以写成$A=WΣW^T$ 注意到要进行特征分解，矩阵$A$必须为方阵。那么如果$A$不是方阵，即行和列不相同时，我们还可以对矩阵进行分解吗？答案是可以，此时我们的SVD登场了。 SVD的定义SVD也是对矩阵进行分解，但是和特征分解不同，SVD并不要求要分解的矩阵为方阵。假设我们的矩阵A是一个$m×n$的矩阵，那么我们定义矩阵$A$的SVD为：$$A=UΣV^T$$ $U$是一个$m×m$ 的矩阵， $Σ$是一个$m×n$的矩阵，除了主对角线上的元素以外全为0，主对角线上的每个元素都称为奇异值， $V$是一个$n×n$的矩阵。$U$和$V$都是酉矩阵，即满足$U^TU=I$,$V^TV=I$。 SVD的几何意义是：任何线性变换都可以分解为三个基本变换的组合： 旋转（U）：在输出空间中旋转坐标系 缩放（Σ）：沿坐标轴进行不同比例的缩放 旋转（Vᵀ）：在输入空间中旋转坐标系 下图可以很形象的看出上面SVD的定义： 那么我们如何求出SVD分解后的$U,Σ,V$ 这三个矩阵呢？ 如果我们将A的转置和A做矩阵乘法，那么会得到$n×n$的一个方阵$A^TA$。既然$A^TA$是方阵，那么我们就可以进行特征分解，得到的特征值和特征向量满足下式：$$(A^TA)v_i=λ_iv_i$$这样我们就可以得到矩阵$A^TA$ 的$n$个特征值和对应的$n$个特征向量$v$了。将$A^TA$的所有特征向量张成一个$n×n$ 的矩阵$V$，就是我们SVD公式里面的$V$矩阵了。一般我们将$V$中的每个特征向量叫做$A$的右奇异向量。 如果我们将$A$和$A$的转置做矩阵乘法，那么会得到$m×m$的一个方阵$AA^T$。既然$AA^T$是方阵，那么我们就可以进行特征分解，得到的特征值和特征向量满足下式：$$(AA^T)u_i=λ_iu_i$$这样我们就可以得到矩阵$AA^T$的$m$个特征值和对应的$m$个特征向量$u$了。将$AA^T$的所有特征向量张成一个$m×m$ 的矩阵$U$，就是我们SVD公式里面的$U$矩阵了。一般我们将$U$中的每个特征向量叫做$A$的左奇异向量。 $U$和$V$我们都求出来了，现在就剩下奇异值矩阵$Σ$ 没有求出了。由于$Σ$除了对角线上是奇异值其他位置都是0，那我们只需要求出每个奇异值$σ$就可以了。我们注意到:$A=UΣV^T⇒AV=UΣV^TV⇒AV=UΣ⇒Av_i=σ_iu_i⇒σ_i=Av_i/u_i$ 这样我们可以求出我们的每个奇异值，进而求出奇异值矩阵$Σ$。上面还有一个问题没有讲，就是我们说$A^TA$ 的特征向量组成的就是我们SVD中的$V$矩阵，而$AA^T$的特征向量组成的就是我们SVD中的$U$矩阵，这有什么根据吗？这个其实很容易证明，我们以$V$矩阵的证明为例。$$A=UΣV^T⇒A^T=VΣ^TU^T⇒A^TA=VΣ^TU^TUΣV^T=VΣ^2V^T$$ 上式证明使用了:$U^TU=I,Σ^TΣ=Σ^2$。可以看出ATA的特征向量组成的的确就是我们SVD中的$V$矩阵。类似的方法可以得到$AA^T$ 的特征向量组成的就是我们SVD中的$U$矩阵。 进一步我们还可以看出我们的特征值矩阵等于奇异值矩阵的平方，也就是说特征值和奇异值满足如下关系：$σ_i=\sqrt{λ_i}$ 这样也就是说，我们可以不用$σ_i=Av_i/u_i$ 来计算奇异值，也可以通过求出$A^TA$的特征值取平方根来求奇异值。 奇异值计算假设我们要处理的矩阵是：$$A = \left( \begin{matrix} 0 &amp; 1 \ 1 &amp; 1 \ 1 &amp; 0 \end{matrix} \right) \tag{1}$$ 我们首先求出$A^TA$和$AA^T$ $$A^TA=\left( \begin{matrix} 0 &amp; 1 &amp; 1\ 1 &amp; 1 &amp; 0\ \end{matrix} \right) \left( \begin{matrix} 0 &amp; 1 \ 1 &amp; 1 \ 1 &amp; 0 \end{matrix} \right) = \left( \begin{matrix} 2 &amp; 1\ 1 &amp; 2\ \end{matrix} \right)$$$$AA^T=\left( \begin{matrix} 0 &amp; 1 \ 1 &amp; 1 \ 1 &amp; 0 \end{matrix} \right) \left( \begin{matrix} 0 &amp; 1 &amp; 1\ 1 &amp; 1 &amp; 0\ \end{matrix} \right) = \left( \begin{matrix} 1 &amp; 1 &amp; 0\ 1 &amp; 2 &amp; 1 \ 0 &amp; 1 &amp; 1 \end{matrix} \right)$$ 进而求出方阵 $A^TA$的特征值和特征向量 $$AA^T - \lambda I =\left(\begin{matrix} 2 &amp; 1\ 1 &amp; 2\\end{matrix}\right) - \lambda \left(\begin{matrix} 1 &amp; 0\ 0 &amp; 1\\end{matrix}\right) = \left(\begin{matrix} 2-\lambda &amp; 1\ 1 &amp; 2-\lambda\\end{matrix}\right) = 0$$ $$a_{11}a_{22} -a_{12}a_{21}=0-&gt;(2-\lambda)(2-\lambda) -11= 4-4\lambda+\lambda^2-1=\lambda^2-4\lambda+3 =0$$$$λ_1=3;u_1=\left( \begin{matrix} 1/\sqrt{2} \ 1/\sqrt{2} \end{matrix} \right); λ_2=1;u_2=\left( \begin{matrix} -1/\sqrt{2} \ 1/\sqrt{2}\end{matrix} \right)$$ 接着求方阵 $AA^T$的特征值和特征向量$$λ_1=3;u_1=\left(\begin{matrix}1/\sqrt{6} \2/\sqrt{6} \1/\sqrt{6} \end{matrix}\right);λ_2=1;u_2=\left(\begin{matrix}1/\sqrt{2} \0 \−1/\sqrt{2}\end{matrix}\right);λ_3=0;u_3=\left(\begin{matrix}1/\sqrt{3} \−1/\sqrt{3} \1/\sqrt{3}\end{matrix}\right)$$ 利用Avi=σiui,i=1,2求奇异值：$$\left(\begin{matrix}0 &amp; 1 \1 &amp; 1 \1 &amp; 0\end{matrix}\right)\left(\begin{matrix}1/\sqrt{2} \1/\sqrt{2}\end{matrix}\right)=\sigma_1\left(\begin{matrix}1/\sqrt{6} \2/\sqrt{6} \1/\sqrt{6}\end{matrix}\right) =&gt;\sigma_1=\sqrt{3}$$$$\left(\begin{matrix}0 &amp; 1 \1 &amp; 1 \1 &amp; 0\end{matrix}\right)\left(\begin{matrix}-1/\sqrt{2} \1/\sqrt{2}\end{matrix}\right)=\sigma_2\left(\begin{matrix}1/\sqrt{2} \0 \−1/\sqrt{2}\end{matrix}\right) =&gt;\sigma_2=1$$ 最终得到A的奇异值分解为：$$A=UΣV^T = \left( \begin{matrix} 1/\sqrt{6} &amp; 1/\sqrt{2} &amp; 1/\sqrt{3}\ 2/\sqrt{6} &amp; 0 &amp; −1/\sqrt{3}\ 1/\sqrt{6} &amp; −1/\sqrt{2} &amp; 1/\sqrt{3} \end{matrix} \right)\left( \begin{matrix} 1/\sqrt{3} &amp; 0\ 0 &amp; 1 \ 0 &amp; 0 \end{matrix} \right) \left( \begin{matrix} 1/\sqrt{2} &amp; 1/\sqrt{2}\ -1/\sqrt{2} &amp; 1/\sqrt{2} \end{matrix} \right)$$ SVD的性质对于奇异值,它跟我们特征分解中的特征值类似，在奇异值矩阵中也是按照从大到小排列，而且奇异值的减少特别的快，在很多情况下，前10%甚至1%的奇异值的和就占了全部的奇异值之和的99%以上的比例。也就是说，我们也可以用最大的k个的奇异值和对应的左右奇异向量来近似描述矩阵。也就是说：$$A_{m×n}=U_{m×m}Σ{m×n}V^T{n×n}≈U_{m×k}Σ{k×k}V^T{k×n}$$ 其中$k$要比$n$小很多，也就是一个大的矩阵$A$可以用三个小的矩阵$U_{m×k},Σ{k×k},V^T{k×n}$ 来表示。如下图所示，现在我们的矩阵A只需要灰色的部分的三个小矩阵就可以近似描述了。由于这个重要的性质，SVD可以用于PCA降维，来做数据压缩和去噪。也可以用于推荐算法，将用户和喜好对应的矩阵做特征分解，进而得到隐含的用户需求来做推荐。同时也可以用于NLP中的算法，比如潜在语义索引（LSI）。下面我们就对SVD用于PCA降维做一个介绍。 reference[1]. https://www.cnblogs.com/pinard/p/6251584.html[2]. 奇异值计算]]></content>
      <categories>
        <category>machine_learning</category>
      </categories>
      <tags>
        <tag>machine_learning</tag>
        <tag>特征工程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据预处理-特征降维]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-20.MachineLearn%2F1004.%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B-%E7%89%B9%E5%BE%81%E9%99%8D%E7%BB%B4%2F</url>
    <content type="text"><![CDATA[随着数据爆炸，针对一个项目，我们可以获得许多测量结果，但其中只有一部分对于决策任务有用。尽管机器学习算法（MLA）可以处理大数据，但其性能会随着维度的增加而下降。当属性数量增加时，观察数量也会成比例增加，结果学习模型变得更加复杂。对许多特征进行训练的模型变得强烈依赖于数据，因此导致对未见过的数据的过度拟合和低性能。由于涉及不显着且相似的特征，模型的准确性会降低。降维算法旨在解决维数灾难，其目标是通过降低数据复杂度来提高数据质量。主要分为特征选择（寻找信息量最大的特征并消除信息量较小的特征）和特征提取（通过代数变换将特征组合成更少的新特征）。所以很多时候我们需要关注特征提取算法（FEA，Feature Extraction Algorithms），来更好地处理现实数据集中的问题，例如噪声、复杂性和稀疏性。这就是有限元分析（Finite element method）吸引了研究界更多关注的原因。 特征过多可能出现特征之间具有多重共线性，即特征之间互相具有关联关系，导致解空间不稳定，模型泛化能力弱，数据冗余等问题；过多特征也会妨碍模型学习规律，因为诸如此类的问题，一般来说，如果特征过多，我们都会在前期处理阶段，进行特征降维就是指可以用更少维度的特征替代更高维度的特征，同时保留有用的信息。从而用尽可能少的数据维度来反应尽可能大的信息量，同时提高模型的泛化能力。另一方面，实际机器学习中处理成千上万甚至几十万维的情况也并不罕见，在这种情况下，机器学习的资源消耗是不可接受的，因此我们必须对数据进行降维，来通过有限的信息损失来极大的降低计算成本。 降维的优势 通过减少误导和冗余，来提高机器学习算法的识别精度。 通过更少的特征来避免过度拟合，从而减少模型的复杂度，简化模型。 更少的特征，对应的计算资源需求减少，同时也可以降低存储空间的需求。 更容易数据可视化和解释。 通过降维的过程，寻找主要的特征，帮助我们理解数据内部的本质特征。 目前在机器学习的应用过程中，如果输入特征过多，特征降维已经成为模型训练前，数据预处理的标准流程。而随着应用场景的不断丰富，目前已经发展出大量特征提取的算法。 降维算法分类线性有限元分析算法基于映射函数的类型分类 线性有限元分析将高维空间线性映射到较低的空间，即较低的维度是原始维度的线性组合。比如：奇异值分解 (SVD)、主成分分析 (PCA)、小波变换 (WT)、线性判别分析 (LDA)、独立成分分析 (ICA)、因子分析 (FA) 和非负矩阵因子（NMF） 非线性FEA非线性映射高位空间进入低纬空间， 本身由氛围： 全局FEA，提供数据店的全局结构的表示。示例包括多为缩放（MDS）、内核PCA（KPCA）和等距映射（ISOMAP） 局部有限元分析在流形上产生更好的性能，其中“局部几何接近欧几里德”，但“全局几何可能不是”[19]。示例包括自动编码器、拉普拉斯特征图 (LE)、核费希尔判别分析 (KFDA)、LDA 的线性扩展、局部线性嵌入 (LLE) 和 t 分布随机邻域嵌入 (t-SNE)。 基于数据标签情况分类 无监督 FEA 专注于数据本身，没有预先存在的类标签 [20]。大多数 FEA 都是无人监督的。例如 MDS、SVD、PCA、LLE、ISOMAP、t-SNE 和 LE。 有监督的 FEA 在从数据中学习时会考虑类标签的信息 [20]。例如 LDA 和 ICA。 基于头型模式 基于随机投影的有限元分析通过利用随机矩阵来有效地投影原始数据，其中每列包含一个单位长度，其中该列的每个元素的平方和的平方根为 1，同时保留实际相对距离数据之间的欧几里得空间。 [21,22]。示例包括 PCA、LDA 和 SVD。 基于流形 大多数基于流形的有限元分析都是非线性的、无监督的 [23] 和基于邻域图的 [4]。 具体FEA算法主成分分析主成分分析 (PCA, Principal component analysis )主成分分析(PCA)是一种线性、无监督的变换算法，它通过确定数据的最大方差来产生新的特征，称为主成分（PC, Principal Components ）。 PCA 将高维数据集投影到一个新的子空间，其中正交轴或 PC 被视为最大数据方差的方向 。在转换过程中，第一个 PC 的方差最高，后续 PC 的方差逐渐减小。优点：它是非迭代的，因此耗时较少，可以很好地减少过度拟合，并且还可以用作去噪和数据压缩技术缺点：仅限于线性投影，因此不能很好地处理非线性数据；在应用PCA之前必须进行数据标准化，否则由于方向对特征尺度非常敏感，将无法找到最佳PC(方差的大,可能是度量单位导致的)；如果不仔细选择PC，则可能会发生信息主程序的排序错误导致重要信息丢失。 内核主成分分析 （KPCA，Kernel Principal component analysis )PCA 在非线性数据上表现不佳，因为生成的子空间不是最优的。因此，内核 PCA (KPCA) 变得很方便。它使用$ K(x_a,x_b)=\emptyset$(X_a)^T *\emptyset$(X_b)$中所示的“内核技巧”K将数据投影到更高的特征空间，以便数据可以线性分离。简单来讲，就是在PCA分析前，通过一个核函数对数据进行处理。常见的核函数有：Linear、Gaussian kernel、Sigmoid、Polynomial 等。 线性判别分析 (LDA, Linear Discriminant Analysis )大多数时候，线性判别分析 (LDA) 被定义为线性、有监督的有限元分析。然而，一些作者指出 LDA 也可以作为线性分类器。 LDA 确定了一个新的特征空间来投影数据，其目标是最大化类的可分离性。从数据集的 d 个独立特征中，它提取 k 个新的独立特征，这些特征将类别（依赖特征）分开最多。LDA和PCA的区别如下图：PCA是在寻找所有数据方差表现最大的方向，LDA是寻找各个类之间差距最大的方向。其次就是LDA进行分析后，特征只有 总类别数-1 的维度，PCA本身就没有这个限制。 多维度缩放 (MDS, Multi-dimensional scaling )是一种非线性、无监督的有限元分析，重点关注多维空间中数据之间的关系（相似性或相异性）。MDS 有两种主要方法：(1).度量MDS（经典）和非度量MDS。在这项工作中，我们介绍了广泛采用的经典 MDS，也称为主坐标分析(Principal Coordinate Analysis)。如算法4所示，MDS一般不关心数据；相反，它更关注数据对之间的成对差异。 MDS 通过计算相异/距离矩阵来定位较低维度的数据点，使得相似的数据在一起，不太相似的数据相距很远[37]。根据距离矩阵 d，MDS 找到使 d 和 d 之间的相似度最大化的输出 Y，其中 d= 奇异值分解 (SVD, Singular Value Decomposition )提供了表示为任意维数矩阵的数据集的精确说明。然而，我们选择的维度（分量）数量越少，SVD 的说明就越不精确。 局部线性嵌入 (LLE, Locally linear embedding)是一种非线性、无监督的提取过程，它通过假设数据位于嵌入高特征空间的平滑非线性流形上来生成低维全局坐标 等距映射 (Isometric mapping)经典缩放是一种公认​​的计算数据差异的方法，旨在保留数据点的成对距离（欧几里德距离），但不考虑相邻数据的分布。 独立成分分析 (ICA, Independent Component Analysis )因子分析 (FA, Factor Analysis )小波变换 (WT, Wavelet Transformation )总结和比较EFA方法的比较 方法 目标 监督 线性 数据特征 迭代 拓扑结构 PCA 最大化方差 无监督 线性 数据有偏差 非迭代 随机投影 KPCA 线性分离数据（基于最大方差） 无监督 非线性 非线性数据表现更好 非迭代 流形 LDA 最大化类分离 简单 线性 连续分布数据表现更好 非迭代 随机投影 MDS 保存数据对的欧几里得和距离 无监督 非线性 关系型数据表现更好 迭代 流形 SVD 最小化重建错误 无监督 线性 处理稀疏数据 迭代 随即投影 LLE 保存局部特征 无监督 非线性 处理稀疏非线性数据 迭代 流形 等距映射 保存数据对的距离 无监督 非线性 处理稀疏有噪音的数据 非迭代 流形 LE 保存局部距离 无监督 非线性 处理噪音数据 非迭代 流形 ICA 最大化独立统计 监督 线性 处理higher order statistics 迭代 随机投影 t-SNE 保存局部结构 无监督 非线性 数字量化数据表现更好 迭代 流形 reference:[1]. Conceptual and empirical comparison of dimensionality reduction algorithms (PCA, KPCA, LDA, MDS, SVD, LLE, ISOMAP, LE, ICA, t-SNE) pdf]]></content>
      <categories>
        <category>machine_learning</category>
      </categories>
      <tags>
        <tag>machine_learning</tag>
        <tag>特征工程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据预处理-特征降维 - PCA]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-20.MachineLearn%2F1004.%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B-%E7%89%B9%E5%BE%81%E9%99%8D%E7%BB%B4-PCA%2F</url>
    <content type="text"><![CDATA[主成分分析，PCA (Principal Component Analysis)主成分分析（PCA）是一种降维方法，通常用于降低大型数据集的维度，通过将大型变量集转换为仍包含大型变量集中大部分信息的较小变量。减少数据集的变量数量自然会以牺牲准确性为代价，但降维的技巧是牺牲一点准确性来换取简单性。因为较小的数据集更容易探索和可视化，从而使机器学习算法分析数据点变得更加容易和更快，而无需处理无关变量。总而言之，PCA的思想很简单：减少数据集的变量数量，同时保留尽可能多的信息。 主成分是被构造为初始变量的线性组合或混合的新变量。这些组合以这样的方式完成：新变量（即主成分）是不相关的，并且初始变量内的大部分信息被挤压或压缩到第一成分中。因此，10 维数据的想法是给你 10 个主成分，但 PCA 尝试将最大可能的信息放入第一个成分中，然后将最大剩余信息放入第二个成分中，依此类推，直到得到如下屏幕图中所示的内容(横坐标是每个主成分排序，纵坐标是每个主成分的方差（信息）百分比)。以这种方式组织主成分中的信息将允许您在不丢失太多信息的情况下降低维度，这是通过丢弃信息量低的成分并将剩余成分视为新变量来实现的。当然这也带来另一个问题，因为主成分分析后保留下来的每个主成分都是初始变量的线性组合的值，是一个单纯的标量，没有对应的实际意义。因此主成分的解释性非常差。 从几何上讲，主成分代表了解释最大方差的数据方向，即捕获数据大部分信息的线。这里方差与信息的关系是，一条线所携带的方差越大，沿该线的数据点的离散度就越大，而沿一条线的离散度越大，则该线所包含的信息越多。简而言之，只需将主成分视为新轴，它提供查看和评估数据的最佳角度，以便更好地看到观察结果之间的差异。 主成分分析过程感官介绍由于数据中有多少个变量，就有多少个主成分，因此主成分的构造方式应使第一个主成分尽可能占数据集中最大的方差。例如，假设我们的数据集的散点图如下所示，我们可以就可以感官的猜测第一个主成分。大约是与紫色标记匹配的线，因为它穿过原点，并且我们的数据投影到这条线上的投影点（红点）最分散的线。或者从数学上讲，它是最大化方差（数据投影到这条线上后，从投影点（红点）到原点的平方距离的平均值）的线。 第一主成分我们是在平面展示的，现在我来想想一个3D数据集。在获取第一个主成分后，我们可以得到第一主成分的一个垂面。将我们的所有数据投影到垂面上，然后重复类似第一主成分获取的方式，就可以得到第二主成分。第二主成分和第一主成分不相关（即垂直）并且它解释了下一个最高方差。针对高纬度的数据，我们以此类推重复此过程，一直持续到计算出总共p个主成分，等于原始变量数量。 理解信息量上图中，如果我们单独看某一个维度的话，比如看x1这个维度可以看到将点投影到x1这个维度上看的话，图1的数据离散性最高，图3较低，图2数据离散性是最低的。数据离散性越大，代表数据在所投影的维度上具有越高的区分度，这个区分度就是信息量。如果我们用方差来形容数据的离散性的话，就是数据方差越大，表示数据的区分度越高，也就是蕴含的信息量是越大的。而我们进行降维的过程，就是用一个或几个较少的维度反应数据的信息量。从数据看，图1我们可以保留图x1维度，对应的图2可以保留x2维度来最大的反应数据的特征，但是相对图3，就是我们在PCA分析中要解决的问题，为了找到要给更好的维度，我们需要基于如下的方式，构建获取一个衡量体系（由原本的某2个或某几个特征组合而成）示例如下：转换后，每个位点的特征表示方式，由原来的 (x1,x2) 转变为 (y1,y2)。同时这时候我们把这个过程简化成一个数学运算，其实相当于是对每个特征点的 原始特征向量（每个维度的数据构成一个向量） * 一个矩阵（两个坐标体系的转移矩阵） 获得新的坐标轴体系（由特征向量确定的方向）上的特征值（在新坐标体系下的对应方向上的向量长度）。 主成分分析具体步骤1. 标准化PCA 对于初始变量的方差非常敏感，如果初始变量的范围之间存在较大差异，则范围较大的变量将主导范围较小的变量（例如，范围在 0 到 100 之间的变量将主导范围在 0 到 1 之间的变量），这会导致结果有偏差。所以需要在进行PCA分析前先要对数据进行标准化来消除初始变量范围差异带来的影响。标准化的过程就是将数据转换为可比较的尺度可以防止这个问题。常见的步骤就是通过减去每个变量的每个值的平均值并除以标准差来完成。$z=\frac{value-mean}{\sigma}$标准化完成后，所有变量都将转换为相同的尺度，确保每个变量对分析的贡献相同。 2. 计算协方差矩阵对所有的初始变量，计算协方差矩阵（协方差为正代表正相关，协方差为负代表负相关；数值越大相关性越强）。 3. 计算协方差矩阵的特征向量和特征值以识别主成分特征向量和特征值是线性代数概念，我们需要根据协方差矩阵计算它们，以确定数据的主成分。 关于特征向量和特征值，您首先需要了解的是它们总是成对出现，因此每个特征向量都有一个特征值。此外，它们的数量等于数据的维数。例如，对于 3 维数据集，有 3 个变量，因此有 3 个特征向量和 3 个对应的特征值。 4. 创建特征向量计算特征向量并按特征值降序对它们进行排序，使我们能够按重要性顺序找到主成分。在这一步中，我们要做的是选择是保留所有这些分量还是丢弃那些不太重要（低特征值）的分量，并与剩余的分量形成一个向量矩阵，我们称之为特征向量。特征向量只是一个矩阵，其列是我们决定保留的分量的特征向量。这使其成为降维的第一步，因为如果我们选择仅保留n中的p 个特征向量（分量），则最终数据集将只有p 个维度。 5. 沿主成分轴重新构建数据最后一步的目的就是使用协方差矩阵的特征向量形成的特征向量，将数据从原始轴重新定向到由主成分表示的轴（因此称为主成分分析） ）。这可以通过将原始数据集的转置乘以特征向量的转置来完成。 注意事项由于PCA算法的特征 reference:[1]. Conceptual and empirical comparison of dimensionality reduction algorithms (PCA, KPCA, LDA, MDS, SVD, LLE, ISOMAP, LE, ICA, t-SNE)]]></content>
      <categories>
        <category>machine_learning</category>
      </categories>
      <tags>
        <tag>machine_learning</tag>
        <tag>特征工程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[0003.机器学习和人工智能相关学习资料]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-20.MachineLearn%2F0003.%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%92%8C%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E7%9B%B8%E5%85%B3%E5%AD%A6%E4%B9%A0%E8%B5%84%E6%96%99%2F</url>
    <content type="text"><![CDATA[《解决几乎所有机器学习问题》是一个打 kaggle 比赛的大佬整理的相关笔记，Approaching (Almost) Any Machine Learning Problem.pdf中文译本在线阅读中文译本 github 仓库]]></content>
      <categories>
        <category>machine_learning</category>
      </categories>
      <tags>
        <tag>machine_learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[VSCode中使用 markdown 的快捷键]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-00.%E7%BC%96%E7%A8%8B%E8%BE%85%E5%8A%A9%E5%B7%A5%E5%85%B7%2FSoftware-%E7%BC%96%E7%A8%8B%E7%BB%88%E7%AB%AF-VSCode-5.%E5%BF%AB%E6%8D%B7%E9%94%AE-ipynb%2F</url>
    <content type="text"><![CDATA[Cell命令模式目前支持的Jupyter Notebook快捷 快捷键 响应 Enter 转入编辑模式 Shift-Enter 运行本单元，选中或插入（最后一个Cell的时候）下个单元 Ctrl-Enter 运行本单元 Alt-Enter 运行本单元，在其下插入新单元 Y 单元转入代码状态 M 单元转入markdown状态 （目前尚不支持R 原生状态） Up 选中上方单元 K 选中上方单元 Down 选中下方单元 J 选中下方单元 A 在上方插入新单元 B 在下方插入新单元 D,D 删除选中的单元 L 转换行号 Shift-Space 向上滚动 Space 向下滚动 可以对于Cell的复制、粘贴、多选、合并、拆分等功能目前尚未支持，等待今后的版本吧 ell编辑模式下支持的Vscode快捷键（只描述与编辑相关的那些快捷键） 快捷键 响应 Ctrl + X 剪切/剪切行（空选定） Ctrl + C 复制/复制行（空选定） Ctrl + Delete / Backspace 删除右边、左边的字 Alt + ↑ / ↓ 向上/向下移动行 Shift + Alt + ↓ / ↑ 向上/向下复制行 Ctrl + Shift + K 删除行 Ctrl + Shift + \ 跳到匹配的括号 Ctrl + ] / [ 缩进/突出行 Ctrl + ← / → 光标到字首/字尾 Ctrl + / 切换行注释 Shift + Alt + A 切换块注释 Ctrl + F/H 查找/替换 Ctrl + D 选择一个/多个（相同）词 Alt + 单击 插入多个光标 Ctrl + Alt + ↑ / ↓ 在上/下插入光标 Ctrl + U 撤消上一个光标操作 Shift + Alt + I 在选定的每一行的末尾插入光标 Ctrl + L 选择当前行 Ctrl + Shift + L 选择当前词的所有出现 Ctrl + F2 选择当前字的所有出现 Shift + Alt + → / Shift + Alt + ← 展开/缩小选择 Shift + Alt + （拖动鼠标） 矩形块选择 Ctrl + K Ctrl + X 修剪尾随空格]]></content>
      <categories>
        <category>software</category>
        <category>Windows</category>
        <category>VSCode</category>
      </categories>
      <tags>
        <tag>环境配置</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[VSCode中使用 markdown 的快捷键]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-00.%E7%BC%96%E7%A8%8B%E8%BE%85%E5%8A%A9%E5%B7%A5%E5%85%B7%2FSoftware-%E7%BC%96%E7%A8%8B%E7%BB%88%E7%AB%AF-VSCode-5.%E5%BF%AB%E6%8D%B7%E9%94%AE-markdown%2F</url>
    <content type="text"><![CDATA[快捷键 响应 Ctrl + k + s 查看与更改键盘快捷方式 Ctrl + b 加粗 加粗 Ctrl + m 放大 放 大 放大 放大 Ctrl + g 跳转到?行 Ctrl + h 查找与替换 Ctrl + j 调出电脑 cmd 控制终端 Ctrl + l 向下选中 Ctrl + w 关闭当前文件 Ctrl + i 斜体 斜体 Ctrl + o 打开文件 Ctrl + Tab 在不同工程之间切换 Ctrl + k + z 全屏显示当前工程 Ctrl + k + c VScode 剪切板 Ctrl + k + m 选择语言模式 Ctrl + k + d 只读模式(会指示你未保存的更改) Ctrl + k + f 关闭当前实例的工作区或文件夹 Ctrl + k + e 打开资源管理器 Ctrl + k + r 打开当前文件所在文件夹 Ctrl + k + u 关闭当前工程之外的所有工程 Ctrl + k + o 当前工程在新窗口开启 Ctrl + Shift + x 扩展商店 Ctrl + Shift + c 进入当前工程文件目录下 cmd 控制终端 Ctrl + Shift + v 检视当前 Markdown 文件的预览 Ctrl + Shift + b 配置生成任务 Ctrl + Shift + n 打开新的 VScode 窗口 Ctrl + Shift + m 打开 VScode 的调试窗口 问题 一栏 Ctrl + Shift + d 打开 运行 和 调试 侧栏 Ctrl + Shift + g 打开 源代码管理 侧栏 Ctrl + Shift + h 搜索和替换 Ctrl + Shift + k 删除当前行 Ctrl + Shift + w 关闭 VScode Ctrl + Shift + e 打开资源管理器 Ctrl + Shift + r 重构操作 Ctrl + Shift + y 打开 VScode 的调试窗口 调试控制台 一栏 Ctrl + Shift + u 打开 VScode 的调试窗口 输出 一栏 Ctrl + Shift + o 查看 Markdown 章节目录 Ctrl + Shift + p 搜索 VScode 功能 Ctrl + k + Ctrl + c/u/q 注释 或 取消注释 Ctrl + k + Ctrl + b 一个类似于书签的东西 Ctrl + k + Ctrl + m 扩展商店 Ctrl + k + Ctrl + l 展开或收起当前章节 Ctrl + k + Ctrl + e 打开资源管理器 Ctrl + k + Ctrl + r 打开键盘快捷方式 PDF Ctrl + k + Ctrl + t 打开颜色主题 Ctrl + k + Ctrl + p 当前打开的文档]]></content>
      <categories>
        <category>software</category>
        <category>Windows</category>
        <category>VSCode</category>
      </categories>
      <tags>
        <tag>环境配置</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[归一化层(Normalization Layers)]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-20.MachineLearn%2F3003.%E6%A6%82%E5%BF%B5-%E5%BD%92%E4%B8%80%E5%8C%96%E5%B1%82%2F</url>
    <content type="text"><![CDATA[Normalization 层主要解决的问题是希望输入数据能大致分布在相同的空间内，从而让训练更好更快的收敛。 最先提出的 Batch normalization 层对于深度网络的收敛起到了很大的促进作用，于是后续又有多种normalization层提出，这里总结下normlization的相关知识。 深度网络中，对数据多次线性和非线性变换，所以可能刚开始处于相同空间内的样本，比如图像数据，经过若干层后的输出差距非常大，那么这些差距较大的数据可能处于不同的数据空间，继而导致更高层需要不断的更新参数去适应每一种分布，使得训练比较难收敛。 Google称这种现象为Internal Covariate Shift, 即网络中间层的输入数据空间分布发生变化， 这种变化带来的问题： 高层的参数需要较大的迭代更新以适应新的数据分布， 使得模型收敛较慢 每层的参数更新对其它层参数影响较大，而且这种影响是累积的 可能导致低层输入较小变化不会反映到最后输出中，导致上层的激活函数处于饱和区，学习过早停止。 而BN层的提出是希望将每一层的输出通过归一化的形式缩放移动到相同的空间分布中，从而使得模型更快的收敛。 主流的Normalization方法 假设上一层的输出数据大小是 $ N×C×H×W $, 那么 Batch Norm: 在batch方向上做归一化，计算的是 $NHW$ 的归一化; Layer Norm: 在channel方向做归一化，即每个样本输出元素做归一化处理， 算的是 $CHW$ 的归一化; Instance Norm: 在每一层feature map上归一化， 即每个样本的每一层的 $ H∗W $ 元素进行归一化; GroupNorm： 这个是Layer Norm的更泛化的方法，将通道划分成多个group， 每个group内进行单独的LayerNorm, 当group数目等于channel数目时等价于Instance norm， 当group=1时就是Layer Norm。 对应的不同使用场景 BN适用于batchsize较大， 且每个batch数据分布比较接近的场景。 BN采用的是平均滑动均值和方差。 LN针对于单个样本进行归一化，避免了BN中batch分布于整体分布差距较大的情形， 适用于小batchsize的场景、动态网络场景和RNN， NLP领域。 LN不需要保存动态的均值和方差，节省空间。 IN 规范化在GAN和style transfer任务中要由于BN， IN和LN一样操作目标是单个样本，另外上面我们也说到LN是所有的通道规范化到相同空间，降低了模型表征能力， 而IN不同，他和BN一样对每一个通道单独操作。 GN相当于与对IN和LN进行了泛化，是对BN的改善。能够处理BN针对于小batch表现不好的情形。 BN归一化是对每一层的featuremap操作的， 而LN是对所有层的featmaps操作的，LN会让所有的通道都处于一个分布空间，BN不同通道可以处于不同空间。 BN相对于LN而言有更强的表征能力。 BN的计算是要受其他样本影响的，由于每个batch的均值和标准差不稳定，对于单个数据而言，相对于是引入了噪声，但在分类这种问题上，结果和数据的整体分布有关系，因此需要通过BN获得数据的整体分布。而instance norm的信息都是来自于自身的图片，相当于对全局信息做了一次整合和调整，在图像转换这种问题上，BN获得的整体信息不会带来任何收益，带来的噪声反而会弱化实例之间的独立性：这类生成式方法，每张图片自己的风格比较独立不应该与batch中其他的样本产生太大联系]]></content>
      <categories>
        <category>neural_network</category>
      </categories>
      <tags>
        <tag>MLP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[概念-激活函数.pytorch实现]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-20.MachineLearn%2F3002.%E6%A6%82%E5%BF%B5-%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0.pytorch%E5%AE%9E%E7%8E%B0%2F</url>
    <content type="text"><![CDATA[Non-linear Activations (weighted sum, nonlinearity) 激活函数 简介 图示 nn.ELU Exponential Linear Unit (ELU) function, element-wise. nn.Hardshrink Hard Shrinkage (Hardshrink) function element-wise. nn.Hardsigmoid Hardsigmoid function element-wise. nn.Hardtanh HardTanh function element-wise. nn.Hardswish Hardswish function, element-wise. nn.LeakyReLU LeakyReLU function element-wise. nn.LogSigmoid Logsigmoid function element-wise. nn.MultiheadAttention Allows the model to jointly attend to information from different representation subspaces. nn.PReLU element-wise PReLU function. nn.ReLU rectified linear unit function element-wise. nn.ReLU6 ReLU6 function element-wise. nn.RReLU randomized leaky rectified linear unit function, element-wise. nn.SELU SELU function element-wise. nn.CELU CELU function element-wise. nn.GELU Gaussian Error Linear Units function. nn.Sigmoid Sigmoid function element-wise. nn.SiLU Sigmoid Linear Unit (SiLU) function, element-wise. nn.Mish Mish function, element-wise. nn.Softplus Softplus function element-wise. nn.Softshrink soft shrinkage function element-wise. nn.Softsign element-wise Softsign function. nn.Tanh Hyperbolic Tangent (Tanh) function element-wise. nn.Tanhshrink element-wise Tanhshrink function. nn.Threshold Thresholds each element of the input Tensor. $\begin{cases} x &amp; x &gt;Threshold\ value &amp; otherwise \end{cases}$ nn.GLU gated linear unit function. 输入矩阵根据输入参数分割成两个子矩阵a和b，计算矩阵 a *b Non-linear Activations (other) 激活函数 简介 说明 nn.Softmin Applies the Softmin function to an n-dimensional input Tensor. 重新缩放输入的n维张量，使输出 Tensor 的元素位于 [0,1] 范围内且总和为 1（每个dim指定方向的切片综合）。softmin 是单调递减（最小的数在经过了softmin后变成最大值） nn.Softmax Applies the Softmax function to an n-dimensional input Tensor. 重新缩放输入的n维张量，使输出 Tensor 的元素位于 [0,1] 范围内且总和为 1（每个dim指定方向的切片综合）。softmax 是单调递增（操作会使得最大的值在激活操作后依然保持最大 nn.Softmax2d Applies SoftMax over features to each spatial location. SoftMax 应用到每个空间位置的要素上。不再是dim指定的切片方向，而是整个输入张量 nn.LogSoftmax Applies the $log⁡(Softmax(x)) $function to an n-dimensional input Tensor. SoftMax 应用到输入的n维张量后，对结果再取$log$ nn.AdaptiveLogSoftmaxWithLoss Applies the $log⁡(Softmax(x))$ function to an n-dimensional input Tensor. 作为输入传递到该模块的标签应根据其频率进行排序,并在排序后根据指定的数目对输入的标签进行分簇处理。]]></content>
      <categories>
        <category>neural_network</category>
      </categories>
      <tags>
        <tag>pytorch</tag>
        <tag>激活函数</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据标准化-平方根转换]]></title>
    <url>%2F01.%E7%9F%A5%E8%AF%86-02.Math%2F%E6%95%B0%E6%8D%AE%E6%A0%87%E5%87%86%E5%8C%96-%E5%B9%B3%E6%96%B9%E6%A0%B9%E8%BD%AC%E6%8D%A2%2F</url>
    <content type="text"><![CDATA[转换共识： $f(x)=\sqrt{x}$ 平方根变换可用于： 标准化偏态分布 将两个变量之间的非线性关系转换为线性关系 减少线性回归中残差的异方差 专注于可视化数据的某些部分 对变量应用平方根变换时，高值会被压缩，低值会变得更加分散。log转换做同样的事情，但更积极。]]></content>
      <categories>
        <category>统计知识</category>
      </categories>
      <tags>
        <tag>数据标准化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pytorch-损失函数]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-20.MachineLearn%2F3004.%E6%A6%82%E5%BF%B5-%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0.pytorch%E5%AE%9E%E7%8E%B0%2F</url>
    <content type="text"><![CDATA[Loss Function pytorch中常用的损失函数| 损失函数 | 名称 | 适用场景 | 说明 || ——————————- | ———————- | ———— | ———————————————————————————— || torch.nn.MSELoss() | 均方误差损失 | 回归 | 取预测值和真实值的绝对误差的平均数 $$ℓ(x,y) = {1 \over N} \sum_{n}^{N}|x_n​−y_n|$$ || torch.nn.L1Loss() | 平均绝对值误差损失 | 回归 || torch.nn.CrossEntropyLoss() | 交叉熵损失 | 多分类 || torch.nn.NLLLoss() | 负对数似然函数损失 | 多分类 || torch.nn.NLLLoss2d() | 图片负对数似然函数损失 | 图像分割 || torch.nn.KLDivLoss() | KL散度损失 | 回归 || torch.nn.BCELoss() | 二分类交叉熵损失 | 二分类 || torch.nn.MarginRankingLoss() | 评价相似度的损失 | || torch.nn.MultiLabelMarginLoss() | 多标签分类的损失 | 多标签分类 || torch.nn.SmoothL1Loss() | 平滑的L1损失 | 回归 | 也叫作 Huber Loss，误差在 (-1,1) 上是平方损失，其他情况是 L1 损失，应用于回归。 || torch.nn.SoftMarginLoss() | 多标签二分类问题的损失 | 多标签二分类 |]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>损失函数</tag>
        <tag>pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pytorch-损失函数]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-20.MachineLearn%2F3001.%E6%A6%82%E5%BF%B5-%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B.pytorch%E5%AE%9E%E7%8E%B0%2F</url>
    <content type="text"><![CDATA[torch.dtype最近进行一些代码实操的过程中，经常遇到一个报错，TypeError: tensor(): argument ‘dtype’ must be torch.dtype, not torch.tensortype，原因则是因为在dtype参数中错误的传递了torch.tensortype。因为有些类似分类任务中，运行报错可能时张量类型要求是LongTensor，但是直接在定义的时候，我们需要使用对应的torch.intdtype。而不能直接使用张量。 Each torch.Tensor has a torch.dtype, torch.device, and torch.layout. Data type dtype Legacy Constructors 32-bit floating point torch.float32 or torch.float torch.*.FloatTensor 64-bit floating point torch.float64 or torch.double torch.*.DoubleTensor 64-bit complex torch.complex64 or torch.cfloat 128-bit complex torch.complex128 or torch.cdouble 16-bit floating point 1 torch.float16 or torch.half torch.*.HalfTensor 16-bit floating point 2 torch.bfloat16 torch.*.BFloat16Tensor 8-bit integer (unsigned) torch.uint8 torch.*.ByteTensor 8-bit integer (signed) torch.int8 torch.*.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.*.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.*.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.*.LongTensor Boolean torch.bool torch.*.BoolTensor 分类任务中，默认是从0起始的，所以针对分类任务进行编号时，需要从 0 开始进行编号。二分类（0，1），三分类（0，1，2）。]]></content>
      <categories>
        <category>pytorch</category>
      </categories>
      <tags>
        <tag>损失函数</tag>
        <tag>pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习-算法-集成学习]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-20.MachineLearn%2F1204.%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0-0.%E6%A6%82%E8%BF%B0%2F</url>
    <content type="text"><![CDATA[集成学习是指使用多种兼容的学习算法/模型来执行单个任务的技术，目的是为了得到更佳的预测表现。集成学习的主要方法可归类为三大类： 堆叠（Stacking）、提升（Boosting） 和 装袋（Bagging/bootstrapaggregating）。其中最流行的方法包括随机森林、梯度提升、AdaBoost、梯度提升决策树（GBDT）和XGBoost。 集成学习集成学习是一种机器学习范式。在集成学习中，我们会训练多个模型（通常称为「弱学习器」）解决相同的问题，并将它们结合起来以获得更好的结果。最重要的假设是：当弱模型被正确组合时，我们可以得到更精确和/或更鲁棒的模型。在集成学习理论中，我们将弱学习器（或基础模型）称为「模型」，这些模型可用作设计更复杂模型的构件。在大多数情况下，这些基本模型本身的性能并不是非常好，这要么是因为它们具有较高的偏置（例如，低自由度模型），要么是因为他们的方差太大导致鲁棒性不强（例如，高自由度模型）。集成方法的思想是通过将这些弱学习器的偏置和/或方差结合起来，从而创建一个「强学习器」（或「集成模型」），从而获得更好的性能。当然这也有两个潜在前提： 基分类器要有一定的性能，至少不差于随机猜测的性能，即基分类器准确率不低于50%。 基学习器要具有多样性，即基学习器间要有差异性，完全一样的结果显然也无法合并出差异的结果。 集成学习的分类为了建立一个集成学习方法，我们首先要选择待聚合的基础模型。在大多数情况下（包括在众所周知的 bagging 和 boosting 方法中），我们会使用单一的基础学习算法，这样一来我们就有了以不同方式训练的同质弱学习器。 这样得到的集成模型被称为「同质的」。然而，也有一些方法使用不同种类的基础学习算法：将一些异质的弱学习器组合成「异质集成模型」。 很重要的一点是：我们对弱学习器的选择应该和我们聚合这些模型的方式相一致。如果我们选择具有低偏置高方差的基础模型，我们应该使用一种倾向于减小方差的聚合方法；而如果我们选择具有低方差高偏置的基础模型，我们应该使用一种倾向于减小偏置的聚合方法。 这就引出了如何组合这些模型的问题。我们可以用三种主要的旨在组合弱学习器的「元算法」： bagging ，该方法通常考虑的是同质弱学习器，相互独立地并行学习这些弱学习器，并按照某种确定性的平均过程将它们组合起来。 boosting ，该方法通常考虑的也是同质弱学习器。它以一种高度自适应的方法顺序地学习这些弱学习器（每个基础模型都依赖于前面的模型），并按照某种确定性的策略将它们组合起来。 stacking ，该方法通常考虑的是异质弱学习器，并行地学习它们，并通过训练一个「元模型」将它们组合起来，根据不同弱模型的预测结果输出一个最终的预测结果。 非常粗略地说，我们可以说 bagging 的重点在于获得一个方差比其组成部分更小的集成模型，而 boosting 和 stacking 则将主要生成偏置比其组成部分更低的强模型（即使方差也可以被减小）。 各类集成学习的特点Bootstrap aggregating 简称为 Bagging。Bagging (Boostrap Aggregating) 是由 Breiman 于 1996 年提出，基本思想如下： 每次采用有放回的抽样从训练集中取出 (n) 个训练样本组成新的训练集。 利用新的训练集，训练得到 (M) 个子模型 ({h_1, h_2, …, h_M})。 对于分类问题，采用投票的方法，得票最多子模型的分类类别为最终的类别；对于回归问题，采用简单的平均方法得到预测值。 Bagging 算法如下所示： 假设对于一个包含 (M) 个样本的数据集 (T)，利用自助采样，则一个样本始终不被采用的概率是 (\left(1 - \frac{1}{M}\right)^M)，取极限有： $$ \lim_{x \to \infty} \left(1 - \dfrac{1}{M}\right)^M = \dfrac{1}{e} \approx 0.368 $$ 即每个学习器仅用到了训练集中 (63.2\%) 的数据集，剩余的 (36.8\%) 的训练集样本可以用作验证集对于学习器的泛化能力进行包外估计 (out-of-bag estimate)。 BoostingBoosting 是一种提升算法，可以将弱的学习算法提升 (boost) 为强的学习算法。基本思路如下： 利用初始训练样本集训练得到一个基学习器。 提高被基学习器误分的样本的权重，使得那些被错误分类的样本在下一轮训练中可以得到更大的关注，利用调整后的样本训练得到下一个基学习器。 重复上述步骤，直至得到 (M) 个学习器。 对于分类问题，采用有权重的投票方式；对于回归问题，采用加权平均得到预测值。 在「顺序化的方法中」，组合起来的不同弱模型之间不再相互独立地拟合。其思想是「迭代地」拟合模型，使模型在给定步骤上的训练依赖于之前的步骤上拟合的模型。「Boosting」是这些方法中最著名的一种，它生成的集成模型通常比组成该模型的弱学习器偏置更小。 Boosting 方法和bagging 方法的工作思路是一样的：我们构建一系列模型，将它们聚合起来得到一个性能更好的强学习器。然而，与重点在于减小方差的 bagging 不同，boosting 着眼于以一种适应性很强的方式顺序拟合多个弱学习器：序列中每个模型在拟合的过程中，会更加重视那些序列中之前的模型处理地很糟糕的观测数据。 直观地说，每个模型都把注意力集中在目前最难拟合的观测数据上。这样一来，在这个过程的最后，我们就获得了一个具有较低偏置的强学习器（我们会注意到，boosting 也有减小方差的效果）。和 bagging 一样，Boosting 也可以用于回归和分类问题。 由于其重点在于减小偏置，用于 boosting 的基础模型通常是那些低方差高偏置的模型。例如，如果想要使用树作为基础模型，我们将主要选择只有少许几层的较浅决策树。而选择低方差高偏置模型作为 boosting 弱学习器的另一个重要原因是：这些模型拟合的计算开销较低（参数化时自由度较低）。 实际上，由于拟合不同模型的计算无法并行处理（与 bagging 不同），顺序地拟合若干复杂模型会导致计算开销变得非常高。一旦选定了弱学习器，我们仍需要定义它们的拟合方式（在拟合当前模型时，要考虑之前模型的哪些信息？）和聚合方式（如何将当前的模型聚合到之前的模型中？）在接下来的两小节中，我们将讨论这些问题，尤其是介绍两个重要的 boosting 算法：自适应提升（adaboost ）和梯度提升（gradient boosting）。这两种元算法在顺序化的过程中创建和聚合弱学习器的方式存在差异。自适应增强算法会更新附加给每个训练数据集中观测数据的权重，而梯度提升算法则会更新这些观测数据的值。这里产生差异的主要原因是：两种算法解决优化问题（寻找最佳模型——弱学习器的加权和）的方式不同。 12345678910111213141516171819from sklearn.ensemble import GradientBoostingClassifier from sklearn.datasets import makeclassification from sklearn.modelselection import traintestsplit from sklearn.metrics import accuracy_score# 生成数据X, y = makeclassification(nsamples=1000, nfeatures=20, ninformative=10, nredundant=10, randomstate=42)# 数据分割Xtrain, Xtest, ytrain, ytest = traintestsplit(X, y, testsize=0.2, randomstate=42)# 模型构建clf = GradientBoostingClassifier(nestimators=100, learningrate=0.1, maxdepth=3, randomstate=42)#模型训练clf.fit(Xtrain, ytrain)# 模型评估ypred = clf.predict(Xtest) print("Accuracy:", accuracyscore(ytest, y_pred)) 堆叠（Stacking）Stacking 是一种用于最小化一个或多个泛化器的泛化误差率的方法。它通过推导泛化器相对于所提供的学习集的偏差来发挥其作用。这个推导的过程包括：在第二层中将第一层的原始泛化器对部分学习集的猜测进行泛化，以及尝试对学习集的剩余部分进行猜测，并且输出正确的结果。当与多个泛化器一起使用时，堆叠泛化可以被看作是一个交叉验证的复杂版本，利用比交叉验证更为复杂的策略来组合各个泛化器。当与单个泛化器一起使用时，堆叠泛化是一种用于估计（然后纠正）泛化器的错误的方法，该泛化器已经在特定学习集上进行了训练并被询问了特定问题。 Criteria 标准 Bagging 套袋 Boosting 提升 Stacking 堆叠 Approach 方法 并行集成学习技术 顺序集成学习技术 元集成学习技术 Base Models 基础型号 独立训练的多个基础模型 按顺序训练的一系列基础模型 多种基础模型与元学习器相结合 Training Method 训练方法 带替换的随机抽样（引导程序） 训练实例的自适应重新加权 结合基础模型的预测 Aggregation 聚合 平均（回归）或投票（分类） 基于绩效的加权投票 通过元学习器进行组合 Bias-Variance Trade-off 偏差-方差权衡 专注于减少方差 专注于减少偏见 可以减少偏差和方差 Model Diversity 模型多样性 具有不同数据子集的相似基础模型 通过关注错误来迭代改进模型 捕捉不同方面的多种基础模型 Interpretability 可解释性 一般可解释（例如，随机森林） 可根据所使用的基本模型进行解释 可以保留基于元学习器的可解释性 Complexity 复杂 通常复杂度较低的模型（例如决策树） 可以适应复杂的模型（例如梯度提升） 灵活，可以包括简单和复杂的模型 Performance 表现 性能稳健，不易过度拟合 通常会产生更高的性能，但可能会过度拟合 性能高度依赖于基础模型和元学习器 常用的集成学习算法实现bagging算法实现： 随机森林：随机森林 (Random Forests) 是一种基于决策树的算法，广泛应用于 Bagging 中。它涉及在训练数据和特征的不同子集上训练多个决策树，然后使用简单的平均或多数投票方法组合它们的输出。当底层模型容易过度拟合或对用于训练的特定特征敏感时，随机森林尤其有效。 Bagged Decision Trees： Bagged Decision Trees 是一种简单的 bagging 算法，涉及在训练数据的不同子集上训练多个决策树。然后使用简单的平均或多数投票方法组合决策树的输出。当底层模型不稳定或容易过度拟合时，Bagged Decision Trees会很有用。 Bagged K-Nearest Neighbors (KNN)： Bagged KNN 是一种装袋算法，涉及在训练数据的不同子集上训练多个 KNN 模型。然后使用简单的平均或多数投票方法组合 KNN 模型的预测。当底层模型容易过度拟合或对用于训练的特定特征敏感时，Bagged KNN 会很有用。 Boosting算法实现： Adaboost-boosting： 其工作原理是迭代地重新加权训练数据中的样本，以便在后续迭代中为错误分类的样本赋予更高的权重。 AdaBoost 在数据不平衡的情况下很有用，因为它有助于提高模型在少数类上的性能。 https://leovan.me/cn/2018/12/ensemble-learning/ GBDT： 基于回归树的梯度提升树模型框架。 XGBoost： XGBoost 是一种基于梯度提升的提升算法，但使用更正则化的模型公式来控制过度拟合。当底层模型容易过度拟合或对用于训练的特定特征敏感时，XGBoost 会特别有效。 LightGBM: GBDT 模型在查找最优分裂点时需要扫描所有的样本计算信息增益，因此其计算复杂度与样本的数量和特征的数量成正比，这使得在处理大数据量的问题时非常耗时。LightGBM 是针对这个问题的一种改进框架。 常见集成方式不论我们使用那种集成模型，bagging和stacking都会基于不同的基学习器产生多个不同的分类结果，所以我们最终需要对所有分类器的结果进行集成，得到最终结果。针对多个子分类器进行集成的方法整体分为两类： 对于回归预测（数值预测） 简单平均（Simple Average），就是取各个分类器结果的平均值。$$G(x) = \frac{1}{T}\sum\limits_{t = 1}^T {g_t(x)}$$ 加权平均（Weighted Average）。$$ G(x) = \frac{1}{T}\sum\limits_{t = 1}^T {\alpha _t \cdot {g_t}(x)} ,{\alpha _t} \ge 0 $$ 对于分类（类别预测） 简单投票（Majority Voting）：就是每个分类器的权重大小一样，少数服从多数，类别得票数超过一半的作为分类结果 。$$ G(x) = sign\left( {\sum\limits_{t = 1}^T {1 \cdot {g_t}(x)} } \right) $$ 加权投票（Weighted Majority Voting）：每个分类器权重不一。$$ G(x) = sign\left( {\sum\limits_{t = 1}^T {\alpha _t \cdot {g_t}(x)} } \right),{\alpha _t} \ge 0 $$ 概率投票（Soft vote）：有的分类器的输出是有概率信息的，因此可用概率投票。 应用集成学习技术已成功应用于广泛的机器学习应用中。这些应用程序的一些示例包括： 图像识别：集成学习已被用来提高图像识别算法的准确性，特别是在图像复杂或有噪声的情况下。 自然语言处理：集成学习已被用来提高自然语言处理算法的准确性，特别是在语言复杂或模糊的情况下。 推荐系统：集成学习已被用来提高推荐系统的准确性，特别是在底层数据有噪声或不完整的情况下。 异常检测：集成学习已用于提高异常检测算法的准确性，特别是在基础数据高度可变或嘈杂的情况下。 references 集成学习算法 (Ensemble Learning)]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>Machine-Learning</tag>
        <tag>集成学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大模型-应用-加载数据]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-25.LLM-%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86%2F4021.%E5%A4%A7%E6%A8%A1%E5%9E%8B-%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%80-datasets-%E5%8A%A0%E8%BD%BD%E6%95%B0%E6%8D%AE%2F</url>
    <content type="text"><![CDATA[在我们进行大模型的训练或微调的过程中，第一步就是要准备自己的训练数据集并导入到代码中。而 datasets 是huggingface开发的一个数据集python库，可以很方便的从Hugging Face Hub里下载数据，也可很方便的从本地加载数据集，本文主要对load_dataset方法的使用进行详细说明。 load_dataset的参数说明如下：123456789101112131415def load_dataset( path: str, name: Optional[str] = None, data_dir: Optional[str] = None, data_files: Union[Dict, List] = None, split: Optional[Union[str, Split]] = None, cache_dir: Optional[str] = None, features: Optional[Features] = None, download_config: Optional[DownloadConfig] = None, download_mode: Optional[GenerateMode] = None, ignore_verifications: bool = False, save_infos: bool = False, script_version: Optional[Union[str, Version]] = None, **config_kwargs,) -&gt; Union[DatasetDict, Dataset]: 常用且比较重要的参数如下： path参数path表示数据集的名字或者路径。可以是如下几种形式（每种形式的使用方式后面会详细说明） 数据集的名字，比如imdb、glue 数据集文件格式，比如json、csv、parquet、txt 数据集目录中的处理数据集的脚本（.py)文件，比如“glue/glue.py” name：参数name表示数据集中的子数据集，当一个数据集包含多个数据集时，就需要这个参数，比如glue数据集下就包含”sst2”、“cola”、”qqp”等多个子数据集，此时就需要指定name来表示加载哪一个子数据集 data_dir：数据集所在的目录 data_files：数据集文件 cache_dir：构建的数据集缓存目录，方便下次快速加载 示例查看数据集12345from datasets import list_datasetsdatasets_list = list_datasets()print( len(datasets_list))print(datasets_list[:10]) 我们就可以得到 hugging Face上的所有数据集列表1247660[&apos;acronym_identification&apos;, &apos;ade_corpus_v2&apos;, &apos;adversarial_qa&apos;, &apos;aeslc&apos;, &apos;afrikaans_ner_corpus&apos;, &apos;ag_news&apos;, &apos;ai2_arc&apos;, &apos;air_dialogue&apos;, &apos;ajgt_twitter_ar&apos;, &apos;allegro_reviews&apos;] 下载数据集官方下载后面通过直接指定 path 等于相关数据集的名字就能下载并加载相关数据集 12from datasets import load_datasetdataset = load_dataset(path=&apos;squad&apos;, split=&apos;train&apos;) 本地加载数据集加载指定格式的文件用path参数指定数据集格式 - son格式，path=&quot;json&quot; - csv格式， path=&quot;csv&quot; - 纯文本格式, path=&quot;text&quot; - dataframe格式， path=&quot;panda&quot; - 图片，path=&quot;imagefolder&quot; 然后用data_files指定文件名称，data_files可以是字符串，列表或者字典，data_dir指定数据集目录。如下case 12345from datasets import load_datasetdataset = load_dataset('csv', data_files='my_file.csv') #加载一个文件，默认数据集名称为 traindataset = load_dataset('csv', data_files=['my_file_1.csv', 'my_file_2.csv', 'my_file_3.csv']) # 所有数据都会汇集到 train 数据集中dataset = load_dataset('csv', data_files=&#123;'train':['my_train_file_1.csv','my_train_file_2.csv'],'test': 'my_test_file.csv'&#125;) # 加载多个文件并指定数据集名称dataset = load_dataset("csv",data_dir="path/2/data/") # 加载目录下的所有csv类型文件，文件名前缀作为数据集名称 加载图片如下我们通过打开指定图片目录进行加载图片数据集1234dataset = load_dataset(path=&quot;imagefolder&quot;, data_dir=&quot;D:\Desktop\workspace\code\loaddataset\data\images&quot;)print(dataset)print(dataset[&quot;train&quot;][0]) 很多情况下加载图片并非只要图片，还会有对应的文本，比如在图片分类的时候，每张图片都对应一个类别。这种情况我们需要在图片所在文件夹中加入一个metadata.jsonl的文件，来指定每个图片对应的类别，格式如下，注意file_name字段必须要有，其他字段可自行命名 12345678&#123; "file_name": "1.jpg", "class": 1&#125;&#123; "file_name": "2.png", "class": 0&#125; 复杂数据加载一些情况下加载数据集的逻辑较为复杂，需要自定义加载方式。比如训练ControlNet时，输入有原始图片，边缘图，以及prompt，这时候我们就需要通过在图片所在的目录下写一个python脚本来处理数据加载方式。如下所示，我们数据处理需要是，每条数据包括两张图片，一个文本。step1: 首先我们先创建一个json文件train.jsonl，把图片和文本对应起来，json文件的格式如下所示123&#123;&quot;text&quot;: &quot;pale golden rod circle with old lace background&quot;, &quot;image&quot;: &quot;images/0.png&quot;, &quot;conditioning_image&quot;: &quot;conditioning_images/0.png&quot;&#125;&#123;&quot;text&quot;: &quot;light coral circle with white background&quot;, &quot;image&quot;: &quot;images/1.png&quot;, &quot;conditioning_image&quot;: &quot;conditioning_images/1.png&quot;&#125;&#123;&quot;text&quot;: &quot;aqua circle with light pink background&quot;, &quot;image&quot;: &quot;images/2.png&quot;, &quot;conditioning_image&quot;: &quot;conditioning_images/2.png&quot;&#125; step2：创建一个python脚本fill50k.py根据json文件中的对应关系加载图片，python脚本如下所示，这个脚本中定义一个 Fill50k类，并继承datasets.GeneratorBasedBuilder，在类中重写_info(self): _split_generators(self, dl_manager)和_split_generators(self, dl_manager)这三个方法12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788import pandas as pdimport datasetsimport osimport logging# 数据集路径设置META_DATA_PATH = "D:\Desktop\workspace\code\loaddataset\\fill50k\\train.jsonl"IMAGE_DIR = "D:\Desktop\workspace\code\loaddataset\\fill50k"CONDITION_IMAGE_DIR = "D:\Desktop\workspace\code\loaddataset\\fill50k"# 定义数据集中有哪些特征，及其类型_FEATURES = datasets.Features( &#123; "image": datasets.Image(), "conditioning_image": datasets.Image(), "text": datasets.Value("string"), &#125;,)# 定义数据集class Fill50k(datasets.GeneratorBasedBuilder): BUILDER_CONFIGS = [datasets.BuilderConfig(name="default", version=datasets.Version("0.0.2"))] DEFAULT_CONFIG_NAME = "default" def _info(self): return datasets.DatasetInfo( description="None", features=_FEATURES, supervised_keys=None, homepage="None", license="None", citation="None", ) def _split_generators(self, dl_manager): return [ datasets.SplitGenerator( name=datasets.Split.TRAIN, # These kwargs will be passed to _generate_examples gen_kwargs=&#123; "metadata_path": META_DATA_PATH, "images_dir": IMAGE_DIR, "conditioning_images_dir": CONDITION_IMAGE_DIR, &#125;, ), ] def _generate_examples(self, metadata_path, images_dir, conditioning_images_dir): metadata = pd.read_json(metadata_path, lines=True) for _, row in metadata.iterrows(): text = row["text"] image_path = row["image"] image_path = os.path.join(images_dir, image_path) # 打开文件错误时直接跳过 try: image = open(image_path, "rb").read() except Exception as e: logging.error(e) continue conditioning_image_path = os.path.join( conditioning_images_dir, row["conditioning_image"] ) # 打开文件错误直接跳过 try: conditioning_image = open(conditioning_image_path, "rb").read() except Exception as e: logging.error(e) continue yield row["image"], &#123; "text": text, "image": &#123; "path": image_path, "bytes": image, &#125;, "conditioning_image": &#123; "path": conditioning_image_path, "bytes": conditioning_image, &#125;, &#125; step3: 通过load_dataset加载数据集123456789101112131415161718192021dataset = load_dataset(path=&quot;D:\Desktop\workspace\code\loaddataset\\fill50k\\fill50k.py&quot;, cache_dir=&quot;D:\Desktop\workspace\code\loaddataset\\fill50k\cache&quot;)print(dataset)print(dataset[&quot;train&quot;][0])# 历史问题及相关处理记录## datasets-2.11.0 本地文件系统不支持```python&gt;&gt;&gt; dataset = load_dataset(&quot;csv&quot;,data_dir=data_dir,data_files=data_files)Downloading and preparing dataset csv/default to file:///home/phoenix/.cache/huggingface/datasets/csv/default-df2512f4672b5719/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1...Downloading data files: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00&lt;00:00, 7839.82it/s]Extracting data files: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00&lt;00:00, 1653.91it/s]Dataset csv downloaded and prepared to file:///home/phoenix/.cache/huggingface/datasets/csv/default-df2512f4672b5719/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1. Subsequent calls will reuse this data.Traceback (most recent call last): File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt; File &quot;/home/phoenix/anaconda3/envs/liubo_llm/lib/python3.8/site-packages/datasets/load.py&quot;, line 1804, in load_dataset ds = builder_instance.as_dataset(split=split, verification_mode=verification_mode, in_memory=keep_in_memory) File &quot;/home/phoenix/anaconda3/envs/liubo_llm/lib/python3.8/site-packages/datasets/builder.py&quot;, line 1108, in as_dataset raise NotImplementedError(f&quot;Loading a dataset cached in a &#123;type(self._fs).__name__&#125; is not supported.&quot;)NotImplementedError: Loading a dataset cached in a LocalFileSystem is not supported. datasets 新版本已解决，安装旧版本datasets导致，进行版本升级即可 pip install -U datasets 。测试升级至 datasets-2.20.0 问题解决。]]></content>
      <categories>
        <category>LLM</category>
      </categories>
      <tags>
        <tag>LLM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2105.神经网络-Transformer]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-20.MachineLearn%2F2105.%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-Transformer%2F</url>
    <content type="text"><![CDATA[论文原文：Attention Is All You Need Transformer之前我们比较了卷积神经网络（CNN）、循环神经网络（RNN）和自注意力（self-attention）。值得注意的是，自注意力同时具有并行计算和最短的最大路径长度这两个优势。因此，使用自注意力来设计深度架构是很有吸引力的。对比之前仍然依赖循环神经网络实现输入表示的自注意力模型 (Cheng et al., 2016, Lin et al., 2017, Paulus et al., 2017)，Transformer模型完全基于注意力机制，没有任何卷积层或循环神经网络层 (Vaswani et al., 2017)。尽管Transformer最初是应用于在文本数据上的序列到序列学习，但现在已经推广到各种现代的深度学习中，例如语言、视觉、语音和强化学习领域。 Transformer 作为一个编码器-解码器架构，结构示意如下： 编码器从宏观角度来看，Transformer的编码器是由多个相同的层叠加而成的，每个层都有两个子层（子层表示为 sublayer）。第一个子层是多头自注意力（multi-head self-attention）汇聚；第二个子层是基于位置的前馈网络（positionwise feed-forward network）。具体来说，在计算编码器的自注意力时，查询、键和值都来自前一个编码器层的输出。受 7.6节中残差网络的启发，每个子层都采用了残差连接（residual connection）。在Transformer中，对于序列中任何位置的任何输入 $x \in R^d$，都要求满足 $sublayer(x) \in R^d$，以便残差连接满足 $x+sublayer(x) \in R^d$。在残差连接的加法计算之后，紧接着应用层规范化（layer normalization） (Ba et al., 2016)。因此，输入序列对应的每个位置，Transformer编码器都将输出一个 $d$ 维表示向量。 解码器Transformer解码器也是由多个相同的层叠加而成的，并且层中使用了残差连接和层规范化。除了编码器中描述的两个子层之外，解码器还在这两个子层之间插入了第三个子层，称为编码器－解码器注意力（encoder-decoder attention）层。在编码器－解码器注意力中，查询来自前一个解码器层的输出，而键和值来自整个编码器的输出。在解码器自注意力中，查询、键和值都来自上一个解码器层的输出。但是，解码器中的每个位置只能考虑该位置之前的所有位置。这种掩蔽（masked）注意力保留了自回归（auto-regressive）属性，确保预测仅依赖于已生成的输出词元。 接下来基于代码进行模型的实现1234import numpy as npimport pandas as pdimport tensorflow as tffrom d2l import tensorflow as d2l 基于位置的前馈网络1234567891011#@saveclass PositionWiseFFN(tf.keras.layers.Layer): """基于位置的前馈网络""" def __init__(self, ffn_num_hiddens, ffn_num_outputs, **kwargs): super().__init__(*kwargs) self.dense1 = tf.keras.layers.Dense(ffn_num_hiddens) self.relu = tf.keras.layers.ReLU() self.dense2 = tf.keras.layers.Dense(ffn_num_outputs) def call(self, X): return self.dense2(self.relu(self.dense1(X))) 残差连接和层规范化编码器解码器reference]]></content>
      <categories>
        <category>neural_network</category>
      </categories>
      <tags>
        <tag>neural_network</tag>
        <tag>transformer</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python-包-增效-pipreqs]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-02.python%2FPython-%E5%8C%85-%E5%A2%9E%E6%95%88-%E5%8C%85%E7%AE%A1%E7%90%86-pipreqs%2F</url>
    <content type="text"><![CDATA[我们都知道获取环境中的所有依赖包命令：1pip freeze &gt; ./requirements.txt 但是，如果我们仅仅想获取当前项目中的安装包，我们可以使用pipreqs工具安装：1pip install pipreqs 在项目根目录下运行如下命令：1pipreqs ./ 执行完上述命令后，则会在当前目录下生成包含当前项目所有依赖包文件requirements.txt后续可以在新环境下安装相关依赖：1pip install -r requirements.txt]]></content>
      <categories>
        <category>Tobeinsert</category>
      </categories>
      <tags>
        <tag>Tobeinsert</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2105.神经网络-注意力机制]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-20.MachineLearn%2F2105.%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%2F</url>
    <content type="text"><![CDATA[我们的视觉系统每时每刻都在接收大量的感官输入， 这些感官输入远远超过了大脑能够完全处理的程度。 然而，并非所有刺激的影响都是相等的。 意识的聚集和专注使灵长类动物能够在复杂的视觉环境中将注意力引向感兴趣的物体。 只关注一小部分信息的能力对进化更加有意义，使人类得以生存和成功。 注意力提示此刻读者正在阅读本书（而忽略了其他的书）， 因此读者的注意力是用机会成本（与金钱类似）来支付的。 为了确保读者现在投入的注意力是值得的， 作者们尽全力（全部的注意力）创作一本好书。 自经济学研究稀缺资源分配以来，人们正处在“注意力经济”时代， 即人类的注意力被视为可以交换的、有限的、有价值的且稀缺的商品。 许多商业模式也被开发出来去利用这一点： 在音乐或视频流媒体服务上，人们要么消耗注意力在广告上，要么付钱来隐藏广告； 为了在网络游戏世界的成长，人们要么消耗注意力在游戏战斗中， 从而帮助吸引新的玩家，要么付钱立即变得强大。 总之，注意力不是免费的。 注意力是稀缺的，而环境中的干扰注意力的信息却并不少。 比如人类的视觉神经系统大约每秒收到 $10^8$ 位的信息， 这远远超过了大脑能够完全处理的水平。 幸运的是，人类的祖先已经从经验（也称为数据）中认识到 “并非感官的所有输入都是一样的”。 在整个人类历史中，这种只将注意力引向感兴趣的一小部分信息的能力， 使人类的大脑能够更明智地分配资源来生存、成长和社交， 例如发现天敌、找寻食物和伴侣。 注意力提示的方式生物学中的注意力提示注意力是如何应用于视觉世界中的呢？ 这要从当今十分普及的双组件（two-component）的框架开始讲起： 这个框架的出现可以追溯到19世纪90年代的威廉·詹姆斯， 他被认为是“美国心理学之父” (James, 2007)。 在这个框架中，受试者基于非自主性提示和自主性提示 有选择地引导注意力的焦点。 非自主性提示是基于环境中物体的突出性和易见性。 想象一下，假如我们面前有五个物品： 一份报纸、一篇研究论文、一杯咖啡、一本笔记本和一本书。 所有纸制品都是黑白印刷的，但咖啡杯是红色的。 换句话说，这个咖啡杯在这种视觉环境中是突出和显眼的， 不由自主地引起人们的注意。 所以我们会把视力最敏锐的地方放到咖啡上， 如图所示。 喝咖啡后，我们会变得兴奋并想读书， 所以转过头，重新聚焦眼睛，然后看看书， 就像下图描述那样。 与上图中的由于突出性导致的选择不同， 此时选择书是受到了认知和意识的控制， 因此注意力在基于自主性提示去辅助选择时将更为谨慎。 受试者的主观意愿推动，选择的力量也就更强大。 查询、键和值 下面来看看如何通过这两种注意力提示， 用神经网络来设计注意力机制的框架， 首先，考虑一个相对简单的状况， 即只使用非自主性提示。 要想将选择偏向于感官输入， 则可以简单地使用参数化的全连接层， 甚至是非参数化的最大汇聚层或平均汇聚层。非自主性提示，我们可以理解为所有输入的权重是一样的，输入会无差别流入下一层。 自主性提示就是将注意力机制与全连接层或汇聚层区别开来。在注意力机制的背景下，自主性提示被称为查询（query）。 给定任何查询，注意力机制通过注意力汇聚（attention pooling） 将选择引导至感官输入（sensory inputs，例如中间特征表示）。 在注意力机制中，这些感官输入被称为值（value）。 更通俗的解释，每个值都与一个键（key）配对， 这可以想象为感官输入的非自主提示。 如下图所示，可以通过设计注意力汇聚的方式， 便于给定的查询（自主性提示）与键（非自主性提示）进行匹配， 这将引导得出最匹配的值（感官输入）。 Keys（键）：在非自主提示下，进入视觉系统的的所有元素的线索，称为 Keys。 Query（查询）：在自主提示下，自主提示的内容或元素的线索，称为 Query。 Values（值)：在由自主提示 Query 限制或者强制下改变注意力的焦点，也就是经过从 Keys 中进行匹配 Query，所得到的进入视觉系统的内容，称为 Values。 注意力机制通过注意力汇聚将查询（自主性提示）和键（非自主性提示）结合在一起，实现对值（感官输入）的选择倾向。自主性提示，相当于我们会通过提供的查询 ，根据输入信息的 键和提供的查询 基于特定的规则进行相关性的计算，从而实现对所有输入的信息实现加权（注意力汇聚），最终只是选择权重重要的信息流入下游。 自主性的与非自主性的注意力提示解释了人类的注意力的方式，使用神经网络来设计注意力机制的框架，就是基于上述非自主性提示和自主性提示的原理，来选择要聚焦的观察对象。具体抽象出来的元素就是最基本的 Key、Query、Value，如下所示： 注意力汇聚Nadaraya-Watson 核回归查询（自主提示）和键（非自主提示）之间的交互形成了注意力汇聚； 注意力汇聚有选择地聚合了值（感官输入）以生成最终的输出。接下来我们来了解下注意力汇聚的细节。1964年提出的Nadaraya-Watson核回归模型是一个简单但完整的例子，这里使用它演示具有注意力机制的机器学习。 生成一组测试数据根据下面的非线性函数生成一个人工数据集， 其中 $\epsilon$ 数据模拟过程中引入的噪音：$$y_i=2\sin(x_i)+x_i^{0.8}+\epsilon$$其中 $\epsilon$ 服从均值为 0 和标准差为 0.5 的正态分布。 在这里生成了 50 个训练样本和 50 个测试样本。 为了更好地可视化之后的注意力模式，需要将训练样本进行排序。1234567891011121314n_train = 50x_train = tf.sort(tf.random.uniform(shape=(n_train,), maxval=5))def f(x): return 2 * tf.sin(x) + x**0.8y_train = f(x_train) + tf.random.normal((n_train,), 0.0, 0.5) # 训练样本的输出x_test = tf.range(0, 5, 0.1) # 测试样本y_truth = f(x_test) # 测试样本的真实输出n_test = len(x_test) # 测试样本数# 下面的函数将绘制所有的训练样本（样本由圆圈表示）， 不带噪声项的真实数据生成函数（标记为“Truth”）， 以及学习得到的预测函数（标记为“Pred”）。def plot_kernel_reg(y_hat): d2l.plot(x_test, [y_truth, y_hat], 'x', 'y', legend=['Truth', 'Pred'], xlim=[0, 5], ylim=[-1, 5]) d2l.plt.plot(x_train, y_train, 'o', alpha=0.5); 上面这部分，我们准备好了进行测试的模拟数据，同时准备好预测结果的可视化函数。 平均汇聚在我们不使用注意力汇聚模型的时候，那么很容易理解，我们模型的所有输入都是等权重的。所以我们会很容易的得到一个简单的估计器：$$f(x)=\frac{1}{n}\sum_{i=1}^n y_i$$因为没有使用注意力汇集，所有输入是等权重的，所以我们得到的是一个直线$f(x)=\overline{y}$，预测结果和$x_i$ 直接无关了，显然这个模型表现并不好。 非参数注意力汇聚为了让我们能获得一个更好的模型，显然，我们必须考虑输入的 $x$ 值（因为我们知道，$y$ 是基于 $x$ 值计算得到的）。于是Nadaraya (Nadaraya, 1964)和 Watson (Watson, 1964)提出了一个更好的想法， 根据输入的位置对输出 $y_i$ 进行加权：$$f(x)=\sum_{i=1}^n \frac{K(x-x_i)}{\sum_{j=1}^n K(x-x_j)}y_i $$其中$K$是核函数（kernel），为了便于理解，当我们假设 $y_i$ 恒等于1 时，那么针对x的权重进行积分，可以得到权重综合不管我们的核函数 $K$ 是如何定义的，最终所有位点的权重积分始终为1。所以我们可以根据我们的需要独立的进行核函数的定义，而不会影响整体的权重分配。$$f(x)= \frac{\sum_{i=1}^n K(x-x_i)}{\sum_{j=1}^n K(x-x_j)} = 1$$所以在我们定义好 $K$ 后，基于上述方案，当我们的输入（要查询的值） $x$ 确定后，就可以计算获得每个输入$y_i$所占的的权重（一般越接近待查询值$x$的键$x_i$ 获得的权重越大），然后对所有输入$y_i$进行加权求和（注意力汇聚），得到一个更好的模型。 值得注意的是，Nadaraya-Watson核回归是一个非参数模型。 因此上述该方法也是 非参数的注意力汇聚（nonparametric attention pooling）模型。我们基于这个非参数的注意力汇聚模型来绘制预测结果如下图，可以看到新的模型预测线是平滑的，并且比平均汇聚的预测更接近真实。同时我们观察注意力的权重。 这里测试数据的输入相当于查询，而训练数据的输入相当于键。 因为两个输入都是经过排序的，因此由观察可知“查询-键”对越接近， 注意力汇聚的注意力权重就越高。 带参数注意力汇聚非参数的Nadaraya-Watson核回归具有一致性（consistency）的优点： 如果有足够的数据，此模型会收敛到最优结果。 尽管如此，我们还是可以轻松地将可学习的参数集成到注意力汇聚中,和之前的区别，主要是我们定义好核函数$K$ 计算$x$到每个$x_i$的距离后，我们再引入一个参数 $w$ 对距离进行修正后计算权重$$f(x)=\sum_{i=1}^n \frac{K(x-x_i)w}{\sum_{j=1}^n K(x-x_j)w}y_i $$ 代码实现定义Nadaraya-Watson核回归的带参数版本：123456789101112class NWKernelRegression(tf.keras.layers.Layer): def __init__(self, **kwargs): super().__init__(**kwargs) self.w = tf.Variable(initial_value=tf.random.uniform(shape=(1,))) def call(self, queries, keys, values, **kwargs): # 对于训练，“查询”是x_train。“键”是每个点的训练数据的距离。“值”为'y_train'。 # queries和attention_weights的形状为(查询个数，“键－值”对个数) queries = tf.repeat(tf.expand_dims(queries, axis=1), repeats=keys.shape[1], axis=1) self.attention_weights = tf.nn.softmax(-((queries - keys) * self.w)**2 /2, axis =1) # values的形状为(查询个数，“键－值”对个数) return tf.squeeze(tf.matmul(tf.expand_dims(self.attention_weights, axis=1), tf.expand_dims(values, axis=-1))) 将训练数据集变换为键和值用于训练注意力模型。 在带参数的注意力汇聚模型中， 任何一个训练样本的输入都会和除自己以外的所有训练样本的“键－值”对进行计算， 从而得到其对应的预测输出。12345678# X_tile的形状:(n_train，n_train)，每一行都包含着相同的训练输入X_tile = tf.repeat(tf.expand_dims(x_train, axis=0), repeats=n_train, axis=0)# Y_tile的形状:(n_train，n_train)，每一行都包含着相同的训练输出Y_tile = tf.repeat(tf.expand_dims(y_train, axis=0), repeats=n_train, axis=0)# keys的形状:('n_train'，'n_train'-1)keys = tf.reshape(X_tile[tf.cast(1 - tf.eye(n_train), dtype=tf.bool)], shape=(n_train, -1))# values的形状:('n_train'，'n_train'-1)values = tf.reshape(Y_tile[tf.cast(1 - tf.eye(n_train), dtype=tf.bool)], shape=(n_train, -1)) 训练带参数的注意力汇聚模型时，使用平方损失函数和随机梯度下降。12345678910111213net = NWKernelRegression()loss_object = tf.keras.losses.MeanSquaredError()optimizer = tf.keras.optimizers.SGD(learning_rate=0.5)animator = d2l.Animator(xlabel='epoch', ylabel='loss', xlim=[1, 5])for epoch in range(5): with tf.GradientTape() as t: loss = loss_object(y_train, net(x_train, keys, values)) * len(y_train) grads = t.gradient(loss, net.trainable_variables) optimizer.apply_gradients(zip(grads, net.trainable_variables)) print(f'epoch &#123;epoch + 1&#125;, loss &#123;float(loss):.6f&#125;') animator.add(epoch + 1, float(loss)) 损失函数下降曲线如下图：1234567891011# keys的形状:(n_test，n_train)，每一行包含着相同的训练输入（例如，相同的键）keys = tf.repeat(tf.expand_dims(x_train, axis=0), repeats=n_test, axis=0)# value的形状:(n_test，n_train)values = tf.repeat(tf.expand_dims(y_train, axis=0), repeats=n_test, axis=0)y_hat = net(x_test, keys, values)plot_kernel_reg(y_hat)# 注意力权重绘图d2l.show_heatmaps(tf.expand_dims( tf.expand_dims(net.attention_weights, axis=0), axis=0), xlabel='Sorted training inputs', ylabel='Sorted testing inputs') 带参数注意力汇聚的拟合结果和注意力权重热图如下：与非参数的注意力汇聚模型相比， 带参数的模型加入可学习的参数后， 曲线在注意力权重较大的区域变得更不平滑。 注意力评分在我们进行注意力汇聚介绍时，提到一个重要的函数 $\alpha(x-x_i) = \frac{K(x-x_i)w}{\sum_{j=1}^n K(x-x_j)w}$ ，它可以帮我们衡量不同查询和键之间的距离，从而确定给定查询下，各个键的权重分布。$K(x-x_i)$ 又称为 注意力评分函数 可以帮助我们衡量查询$q$和键$x_i$之间的相似性/距离，进而确定每个 $y_i$的权重，最后得到的注意力汇聚的输出就是基于这些注意力权重的值的加权和。所以显然，不同的注意力评分函数的选择，会给我们带来不同的权重计算方式，这里记录几个流行的评分函数。 masked softmax operation 掩蔽softmax操作在某些情况下，并非所有的值都应该被纳入到注意力汇聚中。为了仅将有意义的词元作为值来获取注意力汇聚， 可以指定一个有效序列长度（即词元的个数）， 以便在计算softmax时过滤掉超出指定范围的位置。 下面的masked_softmax函数 实现了这样的掩蔽softmax操作（masked softmax operation）， 其中任何超出有效长度的位置都被掩蔽并置为0。12345678910111213141516171819202122232425262728293031323334353637#@savedef masked_softmax(X, valid_lens): &quot;&quot;&quot;通过在最后一个轴上掩蔽元素来执行softmax操作&quot;&quot;&quot; # X:3D张量，valid_lens:1D或2D张量 if valid_lens is None: return tf.nn.softmax(X, axis=-1) else: shape = X.shape if len(valid_lens.shape) == 1: valid_lens = tf.repeat(valid_lens, repeats=shape[1]) else: valid_lens = tf.reshape(valid_lens, shape=-1) # 最后一轴上被掩蔽的元素使用一个非常大的负值替换，从而其softmax输出为0 X = d2l.sequence_mask(tf.reshape(X, shape=(-1, shape[-1])), valid_lens, value=-1e6) return tf.nn.softmax(tf.reshape(X, shape=shape), axis=-1)# 考虑由两个 2*4 矩阵表示的样本， 这两个样本的有效长度分别为2和3。 经过掩蔽softmax操作，超出有效长度的值都被掩蔽为0。masked_softmax(tf.random.uniform(shape=(2, 2, 4)), tf.constant([2, 3]))&lt;tf.Tensor: shape=(2, 2, 4), dtype=float32, numpy=array([[[0.42428792, 0.575712 , 0. , 0. ], [0.5350254 , 0.46497452, 0. , 0. ]], [[0.32676727, 0.47748092, 0.19575192, 0. ], [0.43579608, 0.30782804, 0.2563759 , 0. ]]], dtype=float32)&gt;# 也可以使用二维张量，为矩阵样本中的每一行指定有效长度。masked_softmax(tf.random.uniform(shape=(2, 2, 4)), tf.constant([[1, 3], [2, 4]]))&lt;tf.Tensor: shape=(2, 2, 4), dtype=float32, numpy=array([[[1. , 0. , 0. , 0. ], [0.40658087, 0.38102806, 0.21239112, 0. ]], [[0.3735564 , 0.6264436 , 0. , 0. ], [0.3183936 , 0.22352162, 0.18998112, 0.26810366]]], dtype=float32)&gt; additive attention 加性注意力缩放点积注意力多头注意力当给定相同的查询、键和值的集合时， 我们希望模型可以基于相同的注意力机制学习到不同的行为， 然后将不同的行为作为知识组合起来， 捕获序列内各种范围的依赖关系 （例如，短距离依赖和长距离依赖关系）。 因此，允许注意力机制组合使用查询、键和值的不同 子空间表示（representation subspaces）可能是有益的。 为此，与其只使用单独一个注意力汇聚， 我们可以用独立学习得到的 $h$ 组不同的线性投影（linear projections）来变换查询、键和值。 然后，这组变换后的查询、键和值将并行地送到注意力汇聚中。 最后，将这个注意力汇聚的输出拼接在一起， 并且通过另一个可以学习的线性投影进行变换， 以产生最终输出。 这种设计被称为多头注意力（multihead attention） (Vaswani et al., 2017)。下图展示了使用全连接层来实现可学习的线性变换的多头注意力。 自主机制和位置编码自注意力、卷积神经网络和循环神经网络目标都是将由个词元组成的序列映射到另一个长度相等的序列，其中的每个输入词元或输出词元都由维向量表示。具体来说，将比较的是卷积神经网络、循环神经网络和自注意力这几个架构的计算复杂性、顺序操作和最大路径长度。请注意，顺序操作会妨碍并行计算，而任意的序列位置组合之间的路径越短，则能更轻松地学习序列中的远距离依赖关系 (Hochreiter et al., 2001)[https://www.bioinf.jku.at/publications/older/ch7.pdf] 考虑一个卷积核大小为 $3$ 的卷积层。 在后面的章节将提供关于使用卷积神经网络处理序列的更多详细信息。 目前只需要知道的是，由于序列长度是，输入和输出的通道数量都是， 所以卷积层的计算复杂度为 $O(knd^2) $。 如 上图所示， 卷积神经网络是分层的，因此为有 $O(1)$ 个顺序操作， 最大路径长度为 $O(k/n)$。 例如，$x_1$和$x_5$ 处于 图中卷积核大小为 3 的双层卷积神经网络的感受野内。 当更新循环神经网络的隐状态时，$d*d$ 权重矩阵和维隐状态的乘法计算复杂度为 $O(d^2)$。 由于序列长度为 $n$，因此循环神经网络层的计算复杂度为 $O(nd^2)$。 根据上图，有 $O(n)$个顺序操作无法并行化，最大路径长度也是 $O(n)$。 在自注意力中，查询、键和值都是 $nd$ 矩阵。 考虑其中缩放的”点－积“注意力， 其中 $nd$ 矩阵乘以 $dn$ 矩阵。 之后输出的 $nn$ 矩阵乘以矩阵 $n*d$。 因此，自注意力具有计算复杂性 $O(n^2d)$。每个词元都通过自注意力直接连接到任何其他词元。 因此，有$O(1)$个顺序操作可以并行计算， 最大路径长度也是$O(1)$。 总而言之，卷积神经网络和自注意力都拥有并行计算的优势， 而且自注意力的最大路径长度最短。 但是因为其计算复杂度是关于序列长度的二次方，所以在很长的序列中计算会非常慢。 位置编码在处理词元序列时，循环神经网络是逐个的重复地处理词元的， 而自注意力则因为并行计算而放弃了顺序操作。 为了使用序列的顺序信息，通过在输入表示中添加 位置编码（positional encoding）来注入绝对的或相对的位置信息。 位置编码可以通过学习得到也可以直接固定得到。 接下来描述的是基于正弦函数和余弦函数的固定位置编码 (Vaswani et al., 2017)。$$p_{i,2j}=\sin({i \over {10000^{2j/d}}})$$$$p_{i,2j+1}=\cos({i \over {10000^{2j/d}}})$$分别定义了 $2j$ 和 $2j+1$ 的位置编码（越靠前的列三角函数波动频率越高）。 其中 $i$ 是词元索引(每一行对应输入的一个词元位置)， $d$ 是词元表示的维数。同时可以看到越靠前的编码位置频率越高 绝对位置信息为了明白沿着编码维度单调降低的频率与绝对位置信息的关系， 让我们打印出的二进制表示形式。 正如所看到的，每个数字、每两个数字和每四个数字上的比特值 在第一个最低位、第二个最低位和第三个最低位上分别交替。123456780的二进制是：0001的二进制是：0012的二进制是：0103的二进制是：0114的二进制是：1005的二进制是：1016的二进制是：1107的二进制是：111 在二进制表示中，较高比特位的交替频率低于较低比特位， 与下面的热图所示相似，只是位置编码通过使用三角函数在编码维度上降低频率。 由于输出是浮点数，因此此类连续表示比二进制表示法更节省空间。所以可以理解为基于三角函数的固定位置编码和基于二进制的编码类型，只是二进制中使用的是0和1作为频率波动的幅度，而三角函数中使用的是正弦和余弦函数自身的波动性。二进制中的位置关系，在位置编码矩阵中对应矩阵的不同列。基于三角函数编码位置其本质和基于二进制并无区别，只是利用了三角函数输出是浮点数，连续的表示比二进制表示能更好的节省空间。]]></content>
      <categories>
        <category>neural_network</category>
      </categories>
      <tags>
        <tag>neural_network</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[markdown-数学公式]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-06.markdown%2Fmarkdown-%E6%95%B0%E5%AD%A6%E5%85%AC%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[更多符合可以再 https://www.latexlive.com/## 查阅获取。 符号 代码 描述 $\vec{V}$ 向量 V $\overrightarrow{V}$ 向右向量 V $\overleftarrow{V}$ 向左向量 V $\sim$ \$\sim\$ 满足分布 $\sum$ \$\sum\$ 求和公式 $\sum_{i=0}^n$ \$\sum_{i=0}^n\$ 求和上下标 $\times$ \$\times\$ 乘号 $\pm$ \$\pm\$ 正负号 $\div$ \$\div\$ 除号 $\mid$ \$\mid\$ 竖线 $\cdot$ \$\cdot\$ 点 $\circ$ \$\circ\$ 圈 $\ast $ \$\ast \$ 星号 $\bigotimes$ \$\bigotimes\$ 克罗内克积 $\bigoplus$ \$\bigoplus\$ 异或 $\leq$ \$\leq\$ 小于等于 $\geq$ \$\geq\$ 大于等于 $\neq$ \$\neq\$ 不等于 $\approx$ \$\approx\$ 约等于 $\prod$ \$\prod\$ N元乘积 $\coprod$ \$\coprod\$ N元余积 $\cdots$ \$\cdots\$ 省略号 $\int$ \$\int\$ 积分 $\iint$ \$\iint\$ 双重积分 $\oint$ \$\oint\$ 曲线积分 $\infty$ \$\infty\$ 无穷 $\nabla$ \$\nabla\$ 梯度 $\because$ \$\because\$ 因为 $\therefore$ \$\therefore\$ 所以 $\forall$ \$\forall\$ 任意 $\exists$ \$\exists\$ 存在 $\not=$ \$\not=\$ 不等于 $\not&gt;$ \$\not&gt;\$ 不大于 $\leq$ \$\leq\$ 小于等于 $\geq$ \$\geq\$ 大于等于 $\not\subset$ \$\not\subset\$ 不属于 $\emptyset$ \$\emptyset\$ 空集 $\in$ \$\in\$ 属于 $\notin$ \$\notin\$ 不属于 $\subset$ \$\subset\$ 子集 $\subseteq$ \$\subseteq\$ 真子集 $\bigcup$ \$\bigcup\$ 并集 $\bigcap$ \$\bigcap\$ 交集 $\bigvee$ \$\bigvee\$ 逻辑或 $\bigwedge$ \$\bigwedge\$ 逻辑与 $\biguplus$ \$\biguplus\$ 多重集 $\bigsqcup$ \$\bigsqcup\$ $\hat{y}$ \$\hat{y}\$ 期望值 $\check{y}$ \$\check{y}\$ $\breve{y}$ \$\breve{y}\$ $\overline{a+b+c+d}$ \$\overline{a+b+c+d}\$ 平均值 $\underline{a+b+c+d}$ \$\underline{a+b+c+d}\$ $\overbrace{a+\underbrace{b+c}_{1.0}+d}^{2.0}$ \overbrace{a+\underbrace{b+c}_{1.0}+d}^{2.0} $\uparrow$ \$\uparrow\$ 向上 $\downarrow$ \$\downarrow\$ 向下 $\Uparrow$ \$\Uparrow\$ $\Downarrow$ \$\Downarrow\$ $\rightarrow$ \$\rightarrow\$ 向右 $\leftarrow$ \$\leftarrow\$ 向左 $\Rightarrow$ \$\Rightarrow\$ 向右箭头 $\Longleftarrow$ \$\Longleftarrow\$ 向左长箭头 $\longleftarrow$ \$\longleftarrow\$ 向左单箭头 $\longrightarrow$ \$\longrightarrow\$ 向右长箭头 $\Longrightarrow$ \$\Longrightarrow\$ 向右箭头 $\alpha$ \$\alpha\$ $\beta$ \$\beta\$ $\gamma$ \$\gamma\$ $\Gamma$ \$\Gamma\$ $\delta$ \$\delta\$ $\Delta$ \$\Delta\$ $\epsilon$ \$\epsilon\$ $\varepsilon$ \$\varepsilon\$ $\zeta$ \$\zeta\$ $\eta$ \$\eta\$ $\theta$ \$\theta\$ $\Theta$ \$\Theta\$ $\vartheta$ \$\vartheta\$ $\iota$ \$\iota\$ $\pi$ \$\pi\$ $\phi$ \$\phi\$ $\Phi$ \$\Phi\$ $\psi$ \$\psi\$ $\Psi$ \$\Psi\$ $\omega$ \$\omega\$ $\Omega$ \$\Omega\$ \chi \chi $\rho$ \$\rho\$ $\omicron$ \$\omicron\$ $\sigma$ \$\sigma\$ $\Sigma$ \$\Sigma\$ $\nu$ \$\nu\$ $\xi$ \$\xi\$ $\tau$ \$\tau\$ $\lambda$ \$\lambda\$ $\Lambda$ \$\Lambda\$ \mu \mu $\partial$ \$\partial\$ $\lbrace \rbrace$ \$\lbrace \rbrace\$ $\overline{a}$ \$\overline{a}\$]]></content>
      <categories>
        <category>备忘录</category>
      </categories>
      <tags>
        <tag>markdown</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[神经网络-循环神经网络RNN（Recurrent neural network）]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-20.MachineLearn%2F2104.%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-RNN%2F</url>
    <content type="text"><![CDATA[前面的介绍，我们遇到过两种类型的数据：表格数据和图像数据。 对于图像数据，我们设计了专门的卷积神经网络架构来为这类特殊的数据结构建模。我们在处理这俩各种类型时，这两类数据本身是无序的（顺序不会影响对数据的理解）。 最重要的是，到目前为止我们默认数据都来自于某种分布，并且所有样本都是独立同分布的 （independently and identically distributed，i.i.d.）每次我门处理一个独立数据，数据间是无关的。然而，大多数的数据并非如此，很多元素都是相互连接的，例如，文章中的单词是按顺序写的，如果顺序被随机地重排，就很难理解文章原始的意思。视频中的图像帧、对话中的音频信号以及网站上的浏览行为，股票的变化，都是有顺序的。一个人说了：我喜欢旅游，其中最喜欢的地方是云南，以后有机会一定要去__.这里填空，人应该都知道是填“云南“。因为我们是根据上下文的内容推断出来的，但机器要做到这一步就相当得难了。因此，就有了现在的循环神经网络，他的本质是：像人一样拥有记忆的能力。因此，他的输出就依赖于当前的输入和记忆。 另一个问题来自这样一个事实： 我们不仅仅可以接收一个序列作为输入，而是还可能期望继续猜测这个序列的后续。 例如，一个任务可以是继续预测$2,4,6,8,10…$。 这在时间序列分析中是相当常见的，可以用来预测股市的波动、 患者的体温曲线或者赛车所需的加速度。 同理，我们需要能够处理这些数据的特定模型。如果说卷积神经网络可以有效地处理空间信息，那么我们这里要介绍的 循环神经网络（recurrent neural network，RNN） 则可以更好的处理序列信息。循环神经网络通过引入状态变量存储过去的信息（记忆）和当前的输入，从而可以确定当前的输出。 RNN的网络结构及原理RNN的网络结构如下： 其中每个圆圈可以看作是一个单元，而且每个单元做的事情也是一样的，因此可以折叠呈左半图的样子。用一句话解释RNN，就是一个单元结构重复使用。 RNN是一个序列到序列的模型，假设$x_{t−1},x_t,x_{t+1}$是一个输入：“我是中国“，那么$o_{t−1},o_t$就应该对应”是”，”中国”这两个，预测下一个词最有可能是什么？就是$o_{t+1}$应该是”人”的概率比较大。因此，我们可以做这样的定义： $X_t$:表示t时刻的输入，$o_t$:表示t时刻的输出，$S_t$:表示t时刻的记忆 可以看到和之前的卷积或其他网络的主要区别是，在生成输出时，我们引入记忆 $S_t$ 的信息。输出结果是由记忆和输入共同决定的。而随着我们每处理一个问题，可能我们会更新记忆（学习到新知识）。RNN在这点上也类似，神经网络最擅长做的就是通过一系列参数把很多内容整合到一起，然后学习这个参数，因此就定义了RNN的基础：$S_t=f(U∗X_t+W∗S_{t−1})$可以看到，我们每个时间点的记忆由上一个时间点的记忆，和当前时间点接收到的信息共同构成。大家可能会很好奇，为什么还要加一个 $f()$ 函数，其实这个函数是神经网络中的激活函数，但为什么要加上它呢？举个例子，假如你在大学学了非常好的解题方法，那你初中那时候的解题方法还要用吗？显然是不用了的。RNN的想法也一样，既然我能记忆了，那我当然是只记重要的信息啦，其他不重要的，就肯定会忘记，是吧。但是在神经网络中什么最适合过滤信息呀？肯定是激活函数嘛，因此在这里就套用一个激活函数，来做一个非线性映射，来过滤信息，这个激活函数可能为tanh，也可为其他。 在每个时间点t，我们基于上述方式获得了$S_t$ (整合了t时刻的输入和之前的记忆），下一步就是预测t时刻的输出 $o_t$。运用softmax来预测每个词出现的概率再合适不过了，但预测不能直接带用一个矩阵来预测呀，所有预测的时候还要带一个权重矩阵V,用公式表示为:$o_t=softmax(V S_t)$ RNN中的结构细节： 可以把$S_t$当作隐状态，捕捉了之前时间点上的信息。就像你去考研一样，考的时候记住了你能记住的所有信息。 $o_t$是由当前时间以及之前所有的记忆得到的。就是你考研之后做的考试卷子，是用你的记忆得到的。 很可惜的是，$S_t$并不能捕捉之前所有时间点的信息。就像你考研不能记住所有的英语单词一样。 和卷积神经网络一样，这里的网络中每个cell都共享了一组参数（U，V，W）,这样就能极大的降低计算量了。 $o_t$在很多情况下都是不存在的，因为很多任务，比如文本情感分析，都是只关注最后的结果的。就像考研之后选择学校，学校不会管你到底怎么努力，怎么心酸的准备考研，而只关注你最后考了多少分。 RNN的改进： 双向RNN在有些情况，比如有一部电视剧，在第三集的时候才出现的人物，现在让预测一下在第三集中出现的人物名字，你用前面两集的内容是预测不出来的，所以你需要用到第四，第五集的内容来预测第三集的内容，这就是双向RNN的想法。如图是双向RNN的图解： 从前往后：$\overrightarrow{S_t}=f(\overrightarrow{U}∗X_t+\overrightarrow{W}∗S_{t−1}+\overrightarrow{b})$从后往前：$\overleftarrow{S_t}=f(\overleftarrow{U}∗X_t+\overleftarrow{W}∗S_{t+1}+\overleftarrow{b})$输出：$o_t=softmax(V∗[\overrightarrow{S_t};\overleftarrow{S_t}])$ 这里的 $[\overrightarrow{S_t};\overleftarrow{S_t}]$做的是一个拼接，如果他们都是1000 x 1维的，拼接在一起就是1000 x 2维的了。 双向RNN需要的内存是单向RNN的两倍，因为在同一时间点，双向RNN需要保存两个方向上的权重参数，在分类的时候，需要同时输入两个隐藏层输出的信息。 RNN的改进2：深层双向RNN深层的双向RNN(Stacked Bidirectional RNN)的结构如上图所示。上图是一个堆叠了3个隐藏层的RNN网络。和之前的区别就是这时，我们的forward和backwards的矩阵计算时递归（递归处理不同的层）从前往后：$\overrightarrow{S_t^l}=f(\overrightarrow{U^l}∗S^{l-1}t+\overrightarrow{W^l}∗S^l{t−1}+\overrightarrow{b^l})$从后往前：$\overleftarrow{S^l_t}=f(\overleftarrow{U^l}∗S^{l-1}t+\overleftarrow{W^l}∗S{t+1}+\overleftarrow{b^l})$输出：$o_t=softmax(V∗[\overrightarrow{S_t^l};\overleftarrow{S^l_t}])$ 这个时候，每个隐藏层的每个节点在计算是，依然是需要两个输入，比如我们计算 $S^l_{t}$ （l对应隐藏层的索引、t对应时间点)时，第一个输入就是同一隐藏层前一时刻传递过来的信息$S^l_{t−1}$，和同一时刻上一隐藏层传过来的信息$S^{l-1}_t=[\overrightarrow{S_t^{l-1}};\overleftarrow{S^{l-1}_t}]$，包括前向和后向的 Pyramidal RNN 金字塔RNN 上图是谷歌的W.Chan做的一个测试，它原先要做的是语音识别，他要用序列到序列的模型做语音识别，序列到序列就是说，输入一个序列然后就输出一个序列。 由图我们发现，上一层的两个输出，作为当前层的输入，如果是非常长的序列的话，这样做的话，每一层的序列都比上一层短（示例时上一层的一半长度），但当前层的输入 $f(x)$也会随之增多，貌似看一起相互抵消，运算量并没有什么改进。 但我们知道，对于一层来说，它是从前往后递归的，比如要预测一个股市的变化，数据必须先预测昨天，再预测今天，最后预测明天，也即是说预测必须具有连续性。但每一层的$f$运算是可以并行的，从这个角度来看，运算量还是可以接受的，特别是在原始输入序列较短的时候还是有优势的。 模型的训练通过前面的介绍，我们可以知道，所有的RNN，都可以抽象成如下的两个部分:第一步，结合当前时间点的输入和上一时间点的记忆，生成当前时间点的记忆（如果时双向神经网络会有来自两个方向的记忆）： 从前往后：$\overrightarrow{S_t^l}=f(\overrightarrow{U^l}∗S^{l-1}t+\overrightarrow{W^l}∗S^l{t−1}+\overrightarrow{b^l})$ 从后往前：$\overleftarrow{S^l_t}=f(\overleftarrow{U^l}∗S^{l-1}t+\overleftarrow{W^l}∗S{t+1}+\overleftarrow{b^l})$第二部，基于当前时间点的记忆生成当前时间点的输出（类频率、或频率）输出：$o_t=softmax(V∗[\overrightarrow{S_t^l};\overleftarrow{S^l_t}])$ 所以完成了模型框架后，我们要应用模型，其实要解决的就是寻找最优参数（$U、W、V$）。为了找到最好的参数，我们首先要构建一个评估参数好坏的指标，和之前的模型一样，一般采用损失函数。 损失函数t时刻的损失：$E_t(y_t,y_t)=−y_t\log{\hat{y}_t}$其中 $y_t$是t时刻的标准答案，是一个只有一个维度是1，其他都是0的向量； $y^t$是我们预测出来的结果，与 $yt$ 的维度一样，但它是一个概率向量，里面是每个词出现的概率。因为对结果的影响，肯定不止一个时刻，因此需要把所有时刻的造成的损失都加起来：$$E(y_t,\hat{y}_t)=\sum_t{E_t(y_t,\hat{y}t)}=−\sum{t}{y_tlog\hat{y_t}}$$ 获得随时函数后，我们就可以根据损失函数求解最优参数了。再RNN中计算最优解，用的是BPTT。 序列模型应用输入或者输出中包含有序列数据的模型叫做序列模型。典型应用比如： 语音识别： 输入输出都为序列。 音乐生成： 输出为序列。 情感分析：输入为序列。 DNA序列分析：输入为序列。 机器翻译：输入输出都为序列。 视频行为识别：输入为序列。 命名实体识别：输入输出都为序列。 统计工具要处理序列数据，我们需要统计工具和新的神经网络架构。为了便于后续的理解，我们以 下图所示（富士100指数）股票价格为例。其中，用 $x_t$ 表示价格，即在时间步（time step）$t \in{Z^+}$ 时，观察到的价格。 请注意，$t$对于本文中的序列通常是离散的，并在整数或其子集上变化。 假设一个交易员想在$t$日的股市中表现良好，于是通过以下途径预测：$x_t \sim P(x_t|x_{t-1},…,x_1)$ 自回归模型为了实现这个预测，可以使用回归模型，我们需要解决的只有一个主要问题：输入数据的数量， 输入 $x_{t-1},…,x_1$ 本身因$t$而异。 也就是说，输入数据的数量这个数字将会随着我们遇到的数据量的增加而增加， 因此需要一个近似方法来使这个计算变得容易处理。 本章后面的大部分内容将围绕着如何有效估计 $P(x_t|x_{t-1},…,x_1)$ 展开。 简单地说，它归结为以下两种策略。 在现实情况下相当长的序列 $x_{t-1},…,x_1$ 可能是不必要的， 因此我们只需要满足某个长度为的时间跨度 $\tau$， 即使用观测序列 $x_{t-1},…,x_{t-\tau}$。最直接的好处就是在我们的历史数据达到 $\tau$ 后，参数的数量总是不变的，从而让我们具备训练一个神经网络的基础。因为这种模型是对自己执行回归，所以这种模型被称为自回归模型。 还有一种方案是如下图所示，总是保留一些对过去观测的总结 $h_{t}$,并且同时更新预测结果 $\hat{x}t$ 和总结 $h{t}$。这就形成了，基于 $\hat{x}_t = P(x_t|h_{t})$ 估计 $x_t$ , 以及 $h_t = g(h_{t-1}|x_{t-1})$ 更新的模型。由于总结 $h_t$ 是从未被观测到的，所以这类模型也被成为 隐变量自回归模型。 执行案例文本处理预处理 将文本作为字符串加载到内存中。 将字符串拆分为词元（如单词和字符）。 建立一个词表，将拆分的词元映射到数字索引。 将文本转换为数字索引序列，方便模型操作。]]></content>
      <categories>
        <category>neural_network</category>
      </categories>
      <tags>
        <tag>neural_network</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[神经网络-卷积神经网络-CNN（Convolutional Neural Networks）]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-20.MachineLearn%2F2103.%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-CNN%2F</url>
    <content type="text"><![CDATA[多层感知机十分适合处理表格数据，其中行对应样本，列对应特征。 对于表格数据，我们寻找的模式可能涉及特征之间的交互，但是我们不能预先假设任何与特征交互相关的先验结构。此时，多层感知机可能是最好的选择，然而对于高维感知数据，这种缺少结构的网络可能会变得不实用。例如，在之前猫狗分类的例子中：假设我们有一个足够充分的照片数据集，数据集中是拥有标注的照片，每张照片具有百万级像素，这意味着网络的每次输入都有一百万个维度。 即使将隐藏层维度降低到1000，这个全连接层也将有 $10^6 * 10^3 = 10^9 $ 个参数。 想要训练这个模型将不可实现，因为需要有大量的GPU、分布式优化训练的经验和超乎常人的耐心。然而，如今人类和机器都能很好地区分猫和狗：这是因为图像中本就拥有丰富的结构，而这些结构可以被人类和机器学习模型使用。假设我们想从一张图片中找到某个物体。 合理的假设是：无论哪种方法找到这个物体，都应该和物体的位置无关。 因此我们可以使用一个“目标物体检测器”扫描图像。 该检测器将图像分割成多个区域，并为每个区域包含目标物体的可能性打分。 卷积神经网络正是将空间不变性（spatial invariance）的这一概念系统化，从而基于这个模型使用较少的参数来学习有用的表示。 卷积神经网络（Convolutional Neural Networks，CNN）在卷积神经网络中，卷积层(输入层和输出层不再是全链接)会对输入的数据进行过滤和降维处理。首先对原始输入按着特定的方式划分为多个区域，然后使用卷积核对每个区域进行特征提取实现降维。 卷积是一种数学运算，它描述了两个函数相互作用的过程。在深度学习中，卷积通常用于处理图像、声音等数据。通过卷积操作，我们可以有效地提取数据中的局部特征，从而实现更高层次的抽象表示（即降维）。 在认识卷积神经网络之前，我们先来了解什么是卷积。同时也了解以下另一个概念，互相关运算。这部分数学概念可以帮助我们更好的理解卷积神经网络的技术内核。 卷积层卷积泛函分析中，卷积（Convolution）是通过两个函数 $f$ 和 $g$ 生成第三个函数的一种数学运算，其本质是一种特殊的积分变换，表征函数 $f$ 与 $g$ 经过翻转和平移的重叠部分函数值乘积对重叠长度的积分。如果将参加卷积的一个函数看作区间的指示函数，卷积还可以被看作是“滑动平均”的推广。 定义：卷积是两个变量在某范围内相乘后求和的结果。如果卷积的变量是序列 $x(n)$ 和 $h(n)$，则卷积的结果$$y(n)=\sum_{m=-\infty}^{\infty}x(m)h(n-m) =x(n)h(n)$$其中星号 $$ 表示卷积。当时序 $n=0$ 时，$h(-i)$ 是 $h(i)$ 的时序 $i$ 取反的结果；时序取反使得 $h(i)$ 以纵轴为中心翻转180度，所以这种相乘后求和的计算法称为卷积和，简称卷积。另外，n 是使$h(-i)$位移的量，不同的 n 对应不同的卷积结果。 互相关运算卷积窗口从输入数组的最左上方开始，按从左往右、从上往下的顺序，依次在输入数组上滑动。当卷积窗口滑动到某一位置时，窗口中的输入子数组与核数组按对应元素相乘并求和，得到输出数组中相应位置的元素。$$y_{fh}(n)=\sum_{m=-\infty}^{\infty}f^(m)h(m+n) =x(n)h(n)$$ 卷积和互相关的运算区别互相关运算示例我们对图像中的蓝色区域进行Cross-correlation（互相关运算），互相关运算是按顺序滑动窗口，针对每个窗口按着对应关系，顺序积分求和的过程那么在点E处的计算方式就是：$G[3,3]=a∗A+b∗B+c∗C+d∗D+e∗E+f∗F+g∗G+h∗H+i∗I $我们可以看到，。 卷积示例还是上面的图像，我们对蓝色区域进行卷积，卷积是按顺序滑动窗口，针对每个窗口，逆序（一维）积分求和，若是二维，则是相当于对卷积核进行了原定对称后，在进行了互相关运算的过程。即在点E处的计算为：$G[3,3]=a∗I+b∗H+c∗G+d∗F+e∗E+f∗D+g∗C+h∗B+i∗A$那么这就相当于将‘filter翻转’了，即先上下翻转、再左右翻转，然后进行cross-correlation运算，如下所示： 区别所以在这同时我们可以看到，卷积运算和互相关运算的关系。他们都是一个函数对另一个函数的积分，区别时积分的过程，卷积时 $f(x)g(n-x)$, 互相关是 $f(x)g(n+x)$，在二维图像处理中 $g(n-x)$和g(n+x)是一个关于远端对称，后经过2n平移的矩阵。 互相关运算是顺序积分求和的过程，互相关运算时逆序积分求和。当我们的输入数据时固定的时候，由于我们的卷积核时学习获取的，所以互相关运算和卷积运算在某种意义上来讲是一样的。通过互相关运算的得到的 考虑，一张图如下，我们进行互相关运算，得到的结果如下：如果将上面换成真实点的图我们看到得到的结果就像是filter，只不过翻转了下（原点对称）。因此如果我们将filter翻转了一次再进行cross-correlation，那再加上上面的这次翻转就是两次翻转了，得到的图像就也就不变了。 于是卷积就有了下面的性质Identity:E=[…0,0,1,0,0…]，F*E=F（你可以想下cross-correlation行不行） 由于卷积核是我们基于数据计算获得的，因此单纯计算卷积核的过程，卷积和互相关就没有实际区别（卷积和互相关计算得到的两个卷积核是一个中心对称的矩阵）。所以一般的计算中实际使用的更多的是互相关运算。 卷积层参数像前面介绍的，卷积（实际执行的都是互相关运算，后文如无特殊说明卷积都是指的互相关运算），卷积进行过滤降维的过程，其实就是用核函数对输入数据进行降维过滤的过程，这是因为我们一般使用的核函数通常是宽度和高度都大于 1 所导致的。但是除了核函数，填充和步幅也会影响我们的输出矩阵的大小， 填充层（Padding Layer)在卷积的过程中，输出特征图的大小由输入特征图的大小、内核的大小和步长决定。如果我们简单地将内核应用于输入特征图，那么输出特征图将小于输入。这可能会导致输入特征图边界处的信息丢失。由于我们通常使用小卷积核，因此对于任何单个卷积，我们可能只会丢失几个像素。在应用多层卷积时，随着我们应用许多连续卷积层，累积丢失的像素数就多了。 解决这个问题的简单方法即为填充（padding）：在输入图像的边界填充元素（通常填充元素是(0)）。而这个处理过程就是通过填充层实现。 通过使用填充层，我们可以防止输入图像在多层卷积处理中不断缩小，甚至可以在某些情况下实现特征增加。如下图，我们将 $33$ 的输入数据填充成一个 $55$ 的输入，这样我们得到的输出就从 $22$变为 $44$ 从而可以避免边缘信息丢失。 通常，如果我们添加(p_h)行填充（大约一半在顶部，一半在底部）和(p_w)列填充（左侧大约一半，右侧一半），则输出形状将为[(n_h-k_h+p_h+1)\times(n_w-k_w+p_w+1)。] 这意味着输出的高度和宽度将分别增加(p_h)和(p_w)。 在许多情况下，我们需要设置(p_h=k_h-1)和(p_w=k_w-1)，使输入和输出具有相同的高度和宽度。 这样可以在构建网络时更容易地预测每个图层的输出形状。假设(k_h)是奇数，我们将在高度的两侧填充(p_h/2)行。 如果(k_h)是偶数，则一种可能性是在输入顶部填充(\lceil p_h/2\rceil)行，在底部填充(\lfloor p_h/2\rfloor)行。同理，我们填充宽度的两侧。 卷积神经网络中卷积核的高度和宽度通常为奇数，例如1、3、5或7。 选择奇数的好处是，保持空间维度的同时，我们可以在顶部和底部填充相同数量的行，在左侧和右侧填充相同数量的列。 此外，使用奇数的核大小和填充大小也提供了书写上的便利。对于任何二维张量X，当满足： 1. 卷积核的大小是奇数； 2. 所有边的填充行数和列数相同； 3. 输出与输入具有相同高度和宽度 则可以得出：输出Y[i, j]是通过以输入X[i, j]为中心，与卷积核进行互相关计算得到的。 填充为0 填充为1步幅 计算互相关时，卷积窗口从输入张量的左上角开始，向下、向右滑动。 在前面的例子中，我们默认每次滑动一个元素。 但是，有时候为了高效计算或是缩减采样次数，卷积窗口可以跳过中间位置，每次滑动多个元素。每次滑动元素的数量称为步幅（stride）。到目前为止，我们只使用过高度或宽度为(1)的步幅，那么如何使用较大的步幅呢？通常，当垂直步幅为(s_h)、水平步幅为(s_w)时，输出形状为[\lfloor(n_h-k_h+p_h+s_h)/s_h\rfloor \times \lfloor(n_w-k_w+p_w+s_w)/s_w\rfloor.] 如果我们设置了(p_h=k_h-1)和(p_w=k_w-1)，则输出形状将简化为(\lfloor(n_h+s_h-1)/s_h\rfloor \times \lfloor(n_w+s_w-1)/s_w\rfloor)。 更进一步，如果输入的高度和宽度可以被垂直和水平步幅整除，则输出形状将为((n_h/s_h) \times (n_w/s_w))。 步长为1： 步长为2： 汇聚层/池化层（pooling layer）我们通常希望这些特征保持某种程度上的平移不变性。例如，如果我们拍摄黑白之间轮廓清晰的图像X，并将整个图像向右移动一个像素，即Z[i, j] = X[i, j + 1]，则新图像Z的输出可能大不相同。而在现实中，随着拍摄角度的移动，任何物体几乎不可能发生在同一像素上。所以引入了池化层，来消除像素平移带来的影响。卷积层类似，汇聚层运算符由一个固定形状的窗口组成，该窗口根据其步幅大小在输入的所有区域上滑动，为固定形状窗口（有时称为汇聚窗口）遍历的每个位置计算一个输出。 然而，不同于卷积层中的输入与卷积核之间的互相关计算，汇聚层不包含参数。 相反，池运算是确定性的，我们通常计算汇聚窗口中所有元素的最大值或平均值。这些操作分别称为最大汇聚层（maximum pooling）和平均汇聚层（average pooling）。在这两种情况下，与互相关运算符一样，汇聚窗口从输入张量的左上角开始，从左往右、从上往下的在输入张量内滑动。在汇聚窗口到达的每个位置，它计算该窗口中输入子张量的最大值或平均值。计算最大值或平均值是取决于使用了最大汇聚层还是平均汇聚层。输出张量的高度为，宽度为。这四个元素为每个汇聚窗口中的最大值：max(0,1,3,4) = 4,max(1,2,4,5)=5,max(3,4,6,7)=7,max(4,5,7,8)=8汇聚窗口形状$pq$为的汇聚层称为$pq$汇聚层，汇聚操作称为$p*q$汇聚。 卷积神经网络的特性： 局部连接：卷积层中的神经元仅与输入数据的一个局部区域连接，这有助于减少参数数量。神经网络的前面几层应该只探索输入图像中的局部区域，而不过度在意图像中相隔较远区域的关系，这就是“局部性”原则。 权重共享：卷积核在输入数据的不同位置使用相同的权重和偏置，这进一步减少了参数数量。 平移不变性：由于卷积操作的特性，CNN对图像中的平移变换具有一定的鲁棒性。不管检测对象出现在图像中的哪个位置，神经网络的前面几层应该对相同的图像区域具有相似的反应。 卷积的作用 图像处理在图像处理中，卷积可以用于实现边缘检测、模糊、锐化等功能。通过将图像与特定的卷积核进行卷积操作，我们可以突出或抑制图像中的某些特征，从而达到处理的目的。 信号处理在信号处理中，卷积用于分析和处理信号。例如，通过卷积可以消除噪声、平滑信号，从而提高信号的质量。]]></content>
      <categories>
        <category>neural_network</category>
      </categories>
      <tags>
        <tag>neural_network</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Software-CNV检测-ECOLE]]></title>
    <url>%2F03.%E5%BA%94%E7%94%A8-01.software_in_NGS%2FSoftware-CNV%E6%A3%80%E6%B5%8B-ECOLE%2F</url>
    <content type="text"><![CDATA[官方资料ECOLE: Learning to call copy number variants on whole exome sequencing dataECOLE github ECOLE 是一款基于 深度学习 进行 全外显子测序(WES)数据中精确检测拷贝数变异(CNVs) 的方法。该方法是基于 Transformer 架构的变体，该模型通过对匹配的 WGS 样本进行高置信度调用，学习调用每个外显子的 CNV。我们通过迁移学习通过一小组专家调用进一步训练和微调模型。我们证明 ECOLE 首次在人类专家标记数据上实现了高性能，准确率达到 68.7%，召回率达到 49.6%。与排名第二的最佳方法相比，准确率和召回率分别提高了 18.7% 和 30.8%。 软件开发相关的数据基础训练数据 707 samples（WGS &amp; WES） from the 1000 Genomes Project Byrska-Bishop, M. et al. High-coverage whole-genome sequencing of the expanded 1000 Genomes Project cohort including 602 trios. Cell 185, 3426-3440.e19 (2022).00991-6?_returnURL=https%3A%2F%2Flinkinghub.elsevier.com%2Fretrieve%2Fpii%2FS0092867422009916%3Fshowall%3Dtrue)。数据基于Illumina（WES）和NovaSeq（WGS）测序，使用CNVnator 对WGS进行CNV检测，结果作为最终的分析标签，一共标记了740178个DEL 和 953202 个 DUP， 没有标记的的区域作为阴性区域。 不同测序仪的测试数据 对 NA12878 样本进行的调用来测试 ECOLE 对各种测序平台的通用性。该样本具有以下平台提供的WES数据：BGISEQ 500、Illumina HiSeq 4000、MGISEQ 2000和NovaSeq 6000。我们仅使用该样本进行测试，其数据不以任何方式包含在训练集中。我们再次使用 CNVnator 调用 NA12878 的 WGS 样本来获取每个外显子的半真实标签以进行评估。此样本总共标记有 1,780 个 DEL 和 1,350 个 DUP 调用。 微调数据 15 个 CNV WGS 调用者对来自 1000 个基因组项目的 9 个样本的经过人类专家验证的共识调用。我们获得了8个样本的调用，在1000 Genomes数据集中也有匹配的WES数据，即：HG00512、HG00513、HG00731、HG00732、HG00733、NA19238、NA19239、NA19240。 Chaisson, M. et al. Multi-platform discovery of haplotype-resolved structural variation in human genomes. Nat. Commun. 10, 1–16 (2019). GiaB 为 Ashkenazi 家族（NA12878 - 儿子、NA12891 - 父亲和 NA12892 - 母亲）提供基于 MetaSV 的 CNV 呼叫集。我们使用 NA12891 的调用对 ECOLE 模型进行了 8 个时期的微调。训练样本总共标记有 10,824 个 DEL 和 1,362 个 DUP 调用。真正的阴性是没有进行调用的其余外显子。我们在为 NA12892 提供的基于 MetaSV 的 CNV 调用上测试了该模型。 16 名膀胱癌患者的匹配 WES 和 WGS 样本（登录号：SRP017787）。我们使用该数据集中的 3 个癌症样本（样本 B112、B24、B80）对 ECOLE 模型进行微调 31 。我们使用在这 3 名患者的匹配 WGS 样本上获得的半真实标签进行微调。训练样本总共标记有 23,383 个 DEL 和 282,573 个 DUP 调用。真正的阴性是没有进行调用的其余外显子。Guo, G. et al. Whole-genome and whole-exome sequencing of bladder cancer identifies frequent alterations in genes involved in sister chromatid cohesion and segregation. Nat. Genet. 45, 1459–1463 (2013). 软件原理介绍 模型输入为每个外显子的： readsthe read depth signal (length 1000, padded and masked), chromosome number, and start and end coordinates of the region. 使用全连接 (FC) 层将每个 1000 个读取深度值映射到更高维度的向量$${X}{embed}\in \mathbb{R}^{192}$$ （输入嵌入），该层与染色体特定的分类标记向量 $${c}{t}\in \mathbb{R}^{192}$$对于每条染色体 $t$。这些染色体特定的标记使模型能够学习外显子样本的染色体上下文以执行调用。 Transformer 层使用多头注意力机制，学习碱基对的每个读取深度值相对于给定外显子样本中所有其他碱基对的连接。因此，注意力机制还学习分类标记需要针对相应的 CNV 调用关注哪些读取深度值。为了进一步了解染色体内碱基对的位置上下文，样本的起始和结束坐标用于计算外显子特异性位置编码$${E}{pos}\in \mathbb{R}^{192\times 1001}$$两个矩阵被连接并输入到级联的 3 个转换器编码器，生成输出向量$${O}{3}\in \mathbb{R}^{192\times 1001}$$然后，获取染色体特定分类标记 $${c}_{t}^\prime$$的映射变换，其大小​​为 $$\mathbb{R}^{192}$$最后，对于最终决策（DEL、DUP 或 NO-CALL），我们使用 2 个 FC 层，然后进行 softmax 激活。 使用说明安装部署1234567# install conda$ wget -c https://repo.continuum.io/archive/Anaconda3-vers.num-Linux-x86_64.sh$ bash Anaconda3-version.num-Linux-x86_64.sh# install ECOLE_environment$ conda env create --name ecole_env -f ECOLE_environment.yml$ conda activate ecole_env 执行使用1234$ source preprocess_samples.sh$ source ecole_call.sh 参考#1.]]></content>
      <categories>
        <category>Tobeinsert</category>
      </categories>
      <tags>
        <tag>Tobeinsert</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Software-CNV检测-ClinCNV]]></title>
    <url>%2F03.%E5%BA%94%E7%94%A8-01.software_in_NGS%2FSoftware-CNV%E6%A3%80%E6%B5%8B-ClinCNV%2F</url>
    <content type="text"><![CDATA[官方资料github: ClinCNVgithub: pre-process ngs-bits ClinCNV: multi-sample germline CNV detection in NGS data 流程说明部署应用项目代码拉取1234# CNV 分析项目代码包程度# CNV分析输入数据生成的代码包，用于构建ClinCNV分析的输入文件，视情况判断是否需要git clone https://github.com/imgag/ngs-bits.git R环境安装基于conda创建R基础环境，github说明是基于 3.2.3 版本。但是3.2.3 版本已经不维护。。中间版本可能存在问题： R=3.6.1“MASS”、”party”、”umap” 编译有问题，无法通过 install.packages 安装。其中part的依赖包 mvtnorm安装问题安装问题。未检索到直接解决方案， 经测试相关包安装可以在目前最新的R编译版本（R=4.4.1）中，可以通过install.packages 完成全部依赖包的自动安装。 1conda create -nClinCNV -c conda-forge r-base=4.4.1 R包安装完成镜像后，进入R环境安装所列的依赖包1234567891011121314# 能直接安装的install.packages("optparse")install.packages("robustbase")install.packages("data.table")install.packages("foreach")install.packages("doParallel")install.packages("mclust")install.packages("R.utils")install.packages("RColorBrewer")install.packages("dbscan")install.packages("MASS")install.packages("party")install.packages("umap")install.packages("Rcpp") # Rcpp 安装后可以提高分析速度，不安装也可以运行 R包安装确认测试包安装1234567891011121314# 能直接安装的library("optparse")library("robustbase")library("data.table")library("foreach")library("doParallel")library("mclust")library("R.utils")library("RColorBrewer")library("dbscan")library("MASS")library("party")library("umap")#library("Rcpp") demo数据运行123456# 设置本地拉取的ClinCNV项目存储路径fold=/path/to/ClinCNV-master # 创建输出目录mkdir $fold"/results" # 运行分析Rscript $fold"/clinCNV.R" --bed $fold"/samples/bed_file.bed" --normal $fold"/samples/coverages_normal.cov" --out $fold"/results" 可以看到流程读取两个输入文件 .cov 是一个覆盖度的矩阵文件(包含待分析的所有样本). .bed 进行了GC-含量注释的bed文件 (from 0 to 1, should be in 4th column). bed_file.bedbed文件时在第四列注释了GC含量，第五列标注基因名称信息（第四列是必须的，第五列是可选的）。bed文件中不应该包含文件头，或文件头应该是用 # 注释的。 列号 说明 备注 1 染色体 染色体位置，chr前缀 2 起始坐标 int 3 结束坐标 int 4 GC含量 real, from 0 to 1 5 genes character, 逗号分隔，可选内容 123chr1 12171 12245 0.4595# or, annotated with genes,chr1 12171 12245 0.4595 DDX11L1 coverages_normal.cov.cov 文件是包含染色体区域信息（前三列），每个样本的对应区域的覆盖度（每个样本一列，建议是区域的平均覆盖度）矩阵文件。首行是文件的头文件。 列号 说明 备注 1 染色体 染色体位置，chr前缀 2 起始坐标 int 3 结束坐标 int 4 Sample1 样本在对应区段的平均覆盖深度 …. …. ….. 3+n sample-n 样本在对应区段的平均覆盖深度 12chr start end Sam1 Sam2chr1 11166636 11166864 2374.32 1224.54 文件 *.cov生成建议使用samtools进行区域覆盖深度的生成，一方面这是 clinCNV 官方文档中提供的一个计算覆盖度的方式，另一方面是samtools在大多数NGS分析过程中都会用到，所以一般不需要我们进行额外的安装。 基于samtools统计深度的命令格式如下：1samtools bedcov temp.bed pancancer689__DX1790_daiyufen_23S03853625_23B03853625__Cancer.realign.bam 基于ngs-bits(不建议)ngs-bits 是 clinCNV 官方文档中提供的一个计算覆盖度的方式，但是安装过程发现 ngs-bits 安装的底层依赖和最新版的 clinCNV 已经存在冲突。所以建议更换掉 ngs-bits 软件。 安装 1conda create -n ngs-bits -c bioconda ngs-bits 使用 1BedCoverage -bam $bamPath -in $bedPath -min_mapq 3 -out $sampleName&quot;.cov&quot; mosdepthmosdepth 是另外一个可以实现相关功能的软件。 安装 1conda install -c bioconda mosdepth 使用 1mosdepth --by temp.bed output_prefix input.bam 结果运行后会生成多个结果，其中regions.bed.gz，就是可以用 ClinCNV 进行CNV分析的格式文件（单样本），对每个样本进行处理后，需要对多个样本的结果进行整合。 结果说明批次统计结果批次结果有两部分，1. 统计每个样本的平均深度信息，并计算样本的内部噪音；2.对批次内地所有样本，计算样本间的相关性，完成聚类（聚类使用umap默认参数,聚类数目为15，所以所提供的样本需要超过16例样本。 样本聚类情况统计测试数据载入后，会进行前期的聚类分组，聚类结果见 $fold&quot;/results/clusterization_of_samples.tsv 会输出聚类后所有样本的cluster分类情况(sample_id，cluster_id)123456789101112131420240827__***********47 020240827__***********48 020240827__***********49 020240827__***********50 020240827__***********51 020240827__***********52 020240827__***********53 020240827__***********54 020240827__***********56 020240827__***********57 020240827__***********58 020240827__***********59 020240827__***********60 020240827__***********61 0 同时也会生成聚类图$fold&quot;/results/clusteringSolution.png&quot; 样本覆盖度情况统计$fold&quot;/results/ontargetNormal.summary.xls&quot; 会统计整批分析样本的平均深度信息以及计算得到的噪音信息(noises &lt;- round(apply(gcNormalisedCov[which(!bedFile[,1] %in% c(“chrX”,”chrY”)),], 2, mad), digits = 2))。123456789namesOfOutputFile noises avgDepth20240827__*********47 0.31 183.0165289256220240827__*********48 0.33 155.84251968503920240827__*********49 0.33 140.25746268656720240827__*********50 0.34 142.70769230769220240827__*********51 0.33 165.57024793388420240827__*********52 0.34 151.57024793388420240827__*********53 0.34 119.43801652892620240827__*********54 0.33 162.801418439716 样本CNV检测结果带 cnvs 标记的，都是样本的CNV检测结果对应的信息，其中 tsv 文件为 ClinCNV 输出的CNV详细信息（涵盖了bin的输入，CNV的长度，覆盖的基因等。两个 seg 后缀的文件为IGV的tracks文件， _cov.seg，提供了实际覆盖率的值（覆盖率值是 GC 归一化和中位数归一化，以常染色体中的拷贝数 2 为中心） _cnvs.seg文件中，拷贝数范围只有 [0,1,2,3,4,5,6] ,所有6拷贝以上的CNV，都按6进行输出。 **_cnvs.tsv最主要的CNV结果，结果由两部分构成，标签标题行和结果表格。 标签标题行示例如下：12345678##ANALYSISTYPE=CLINCNV_GERMLINE_SINGLE##ClinCNV version: v1.19.0##Analysis finished on: 2024-09-25 14:54:39.518772##gender of sample: M##number of iterations: 1##quality used at final iteration: 20##was it outlier after clustering: FALSE##fraction of outliers: 0.053 前 1 行 是技术性的记录。 前 2 行 是软件版本。 前 3 行 是分析的时间。 第 4 行 显示推断的性别。 第 5 行 自从样本中的 CNV 数量变得可接受以来，质量分数增加迭代了多少次。 第 6 行 最终实际使用的质量值。 第 7 行 显示该样本在批次效应聚类后是否为异常值。 第 8 行 异常值的分数，显示有多少个覆盖数据点的 p 值小于 0.05。ps 它不能真正用于 QC 控制，因为较大的非整倍性会导致高比例的异常值。 The last line – fraction of outliers – shows how many coverage data points had p-value less than 0.05. It can not really be used for the QC control since having large aneuploidy will lead to high fraction of outliers.最后一行（异常值的分数）显示有多少个覆盖数据点的 p 值小于 0.05。它不能真正用于 QC 控制，因为较大的非整倍性会导致高比例的异常值。 结果表格下： #chr start end CN_change loglikelihood no_of_regions length_KB potential_AF genes qvalue chr1 22315703 22336693 1 231 10 20.990 0.076 - 0.00000 chr1 17248483 17275039 1 48 18 26.556 0.152 - 0.00419 chr1 25599018 25655627 1 205 11 56.609 0.127 - 0.00000 chr1 17060426 17086502 3 18 10 26.076 0.114 - 0.04763 chr1 110230453 110236367 4 25 9 5.914 0.253 - 0.01483 #chr： 染色体; start： CNV的起始; end：CNV的终止; CN_change：检测到的变体的拷贝数; loglikelihood：与基线（二倍体或男性性染色体的 1 个拷贝）相比，检测到的特定区域的拷贝数的可能性。,对数似然 &gt; 10 意味着支持替代拷贝数的证据强度“非常强”，但是，我们建议您保持更高的值（40 或至少 20），因为基因组覆盖度受到影响诸如短插入缺失等众多事件导致比对问题、批次效应、测序深度、技术假象等，而且基因组相当长，因此看到大的对数似然变化并不罕见。 no_of_regions: 显示变体中包含多少个数据点。较长的变体通常更可信（但并非总是如此，例如，对loglikelihood值较小的长变体可能是误报）。 length_KB：CNV的长度（start和end间的距离） potential_AF：显示特定区域的覆盖率异常较低/较高的频率（如果变体分别为删除/重复）。 genes：输入的.bed文件包含基因信息时， 会填充 genes 信息。 qvalue： Z-test的p值，高p值，则对应区域可能是 CNV多态性 区域。 **_cnvs.seg ID chr start end value 20240827__24B07832897 chr1 22315703 22336693 231.01 1 20240827__24B07832897 chr1 17248483 17275039 47.98 1 20240827__24B07832897 chr1 25599018 25655627 205.2 1 **_cov.seg记录了bed上每个区域的异常值和折算的拷贝数信息。 ID chr start end variance value 20240827__24B07832897 chr1 65498 65638 0.0934 1.98 20240827__24B07832897 chr1 69036 70008 0.0604 2.3 20240827__24B07832897 chr1 367648 368607 0.0836 2.24 20240827__24B07832897 chr1 564537 564657 0.1381 2.14 20240827__24B07832897 chr1 621085 622044 0.0839 2.17 软件使用针对遗传检测基础分析参数1234567Rscript /ifstj2/B2C_RD_H1/Personal/liubo/3.Software/ClinCNV-master/clinCNV.R \# 待检测的目标区域文件 bed，详见demo测试部分具体说明--bed /jdfstj6/B2C_RD/liubo4/product/Large_CNV/00.bed.info/WESv5withSafeRisk.addGCRate.bed \# 待检测的目标区域覆盖度文件，详见demo测试部分具体说明--normal /jdfstj6/B2C_RD/liubo4/product/Large_CNV/by_ClinCNV/20240915_DNBSEQ-T7A2302409150004.mergeCov.tsv \# 指定输出目录，将在输出文件夹中创建normal文件夹，结果将放入子文件夹中，每个子文件夹一个样本。--out /jdfstj6/B2C_RD/liubo4/product/Large_CNV/by_ClinCNV/20240915_DNBSEQ-T7A2302409150004_ClinCNV 可选参数 –normalOfftarget &amp; –bedOfftarget添加脱靶覆盖率（不会大幅提高灵敏度，因为种系 CNV 很少与脱靶窗口一样长，但可以提高特异性，有效去除彼此远离的探针形成的 CNV，必须提取足够大的脱靶覆盖率样本数，但不一定是normal.cov文件中的所有样本）。e.g.: --normalOfftarget normalOff.cov --bedOfftarget bedFileOff.bed –scoreG平衡灵敏度和特异性（增加阈值–scoreG 会导致更高的特异性和更低的灵敏度，反之亦然，默认值为 20）。e.g.: --scoreG 60 –lengthG增加或减少检测到的变体的最小长度（默认 = 2 个数据点或更大，实际数据点数量实际上是您在此处指定的 +1，0 表示至少 1 个数据点）。e.g.: --lengthG 0 –maxNumGermCNVs &amp; –maxNumIter增加您期望从该样本中获得的 CNV 数量（默认 = 10000，但对于 WGS 样本，最好将其设置为 1000 甚至更大，当超过此阈值时，质量阈值会增加，并且样本会重新分析多次（由 –maxNumIter 指定）e.g.: --maxNumGermCNVs 2000 --maxNumIter 5 –minimumNumOfElemsInCluster 50将大队列分成几个相似样本的集群，以进行更准确的参数估计并加快工具的速度。您指定集群的最小大小（我们建议将其保持在 20 到 100 之间）：e.g.: --minimumNumOfElemsInCluster 50 –numberOfThreads 4通过使用多个核心来加速分析过程中的某些部分，以提高分析速度。e.g.: --numberOfThreads 4 Polymorphic CNVs人群频率大于 2.5% 的 CNV 通常更难以使用常规方法检测。因此，我们使用高斯混合模型检测它们。这是同一脚本中的单独例程。要调用此类变体，您需要指定 –polymorphicCalling YES如果提供的参数不是 YES 而是带有多态区域坐标的.bed文件路径， ClinCNV将在调用过程中忽略他们。 嵌合体嵌合体CNV的检测范围为 1.1 到 2.9，步长为 0.05。 –mosaicism 软件原理介绍因为目前预期用途是进行胚系的WES应用评估，所以相关方法说明参考的 2022年的ClinCNV: multi-sample germline CNV detection in NGS data方法检测整体是基于深度的，算法可以比较容易的将等位基因频率纳入考量，但是文章认为等位基因频率对体系CNV的帮助比较大，但是对于胚系CNV检测作用并没有体系那么大。 method检测方法整体分为两部分： 数据的标准化； CNV的Calling（这部分基于作者2019年发布的前一篇文章ClinCNV: novel method for allele-specific somatic copy-number alterations detection。 标准化在这边文章所述方法中，作者针对标准化进行了调整，来实现以下目的（源于补充材料）： 构建bin文件，分析的区段单元，通常是120bp，可以是WES富集捕获的区段，也可以是其他有依据的划分方式。（前期准备） 针对每个bin文件，统计reads的数目。（前期准备） 利用平方根转换$f(x)=\sqrt(x)$，根据(GC-content, region-length based, variance stabilization)进行方差稳定的数据标准化。 CNV检测 在所有样本中，寻找和目标样本的覆盖度相似度最高的样本，根据coverage 对样本进行聚类。 进行样本间的标准化，对聚类到一起的样本使用覆盖度的中位值进行标准化。 对统计模型进行参数估计用来描述不同的拷贝数状态（针对每个样本的每个区域）。 构建似然矩阵，矩阵的行对应预先定义的拷贝数状态，矩阵的列对应我们在 step1 中定义的bin的数目。 对bin进行片段化，并进行结果检出 使用各种QC矩阵进行注释，并对结果进行可视化。引自2019年发表的第一篇文章 12345678910graph TB;A(*bed 提前根据探针或其他方式确定分析的bin)--&gt;CB(*bam 经过上游预处理得到的样本数据)--&gt;CC(*cov 针对每个bin统计reads数目)--平方根转换--&gt;DD(标准化后的覆盖度数据)--样本间覆盖相似度比较--&gt;EE(样本覆盖度的聚类结果)--聚类一组的样本进行中位值标准化--&gt;FF(对标准化的覆盖度使用参数模型来描述拷贝数状态)--&gt;GG(构建似然矩阵)--&gt;HH(对bin进行片段化生成大区段CNV)--&gt;II(生成QC，并进行可视化)]]></content>
      <categories>
        <category>NGS</category>
        <category>software</category>
        <category>编程拾慧</category>
        <category>R</category>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>CNV</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[神经网络-深度学习]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-20.MachineLearn%2F2102.%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[前文介绍了多层感知机，而多层感知机，其实是神经网络的一种特殊情况（只有一层隐藏层 &amp; 全链接），而当我们引入多层隐藏层进行分析时，就是常说的深度学习，其中的深度，就是指的有多个隐藏层。整体框架结构上，深度学习和多层感知机并没有基础结构件的区别，但是就像我们代码为了提高代码的复用率一样，当我们具有非常多隐藏层的时候，我们往往需要定义一个抽象的层以及定义一个抽象的块，来帮助我们更好的对模型结构进行复用。 层和块介绍神经网络时，我们 开始我们关注的是具有单一输出的线性模型。，整个模型只有一个输出。 单个神经网络 （1）接受一些输入； （2）生成相应的标量输出； （3）具有一组相关 参数（parameters），更新这些参数可以优化某目标函数。 然后，考虑具有多个输出的网络时， 我们利用矢量化算法来描述整层神经元。 像单个神经元一样，层（1）接受一组输入， （2）生成相应的输出， （3）由一组可调整参数描述。 当我们使用softmax回归时，一个单层本身就是模型。 然而，即使我们随后引入了多层感知机，我们仍然可以认为该模型保留了上面所说的基本架构。 对于多层感知机而言，整个模型及其组成层都是这种架构。 整个模型接受原始输入（特征），生成输出（预测）， 并包含一些参数（所有组成层的参数集合）。 同样，每个单独的层接收输入（由前一层提供）， 生成输出（到下一层的输入），并且具有一组可调参数， 这些参数根据从下一层反向传播的信号进行更新。 我们可以看到，整体模式其实都是 根据一些参数， 获取原始输入，得到一个输出结果。而一些项目我们可能会用到非常多的层， 例如，在计算机视觉中广泛流行的ResNet-152架构就有数百层， 这些层是由层组（groups of layers）的重复模式组成。 这个ResNet架构赢得了2015年ImageNet和COCO计算机视觉比赛 的识别和检测任务 (He et al., 2016)。 目前ResNet架构仍然是许多视觉任务的首选架构。 在其他的领域，如自然语言处理和语音， 层组以各种重复模式排列的类似架构现在也是普遍存在。为了实现这些复杂的网络，我们引入了神经网络块的概念。 块（block）可以描述单个层、由多个层组成的组件或整个模型本身。 使用块进行抽象的一个好处是可以将一些块组合成更大的组件， 这一过程通常是递归的，如下所示。 通过定义代码来按需生成任意复杂度的块， 我们可以通过简洁的代码实现复杂的神经网络。]]></content>
      <categories>
        <category>neural_network</category>
      </categories>
      <tags>
        <tag>deep_learning</tag>
        <tag>neural_network</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2101.神经网络-算法-多层感知机]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-20.MachineLearn%2F3001.%E6%A6%82%E5%BF%B5-%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B-%E7%B2%BE%E5%BA%A6%2F</url>
    <content type="text"><![CDATA[在大模型训练中，数据格式的选择对于计算资源的利用、内存占用和训练速度具有重要意义。合适的数据格式可以提高模型的性能，减少内存占用，并加速训练过程。数据类型在机器学习中被称为精度。 IEEE 754-2008标准定义了浮点数算术的表示和操作，包括单精度（FP32） 和 双精度（FP64）浮点数、以及新增的16位的半精度浮点数。不同的数据格式表示是完全一样的，都是指数小数形式，区别只有指数的位数(决定了数值的范围，即数值的大小)和小数的位数(决定了数值的精度，即数值的小数部分的精确度)。常见的数据类型 32位浮点数（FP32）：FP32是标准的 IEEE 32 位浮点表示。使用该数据类型，可以表示大范围的浮点数。在 FP32 中，为“指数”保留了 8 位，为“尾数”保留了 23 位，为符号保留了 1 位。因为是标准数据类型，所以大部分硬件都支持 FP32 运算指令。 16位浮点数（FP16）：FP16使用一半的位数表示浮点数，相较于FP32，可以提高计算速度，降低内存占用，但精度略有损失。在 Float16 (FP16) 数据类型中，指数保留 5 位，尾数保留 10 位。这使得 FP16 数字的数值范围远低于 FP32。因此 FP16 存在上溢 (当用于表示非常大的数时) 和下溢 (当用于表示非常小的数时) 的风险。 脑浮点数（BF16）：BF16是一种针对深度学习优化的半精度浮点数格式，与FP16具有相似的精度和性能特点，但在某些硬件平台上可能具有更好的兼容性。BF16数据格式是由谷歌大脑（Google Brain）团队提出的一种针对深度学习的半精度浮点数表示。BF16在2019年被纳入了ONNX（Open Neural Network Exchange）格式，这是一种用于表示深度学习模型的开放标准。与FP16相比，BF16具有更宽的动态范围，但具有较少的尾数位数（7位）。这种设计使得BF16在表示较大数值时具有更高的精度，同时仍然保持了较低的内存占用和计算成本。 TF32：由NVIDIA提出开发的，全称 Tensor float32，也是为了加速机器学习的一种格式。TF32目前只有在Ampere架构以上才支持。是为了专门在 Tensor Core 上加速的一种数据结构。 在选择合适的数据格式时，需要考虑以下几个关键要素： 精度：数据格式应能够在保持足够精度的同时，减少内存占用。通常，较低精度的数据格式（如FP16、BF16）可以提高计算速度，但可能牺牲一定的精度。如果模型对精度要求较高，可以选择FP32；如果可以接受一定程度的精度损失，可以考虑使用FP16或BF16。 计算效率：数据格式应能够充分利用硬件资源，提高计算效率。例如，某些硬件加速器对特定数据格式有更好的支持，从而提高计算性能。了解所使用的硬件平台对不同数据格式的支持情况，选择能够在当前硬件上发挥最佳性能的数据格式。 内存占用：在大模型训练中，内存资源通常是非常宝贵的。选择能够降低内存占用的数据格式，可以在有限的硬件资源下训练更大的模型。 兼容性：数据格式应与所使用的深度学习框架和硬件平台兼容，以便于实现端到端的训练和部署。]]></content>
      <categories>
        <category>neural_network</category>
      </categories>
      <tags>
        <tag>MLP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2101.神经网络-算法-多层感知机]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-20.MachineLearn%2F2101.%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA-MLP%2F</url>
    <content type="text"><![CDATA[多层感知器(MLP)是一种前馈人工神经网络，它拥有输入层、隐藏层（至少1层）和输出层。如果隐藏层超过1个，则又称为深度人工神经网络。多层感知机（Multilayer Perceptron，MLP）是神经网络（Neural Network）的一种特殊类型。 MLP是一种前馈神经网络，它由至少三层节点组成：输入层、一个或多个隐藏层以及输出层。 在标准的MLP中，每一层的节点都是全连接的，即每一层的所有节点都与下一层的所有节点相连。 MLP中的节点（或神经元）通常使用非线性激活函数，允许网络学习复杂的函数映射。 MLP通常使用梯度下降和反向传播算法来训练，调整网络权重以最小化损失函数。 多层感知器(MLP)工作机制其实大多数线性问题，我们可以用更简单的线性回归（可以简单的理解为只有输入层和输出层的简化神经网络）来解决，但线性意味着单调假设：任何特征的增大都会导致模型输出的增大（如果对应的权重为正）， 或者导致模型输出的减小（如果对应的权重为负）。 有时这是有道理的。例如，如果我们试图预测一个人是否会偿还贷款。 我们可以认为，在其他条件不变的情况下， 收入较高的申请人比收入较低的申请人更有可能偿还贷款。 但是，虽然收入与还款概率存在单调性，但它们不是线性相关的。 收入从0增加到5万，可能比从100万增加到105万带来更大的还款可能性。 处理这一问题的一种方法是对我们的数据进行预处理， 使线性变得更合理，如使用收入的对数作为我们的特征。 然而我们可以很容易找出违反单调性的例子。 例如，我们想要根据体温预测死亡率。 对体温高于37摄氏度的人来说，温度越高风险越大。 然而，对体温低于37摄氏度的人来说，温度越高风险就越低。 在这种情况下，我们也可以通过一些巧妙的预处理来解决问题。 例如，我们可以使用与37摄氏度的距离作为特征。 但是，如何对猫和狗的图像进行分类呢？ 增加位置处像素的强度是否总是增加（或降低）图像描绘狗的似然？ 对线性模型的依赖对应于一个隐含的假设， 即区分猫和狗的唯一要求是评估单个像素的强度。 在一个倒置图像后依然保留类别的世界里，这种方法注定会失败。这里的线性很荒谬， 而且我们难以通过简单的预处理来解决这个问题。 这是因为任何像素的重要性都以复杂的方式取决于该像素的上下文（周围像素的值）。 我们的数据可能会有一种表示，这种表示会考虑到我们在特征之间的相关交互作用。 在此表示的基础上建立一个线性模型可能会是合适的， 但我们不知道如何手动计算这么一种表示。 对于深度神经 网络，我们使用观测数据来联合学习隐藏层表示和应用于该表示的线性预测器。 多层感知机结构所以这时候，我们可以在网络中加入一个或多个隐藏层来克服线性模型的限制， 使其能处理更普遍的函数关系类型。 要做到这一点，最简单的方法是将许多全连接层堆叠在一起。 每一层都输出到上面的层，直到生成最后的输出。 我们可以把前 $ L-1 $层看作标识，把最后一层看作线性预测器。 这种架构通常称为多层感知机（multilayer perceptron），通常缩写为MLP。 下面，我们以图的方式描述了多层感知机。借助隐藏层的存在，从而可以使多层感知机可以实现复杂的非线性映射。为了便于理解，我们可以带入一个简单的示例： 产品经理向我们剔除一个需求，问我们能不能做，那么一个简化的多层感知机模型示意如下：输入层会有很多实际的现状情况，对应隐藏层，其实就是我们基于不同的已知信息（输入层）按不同权重、偏置来计算隐藏层的输出（每个节点可以抽象为我们需要考虑的一个特定方面，每个方面都会不同程度上决定我们最终是否进行开发），然后通过输出层，综合考虑隐藏层（各个方向的结论），得到最终的输出。从这个示例，我们其实可以看到，针对每个输入的信息，我们都可以进行加权判定生成隐藏层，在多隐藏层的情况下，我们也可以对数据进行多轮加权判定，从而理论上可以实现基于已知输入的所有逻辑类型。 需要注意的时，具有全连接层的多层感知机的参数开销可能会高得令人望而却步。 即使在不改变输入或输出大小的情况下， 可能在参数节约和模型有效性之间进行权衡。 激活函数如前文提到的，我们引入神经网络，引入隐藏层，实际是为了解决线性模型不能解决的复杂问题，实现非线性变换。使得神经网络能够学习和逼近复杂的函数映射。而这里实现非线性变换，就是依靠激活函数来实现的，对特定的输入使用激活函数进行处理，形成隐藏层数据。激活函数接收神经元的加权输入和偏置之和作为输入，然后输出一个经过某种非线性变换的值。这种非线性变换允许神经网络在多层结构中堆叠时，捕捉到数据中的复杂模式和层次结构。 激活函数在神经网络中至关重要，它们为神经网络引入非线性，使网络能够学习和逼近复杂的函数映射。激活函数都是非线性的，而我们在计算激活值时使用的都是线性运算，引入激活函数能让我们在后续的计算中逼近任何的非线性函数，从而使神经网络有更好的普适性。 Sigmoid函数 公式: f(x) = 1 / (1 + e^-x) 应用场景: 早期神经网络中常用，尤其适用于输出层，当需要输出值在0到1之间时。 缺点: 易发生梯度消失，计算效率不高。tanh函数 公式: f(x) = tanh(x) 应用场景: 类似于Sigmoid函数，但输出范围在-1到1之间，有助于网络收敛。 缺点: 存在梯度消失问题。 ReLU函数 (Rectified Linear Unit) 公式: f(x) = max(0, x) 应用场景: 广泛用于深层神经网络的隐藏层，特别是在卷积神经网络中。 优点: 计算简单，缓解梯度消失问题。 缺点: 对于负输入，梯度为0，可能导致神经元“死亡”。 Leaky ReLU函数 公式: f(x) = max(alpha * x, x)，其中alpha是一个小的正数。 应用场景: 解决ReLU函数在负数区域梯度为0的问题。 优点: 避免神经元“死亡”。 ELU函数 (Exponential Linear Units) 公式: f(x) = x 如果 x &gt; 0；f(x) = alpha * (e^x - 1) 如果 x &lt;= 0 应用场景: 类似于Leaky ReLU，但输出均值更接近于0。 优点: 输出均值接近0，有助于梯度传播。 SELU函数 (Scaled Exponential Linear Units) 公式: f(x) = lambda * x 如果 x &gt; 0；f(x) = lambda * (alpha * (e^x - 1)) 如果 x &lt;= 0 应用场景: 设计用于自动归一化网络。 优点: 使网络自动达到归一化状态。 Softmax函数 公式: f(x_i) = e^(x_i) / sum(e^(x_j))，对输出层，将向量转换为概率分布。 应用场景: 主要用于多分类问题的输出层。 选择激活函数时，需要根据模型的具体需求、数据特性和任务类型来决定。例如，对于回归任务，输出层可能不需要激活函数（或使用线性激活函数）。而在隐藏层中，ReLU和它的变体因其计算效率和梯度传播的优势而广泛使用。 权重与偏置的调整在多层感知机（MLP）中，权重（weights）和偏置（biases）是神经网络模型中的关键参数，它们在神经元的计算过程中起着核心作用。下面分别解释这两个概念： 权重（Weights）权重代表了神经网络中神经元之间的连接强度。每个连接都有一个对应的权重值，这个值决定了输入信号对于后续神经元输出的影响程度。在数学上，权重通常表示为矩阵，用于将前一层神经元的输出与当前层神经元的输入相连接。 当权重较大时，意味着前一层的神经元对该神经元的输出有较大的影响。 当权重较小时，意味着前一层神经元的影响较小，或者可以说该神经元对于当前神经元的输出贡献不大。 权重的学习和调整是通过训练过程中的反向传播算法完成的，目的是最小化网络输出与期望输出之间的差异（损失函数）。 偏置（Biases）偏置是附加到每个神经元上的一个额外参数，它不依赖于任何输入，而是在神经元的激活计算中始终存在。偏置的作用类似于数学方程中的截距，它允许神经元的激活阈值进行调整，从而影响神经元何时被激活。 偏置允许每个神经元的激活函数在不考虑输入的情况下也能产生输出。如果没有偏置，神经元只能通过输入信号的组合来激活，这会限制模型的灵活性。 调整偏置值可以让神经元即使在输入为零时也能产生非零输出，或者改变神经元被激活的阈值，这对于学习更复杂的决策边界是非常重要的。 偏置的值也是通过训练过程进行学习和更新的，以优化网络的整体性能。 在MLP中，每个神经元的净输入（即神经元的加权输入总和加上偏置）通过激活函数进行转换，产生该神经元的最终输出。这个过程可以表示为：$step1: z=Wx+b $$step2: a=f(z) $z 是神经元的净输入， W 是权重矩阵， x 是输入向量， b 是偏置向量， a 是经过激活函数 $f(z)$ 转换后的输出。权重和偏置的优化是神经网络训练的核心，机器学习的过程其实就是模型根据提供的训练数据不断调整和优化权重和偏置的过程。 参数性能的度量：损失函数和代价函数我们想要让模型自己基于数据进行训练学习，寻找最优的权重和偏置，以使得模型在训练数据上的表现达到最好的效果。有一个前提，就是我们首先须能够评估（量化）每个不同的权重或偏置的表现如何，因此为了评估不同参数对应的性能表现，我们引入了损失函数（对单个样本而言）、代价函数（对训练集所有样本的损失函数值的平均函数）。损失函数就是给矫正参数提供数据支撑的，损失函数因不同的算法而异，我们可以根据项目需求进行选择。但是无论是哪种损失函数，它都要符合一个原则：当网络能对图像进行正确分类时，损失函数值要比较小，偏差的越大，损失值越大。通过损失函数的构建，我们完成了对要解决问题的转换和抽象化，现在我们要解决的问题是：如何找到一个参数组合，使得训练集上的损失函数值最小。 参数调整的方法通过损失函数，我们可以评估一套参数表现的好和坏，但是我们的目的，是找到最优参数，针对多层感知机的特殊性（是一种全链接的神经网络），我们不难发现随着隐藏层的增加和隐藏节点的增加，模型的参数量会急速增加，而每个参数都可能是从(-∞,+∞)的庞大范围，参数组合更是无法枚举的量级。所以显然我们还需要一些方法/技巧来帮助我们更合理的对参数进行快速优化，找出最优解（而不是随机抽签，或者遍历宇宙间的所有可能┑(￣Д ￣)┍）。 目前比较常用的方法有： 梯度下降法：梯度下降法是一种迭代的方法，通过逐步更新参数向量来逼近目标函数的最小值。其核心思想是通过梯度下降的方式逐步更新参数向量，使目标函数的值逐步减小。为了方便理解，我们在这先介绍两个概念，梯度和学习率，分别决定我们更新参数的方向和步长。 梯度： 梯度的数学定义：梯度是一个向量（或矢量），用于表示某一函数在该点处的方向导数沿着该方向取得最大值。简言之，梯度指明了函数在该点处变化最快的方向和变化率。 学习率：学习率是梯度下降法的一个超参数，它控制着每次调整参数的大小， learning rate 越大，每次调整的参数越大，收敛速度越快，但 learning rate 过大可能会导致梯度下降过程不稳定， learning rate 过小可能会导致梯度下降过程收敛速度过慢。 更直观的讲,梯度可以被想象为多维空间中的“斜率指南”，它告诉我们函数在哪个方向上增长（或下降）最快。下面我们分别借助一元函数（方向是线的两面）、二元函数（方向是一个发散的平面）更好的理解梯度的概念。 先抛开自变量众多的代价函数，我们先看看如何求解一元函数的最小值，其方法在任意一本微积分教材上应该都有：计算导数为0的点，确定极小值，计算极小值中的最小值。但我们本身使用神经网络就是为了拟合一些复杂函数，这类函数有些可能求导本身可能就已经非常困难了，而同时这些复杂网络中，我们要处理的节点数量成千上万。更巧妙的方法是：随便挑一个输入值，然后判断，向左走还是向右走函数值才会变小呢？如果随机的这一点处斜率为正，向左走一点能让函数值变小；如果斜率为负那么就向右走一点。走的时候还需要注意：斜率越平缓时走的每一步应该越来越小，这样可以保证不会“走过头”而越过极小值点。（但我们需要注意：极小值点不一定是最小值点，神经网络也会遇到高等数学解题中同样的问题。） 拓展到二元函数：类比一元函数，在二元函数的图像中，我们参数调整的范围从左右线性编程了一个平面，这时候我们要思考的应该是“哪个方向下山最快？”，尽管尚未接触二元函数知识，但我们知道，按梯度的方向走，函数值增长的最快，那么按梯度的反方向，函数值（对应损失函数）就减少的最快，而且梯度向量的长度代表了这个最陡的斜坡到底有多陡。这样我们就得到了一种让函数值最小的算法：计算梯度 → 按梯度反方向调整自变量 → 循环，这对多个自变量为输入的代价函数也是一样，将所有输入作为n维的列向量并计算负梯度并加在列向量上就能计算出调整后的神经网络参数。 扩展到多元函数，我们只需要计算出每个参数的偏导数，然后所有参数的偏导共同构成了我们的梯度向量$ ∇f=({∂f \over ∂x}​,{∂f \over ∂y​})$，这个梯度向量在几何上表示了函数在该点处增长最快的方向（即函数值/损失函数 增加最快的方向），而梯度的负方向则是指向函数值减小最快的方向，这正是梯度下降法所利用的性质。然后按照梯度反方向调整参数，就可以得到一个梯度下降法。 需要注意的是，代价函数是对训练集而言，也就是很可能出现这样一种情况：随机的参数在特定输入上得到了正确答案，但训练后的参数反而给出了错误的答案，这是因为上述梯度下降的过程是针对所有样本而言的，训练后的参数对所有样本得到的总体结果会更好一些。 几种应用类型： 批量梯度下降法，是梯度下降法最常用的形式，具体做法也就是在更新参数时使用所有的样本来进行更新。由于我们有m个样本，这里求梯度的时候就用了所有m个样本的梯度数据。 随机梯度下降法，其实和批量梯度下降法原理类似，区别在与求梯度时没有用所有的m个样本的数据，而是仅仅选取一个样本j来求梯度。对应的更新公式是： 小批量梯度下降法，是批量梯度下降法和随机梯度下降法的折衷，也就是对于m个样本，我们采用x个样本来迭代，$1&lt;x&lt;m$。一般可以取$x=10$，当然根据样本的数据，可以调整这个x的值。 随机梯度下降法和批量梯度下降法是两个极端，一个采用所有数据来梯度下降，一个用一个样本来梯度下降。自然各自的优缺点都非常突出。对于训练速度来说，随机梯度下降法由于每次仅仅采用一个样本来迭代，训练速度很快，而批量梯度下降法在样本量很大的时候，训练速度不能让人满意。对于准确度来说，随机梯度下降法用于仅仅用一个样本决定梯度方向，导致解很有可能不是最优。对于收敛速度来说，由于随机梯度下降法一次迭代一个样本，导致迭代方向变化很大，不能很快的收敛到局部最优解。 牛顿法而我们要寻找损失函数的极值，除了像梯度下降法一样根据斜率和学习率进行迭代遍历（效率会收到学习率的影响，同时最终精度也会受到学习率的影响），还可以借助目标函数（$f(x)$)的导数（$f’(x)$)来更快地找到目标函数的最小值，也就是寻找（$f’(x)=0$)的点。这时候，我们又进一步将问题转换为求解$f’(x)=0$的问题。具体步骤如下： 选择一个接近函数 $f(x)$零点的 $x_0$，计算相应的 $f (x0)$ 和切线斜率$f’(x_0)$（这里$f’$ 表示函数 $f$ 的导数）。然后我们计算穿过点$(x_0, f(x_0))$ 并且斜率为$f’(x_0)$的直线和 x 轴的交点的x坐标，也就是求如下方程的解：$$xf’(x_0)+f(x_0)-x_0f’(x_0)=0 等价于 x_1=x_0-{f(x_0)\over f’(x_0)}$$ 我们将新求得的点的 $x$ 坐标命名为$x_1$，通常$x_1$会比$x_0$更接近方程$f(x) =0$的解。因此我们现在可以利用$x_1$开始下一轮迭代。迭代公式可化简为如下所示： $$x_{n+1}=x_n-{f(x_n)\over f’(x_n)}$$ 已经证明，如果 $f’$ 是连续的，并且待求的零点$x$是孤立的，那么在零点x周围存在一个区域，只要初始值$x0$位于这个邻近区域内，那么牛顿法必定收敛。 并且，如果 $f’(x)$不为0, 那么牛顿法将具有平方收敛的性能. 粗略的说，这意味着每迭代一次，牛顿法结果的有效数字将增加一倍。 由于牛顿法是基于当前位置的切线来确定下一次的位置，所以牛顿法又被很形象地称为是”切线法”。牛顿法的搜索路径（二维情况）如下图所示：下图为一个牛顿法执行过程的例子。 牛顿法是一种高效的方法，通过使用目标函数的二阶导数来更快地找到目标函数的最小值。对于初始位点的需求没有梯度下降法这么高，同时由于具有收敛性，所以经过足够迭代我们总可以获取所需精度的结果，而不需要依赖于学习率。]]></content>
      <categories>
        <category>neural_network</category>
      </categories>
      <tags>
        <tag>MLP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2101.神经网络-算法-多层感知机]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-20.MachineLearn%2F3002.%E6%A6%82%E5%BF%B5-%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%2F</url>
    <content type="text"><![CDATA[激活函数我们引入神经网络，引入隐藏层，实际是为了解决线性模型不能解决的复杂问题，实现非线性变换。使得神经网络能够学习和逼近复杂的函数映射。而这里实现非线性变换，就是依靠激活函数来实现的，对特定的输入使用激活函数进行处理，形成隐藏层数据。激活函数接收神经元的加权输入和偏置之和作为输入，然后输出一个经过某种非线性变换的值。这种非线性变换允许神经网络在多层结构中堆叠时，捕捉到数据中的复杂模式和层次结构。 激活函数在神经网络中至关重要，它们为神经网络引入非线性，使网络能够学习和逼近复杂的函数映射。激活函数都是非线性的，而我们在计算激活值时使用的都是线性运算，引入激活函数能让我们在后续的计算中逼近任何的非线性函数，从而使神经网络有更好的普适性。 Sigmoid函数 公式: f(x) = 1 / (1 + e^-x) 应用场景: 早期神经网络中常用，尤其适用于输出层，当需要输出值在0到1之间时。 缺点: 易发生梯度消失，计算效率不高。 tanh函数 公式: f(x) = tanh(x) 应用场景: 类似于Sigmoid函数，但输出范围在-1到1之间，有助于网络收敛。 缺点: 存在梯度消失问题。 ReLU函数 (Rectified Linear Unit) 公式: f(x) = max(0, x) 应用场景: 广泛用于深层神经网络的隐藏层，特别是在卷积神经网络中。 优点: 计算简单，缓解梯度消失问题。 缺点: 对于负输入，梯度为0，可能导致神经元“死亡”。 Leaky ReLU函数 公式: f(x) = max(alpha * x, x)，其中alpha是一个小的正数。 应用场景: 解决ReLU函数在负数区域梯度为0的问题。 优点: 避免神经元“死亡”。 ELU函数 (Exponential Linear Units) 公式: f(x) = x 如果 x &gt; 0；f(x) = alpha * (e^x - 1) 如果 x &lt;= 0 应用场景: 类似于Leaky ReLU，但输出均值更接近于0。 优点: 输出均值接近0，有助于梯度传播。 SELU函数 (Scaled Exponential Linear Units) 公式: f(x) = lambda * x 如果 x &gt; 0；f(x) = lambda * (alpha * (e^x - 1)) 如果 x &lt;= 0 应用场景: 设计用于自动归一化网络。 优点: 使网络自动达到归一化状态。 Softmax函数 公式: f(x_i) = e^(x_i) / sum(e^(x_j))，对输出层，将向量转换为概率分布。 应用场景: 主要用于多分类问题的输出层。 选择激活函数时，需要根据模型的具体需求、数据特性和任务类型来决定。例如，对于回归任务，输出层可能不需要激活函数（或使用线性激活函数）。而在隐藏层中，ReLU和它的变体因其计算效率和梯度传播的优势而广泛使用。。当然这不是恒定的规律，我们可以尝试使用 Sigmoid 函数作为隐层激活函数，但注意使用时尽量不要超过太多隐层。另外可以使用 Tanh 函数来代替 Sigmoid 函数观察模型的精确率曲线图。如果直接使用 ReLU 函数作为激活函数，注意梯度下降算法的学习率参数不能设置得过高，避免神经元的大量“消亡”。对于输出层，一般使用 softmax 函数获得同分布最高概率作为输出结果。 此外，可以加入 Batch Normalization (BN)层，让下一层的输入数据具有相同的分布。如果遇到神经网络训练时收敛速度慢，或梯度爆炸或者梯度消失等无法训练的状况都可以尝试加入 BN层，然后观察其训练结果。]]></content>
      <categories>
        <category>neural_network</category>
      </categories>
      <tags>
        <tag>MLP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2101.神经网络-算法-多层感知机]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-20.MachineLearn%2F3004.%E6%A6%82%E5%BF%B5-%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%2F</url>
    <content type="text"><![CDATA[参数性能的度量：损失函数和代价函数我们想要让模型自己基于数据进行训练学习，寻找最优的权重和偏置，以使得模型在训练数据上的表现达到最好的效果。有一个前提，就是我们首先须能够评估（量化）每个不同的权重或偏置的表现如何，因此为了评估不同参数对应的性能表现，我们引入了损失函数（对单个样本而言）、代价函数（对训练集所有样本的损失函数值的平均函数）。损失函数就是给矫正参数提供数据支撑的，损失函数因不同的算法而异，我们可以根据项目需求进行选择。但是无论是哪种损失函数，它都要符合一个原则：当网络能对图像进行正确分类时，损失函数值要比较小，偏差的越大，损失值越大。 通过损失函数的构建，我们完成了对要解决问题的转换和抽象化，现在我们要解决的问题是：如何找到一个参数组合，使得训练集上的损失函数值最小。 在BP神经网络中，一般推导中，使用均方误差作为损失函数，而在实际中，常用交叉熵作为损失函数。如下图所示，我们可以清晰地观察到不同的损失函数在梯度下降过程中的收敛速度和性能都是不同的。 均方误差作为损失函数收敛速度慢，可能会陷入局部最优解； 而交叉熵作为损失函数的收敛速度比均方误差快，且较为容易找到函数最优解.]]></content>
      <categories>
        <category>neural_network</category>
      </categories>
      <tags>
        <tag>MLP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[0002.基于机器学习方向的重点学习内容]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-20.MachineLearn%2F0002.%E5%9F%BA%E4%BA%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%96%B9%E5%90%91%E7%9A%84%E9%87%8D%E7%82%B9%E5%AD%A6%E4%B9%A0%E5%86%85%E5%AE%B9%2F</url>
    <content type="text"><![CDATA[方向 Transformer CNN GNN 迁移学习 多模态 自然语言处理 框架 TensorFlow 2 PyTorch 2 Caffe MXNet Theano 开发环境 isaac mujoco Hadoop 2 Spark ODPS]]></content>
      <categories>
        <category>machine_learning</category>
      </categories>
      <tags>
        <tag>machine_learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[框架介绍.pytorch和tensorflow]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-20.MachineLearn%2F0101.%E6%A1%86%E6%9E%B6%E4%BB%8B%E7%BB%8D.pytorch%E5%92%8Ctensorflow%2F</url>
    <content type="text"><![CDATA[Pytorch Vs TensorFlow 是接触机器学习和深度学习方向非常成熟和主流的框架，AI、ML和DL框架不仅仅是工具；它们是决定我们如何创建、实施和部署智能系统的基础构建块。这些框架配备了库和预构建的功能，使开发人员能够在不从头开始的情况下制定复杂的人工智能算法。它们简化了开发过程，确保了各个项目的一致性，并使人工智能功能能够集成到不同的平台和应用程序中。但是学习过程总归是递进的，开始的选择可能直接影响我们后续的学习进度，和对整体方法的理解落地过程。通过了解他们的优势和局限性，一遍我们能够更好地做出符合我们项目要求和学习风格的明智决策。 使用方便 PyTorch： PyTorch 以其 Python 性质和简单性而闻名，通常因其直观的语法和易于理解而吸引初学者。它反映了 Python 的做事方式，让熟悉它的人可以使用它。用户经常称赞 PyTorch 构建和训练神经网络的简单方法，特别是其动态计算图，允许动态更改。这使得初学者的实验和调试相对容易。 一位 PyTorch 初学者评论道：“我发现将我的 Python 知识转化为在 PyTorch 中构建简单模型非常简单。” TensorFlow：从历史上看，TensorFlow 被认为具有更陡峭的学习曲线，主要是由于其静态计算图和更详细的语法。然而，随着 Keras 作为 TensorFlow 中的高级 API 的引入，这种情况发生了显着变化。Keras 以其用户友好的界面为初学者提供了一个更简单的入门点。TensorFlow 的最新版本专注于提高用户友好性，但它最初可能仍然被认为比 PyTorch 更具挑战性。 一位 TensorFlow 新用户提到：“我花了一些时间才掌握 TensorFlow 定义模型的方式，但广泛的文档和社区支持非常有帮助。” 灵活性和设计理念 PyTorch：PyTorch 的设计以灵活性和用户友好性为中心。其动态计算图（热切执行）允许开发人员动态更改模型的行为并使用本机 Python 控制流操作。这种动态性特别适合经常进行更改的复杂、迭代模型架构。 这就像塑造粘土一样——您可以边做边塑造和重塑您的模型。 TensorFlow：另一方面，TensorFlow 使用静态计算图，这需要在任何实际计算发生之前预先定义整个模型架构。这种方法虽然不如 PyTorch 灵活，但可以更直接地优化模型，从而可能在规模上带来更好的性能。TensorFlow 的理念类似于建造一座大楼——在建造之前你需要一个详细的蓝图。 对实际模型构建的影响：PyTorch：PyTorch 的灵活性使其成为研究和原型设计的理想选择，其中快速调整模型的能力至关重要。然而，这种灵活性有时会导致模型的优化程度低于 TensorFlow，特别是对于生产环境中的部署而言。TensorFlow：TensorFlow 的结构化方法有利于模型必须可扩展且高度优化的生产环境。然而，这有时会减慢实验过程，使其不太适合需要快速原型设计的研究目的。底线：PyTorch 可能对初学者更有吸引力，而研究人员则专注于实验和学习。相反，对于那些希望在生产中部署可扩展和优化模型的人来说，TensorFlow 可能是更合适的选择。 速度和效率基准测试场景：假设我们正在 MNIST 等标准数据集上训练基本的卷积神经网络 (CNN)。CNN 将具有一些卷积层、池化层和全连接层。要关注的性能指标是训练时间和内存使用情况。结果（假设）： 在此类测试中，您可能会发现 PyTorch 和 TensorFlow 在 GPU 上运行时在训练速度方面表现相似。但是，根据框架的特定版本和所使用的硬件，可能会出现变化。例如，由于其静态图性质，TensorFlow 在 GPU 使用效率方面可能会略有优势，底层引擎可以更轻松地对其进行优化。 资源使用：与 PyTorch 相比，TensorFlow 可能会在内存使用方面表现出更高的效率，尤其是在更大、更复杂的模型中，这要归功于它的图形优化。PyTorch 具有动态图，对于同一任务可能会消耗更多内存。 可扩展性PyTorch：PyTorch 具有高度可扩展性，并且越来越多地被大规模应用程序采用。其动态特性并不妨碍其可扩展性。随着 TorchScript 等功能的引入以及 PyTorch 支持分布式训练的能力，它能够处理大规模部署。但是，动态图在某些情况下可能会增加开销，特别是在扩展到非常大的模型或数据大小时。 TensorFlow：TensorFlow 以其可扩展性而闻名，特别是在生产环境中。它在涉及大型数据集和复杂神经网络架构的情况下表现出色。TensorFlow 的静态计算图可以针对不同的硬件配置进行优化，使其成为企业级大规模机器学习项目的稳健选择。TensorFlow 对分布式训练的支持和 TensorFlow Serving 对模型部署的支持也是其可扩展性的关键因素。 底线：这两个框架都提供了具有竞争力的性能和可扩展性，其中 TensorFlow 在大型项目的优化和资源管理方面稍有优势，而 PyTorch 提供的灵活性在快速变化和实验场景中具有优势。它们之间的选择应该受到项目的具体需求的影响，例如模型的大小、任务的复杂性和部署环境 PyTorch 的案例研究：Microsoft采用 PyTorch 进行语言建模展示了其灵活性如何有助于高级任务和架构的顺利迁移和开发。 丰田的实施展示了 PyTorch 在处理复杂的现实用例（例如自动驾驶汽车的视频处理）方面的能力。 Airbnb 的对话助手充分利用了 PyTorch 的神经机器翻译功能，在客户交互和服务增强方面的适用性。 Genentech在癌症治疗和药物发现中使用 PyTorch 说明了其在挽救生命的医学研究和个性化医疗应用中的潜力。 TensorFlow 案例研究：TensorFlow 的广泛行业采用包括从语音识别和Google照片搜索到实时翻译，甚至药物发现和基因组测序等复杂任务的应用。 这些现实世界的应用程序和案例研究反映了 PyTorch 和 TensorFlow 的不同功能，突出了它们在不同行业和用例中的适用性。PyTorch 经常因其在研究和快速原型设计中的易用性而受到称赞，而 TensorFlow 因其在生产级应用程序中的可扩展性和效率而受到认可 谁应该选择 PyTorch？PyTorch 特别适合优先考虑以下事项的个人和项目： 快速原型制作和研究：非常适合需要灵活框架来试验新颖想法和算法的学生、学者和研究人员。 动态环境：由于其动态计算图，对于需要动态更改模型的项目是有益的。 以 Python 为中心的开发：非常适合那些熟悉 Python 并寻求直观的 Python 界面的人。 学习和实验：由于其简单的语法和强大的社区学习支持，非常适合初学者。谁应该选择 TensorFlow？ TensorFlow 更适合：生产级项目：适合专注于在生产中部署可扩展和优化模型的行业和开发人员。 大规模应用程序：非常适合处理大型数据集和复杂的神经网络架构，尤其是在企业环境中。 全面的生态系统：对于那些需要大量工具和社区贡献资源的人来说是有利的。 边缘和移动部署：首选将模型部署到移动设备或边缘计算平台的项目。 这两种框架都具有独特的优势，选择很大程度上取决于项目的具体要求以及学习者或开发人员的偏好。 要点 PyTorch vs TensorFlow：两者都是强大的框架，具有独特的优势；PyTorch 受到研究和动态项目的青睐，而 TensorFlow 在大规模和生产环境中表现出色。 易于使用： PyTorch 提供了更直观的 Python 方法，非常适合初学者和快速原型设计。TensorFlow 经过最近的更新，变得更加用户友好。 性能和可扩展性：TensorFlow 针对性能进行了优化，尤其是在大型应用程序中。PyTorch 提供灵活性，有利于动态模型调整。 社区和资源： TensorFlow 拥有一个广泛且成熟的社区，拥有丰富的资源，而 PyTorch 拥有一个快速增长的社区，尤其在学术研究领域很受欢迎。 实际应用： PyTorch 在学术界和以研究为重点的行业中占据主导地位，而 TensorFlow 在工业界广泛用于大规模应用。 未来展望：这两个框架都在不断发展，PyTorch 专注于可用性，TensorFlow 专注于可扩展性和优化。 做出正确的选择：您的决定应该基于项目的需求 - PyTorch 用于灵活性和研究，TensorFlow 用于可扩展性和生产。]]></content>
      <categories>
        <category>machine_learning</category>
      </categories>
      <tags>
        <tag>machine_learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度学习-概述]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-20.MachineLearn%2F2001.%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-%E6%A6%82%E8%BF%B0%2F</url>
    <content type="text"><![CDATA[思维学普遍认为，人类大脑的思维分为抽象（逻辑）思维、形象（直观）思维和灵感（顿悟）思维三种基本方式。人工神经网络就是模拟人思维的第二种方式。这是一个非线性动力学系统，其特色在于信息的分布式存储和并行协同处理。虽然单个神经元的结构极其简单，功能有限，但大量神经元构成的网络系统所能实现的行为却是极其丰富多彩的。 神经网络的发展神经网络的灵感取自于生物上的神经元细胞,如下图这是人体神经元的基本构成，其中树突主要用于接收其他神经元的信号，轴突用于输出该神经元的信号。人的大脑皮层包括有100亿个以上的神经元，每立方毫米约有数万个，它们互相联结形成神经网络，通过感觉器官和神经接受来自身体内外的各种信息，传递至中枢神经系统内，经过对信息的分析和综合，再通过运动神经发出控制信息，以此来实现机体与内外环境的联系，协调全身的各种机能活动。使得我们人类能够进行高级的思考，能够不断地对新事物进行学习，做出反应。 仿照人类神经网络的结构，搭建一种人为的神经网络结构，从而使其能够完成一些计算任务，这也是神经网络名字的由来。 [[人工神经网络]]早期的研究工作应追溯至上世纪40年代。下面以时间顺序，以著名的人物或某一方面突出的研究成果为线索，简要介绍人工[[神经网络]]的发展历史。 1943年，心理学家W·Mcculloch和数理逻辑学家W·Pitts在分析、总结神经元基本特性的基础上首先提出神经元的数学模型“A logical calculus of the ideas immanent in nervous activity”。这项研究试图了解人脑如何通过连接的脑细胞或神经元形成复杂的模式。这项工作产生的一个主要想法是将具有二进制阈值的神经元与布尔逻辑（即 0/1 或真/假语句）进行比较。此模型沿用至今，并且直接影响着这一领域研究的进展。因而，他们两人可称为人工神经网络研究的先驱。 1945年[[冯·诺依曼]]领导的设计小组试制成功存储程序式[[电子计算机]]，[[标志]]着电子计算机时代的开始。1948年，他在研究工作中比较了人脑结构与存储程序式计算机的根本区别，提出了以简单神经元构成的再生自动机[[网络结构]]。但是，由于指令存储式[[计算机技术]]的发展非常迅速，迫使他放弃了神经网络研究的新途径，继续投身于指令存储式计算机技术的研究，并在此领域作出了巨大贡献。虽然，冯·诺依曼的名字是与普通计算机联系在一起的，但他也是人工神经网络研究的先驱之一。 50年代末，F·Rosenblatt设计制作了“感知机”，它是一种多层的神经网络。这项工作首次把人工神经网络的研究从理论探讨付诸工程实践。当时，世界上许多实验室仿效制作感知机，分别应用于[[文字识别]]、声音识别、声纳信号识别以及学习[[记忆]]问题的研究。然而，这次[[人工神经网络]]的研究高潮未能持续很久，许多人陆续放弃了这方面的研究工作，这是因为当时[[数字计算机]]的发展处于全盛时期，许多人误以为数字计算机可以解决[[人工智能]]、[[模式识别]]、[[专家系统]]等方面的一切问题，使感知机的工作得不到重视；其次，当时的[[电子]][[技术]][[工艺]]水平比较落后，主要的元件是电子管或[[晶体管]]，利用它们制作的[[神经网络]]体积庞大，价格昂贵，要制作在规模上与真实的神经网络相似是完全不可能的；另外，在1968年一本名为《感知机》的著作中指出线性感知机功能是有限的，它不能解决如异感这样的基本问题，而且多层网络还不能找到有效的计算方法，这些论点促使大批研究人员对于人工神经网络的前景失去信心。60年代末期，人工神经网络的研究进入了低潮。 另外，在60年代初期，Widrow提出了自适应线性元件网络，这是一种连续取值的线性加权求和阈值网络。后来，在此基础上发展了非线性多层自适应网络。当时，这些工作虽未标出神经网络的名称，而实际上就是一种[[人工神经网络模型]]。 随着人们对感知机兴趣的衰退，神经网络的研究沉寂了相当长的时间。80年代初期，[[模拟]]与数字混合的超[[大规模集成电路]]制作技术提高到新的水平，完全付诸实用化，此外，数字计算机的发展在若干应用领域遇到困难。这一背景预示，向[[人工神经网络]]寻求出路的时机已经成熟。美国的物理学家Hopfield于1982年和1984年在美国科学院院刊上发表了两篇关于人工神经网络研究的论文，引起了巨大的反响。人们重新认识到神经网络的威力以及付诸应用的现实性。随即，一大批学者和研究人员围绕着 Hopfield提出的方法展开了进一步的[[工作]]，形成了80年代中期以来人工神经网络的研究热潮。 1985年，Ackley、Hinton和Sejnowski将[[模拟退火算法]]应用到神经网络训练中，提出了Boltzmann机，该算法具有逃离极值的优点，但是训练时间[[需要]]很长。 1986年，Rumelhart、Hinton和Williams提出了多层前馈神经网络的学习算法，即BP算法。它从证明的角度推导算法的正确性，是学习算法有理论依据。从学习算法角度上看，是一个很大的进步。 1988年，Broomhead和Lowe第一次提出了径向基[[网络]]：RBF网络。 神经网络的组成一个简单的神经网络主要由：输入层，隐藏层，输出层三部分构成。当隐藏层只有一层时，该网络为两层神经网络，由于输入层未做任何变换，可以不看做单独的一层。 输入层，每个节点代表了一个输入特征，在输入节点中，不进行任何的计算，仅向隐藏节点传递信息。 隐藏层，隐藏节点和外部世界没有直接联系（由此得名）。这些节点进行计算，并将信息从输入节点传递到输出节点。隐藏节点总称为「隐藏层」。尽管一个前馈神经网络只有一个输入层和一个输出层，但网络里可以没有隐藏层（如果没有隐藏层，激活函数选择sigmod，那么就变成逻辑回归了），也可以有多个隐藏层。隐藏层的层数以及隐藏层神经元是由人工设定，根据隐藏层的数目，又分为多层感知机（至少一层隐藏），深度神经网络（一般指至少两个隐藏层）。 输出层，输出节点总称为「输出层」，负责计算，并从网络向外部世界传递信息。个数代表了分类标签的个数（在做二分类时，如果采用sigmoid分类器，输出层的神经元个数为1个；如果采用softmax分类器，输出层神经元个数为2个；如果是多分类问题，即输出类别&gt;=3时，输出层神经元为类别个数）， 一个基本的两层神经网络可见下图（注意：说神经网络多少层数的时候一般不包括输入层。 在神经网络中的激活主要讲的是梯度的更新的激活）：除了上述这些常用的组件外，还有 全连接层（Fully Connected Layer，也称为密集层或Dense Layer）： 全连接层中的每个神经元与前一层的所有神经元相连，形成完全的连通性。 这种层用于学习输入特征的复杂组合，常用于分类任务的最后几层。 全连接层的权重矩阵与偏置向量在训练过程中会被调整，以优化模型性能。 卷积层（Convolutional Layer）： 卷积层主要用于处理具有网格结构的数据，如图像。 它使用卷积核（滤波器）在输入数据上滑动，执行点乘操作，以检测局部特征。 卷积层可以捕获空间上的邻近关系，非常适合图像识别和计算机视觉任务。 池化层（Pooling Layer）： 池化层用于减少卷积层输出的空间尺寸，从而减少计算量和过拟合风险。 最常用的池化方法是最大池化（Max Pooling）和平均池化（Average Pooling），前者选取区域内最大值，后者取平均值。 递归层（Recurrent Layer）： 递归层（RNN层）用于处理序列数据，如时间序列或自然语言。 它们具有反馈连接，允许信息在时间步骤间循环，使得模型能够记住先前的信息。 基本的RNN可能遭受梯度消失或梯度爆炸的问题，因此有时使用改进的架构，如LSTM或GRU。 长短期记忆层（Long Short-Term Memory，LSTM Layer）： LSTM是一种特殊的RNN，通过引入门控机制（输入门、遗忘门和输出门）来控制信息流，避免长期依赖问题。 LSTM能够记住长期的信息，非常适合处理需要长时间记忆的任务。 门控循环单元层（Gated Recurrent Unit，GRU Layer）： GRU是LSTM的简化版本，通过合并状态和记忆细胞，并减少门控单元的数量，降低了计算复杂度。 GRU同样能够处理长期依赖问题，但通常比LSTM更高效。 激活层（Activation Layer）： 激活层通常紧随其他层之后，为网络引入非线性，使模型能够学习更复杂的函数。 常见的激活函数有ReLU（Rectified Linear Unit）、sigmoid、tanh等。 批归一化层（Batch Normalization Layer）： 批规范化层用于标准化输入数据，以加速训练过程并减少内部协变量移位问题。 它通过对每一批数据进行归一化处理，使各层的输入分布更加稳定。 丢弃层（Dropout Layer）： 丢弃层通过随机“丢弃”一部分神经元，防止模型过度依赖某些特征，有助于防止过拟合。 神经网络算法的分类根据不同应用场景和处理任务的类别，神经网络算法主要分为 3 类： 前馈神经网络 feedforward neural network，FNN前馈神经网络采用一种单向多层结构。各神经元分层排列，每一层包含若干个神经元。在此种神经网络中，每个神经元只与前一层的神经元，接收前一层的输出，并输出给下一层，各层间没有反馈。第0层叫输入层，最后一层叫输出层，其他中间层叫做隐含层（或隐藏层、隐层）。隐层可以是一层或多层。整个网络中无反馈，信号从输入层向输出层单向传播，可用一个有向无环图表示。一个典型的多层前馈神经网络如下所示。这类神经网络算法主要用于处理信息分类、预测等任务，可实现信息快速传递和处理，前馈神经网络的不同神经元之间都是向前连接的 循环神经网络：循环神经网络（Recurrent Neural Network, RNN）是一类以序列（sequence）数据为输入，在序列的演进方向进行递归（recursion）且所有节点（循环单元）按链式连接的递归神经网络（recursive neural network）。双向循环神经网络（Bidirectional RNN, Bi-RNN）和长短期记忆网络（Long Short-Term Memory networks，LSTM）是常见的循环神经网络 循环神经网络采用循环连接的方式，常用于自然语言处理和识别等任务，因为这种神经网络算法可以保留数据处理前的状态 卷积神经网络：卷积神经网络（Convolutional Neural Networks, CNN）是一类包含卷积计算且具有深度结构的前馈神经网络（Feedforward Neural Networks），是深度学习（deep learning）的代表算法之一 [1-2]。卷积神经网络具有表征学习（representation learning）能力，能够按其阶层结构对输入信息进行平移不变分类（shift-invariant classification），因此也被称为“平移不变人工神经网络（Shift-Invariant Artificial Neural Networks, SIANN）” 这种神经网络算法主要通过卷积层和池化层处理数据，能有效减少参数数量，一般用于图像分类、目标检测等场景。 神经网络算法的基本原理神经网络算法的基本原理主要分为以下四类： 自适应谐振理论（ART）网络：这种神经网络算法包含输入层和输出层，且两层完全互联，数据可沿着正向和反向两个方向传播； 学习矢量量化（LVQ）网络：此类神经网络算法方式包含输入层、隐含层和输出层，输入层和隐含层之间神经元完全连接，但输出的神经元则不完全与隐含神经元相连 Kohonen网络：这类神经网络算法是以二维阵列排列，各个输出神经元可与对应的输入神经元相连接 Hopfield网络：该神经网络算法也被称为递归网络，这种算法只支持二进制和双极输入，其中每个神经元都与不同的神经元相连。]]></content>
      <categories>
        <category>neural_network</category>
      </categories>
      <tags>
        <tag>deep_learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大模型-部署工具-Ollama]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-25.LLM-%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86%2F5011.%E5%A4%A7%E6%A8%A1%E5%9E%8B-%E9%83%A8%E7%BD%B2-%E9%83%A8%E7%BD%B2%E5%B7%A5%E5%85%B7-Ollama%2F</url>
    <content type="text"><![CDATA[ollama是一个旨在简化大型语言模型本地部署和运行过程的工具。它提供了一个轻量级、易于扩展的框架，让开发者能够在本地机器上轻松构建和管理LLMs。通过ollama，开发者可以访问和运行一系列预构建的模型，或者导入和定制自己的模型，无需关注复杂的底层实现细节。 官网：Ollama GitHub地址：GitHub - ollama/ollama: Get up and running with Llama 2, Mistral, Gemma, and other large language models. 安装平台安装Ollama 提供了针对多种平台版本（Mac、Windows、Linux）的安装包，用户可以根据自己的操作系统选择对应的安装包（Linux是命令行，其他两个平台是下载包）。 常用命令12345678910111213141516171819202122232425262728293031323334353637383940# 创建模型：使用 Modelfile 创建模型的命令是 ollama create。ollama create mymodel -f ./Modelfile # 拉取模型：此命令还可用于更新本地模型。只会拉取差异。ollama pull llama2# 启动服务器./ollama serve# 运行模型./ollama run llama3# 删除模型ollama rm llama2 # 复制模型ollama cp llama2 my-llama2# 多行输入：对于多行输入，可以使用"""&gt;&gt;&gt; """Hello,... world!... """I’m a basic program that prints the famous "Hello, world!" message to the console.# 多模态模型&gt;&gt;&gt; What’s in this image? /Users/jmorgan/Desktop/smile.pngThe image features a yellow smiley face, which is likely the central focus of the picture.# 将prompt作为参数传递$ ollama run llama2 "Summarize this file: $(cat README.md)"Ollama is a lightweight, extensible framework for building and running language models on the local machine. It provides a simple API for creating, running, and managing models, as well as a library of pre-built models that can be easily used in a variety of applications.# 列出模型的信息ollama show llama3# 列出计算机上的型号ollama list# 开始Ollama：在不运行桌面应用程序的情况下启动Ollama时使用Ollama服务。ollama serve 运行模型模型获取（非必须） pull完成平台安装后，就是获取模型， Ollama library 提供了一系列可供选择的模型。比如 llama3、qwen2、phi3 等。Ollama 采取了与 Docker 组织镜像相似的方案，使用模型名加上标签的形式（ model:tag ）来确定具体的模型版本，不加标签时默认为 latest ，通常对应 7B 参数量 4bit 量化版。而如果要运行 70B 版本，就可以使用 70B 标签：例如：123456789#Instruct is fine-tuned for chat/dialogue use cases.Example:ollama pull llama3 # 运行默认 8B 模型 ollama pull llama3:70b # 运行 70B 参数模型# Pre-trained is the base model.Example:ollama pull llama3:textollama pull llama3:70b-text 在这里列出几个中文支持比较好的模型： DeepSeek 系列，深度求索团队推出，包括针对代码训练的 DeepSeek-Coder 和 通用的 DespSeek-LLM； Yi 系列，零一万物团队推出，有支持 20 万上下文窗口的版本可选； 如果碰巧财力雄厚，不妨试试法国明星初创团队 Mistral 最新推出的首个开源混合专家 MoE 模型 Mixtral-8x7B，需要 48GB 内存以运行； 如果不巧硬件紧张，无需气馁，Phi-2 由微软团队针对逻辑和理解精调，2.7B 的尺寸只需 4 GB 内存即可运行，吐字速度飞快，只是不太懂中文。 运行模型 run下载模型完成后，我们可以使用 run 命令运行模型（如果没有提前下载，那运行run时会开始进行下载），可直接将消息附在命令后，或留空进入对话模式，对话模式内置了几个以斜杠引出的命令：1234567891011121314151617181920# 单条输入》(base) ➜ ~ ollama run llama3 "天街小雨润如酥，的下一句"A classic line from a famous Chinese poem!The full poem is:天街小雨润如酥，春暖花朝瑞。风前水后山寺静，月明人影自在。The next line would be:春暖花朝瑞# 对话模式》ollama run llama2-chinese # 运行时不提供对话内容就会进入对话模式(base) ➜ ~ ollama run llama3&gt;&gt;&gt; #Send a message (/? for help) 其实至此，我们已经完成了大模型的本地部署和测试工作。这个运行时在命令行状态的，对于很多非IT背景的人员这并不又要，对此 Ollama 有一系列的周边工具可供使用，包含了网页、桌面、终端等交互界面及诸多插件和拓展。 自定义模型有时候我们需要运行一些自定义模型，比如我们想要使用自己的模型，或者想要使用其他模型，我们可以使用 create 命令结合 Modelfile 来创建自定义模型。 使用其他来源的模型其他来源的模型比较容易，我们只需要构建一个 Modelfile 文件，告诉 Ollama 模型所在路径即可。12345678# 从 GGUF 导入：Ollama 支持在 Modelfile 中导入 GGUF 模型：创建一个名为 Modelfile 的文件，其中包含一个 FROM 指令，指向要导入的模型的本地文件路径。FROM ./vicuna-33b.Q4_0.gguf# 在 Ollama 中创建模型ollama create example -f Modelfile# 运行模型ollama run example 调整参数除了模型来源，有时候，我们可能也需要对模型的参数进行调整，那么相关调整我们也可以通过 Modelfile文件进行个性化的定义。12345678FROM llama2PARAMETER temperature 1 将温度设置为 1 [较高为更具创造性，较低为更连贯]# set the system message设置系统消息SYSTEM """You are Mario from Super Mario Bros. Answer as Mario, the assistant, only.""FROM llama2 然后就是像运行其他来源的模型一样，创建我们的新模型12345ollama create mario -f ./Modelfile​​​​​​​ollama run mario&gt;&gt;&gt; hiHello! It&apos;s your friend Mario. ModelFile 类似DockerFile，具体参数配置可以参考官方文档，其中主要的关键字如下： Instruction Description FROM (required) Defines the base model to use. PARAMETER Sets the parameters for how Ollama will run the model. TEMPLATE The full prompt template to be sent to the model. SYSTEM Specifies the system message that will be set in the template. ADAPTER Defines the (Q)LoRA adapters to apply to the model. LICENSE Specifies the legal license. MESSAGE Specify message history. 模型套壳Ollama 有一系列的周边工具可供使用，包含了网页、桌面、终端等交互界面及诸多插件和拓展。之所以 Ollama 能快速形成如此丰富的生态，是因为它自立项之初就有清晰的定位：让更多人以最简单快速的方式在本地把大模型跑起来。于是，Ollama 不是简单地封装 llama.cpp，而是同时将繁多的参数与对应的模型打包放入；Ollama 因此约等于一个简洁的命令行工具和一个稳定的服务端 API。这为下游应用和拓展提供了极大便利。 就 Ollama GUI 而言，根据不同偏好，有许多选择： Web 版：Ollama WebUI 具有最接近 ChatGPT 的界面和最丰富的功能特性，需要以 Docker 部署；终端 TUI 版：oterm 提供了完善的功能和快捷键支持，用 brew 或 pip 安装；Raycast 插件：即 Raycast Ollama]]></content>
      <categories>
        <category>LLM</category>
      </categories>
      <tags>
        <tag>LLM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[5020.大模型-开源模型资源]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-25.LLM-%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86%2F5010.%E5%A4%A7%E6%A8%A1%E5%9E%8B-%E9%83%A8%E7%BD%B2-%E5%BC%80%E6%BA%90%E6%A8%A1%E5%9E%8B%E8%B5%84%E6%BA%90-00.%E6%A6%82%E8%BF%B0%2F</url>
    <content type="text"><![CDATA[开源模型测评仓库CLiB中文大模型能力评测榜单-github 模型列表 模型 git仓库 模型特点 模型大小 运行资源 Llama 3 github 8B / 70B Phi-3 huggingface 小巧可以运行在手机上 Qwen1.5-110B huggingface BLOOM bigscience.huggingface.co BERT github Yi-Chat github huggingface由于网络问题，可能部分环境被墙或被限速，可以切换至国内镜像站点 LlamaMeta 推出 Phi-3Phi-3是微软AI研究院的新开源语言模型，具备小巧且高效的特性，赢得市场青睐。系列包括Phi-3-Mini、Phi-3-Small和Phi-3-Medium三种规模。Phi-3-Mini虽小，但性能与大型模型相当，适合资源有限环境。Phi-3-Small和Phi-3-Medium在扩展数据集支持下性能更佳。Phi-3系列以小巧设计、卓越性能和灵活扩展能力，为语言模型领域注入新活力，满足不同用户需求。 https://arxiv.org/abs/2404.14219 Qwen1.5-110B通义千问公司发布的一款千亿级参数模型——Qwen1.5-110B。经过详尽的性能测试，Qwen1.5-110B凭借其卓越表现重返SOTA开源模型之巅，甚至超越了强大的Llama 3 70B，成为了当前最顶尖的开源大模型。值得一提的是，Qwen1.5-110B与Qwen1.5系列的其他模型在结构上保持了一致性，均采用了分组查询注意力机制，保证了推理的高效性。此外，该模型还支持高达32K的上下文，同时兼容多种语言，包括英语、中文、法语、西班牙语、德语、俄语、韩语和日语等，满足了全球用户的需求。 BLOOMBLOOM是一个经过一年合作开发的自回归LLM训练模型，利用了工业级计算资源和大量文本数据生成文本。其发布是生成式AI民主化的里程碑。拥有1760亿参数的BLOOM，是强大的开源LLMs之一，能以46种语言和13种编程语言生成连贯准确的文本。其特点是透明度高，源代码和训练数据均可访问，方便运行、研究和改进。此外，BLOOM可通过Hugging Face生态系统免费使用。 BERTBERT是早期大型语言模型的代表作，作为Transformer潜力的首批实验之一，BERT在2018年开源后迅速在自然语言处理任务中取得先进性能。因其创新和开源性质，BERT成为最受欢迎的LLMs之一，有数千种开源、免费和预训练的模型用于各种用例。但近年来，谷歌对开源大模型的态度有所冷漠。 Falcon 180BFalcon 40B在开源LLM社区备受赞誉，成为Hugging Face榜首。新推出的Falcon 180B展现出专有与开源LLM间的差距正迅速缩小。阿联酋技术创新研究所透露，Falcon 180B正在接受1800亿参数的训练，计算能力强大，已在多种NLP任务中超越LLaMA 2和GPT-3.5。虽然免费供商业和研究使用，但运行Falcon 180B需要庞大计算资源。 Yi-ChatYi系列模型是01.AI推出的强大开源语言模型，以双语能力领先领域。利用3T多语言语料库训练，具备卓越的语言理解、常识推理和阅读理解等能力。2024年1月数据显示，Yi-34B-Chat在AlpacaEval排名第二，仅次于GPT-4 Turbo，超越其他LLM如GPT-4、Mixtral、Claude。在各种基准测试中，Yi-34B排名第一，超越Falcon-180B、Llama-70B、Claude等开源模型。这使得Yi系列模型成为全球领先的LLM之一，展现出广阔的应用前景。]]></content>
      <categories>
        <category>LLM</category>
      </categories>
      <tags>
        <tag>LLM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[开源模型介绍 - Llama]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-25.LLM-%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86%2F5010.%E5%A4%A7%E6%A8%A1%E5%9E%8B-%E9%83%A8%E7%BD%B2-%E5%BC%80%E6%BA%90%E6%A8%A1%E5%9E%8B%E8%B5%84%E6%BA%90-01.Llama%2F</url>
    <content type="text"><![CDATA[开源模型测评仓库CLiB中文大模型能力评测榜单-github 模型列表 模型 git仓库 模型特点 模型大小 运行资源 Llama 3 github 8B / 70B Phi-3 huggingface 小巧可以运行在手机上 Qwen1.5-110B huggingface BLOOM bigscience.huggingface.co BERT github Yi-Chat github huggingface由于网络问题，可能部分环境被墙或被限速，可以切换至国内镜像站点 目前已经发展出来非常多的模型，但是不同 功能or 用途的模型 本身支持的 prompt 格式都存在差异，而不了解这些差异往往导致我们在只用过程中陷入错误而不自知，因此我们推荐大家先了解这些模型的 prompt 格式，然后根据需要选择合适的模型。所以开启这个系列后续会记录自己接触测试模型的 prompt 当然也许后续会发现其他和模型绑定的 tips 也会一起补充。，]]></content>
      <categories>
        <category>LLM</category>
      </categories>
      <tags>
        <tag>LLM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[5004.大模型-优化部署模型]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-25.LLM-%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86%2F5012.%E5%A4%A7%E6%A8%A1%E5%9E%8B-%E9%83%A8%E7%BD%B2-%E4%BC%98%E5%8C%96%E9%83%A8%E7%BD%B2%E6%A8%A1%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[Distillation用一个teacher模型来训练一个student模型 Quantization将训练后的模型进行量化，降低数据的精度，从而降低模型的复杂度和对资源的需求。 Pruning模型剪枝，删除冗余模型，或去除模型中权重很低，贡献不大的节点， Full model re-training 被保留参数进行重新训练 PEFT/LoRA Post-training 训练以后在进行剪枝]]></content>
      <categories>
        <category>LLM</category>
      </categories>
      <tags>
        <tag>LLM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大模型-效果评估]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-25.LLM-%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86%2F5040.%E5%A4%A7%E6%A8%A1%E5%9E%8B-%E8%AF%84%E4%BB%B7-%E6%95%88%E6%9E%9C%E8%AF%84%E4%BC%B0%2F</url>
    <content type="text"><![CDATA[一些评估模型的指标传统的机器学习中，我们可以通过已知输出的训练集和验证数据集的性能表现来评估模型的性能表现，通过计算获得简单的指标，比如使用 准确性（=correct predictions/total predictions）来计算模型的性能表现。但是对于输出不确定的大预言模型，评估语言的性能表现，需要通过一些特殊的指标来评估。比如eg1: “Mike really loves drinking tea“ 和 “Mike adores sipping tea“，eg2: “Mike does not drink coffee“ 和 “Mike does drink coffee“，我们该如何衡量其相似性，eg1中的单词差异很大，但是含义相近，而eg2中的单词只有一个 not的区别，但是含义完全相反。 所以我们需要一些自动化，结构化的方法来进行测量，量化生成的结果和预期结果的相似性。而 ROUGE 和 BLEU SCORE 是两种广泛用于不同任务的评估指标。这两种简单的指标都具有较低的计算成本。 语言解刨学的一些概念首先理解一下再语言解刨学中的一些概念，我们日常中的一句话，可以根据拆解单位拆分成不同的单词、词组 或 多词的组合。而不同的拆解模式会影响我们最终的评价结果。 ROUGEROUGE 或者 召回率导向的 jesting evaluation 用来评估自动生成的摘要和人工摘要的质量。基于语言解刨学的介绍，我们再比较的时候，可以使用单词组计算我们的模型性能表现如下。这是最基本的指标，我们只考虑单词的出现，没考虑单词的顺序问题，所以结论可能存在很大的欺骗性（比如示例中的差别单词不是 very 而是 not，其实得分不会存在差别）。所以我们可以使用 biggrams 计算我们模型的性能表现，得到一个更合适的分数。比如我们使用两个词（ROUGE-2) 一组进行评分,可以看到分数显著低于ROUGE-1 的分支，句子越长这个差距可能越大。 当然句子基于加长时，我们可能下意识的想到的是 使用ROUGE-3、ROUGE-4等等更长的序列。但是这时候，我们其实更好的方法是查找我们生成序列和参考序列的最长公共子序列的长度（LCS Long common subsequence），然后我们可以使用最长公共子序列来计算我们的 ROUGE-LCS 得分： 但是ROUGE分值可能会由于数据的特殊性导致一些差的结果也会得到比较高的评分，比如下面这种情况： BLEU SCORE用于机器翻译翻译的结果和人工翻译结果的场景。分数本身是使用多个 n-gram 大小的平均精度计算得出的。就像我们之前看过的 Rouge-1 分数一样，但计算得出了n 个大小范围，然后求平均值 基准数据集GLUEsuperGLUEHELMMMLUBig-Bench]]></content>
      <categories>
        <category>LLM</category>
      </categories>
      <tags>
        <tag>LLM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大模型 - 微调方法 - PEFT]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-25.LLM-%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86%2F5021.%E5%A4%A7%E6%A8%A1%E5%9E%8B-%E6%A8%A1%E5%9E%8B%E4%BC%98%E5%8C%96-PEFT-00.%E6%A6%82%E8%BF%B0%2F</url>
    <content type="text"><![CDATA[微调的方法分类参数规模从参数规模的角度，大模型的微调分成两条技术路线： 全量微调FFT(Full Fine Tuning)，是对全量的参数，进行全量的训练。用特定的数据，对大模型进行训练，将W 变成W’ ，W’相比W ，最大的优点就是上述特定数据领域的表现会好很多。主要问题： 一个是训练的成本会比较高，因为微调的参数量跟预训练的是一样的多的； 一个是叫灾难性遗忘(Catastrophic Forgetting)，用特定训练数据去微调可能会把这个领域的表现变好，但也可能会把原来表现好的别的领域的能力变差。 PEFT(Parameter-Efficient Fine Tuning)，旨在通过最小化微调参数数量和计算复杂度，提升预训练模型在新任务上的表现，从而减轻大型预训练模型的训练负担。PEFT的出现就是为了解决 FFT中存在的主要问题。 PEFT 是目前比较主流的微调方案。 训练的方法 监督式微调SFT(Supervised Fine Tuning) 主要是用人工标注的数据，用传统机器学习中监督学习的方法，对大模型进行微调； 基于人类反馈的强化学习微调RLHF(Reinforcement Learning with Human Feedback) 是把人类的反馈，通过强化学习的方式，引入到对大模型的微调中去，让大模型生成的结果，更加符合人类的一些期望； 基于AI反馈的强化学习微调RLAIF(Reinforcement Learning with AI Feedback) 大致跟RLHF类似，但是反馈的来源是AI。这里是想解决反馈系统的效率问题，因为收集人类反馈，相对来说成本会比较高、效率比较低。 技术路径每种微调，都要对 权衡参数效率、内存效率、训练速度、模型质量和推理成本 进行权衡。针对微调，一些路径技术会冻结大部分模型权重，并专注于微调现有模型参数的子集，例如特定的层或组件，比如PEFT。其他技术根本不会触及原始模型的权重，而是添加少量新参数或层，只对新组件进行微调。 Selective仅微调原始 LLM 参数子集的方法。您可以采用多种方法来确定要更新的参数。您可以选择仅训练模型或特定层的某些组件，甚至可以选择单独的参数类型。研究人员发现，这些方法的性能好坏参半，参数效率和计算效率之间存在显著的权衡取舍。 Reparameterization重新参数化方法也可以使用原始的 LLM 参数，但是通过创建原始网络权重的新的低等级变换来减少要训练的参数数量。这种常用的技术是LoRa。 Additive通过冻结所有原始 LLM 权重并引入新的可训练组件来进行微调。这里有两种主要方法。 Adapters在模型架构中添加了新的可训练层，通常位于注意层或前馈层之后的编码器或解码器组件内。 Soft Prompts 可以保持模型架构的固定和冻结，并专注于操纵输入以实现更好的性能。这可以通过在提示嵌入中添加可训练的参数或保持输入固定并重新训练嵌入权重来实现。 微调的步骤 数据准备：与全面微调相同。 冻结大部分参数：在训练过程中冻结模型的大部分参数。 训练设置：设定需要调整的参数部分的学习率。 训练过程：只更新选定的参数。 评估与迭代：与全面微调类似，但聚焦于微调参数的 基于PEFT的相关技术方法PEFT（Parameter-Efficient Fine-Tuning）是hugging face开源的一个参数高效微调大模型的工具，里面集成了4中微调大模型的方法，可以通过微调少量参数就达到接近微调全量参数的效果，使得在GPU资源不足的情况下也可以微调大模型。 PEFT技术 即使在计算资源受限的情况下，也能够利用预训练模型的知识快速适应新任务，实现有效的迁移学习。因此，PEFT不仅能提升模型效果，还能显著缩短训练时间和计算成本，使更多研究者能够参与到深度学习的研究中。PEFT包括LoRA、QLoRA、适配器调整(Adapter Tuning)、前缀调整(Prefix Tuning)、提示调整(Prompt Tuning)、P-Tuning及P-Tuning v2等多种方法。 以下图表示了7种主流微调方法在Transformer网络架构中的作用位置及其简要说明，接下来将详细介绍每一种方法。 1. LoRA（基于新数据训练一个地址分解矩阵，使用时加到原模型矩阵上）LoRA 背后有一个假设：我们现在看到的这些大语言模型，它们都是被过度参数化的。而过度参数化的大模型背后，都有一个低维的本质模型。。也就是说 大模型中有其中一部分参数，是非常重要的，是影响大模型生成结果的关键参数，这部分关键参数就是上面提到的低维的本质模型。LoRA（Low-Rank Adaptation）是一种旨在微调大型预训练语言模型（如GPT-3或BERT）的技术。其核心理念在于，在模型的决定性层次中引入一个额外的小型矩阵(“旁路”用A、B两个矩阵组合表示，维度分别是d × r和 r × d，其中r远小于d，A随机初始化，B初始化为0)，来实现模型行为的微调，而无需对整个模型结构进行大幅度修改。在微调模型的过程中，左边的W不更新，只更新右边的A和B的参数。前向传播时是左右的输出和，反向传播时只更新右边，因此计算的梯度以及优化器的中间值也只和右边有关，最终右边的参数会单独保存下来。LORA这种训练方式不会改变大模型的参数，且针对每个下游任务生成自己的LORA参数，在预测阶段只要将大模型的参数和LORA参数叠加在一起即可。 LoRA的基本思路，包括以下几步： 首先, 要适配特定的下游任务，要训练一个特定的模型，将 $Y=WX$ 变成 $Y=(W+∆W)X$ ，这里面 $∆W$ 主是我们要微调得到的结果； 其次，将 $∆W$进行低维分解 $∆W=AB$ (∆W为m n维，A为m r维，B为r * n维，r就是上述假设中的低维)； 接下来，用特定的训练数据，训练出A和B即可得到∆W，在推理的过程中直接将∆W加到W上去，再没有额外的成本。 另外，如果要用LoRA适配不同的场景，切换也非常方便，做简单的矩阵加法即可：(W + ∆W) - ∆W + ∆W`。 该方法认为模型权重矩阵在特定微调后具有较低的本征秩，故基于秩分解的概念，将预训练模型的现有权重矩阵分成两个较小的矩阵。LoRA的特点 将矩阵乘积BA加到原模型参数矩阵W上可以避免推理延迟。 可插拔的低秩分解矩阵模块，方便切换到不同的任务。 不显著增加额外计算负担的前提下，能够有效地微调模型，同时保留模型原有的性能水准。 LoRA的实验结果使用的模型是RoBERTa、DeBERTa、GPT-2、GPT-3 175B。在多个数据集上，LoRA在性能上能和全量微调相近，且在某些任务上优于全量微调。 2、QLoRA（低精度加载高精度重训练）QLoRA（Quantized Low-Rank Adaptation）是一种结合了LoRA（Low-Rank Adaptation）方法与深度量化技术的高效模型微调手段。QLoRA的核心在于： 量化技术：QLoRA采用创新的技术将预训练模型量化为4位。这一技术包括低精度存储数据类型（4-bit NormalFloat，简称NF4）和计算数据类型（16-bit BrainFloat）。这种做法极大地减少了模型存储需求，同时保持了模型精度的最小损失。 量化操作：在4位量化中，每个权重由4个比特表示，量化过程中需选择最重要的值并将它们映射到16个可能的值之一。首先确定量化范围（例如-1到1），然后将这个范围分成16个区间，每个区间对应一个4-bit值。然后，原始的32位浮点数值将映射到最近的量化区间值上。 微调阶段：在训练期间，QLoRA先以4-bit格式加载模型，训练时将数值反量化到bf16进行训练，这样大幅减少了训练所需的显存。例如，33B的LLaMA模型可以在24 GB的显卡上进行训练。 量化过程的挑战在于设计合适的映射和量化策略，以最小化精度损失对性能的影响。在大型模型中，这种方法可以显著减少内存和计算需求，使得在资源有限的环境下部署和训练成为可能。大模型入门（二）—— PEFT 3.适配器调整 Adapter Tuning（增加一个Adapter层，与LoRA技术类似，适配器调整的目标是在保留预训练模型原始参数不变的前提下，使模型能够适应新的任务。适配器调整的方法是在模型的每个层或选定层之间插入小型神经网络模块，称为“适配器”。这些适配器是可训练的，而原始模型的参数则保持不变。 Adapter的架构如下： 在每一个Transformer层中的每个子层之后插入两个串行的Adapter。在Adapter微调期间，绿色层是根据下游数据进行训练的，而预训练模型的原参数保持不变。Adapter 模块主要由两个前馈（Feed-forward）子层组成。 第一个前馈子层将原始特征的维度d投影到一个更小的维度m，应用非线性函数，再投影回维度d的特征（作为Adapter模块的输出）。 总参数量为2md + d + m。通过设置m &lt; d，我们限制了每个任务添加的参数数量。 当投影层的参数初始化接近零时，根据一个skip-connection，将该模块就初始化为近似恒等函数，以确保微调的有效性。 LoRA 与 Adapter Tuning 的主要区别在于： LoRA：在模型的权重矩阵中引入低秩矩阵来实现微调。这些低秩矩阵作为原有权重矩阵的修改项，在实际计算时对原有权重矩阵进行调整。 适配器调整：通过在模型各层中添加小型神经网络模块，即“适配器”，来实现微调。适配器独立于模型的主体结构，仅适配器的参数在微调过程中更新，而模型的其他预训练参数保持不变。 早期的**提示微调**通过修改输入文本来控制语言模型的行为，称为硬提示（Hard Prompts）微调。这些方法很难优化，且受到最大模型输入长度的限制。下图为离散的人工设计的Prompt示例： ![image](https://img-blog.csdnimg.cn/img_convert/d8cb7200c1a4dd1d2f24ea020554babe.png) 软提示（Soft Prompts）将离散的“提示”问题转为连续的“提示”问题，通过过反向传播和梯度下降更新参数来学习Prompts，而不是人工设计Prompts。 硬提示和软提示之间的区别在于，**硬提示提供了明确的指令来指导模型的响应，并且可能需要针对不同的任务定制模型**，而软提示通过**调整提示参数**来指导模型的行为，提供更通用和更有效的策略跨越各种任务。 有仅对输入层进行训练，也有对所有层进行训练的类型。下面将介绍几种热门的Soft Prompts微调方法。 4.前缀调整 Prefix Tuning()Prefix Tuning的灵感来源是，基于Prompt Engineering的实践表明，在不改变大模型的前提下，在Prompt上下文中添加适当的条件，可以引导大模型有更加出色的表现。前缀调整提出了一种新的策略，即在预训练的语言模型（LM）输入序列前添加可训练、任务特定的前缀（添加的是参数或矩阵），从而实现针对不同任务的微调。除了在embedding层加入这个前缀之外，还在其他的所有层都添加这样一个前缀。最后微调时只调整前缀的参数，大模型的参数保持不变。这意味着我们只需要为不同任务保存各自特异的前缀部分的参数，而不是为每个任务保存一整套微调后的模型权重，从而节省了大量的存储空间和微调成本。 前缀实际上是一种连续可微的虚拟标记（Soft Prompt/Continuous Prompt），与离散的Token相比，它们更易于优化并且效果更佳。这种方法的优势在于不需要调整模型的所有权重（基座不变），而是通过在输入中添加前缀来调整模型的行为，从而节省大量的计算资源，同时使得单一模型能够适应多种不同的任务。前缀可以是固定的（即手动设计的静态提示）或可训练的（即模型在训练过程中学习的动态提示） Prefix Tuning的特点 冻结预训练语言模型的参数，为每个任务存储特定的连续可微的前缀，节省空间。 训练间增加MLP层以达到稳定。 对于不同模型构造不同的Prefix。 Prefix Tuning的实验结果：对于表格到文本任务，使用GPT-2MEDIUM和GPT-2LARGE模型。在表格到文本任务上，Prefix Tuning优于Fine-Tuning（全量微调）和Adapter-Tuning。对于摘要任务，使用BART-LARGE模型。在摘要任务上，Prefix Tuning比全量微调弱。 5.提示调整（Prompt Tuning）Prompt Tuning的出发点，是基座模型(Foundation Model)的参数不变，为每个特定任务，训练一个少量参数的小模型，在具体执行特定任务的时候按需调用。这些可训练的提示向量在训练过程中更新，以指导模型输出更适合特定任务的响应。 像GPT3中那种，通过人工构造一些token作为前缀输入到模型中，因为这些token是从vocab中选择的，因此会受到大模型的参数的影响，所以要取得好的结果的话，人工构造的提示语必须要符合模型训练语料的特性。而Prompt tuning是为Prompt单独生成一份参数，在微调的过程中大模型的参数冻结不变，只更新Prompt的参数。且文章实验表明对于Prompt的参数使用大模型的vocab中的一些token 的embedding初始化，或者使用标签词的嵌入（当标签词的token数大于1时，对所有token取平均，即将一个标签词看作一个整体）初始化要比随机初始化的效果要好。此外Prompt的长度对结果也会有影响，长度越长效果会越好，但随着模型的规模变大，不同长度或者不同初始化的Prompt之间的差距会被缩小。 Prompt Tuning与Prefix Tuning都涉及在输入数据中添加可学习的向量，这些向量是在输入层添加的，但两者的策略和目的不同： 提示调整：旨在模仿自然语言中的提示形式，将可学习向量（通常称为提示标记）设计为模型针对特定任务生成特定类型输出的引导。这些向量通常被视为任务指导信息的一部分，倾向于使用较少的向量来模仿传统的自然语言提示。 前缀调整：可学习前缀更多地用于提供输入数据的直接上下文信息，作为模型内部表示的一部分，可以影响整个模型的行为。 Prompt Tuning的基本原理是在输入序列X之前，增加一些特定长度的特殊Token，以增大生成期望序列的概率。如果将大模型比做一个函数：Y=f(X)，那么Prompt Tuning就是在保证函数本身不变的前提下，在X前面加上了一些特定的内容，而这些内容可以影响X生成期望中Y的概率。 其结构如下：上图中，仅Virtual Token部分会由梯度下降法去更新参数。 Prompt Tuning的实验结果:使用的是预训练的各种T5模型。在流行的SuperGLUE基准测试中，Prompt Tuning的任务性能与传统的模型调优相当，且随着模型规模的增加，差距逐渐减小。在零样本领域迁移中，Prompt Tuning可以改善泛化性能。 Prompt Tuning 是在 Embedding环节，往输入序列X前面加特定的Token。而Prefix Tuning是在Transformer的Encoder和Decoder的网络中都加了一些特定的前缀。 具体来说，就是将Y=WX中的W，变成W= [Wp; W]，Y=WX。 6. P-Tuning的特点 一般在通过Prompt的方式使用大模型时，通常需要人工构造一些模板，P-tuning将自然语言模板的构建转换成连续参数优化的问题，用一些特殊的token替代人工构造的自然语言模板，让模型自己去学习这些连续的token，在学习的过程中只微调这些token的embedding参数，并且为了保证token之间的联系，并不是随机初始化embedding，而是通过lstm层学习这些token的embedding。 P-Tuning（基于提示的微调）和提示调整都是为了调整大型预训练语言模型（如GPT系列）以适应特定任务而设计的技术。两者都利用预训练的语言模型执行特定的下游任务，如文本分类、情感分析等，并使用某种形式的“提示”或“指导”来引导模型输出，以更好地适应特定任务。 提示调整与P-Tuning的主要区别在于： 提示调整：使用静态的、可训练的虚拟标记嵌入，在初始化后保持固定，除非在训练过程中更新。这种方法相对简单，因为它只涉及调整一组固定的嵌入参数，在处理多种任务时表现良好，但可能在处理特别复杂或需要细粒度控制的任务时受限。 P-Tuning：使用一个可训练的LSTM模型（称为提示编码器prompt_encoder）来动态生成虚拟标记嵌入，允许根据输入数据的不同生成不同的嵌入，提供更高的灵活性和适应性，适合需要精细控制和理解复杂上下文的任务。这种方法相对复杂，因为它涉及一个额外的LSTM模型来生成虚拟标记嵌入。 P-Tuning中使用LSTM（长短期记忆网络）作为生成虚拟标记嵌入的工具，利用了LSTM的以下优势： 更好的适应性和灵活性：LSTM可以捕捉输入数据中的时间序列特征，更好地理解和适应复杂的、顺序依赖的任务，如文本生成或序列标注。 改进的上下文理解：LSTM因其循环结构，擅长处理和理解长期依赖关系和复杂的上下文信息。 参数共享和泛化能力：在P-Tuning中，LSTM模型的参数可以在多个任务之间共享，这提高了模型的泛化能力，并减少了针对每个单独任务的训练需求。而在提示调整中，每个任务通常都有其独立的虚拟标记嵌入，这可能限制了跨任务泛化的能力。 其结构如下： P-Tuning只在输入层加入可微的Virtual Token，其会自动插入到文本提示的离散Token嵌入中。 Virtual Token不一定作为前缀，其插入位置是可选的。 P-Tuning的实验结果 使用的是GPT系列和BERT系列的模型。P-Tuning与全参数效果相当，且在一些任务上优于全参数微调，可以显著提高GPT模型在自然语言理解方面的性能，并且BERT风格的模型也可以获得较小的增益。 7.P-Tuning v2P-Tuning v2是P-Tuning的进一步改进版，在P-Tuning中，连续提示被插入到输入序列的嵌入层中，除了语言模型的输入层，其他层的提示嵌入都来自于上一层。这种设计存在两个问题： 第一，它限制了优化参数的数量。由于模型的输入文本长度是固定的，通常为512，因此提示的长度不能过长。 第二，当模型层数很深时，微调时模型的稳定性难以保证；模型层数越深，第一层输入的提示对后面层的影响难以预测，这会影响模型的稳定性。 其结构如下： P-Tuning v2的改进在于，不仅在第一层插入连续提示，而是在多层都插入连续提示，且层与层之间的连续提示是相互独立的。这样，在模型微调时，可训练的参数量增加了，P-Tuning v2在应对复杂的自然语言理解(NLU)任务和小型模型方面，相比原始P-Tuning具有更出色的效能。 P-Tuning v2的实验结果:使用的是BERT系列和GLM系列模型。P-Tuning v2是一种在不同规模和任务中都可与微调相媲美的提示方法。在NLU任务中，整体上P-Tuning v2与全量微调的性能相差很小。 除了上述方法外，随着大模型行业的发展，也有一些新的微调方法相继涌现 8.BitFit属于选择性方法，是对模型的现有参数进行微调，可以根据层的深度、层类型或者甚至是个别参数进行选择。2022年9月5日，BitFit出现，这是一种稀疏微调方法，仅修改模型的Bias（偏置项）或其中的子集。 BitFit的特点 冻结大部分Transformer编码器的参数，只训练偏置项和任务特定的分类层。 优化的偏置项参数包括Attention模块中计算Query、Key、Value时，计算MLP层时，计算Layernormalization层时遇到的偏置项参数。 每个新任务只需要存储偏置项参数向量（占总参数数量的不到0.1%）和任务特定的最终线性分类器层。 BitFit的实验结果 使用公开可用的预训练BERTBASE、BERTLARGE和RoBERTaBA模型。BitFit微调结果不及全量参数微调，但在极少数参数可更新的情况下，远超Frozen（冻结模型参数）方式。 9.AdaLoRA该方法基于权重矩阵的重要性而自适应调整不同模块的秩，节省计算量，可理解为LoRA的升级版。AdaLoRA的做法是让模型学习SVD分解的近似。在损失函数中增加了惩罚项，防止矩阵P和Q偏离正交性太远，以实现稳定训练。AdaLoRA的实验结果:使用的模型是DeBERTaV3-base 和BART-large模型。AdaLoRA的性能通常高于参数量更高的方法。其中，AdaLoRA在0.32M微调参数时，在CoLA数据集上达到了70.04的Mcc分数。 其他方法RLHF (Reinforcement Learning fromHuman feedback)RLHF使用强化学习（简称RL）使用人类反馈数据对LLM进行微调，从而生成更符合人类偏好的模型。您可以使用 RLHF 来确保模型生成的输出能够最大限度地提高输入提示的实用性和相关性。也许最重要的是，RLHF可以帮助最大限度地减少可能的伤害。你可以训练你的模型，让它给出承认其局限性的注意事项，并避免使用有害的语言和话题。 PPO(Proximal Policy Optimization) KL(Kullback-Leibler)发散KL发散是PPO算法中一个重要的概念，它表示两个概率分布之间的差异。在PPO算法中，我们希望新的策略（新的策略）与旧的策略（旧的策略）之间的差异尽可能小，避免单纯的提高强化评分的结果，而生成无效的文本输出，从而实现策略的可靠更新。 FLAN]]></content>
      <categories>
        <category>LLM</category>
        <category>微调</category>
      </categories>
      <tags>
        <tag>LLM</tag>
        <tag>微调</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大模型 - 微调方法 - PEFT]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-25.LLM-%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86%2F5021.%E5%A4%A7%E6%A8%A1%E5%9E%8B-%E6%A8%A1%E5%9E%8B%E4%BC%98%E5%8C%96-PEFT-01%2F</url>
    <content type="text"><![CDATA[微调的方法分类参数规模从参数规模的角度，大模型的微调分成两条技术路线： 全量微调FFT(Full Fine Tuning)，是对全量的参数，进行全量的训练。用特定的数据，对大模型进行训练，将W 变成W’ ，W’相比W ，最大的优点就是上述特定数据领域的表现会好很多。主要问题： 一个是训练的成本会比较高，因为微调的参数量跟预训练的是一样的多的； 一个是叫灾难性遗忘(Catastrophic Forgetting)，用特定训练数据去微调可能会把这个领域的表现变好，但也可能会把原来表现好的别的领域的能力变差。 PEFT(Parameter-Efficient Fine Tuning)，旨在通过最小化微调参数数量和计算复杂度，提升预训练模型在新任务上的表现，从而减轻大型预训练模型的训练负担。PEFT的出现就是为了解决 FFT中存在的主要问题。 PEFT 是目前比较主流的微调方案。 训练的方法 监督式微调SFT(Supervised Fine Tuning) 主要是用人工标注的数据，用传统机器学习中监督学习的方法，对大模型进行微调； 基于人类反馈的强化学习微调RLHF(Reinforcement Learning with Human Feedback) 是把人类的反馈，通过强化学习的方式，引入到对大模型的微调中去，让大模型生成的结果，更加符合人类的一些期望； 基于AI反馈的强化学习微调RLAIF(Reinforcement Learning with AI Feedback) 大致跟RLHF类似，但是反馈的来源是AI。这里是想解决反馈系统的效率问题，因为收集人类反馈，相对来说成本会比较高、效率比较低。 技术路径每种微调，都要对 权衡参数效率、内存效率、训练速度、模型质量和推理成本 进行权衡。针对微调，一些路径技术会冻结大部分模型权重，并专注于微调现有模型参数的子集，例如特定的层或组件，比如PEFT。其他技术根本不会触及原始模型的权重，而是添加少量新参数或层，只对新组件进行微调。 Selective仅微调原始 LLM 参数子集的方法。您可以采用多种方法来确定要更新的参数。您可以选择仅训练模型或特定层的某些组件，甚至可以选择单独的参数类型。研究人员发现，这些方法的性能好坏参半，参数效率和计算效率之间存在显著的权衡取舍。 Reparameterization重新参数化方法也可以使用原始的 LLM 参数，但是通过创建原始网络权重的新的低等级变换来减少要训练的参数数量。这种常用的技术是LoRa。 Additive通过冻结所有原始 LLM 权重并引入新的可训练组件来进行微调。这里有两种主要方法。 Adapters在模型架构中添加了新的可训练层，通常位于注意层或前馈层之后的编码器或解码器组件内。 Soft Prompts 可以保持模型架构的固定和冻结，并专注于操纵输入以实现更好的性能。这可以通过在提示嵌入中添加可训练的参数或保持输入固定并重新训练嵌入权重来实现。 基于PEFT的相关技术方法PEFT（Parameter-Efficient Fine-Tuning）是hugging face开源的一个参数高效微调大模型的工具，里面集成了4中微调大模型的方法，可以通过微调少量参数就达到接近微调全量参数的效果，使得在GPU资源不足的情况下也可以微调大模型。 PEFT技术 即使在计算资源受限的情况下，也能够利用预训练模型的知识快速适应新任务，实现有效的迁移学习。因此，PEFT不仅能提升模型效果，还能显著缩短训练时间和计算成本，使更多研究者能够参与到深度学习的研究中。PEFT包括LoRA、QLoRA、适配器调整(Adapter Tuning)、前缀调整(Prefix Tuning)、提示调整(Prompt Tuning)、P-Tuning及P-Tuning v2等多种方法。 以下图表示了7种主流微调方法在Transformer网络架构中的作用位置及其简要说明，接下来将详细介绍每一种方法。 1. LoRA（基于新数据训练一个地址分解矩阵，使用时加到原模型矩阵上）LoRA 背后有一个假设：我们现在看到的这些大语言模型，它们都是被过度参数化的。而过度参数化的大模型背后，都有一个低维的本质模型。。也就是说 大模型中有其中一部分参数，是非常重要的，是影响大模型生成结果的关键参数，这部分关键参数就是上面提到的低维的本质模型。LoRA（Low-Rank Adaptation）是一种旨在微调大型预训练语言模型（如GPT-3或BERT）的技术。其核心理念在于，在模型的决定性层次中引入一个额外的小型矩阵(“旁路”用A、B两个矩阵组合表示，维度分别是d × r和 r × d，其中r远小于d，A随机初始化，B初始化为0)，来实现模型行为的微调，而无需对整个模型结构进行大幅度修改。在微调模型的过程中，左边的W不更新，只更新右边的A和B的参数。前向传播时是左右的输出和，反向传播时只更新右边，因此计算的梯度以及优化器的中间值也只和右边有关，最终右边的参数会单独保存下来。LORA这种训练方式不会改变大模型的参数，且针对每个下游任务生成自己的LORA参数，在预测阶段只要将大模型的参数和LORA参数叠加在一起即可。 LoRA的基本思路，包括以下几步： 首先, 要适配特定的下游任务，要训练一个特定的模型，将 $Y=WX$ 变成 $Y=(W+∆W)X$ ，这里面 $∆W$ 主是我们要微调得到的结果； 其次，将 $∆W$进行低维分解 $∆W=AB$ (∆W为m n维，A为m r维，B为r * n维，r就是上述假设中的低维)； 接下来，用特定的训练数据，训练出A和B即可得到∆W，在推理的过程中直接将∆W加到W上去，再没有额外的成本。 另外，如果要用LoRA适配不同的场景，切换也非常方便，做简单的矩阵加法即可：(W + ∆W) - ∆W + ∆W`。 该方法认为模型权重矩阵在特定微调后具有较低的本征秩，故基于秩分解的概念，将预训练模型的现有权重矩阵分成两个较小的矩阵。LoRA的特点 将矩阵乘积BA加到原模型参数矩阵W上可以避免推理延迟。 可插拔的低秩分解矩阵模块，方便切换到不同的任务。 不显著增加额外计算负担的前提下，能够有效地微调模型，同时保留模型原有的性能水准。 LoRA的实验结果使用的模型是RoBERTa、DeBERTa、GPT-2、GPT-3 175B。在多个数据集上，LoRA在性能上能和全量微调相近，且在某些任务上优于全量微调。 2、QLoRA（低精度加载高精度重训练）QLoRA（Quantized Low-Rank Adaptation）是一种结合了LoRA（Low-Rank Adaptation）方法与深度量化技术的高效模型微调手段。QLoRA的核心在于： 量化技术：QLoRA采用创新的技术将预训练模型量化为4位。这一技术包括低精度存储数据类型（4-bit NormalFloat，简称NF4）和计算数据类型（16-bit BrainFloat）。这种做法极大地减少了模型存储需求，同时保持了模型精度的最小损失。 量化操作：在4位量化中，每个权重由4个比特表示，量化过程中需选择最重要的值并将它们映射到16个可能的值之一。首先确定量化范围（例如-1到1），然后将这个范围分成16个区间，每个区间对应一个4-bit值。然后，原始的32位浮点数值将映射到最近的量化区间值上。 微调阶段：在训练期间，QLoRA先以4-bit格式加载模型，训练时将数值反量化到bf16进行训练，这样大幅减少了训练所需的显存。例如，33B的LLaMA模型可以在24 GB的显卡上进行训练。 量化过程的挑战在于设计合适的映射和量化策略，以最小化精度损失对性能的影响。在大型模型中，这种方法可以显著减少内存和计算需求，使得在资源有限的环境下部署和训练成为可能。大模型入门（二）—— PEFT 3.适配器调整 Adapter Tuning（增加一个Adapter层，与LoRA技术类似，适配器调整的目标是在保留预训练模型原始参数不变的前提下，使模型能够适应新的任务。适配器调整的方法是在模型的每个层或选定层之间插入小型神经网络模块，称为“适配器”。这些适配器是可训练的，而原始模型的参数则保持不变。 Adapter的架构如下： 在每一个Transformer层中的每个子层之后插入两个串行的Adapter。在Adapter微调期间，绿色层是根据下游数据进行训练的，而预训练模型的原参数保持不变。Adapter 模块主要由两个前馈（Feed-forward）子层组成。 第一个前馈子层将原始特征的维度d投影到一个更小的维度m，应用非线性函数，再投影回维度d的特征（作为Adapter模块的输出）。 总参数量为2md + d + m。通过设置m &lt; d，我们限制了每个任务添加的参数数量。 当投影层的参数初始化接近零时，根据一个skip-connection，将该模块就初始化为近似恒等函数，以确保微调的有效性。 LoRA 与 Adapter Tuning 的主要区别在于： LoRA：在模型的权重矩阵中引入低秩矩阵来实现微调。这些低秩矩阵作为原有权重矩阵的修改项，在实际计算时对原有权重矩阵进行调整。 适配器调整：通过在模型各层中添加小型神经网络模块，即“适配器”，来实现微调。适配器独立于模型的主体结构，仅适配器的参数在微调过程中更新，而模型的其他预训练参数保持不变。 早期的**提示微调**通过修改输入文本来控制语言模型的行为，称为硬提示（Hard Prompts）微调。这些方法很难优化，且受到最大模型输入长度的限制。下图为离散的人工设计的Prompt示例： ![image](https://img-blog.csdnimg.cn/img_convert/d8cb7200c1a4dd1d2f24ea020554babe.png) 软提示（Soft Prompts）将离散的“提示”问题转为连续的“提示”问题，通过过反向传播和梯度下降更新参数来学习Prompts，而不是人工设计Prompts。 硬提示和软提示之间的区别在于，**硬提示提供了明确的指令来指导模型的响应，并且可能需要针对不同的任务定制模型**，而软提示通过**调整提示参数**来指导模型的行为，提供更通用和更有效的策略跨越各种任务。 有仅对输入层进行训练，也有对所有层进行训练的类型。下面将介绍几种热门的Soft Prompts微调方法。 4.前缀调整 Prefix Tuning()Prefix Tuning的灵感来源是，基于Prompt Engineering的实践表明，在不改变大模型的前提下，在Prompt上下文中添加适当的条件，可以引导大模型有更加出色的表现。前缀调整提出了一种新的策略，即在预训练的语言模型（LM）输入序列前添加可训练、任务特定的前缀（添加的是参数或矩阵），从而实现针对不同任务的微调。除了在embedding层加入这个前缀之外，还在其他的所有层都添加这样一个前缀。最后微调时只调整前缀的参数，大模型的参数保持不变。这意味着我们只需要为不同任务保存各自特异的前缀部分的参数，而不是为每个任务保存一整套微调后的模型权重，从而节省了大量的存储空间和微调成本。 前缀实际上是一种连续可微的虚拟标记（Soft Prompt/Continuous Prompt），与离散的Token相比，它们更易于优化并且效果更佳。这种方法的优势在于不需要调整模型的所有权重（基座不变），而是通过在输入中添加前缀来调整模型的行为，从而节省大量的计算资源，同时使得单一模型能够适应多种不同的任务。前缀可以是固定的（即手动设计的静态提示）或可训练的（即模型在训练过程中学习的动态提示） Prefix Tuning的特点 冻结预训练语言模型的参数，为每个任务存储特定的连续可微的前缀，节省空间。 训练间增加MLP层以达到稳定。 对于不同模型构造不同的Prefix。 Prefix Tuning的实验结果：对于表格到文本任务，使用GPT-2MEDIUM和GPT-2LARGE模型。在表格到文本任务上，Prefix Tuning优于Fine-Tuning（全量微调）和Adapter-Tuning。对于摘要任务，使用BART-LARGE模型。在摘要任务上，Prefix Tuning比全量微调弱。 5.提示调整（Prompt Tuning）Prompt Tuning的出发点，是基座模型(Foundation Model)的参数不变，为每个特定任务，训练一个少量参数的小模型，在具体执行特定任务的时候按需调用。这些可训练的提示向量在训练过程中更新，以指导模型输出更适合特定任务的响应。 像GPT3中那种，通过人工构造一些token作为前缀输入到模型中，因为这些token是从vocab中选择的，因此会受到大模型的参数的影响，所以要取得好的结果的话，人工构造的提示语必须要符合模型训练语料的特性。而Prompt tuning是为Prompt单独生成一份参数，在微调的过程中大模型的参数冻结不变，只更新Prompt的参数。且文章实验表明对于Prompt的参数使用大模型的vocab中的一些token 的embedding初始化，或者使用标签词的嵌入（当标签词的token数大于1时，对所有token取平均，即将一个标签词看作一个整体）初始化要比随机初始化的效果要好。此外Prompt的长度对结果也会有影响，长度越长效果会越好，但随着模型的规模变大，不同长度或者不同初始化的Prompt之间的差距会被缩小。 Prompt Tuning与Prefix Tuning都涉及在输入数据中添加可学习的向量，这些向量是在输入层添加的，但两者的策略和目的不同： 提示调整：旨在模仿自然语言中的提示形式，将可学习向量（通常称为提示标记）设计为模型针对特定任务生成特定类型输出的引导。这些向量通常被视为任务指导信息的一部分，倾向于使用较少的向量来模仿传统的自然语言提示。 前缀调整：可学习前缀更多地用于提供输入数据的直接上下文信息，作为模型内部表示的一部分，可以影响整个模型的行为。 Prompt Tuning的基本原理是在输入序列X之前，增加一些特定长度的特殊Token，以增大生成期望序列的概率。如果将大模型比做一个函数：Y=f(X)，那么Prompt Tuning就是在保证函数本身不变的前提下，在X前面加上了一些特定的内容，而这些内容可以影响X生成期望中Y的概率。 其结构如下：上图中，仅Virtual Token部分会由梯度下降法去更新参数。 Prompt Tuning的实验结果:使用的是预训练的各种T5模型。在流行的SuperGLUE基准测试中，Prompt Tuning的任务性能与传统的模型调优相当，且随着模型规模的增加，差距逐渐减小。在零样本领域迁移中，Prompt Tuning可以改善泛化性能。 Prompt Tuning 是在 Embedding环节，往输入序列X前面加特定的Token。而Prefix Tuning是在Transformer的Encoder和Decoder的网络中都加了一些特定的前缀。 具体来说，就是将Y=WX中的W，变成W= [Wp; W]，Y=WX。 6. P-Tuning的特点 一般在通过Prompt的方式使用大模型时，通常需要人工构造一些模板，P-tuning将自然语言模板的构建转换成连续参数优化的问题，用一些特殊的token替代人工构造的自然语言模板，让模型自己去学习这些连续的token，在学习的过程中只微调这些token的embedding参数，并且为了保证token之间的联系，并不是随机初始化embedding，而是通过lstm层学习这些token的embedding。 P-Tuning（基于提示的微调）和提示调整都是为了调整大型预训练语言模型（如GPT系列）以适应特定任务而设计的技术。两者都利用预训练的语言模型执行特定的下游任务，如文本分类、情感分析等，并使用某种形式的“提示”或“指导”来引导模型输出，以更好地适应特定任务。 提示调整与P-Tuning的主要区别在于： 提示调整：使用静态的、可训练的虚拟标记嵌入，在初始化后保持固定，除非在训练过程中更新。这种方法相对简单，因为它只涉及调整一组固定的嵌入参数，在处理多种任务时表现良好，但可能在处理特别复杂或需要细粒度控制的任务时受限。 P-Tuning：使用一个可训练的LSTM模型（称为提示编码器prompt_encoder）来动态生成虚拟标记嵌入，允许根据输入数据的不同生成不同的嵌入，提供更高的灵活性和适应性，适合需要精细控制和理解复杂上下文的任务。这种方法相对复杂，因为它涉及一个额外的LSTM模型来生成虚拟标记嵌入。 P-Tuning中使用LSTM（长短期记忆网络）作为生成虚拟标记嵌入的工具，利用了LSTM的以下优势： 更好的适应性和灵活性：LSTM可以捕捉输入数据中的时间序列特征，更好地理解和适应复杂的、顺序依赖的任务，如文本生成或序列标注。 改进的上下文理解：LSTM因其循环结构，擅长处理和理解长期依赖关系和复杂的上下文信息。 参数共享和泛化能力：在P-Tuning中，LSTM模型的参数可以在多个任务之间共享，这提高了模型的泛化能力，并减少了针对每个单独任务的训练需求。而在提示调整中，每个任务通常都有其独立的虚拟标记嵌入，这可能限制了跨任务泛化的能力。 其结构如下： P-Tuning只在输入层加入可微的Virtual Token，其会自动插入到文本提示的离散Token嵌入中。 Virtual Token不一定作为前缀，其插入位置是可选的。 P-Tuning的实验结果 使用的是GPT系列和BERT系列的模型。P-Tuning与全参数效果相当，且在一些任务上优于全参数微调，可以显著提高GPT模型在自然语言理解方面的性能，并且BERT风格的模型也可以获得较小的增益。 7.P-Tuning v2P-Tuning v2是P-Tuning的进一步改进版，在P-Tuning中，连续提示被插入到输入序列的嵌入层中，除了语言模型的输入层，其他层的提示嵌入都来自于上一层。这种设计存在两个问题： 第一，它限制了优化参数的数量。由于模型的输入文本长度是固定的，通常为512，因此提示的长度不能过长。 第二，当模型层数很深时，微调时模型的稳定性难以保证；模型层数越深，第一层输入的提示对后面层的影响难以预测，这会影响模型的稳定性。 其结构如下： P-Tuning v2的改进在于，不仅在第一层插入连续提示，而是在多层都插入连续提示，且层与层之间的连续提示是相互独立的。这样，在模型微调时，可训练的参数量增加了，P-Tuning v2在应对复杂的自然语言理解(NLU)任务和小型模型方面，相比原始P-Tuning具有更出色的效能。 P-Tuning v2的实验结果:使用的是BERT系列和GLM系列模型。P-Tuning v2是一种在不同规模和任务中都可与微调相媲美的提示方法。在NLU任务中，整体上P-Tuning v2与全量微调的性能相差很小。 除了上述方法外，随着大模型行业的发展，也有一些新的微调方法相继涌现 8.BitFit属于选择性方法，是对模型的现有参数进行微调，可以根据层的深度、层类型或者甚至是个别参数进行选择。2022年9月5日，BitFit出现，这是一种稀疏微调方法，仅修改模型的Bias（偏置项）或其中的子集。 BitFit的特点 冻结大部分Transformer编码器的参数，只训练偏置项和任务特定的分类层。 优化的偏置项参数包括Attention模块中计算Query、Key、Value时，计算MLP层时，计算Layernormalization层时遇到的偏置项参数。 每个新任务只需要存储偏置项参数向量（占总参数数量的不到0.1%）和任务特定的最终线性分类器层。 BitFit的实验结果 使用公开可用的预训练BERTBASE、BERTLARGE和RoBERTaBA模型。BitFit微调结果不及全量参数微调，但在极少数参数可更新的情况下，远超Frozen（冻结模型参数）方式。 9.AdaLoRA该方法基于权重矩阵的重要性而自适应调整不同模块的秩，节省计算量，可理解为LoRA的升级版。AdaLoRA的做法是让模型学习SVD分解的近似。在损失函数中增加了惩罚项，防止矩阵P和Q偏离正交性太远，以实现稳定训练。AdaLoRA的实验结果:使用的模型是DeBERTaV3-base 和BART-large模型。AdaLoRA的性能通常高于参数量更高的方法。其中，AdaLoRA在0.32M微调参数时，在CoLA数据集上达到了70.04的Mcc分数。 其他方法RLHF (Reinforcement Learning fromHuman feedback)RLHF使用强化学习（简称RL）使用人类反馈数据对LLM进行微调，从而生成更符合人类偏好的模型。您可以使用 RLHF 来确保模型生成的输出能够最大限度地提高输入提示的实用性和相关性。也许最重要的是，RLHF可以帮助最大限度地减少可能的伤害。你可以训练你的模型，让它给出承认其局限性的注意事项，并避免使用有害的语言和话题。 PPO(Proximal Policy Optimization) KL(Kullback-Leibler)发散KL发散是PPO算法中一个重要的概念，它表示两个概率分布之间的差异。在PPO算法中，我们希望新的策略（新的策略）与旧的策略（旧的策略）之间的差异尽可能小，避免单纯的提高强化评分的结果，而生成无效的文本输出，从而实现策略的可靠更新。 FLAN]]></content>
      <categories>
        <category>LLM</category>
        <category>微调</category>
      </categories>
      <tags>
        <tag>LLM</tag>
        <tag>微调</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[模型优化 使用Lora方法优化Qwen2模型，实现NER性能提]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-25.LLM-%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86%2F5021.%E5%A4%A7%E6%A8%A1%E5%9E%8B-%E6%A8%A1%E5%9E%8B%E4%BC%98%E5%8C%96-PEFT-01.Lora%E4%BC%98%E5%8C%96Qwen2%E5%AE%9E%E7%8E%B0NER%E4%BC%98%E5%8C%96-%E5%AE%9E%E4%BE%8B%2F</url>
    <content type="text"><![CDATA[参考 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197import jsonimport pandas as pdimport torchfrom datasets import Datasetfrom modelscope import snapshot_download, AutoTokenizerfrom swanlab.integration.huggingface import SwanLabCallbackfrom peft import LoraConfig, TaskType, get_peft_modelfrom transformers import AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForSeq2Seqimport osimport swanlabdef dataset_jsonl_transfer(origin_path, new_path): """ 将原始数据集转换为大模型微调所需数据格式的新数据集 """ messages = [] # 读取旧的JSONL文件 with open(origin_path, "r") as file: for line in file: # 解析每一行的json数据 data = json.loads(line) input_text = data["text"] entities = data["entities"] match_names = ["地点", "人名", "地理实体", "组织"] entity_sentence = "" for entity in entities: entity_json = dict(entity) entity_text = entity_json["entity_text"] entity_names = entity_json["entity_names"] for name in entity_names: if name in match_names: entity_label = name break entity_sentence += f"""&#123;"entity_text": "&#123;entity_text&#125;", "entity_label": "&#123;entity_label&#125;"&#125;""" if entity_sentence == "": entity_sentence = "没有找到任何实体" message = &#123; "instruction": """你是一个文本实体识别领域的专家，你需要从给定的句子中提取 地点; 人名; 地理实体; 组织 实体. 以 json 格式输出, 如 &#123;"entity_text": "南京", "entity_label": "地理实体"&#125; 注意: 1. 输出的每一行都必须是正确的 json 字符串. 2. 找不到任何实体时, 输出"没有找到任何实体". """, "input": f"文本:&#123;input_text&#125;", "output": entity_sentence, &#125; messages.append(message) # 保存重构后的JSONL文件 with open(new_path, "w", encoding="utf-8") as file: for message in messages: file.write(json.dumps(message, ensure_ascii=False) + "\n")def process_func(example): """ 将数据集进行预处理 """ MAX_LENGTH = 384 input_ids, attention_mask, labels = [], [], [] system_prompt = """你是一个文本实体识别领域的专家，你需要从给定的句子中提取 地点; 人名; 地理实体; 组织 实体. 以 json 格式输出, 如 &#123;"entity_text": "南京", "entity_label": "地理实体"&#125; 注意: 1. 输出的每一行都必须是正确的 json 字符串. 2. 找不到任何实体时, 输出"没有找到任何实体".""" instruction = tokenizer( f"&lt;|im_start|&gt;system\n&#123;system_prompt&#125;&lt;|im_end|&gt;\n&lt;|im_start|&gt;user\n&#123; ['input']&#125;&lt;|im_end|&gt;\n&lt;|im_start|&gt;assistant\n", add_special_tokens=False, ) response = tokenizer(f"&#123;example['output']&#125;", add_special_tokens=False) input_ids = instruction["input_ids"] + response["input_ids"] + [tokenizer.pad_token_id] attention_mask = ( instruction["attention_mask"] + response["attention_mask"] + [1] ) labels = [-100] * len(instruction["input_ids"]) + response["input_ids"] + [tokenizer.pad_token_id] if len(input_ids) &gt; MAX_LENGTH: # 做一个截断 input_ids = input_ids[:MAX_LENGTH] attention_mask = attention_mask[:MAX_LENGTH] labels = labels[:MAX_LENGTH] return &#123;"input_ids": input_ids, "attention_mask": attention_mask, "labels": labels&#125;def predict(messages, model, tokenizer): device = "cuda" text = tokenizer.apply_chat_template( messages, tokenize=False, add_generation_prompt=True ) model_inputs = tokenizer([text], return_tensors="pt").to(device) generated_ids = model.generate( model_inputs.input_ids, max_new_tokens=512 ) generated_ids = [ output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids) ] response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0] print(response) return responsemodel_id = "qwen/Qwen2-1.5B-Instruct"model_dir = "./qwen/Qwen2-1___5B-Instruct"# 在modelscope上下载Qwen模型到本地目录下model_dir = snapshot_download(model_id, cache_dir="./", revision="master")# Transformers加载模型权重tokenizer = AutoTokenizer.from_pretrained(model_dir, use_fast=False, trust_remote_code=True)model = AutoModelForCausalLM.from_pretrained(model_dir, device_map="auto", torch_dtype=torch.bfloat16)model.enable_input_require_grads() # 开启梯度检查点时，要执行该方法# 加载、处理数据集和测试集train_dataset_path = "ccfbdci.jsonl"train_jsonl_new_path = "ccf_train.jsonl"if not os.path.exists(train_jsonl_new_path): dataset_jsonl_transfer(train_dataset_path, train_jsonl_new_path)# 得到训练集total_df = pd.read_json(train_jsonl_new_path, lines=True)train_df = total_df[int(len(total_df) * 0.1):]train_ds = Dataset.from_pandas(train_df)train_dataset = train_ds.map(process_func, remove_columns=train_ds.column_names)config = LoraConfig( task_type=TaskType.CAUSAL_LM, target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], inference_mode=False, # 训练模式 r=8, # Lora 秩 lora_alpha=32, # Lora alaph，具体作用参见 Lora 原理 lora_dropout=0.1, # Dropout 比例)model = get_peft_model(model, config)args = TrainingArguments( output_dir="./output/Qwen2-NER", per_device_train_batch_size=4, per_device_eval_batch_size=4, gradient_accumulation_steps=4, logging_steps=10, num_train_epochs=2, save_steps=100, learning_rate=1e-4, save_on_each_node=True, gradient_checkpointing=True, report_to="none",)swanlab_callback = SwanLabCallback( project="Qwen2-NER-fintune", experiment_name="Qwen2-1.5B-Instruct", description="使用通义千问Qwen2-1.5B-Instruct模型在NER数据集上微调，实现关键实体识别任务。", config=&#123; "model": model_id, "model_dir": model_dir, "dataset": "qgyd2021/chinese_ner_sft", &#125;,)trainer = Trainer( model=model, args=args, train_dataset=train_dataset, data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, padding=True), callbacks=[swanlab_callback],)trainer.train()# 用测试集的随机20条，测试模型# 得到测试集test_df = total_df[:int(len(total_df) * 0.1)].sample(n=20)test_text_list = []for index, row in test_df.iterrows(): instruction = row['instruction'] input_value = row['input'] messages = [ &#123;"role": "system", "content": f"&#123;instruction&#125;"&#125;, &#123;"role": "user", "content": f"&#123;input_value&#125;"&#125; ] response = predict(messages, model, tokenizer) messages.append(&#123;"role": "assistant", "content": f"&#123;response&#125;"&#125;) result_text = f"&#123;messages[0]&#125;\n\n&#123;messages[1]&#125;\n\n&#123;messages[2]&#125;" test_text_list.append(swanlab.Text(result_text, caption=response)) swanlab.log(&#123;"Prediction": test_text_list&#125;)swanlab.finish()]]></content>
      <categories>
        <category>LLM</category>
        <category>微调</category>
      </categories>
      <tags>
        <tag>LLM</tag>
        <tag>微调</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基本概念]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-25.LLM-%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86%2F5001.%E5%A4%A7%E6%A8%A1%E5%9E%8B-%E6%A6%82%E8%BF%B0-%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%2F</url>
    <content type="text"><![CDATA[怎么部署一个开源的模型 初步了解执行的原理 怎么用自己的数据对模型进行微调 如何评估、量化模型的表现 概念定义Learning technique of Machine Learning Model: One-shot learning – each new class has one labeled example. The goal is to make predictions for the new classes based on this single example. Few-shot learning – there is a limited number of labeled examples for each new class. The goal is to make predictions for new classes based on just a few examples of labeled data. Zero-shot learning – there is absolutely no labeled data available for new classes. The goal is for the algorithm to make predictions about new classes by using prior knowledge about the relationships that exist between classes it already knows. In the case of large language models (LLMs) like ChatGPT, for example, prior knowledge is likely include semantic similarities. configuration parameters一些相关的参数会影响在进行推理输出时结果的自由度。 max new tokens : 生成文本的长度(最大长度)实际不一定生成到这么长。 top-k : 指定模型，仅从概率最高的钱 k 个单词中进行选择。提供一定的随机性同时限制文本的过度扩散。 top-p : 随机抽样限制为组合概率不超过 p 的预测。对概率从高到低排序后，累计概率小于等于p的单词会被进行随机选择。 temperature : 调整单词概率的随机性。temperature 越大，则生成的文本概率越均衡（不同词的概率越接近），反之则越概率峰值越强（不同词的概率差异越大，大部分概率集中在少量词上）。 分词器LLM 的三种主要架构以上三种架构各有优劣，其应用场景和前景也不尽相同。 Encoder-Only 架构适用于文本分类和情感分析等任务，其前景主要取决于其在这些任务中的性能和准确性。 Decoder-Only 架构适用于文本生成和机器翻译等任务，其前景主要取决于其生成文本的质量和多样性。 Encoder-Decoder 架构适用于机器翻译和对话生成等任务，其前景主要取决于其在这些任务中的性能和准确性。 Decoder-Only也被称为生成式架构，仅包含解码器部分。它通常用于序列生成任务，如文本生成、机器翻译等。这种架构的模型适用于需要生成序列的任务，可以从输入的编码中生成相应的序列。Decoder-Only 架构的优点是擅长创造性的写作，比如写小说或自动生成文章。它更多关注于从已有的信息（开头）扩展出新的内容。其缺点是需要大量的训练数据来提高生成文本的质量和多样性。 定义和特点: Decoder-Only架构专注于从一系列输入生成或预测输出。这种架构通常用于文本生成任务，如语言模型。优点: 强大的生成能力：能够生成连贯、有创造性的文本。灵活性：适用于各种生成型任务。缺点: 有限的理解能力：不擅长理解复杂的输入数据。示例模型: GPT系列、LLaMA、OPT、BLOOM Encoder-Only这种架构主要用于处理输入数据，专注于理解和编码信息，而不是生成新的文本。 定义和特点：Encoder-Only架构专注于理解和编码输入信息，常用于主题分类、情感分析等任务。优点: 强大的理解能力：能够有效处理和理解输入数据。适用性广泛：适用于多种分析型任务。缺点: 生成能力有限：不擅长自主生成文本或内容。示例模型: Google的BERT是一个典型的Encoder-Only架构模型。 Encoder-DecoderEncoder-Decoder 架构，也被称为序列到序列架构，同时包含编码器和解码器部分。它通常用于序列到序列（Seq2Seq）任务，如机器翻译、对话生成等。这种架构的代表是以 Google 训练出来的 T5 为代表的相关大模型。 定义和特点 : Encoder-Decoder架构结合了编码器和解码器的优点，通常用于需要理解输入并生成相应输出的任务，如机器翻译。优点 : 灵活强大：能够理解复杂输入并生成相关输出。 适用于复杂任务：如机器翻译、文本摘要等。缺点 : 架构复杂：相比单一的Encoder或Decoder，它更复杂。 训练挑战：需要更多的数据和计算资源。示例模型 : Google的T5是一个著名的Encoder-Decoder架构模型，智谱AI的ChatGLM也是Encoder-Decoder架构模型。 这就像是翻译家。他先听你说一段话（比如英文），理解它，然后把它翻译成另一种语言（比如中文）。 Encoder-Decoder模型就是这样，先理解输入的信息（Encoder部分），然后基于这个理解生成新的、相关的内容（Decoder部分）。 特点：擅长处理需要理解输入然后生成相关输出的任务，比如翻译或问答系统。 这种模型在需要深入理解输入内容并生成相关响应的任务中表现良好，例如机器翻译、问答系统等。 Generative IA Project lifecycle Scope 尽可能准确、狭义地定义范围。正如你到目前为止在本课程中看到的那样，LLM 能够执行许多任务，但他们的能力在很大程度上取决于模型的大小和架构。 Select 选择使用现有的模型还是自己进行训练 Adapt and align model prompt engineering fine-tuning alignment with human feedback Evaluate Application intergration Optimize and deploy model for inference 优化和部署推理模型 Augment model and build LLM-powered Application 增强模型和构建llm驱动的应用程序 一些概念 灾难性遗忘： 在进行微调后，进行任务推理时，LLM 会忘记之前可以识别推理的内容。扩展阅读资料 生成式 AI 生命周期AWS 上的生成式 AI：构建情境感知、多模态推理应用 》–O’Reilly 的这本书深入探讨了生成式 AI 生命周期的各个阶段，包括模型选择、微调、适应、评估、部署和运行时优化。 多任务、指令微调 Scaling Instruction-Finetuned Language Model 以任务、模型大小和思维链数据为重点进行 Scaling 微调。介绍 FLAN：通过指令微调实现更通用的语言模型 这篇博客（和文章）探讨了指令微调，其目的是使语言模型在执行 NLP 任务时更好地进行零点推理。 模型评估指标 HELM - 语言模型的整体评估 HELM 是一个活的基准，用于更透明地评估语言模型。 通用语言理解评估（GLUE）基准 本文介绍了 GLUE，这是一个在多样化自然语言理解（NLU）任务中评估模型的基准，并强调了改进通用 NLU 系统的重要性。 SuperGLUE 本文介绍了 SuperGLUE 基准，该基准旨在评估各种 NLP Model 在一系列具有挑战性的语言理解任务中的表现。 ROUGE：摘要自动评估软件包 本文介绍并评估了 ROUGE 摘要评估软件包中的四种不同测量方法（ROUGE-N、ROUGE-L、ROUGE-W 和 ROUGE-S），它们通过将摘要与理想的人工生成摘要进行比较来评估摘要的质量。 测量大规模多任务语言理解MMLU 本文提出了一种新的测试方法来测量文本模型的多任务准确性，强调了在实现专家级准确性方面进行实质性改进的必要性，并解决了在社会重要主题上的片面表现和低准确性问题。 BigBench-Hard - 超越模仿游戏：量化和推断语言模型的能力 该论文介绍了 BIG-bench，这是一个在具有挑战性的任务中评估语言模型的基准，提供了关于 Scale、校准和社会偏见的见解。 参数高效微调（PEFT） Scaling Down to Scale Up: A Guide to Parameter-Efficient Fine-Tuning 本文系统概述了讲座视频中讨论的所有三个类别中的参数效率微调（PEFT）方法。 论参数效率微调的有效性 本文分析了 NLP 中预训练模型的稀疏微调方法。 LoRALoRA Large Language Model 的低秩适应 本文提出了一种参数高效微调方法，利用低秩分解矩阵减少微调语言模型所需的可训练参数数量。 QLoRA：efficient Finetuning of Quantized LLMs 本文介绍了一种基于量化的、在单个 GPU 上对 Large Language Model 进行微调的高效方法，在基准测试中取得了令人印象深刻的结果。 使用软提示进行 Prompt 调整 The Power of Scale for Parameter-Efficient Prompt Tuning 这篇论文探讨了 “Prompt-tuning”，这是一种利用学习到的软提示对语言模型进行调节的方法，与完全微调相比，这种方法取得了具有竞争力的性能，并使模型能够在许多任务中重复使用。]]></content>
      <categories>
        <category>LLM</category>
      </categories>
      <tags>
        <tag>LLM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Dag-有向无环图]]></title>
    <url>%2F03.%E5%BA%94%E7%94%A8-99.other%2FDag-%E6%9C%89%E5%90%91%E6%97%A0%E7%8E%AF%E5%9B%BE%2F</url>
    <content type="text"><![CDATA[有向无环图（DAG），数据流的基石。在当今快速发展的信息技术领域，数据流的管理和优化是至关重要的。有向无环图（DAG）作为一种数据结构，因其独特的优势，在多个领域中发挥着关键作用。本文将深入探讨DAG的用途、应用场景、基本逻辑和语法。 DAG简介有向无环图（Directed Acyclic Graph，简称DAG）是一种特殊的图，其中的边具有方向，且不存在环。这种结构保证了数据流的单向性和无回环性，非常适合用于表示任务之间的依赖关系。 DAG的用途任务调度：在项目管理和任务调度中，DAG可以清晰地表示任务的先后顺序和依赖关系。 数据流处理：在数据科学和大数据处理中，DAG用于定义数据的流向和转换过程。 工作流管理：在工作流管理系统中，DAG帮助自动化和优化工作流程。 网络拓扑：在网络设计中，DAG可以用于表示网络设备的连接关系，确保没有回路。 应用场景Apache Airflow：一个流行的开源工作流管理系统，使用DAG来定义、调度和监控复杂的数据管道。 TensorFlow：一个广泛使用的机器学习框架，使用DAG来构建和执行计算图。 区块链技术：在区块链中，DAG可以用于表示交易和区块的依赖关系。 基本逻辑节点：DAG中的每个点代表一个任务或操作。 边：边表示节点之间的依赖关系，即一个任务必须在另一个任务完成后才能开始。 拓扑排序：一种对DAG中的节点进行排序的方法，使得对于任意一条从节点u到节点v的边，u都在v之前。 语法DAG的表示通常依赖于特定的工具或框架，但基本的语法元素包括： 定义节点：指定每个节点的名称和属性。 定义边：指定节点之间的依赖关系。 配置参数：设置任务执行的参数，如执行时间、资源需求等。 示例以下是使用Python代码定义一个简单的DAG的示例：1234567891011121314151617from airflow import DAGfrom airflow.operators.dummy import DummyOperator# 定义DAGdag = DAG( &apos;example_dag&apos;, schedule_interval=&apos;0 12 * * *&apos;, # 每天中午执行 start_date=datetime(2024, 7, 8), catchup=False,)# 定义任务task1 = DummyOperator(task_id=&apos;task1&apos;, dag=dag)task2 = DummyOperator(task_id=&apos;task2&apos;, dag=dag)# 定义依赖关系task1 &gt;&gt; task2 DAG作为一种强大的数据结构，其在现代数据处理和任务调度中的重要性不言而喻。通过本文的介绍，希望能帮助读者更好地理解和应用DAG，以优化自己的工作流程和数据处理策略。]]></content>
      <categories>
        <category>Tobeinsert</category>
      </categories>
      <tags>
        <tag>Tobeinsert</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[健康管理师-重点记忆知识点]]></title>
    <url>%2F05.%E5%A4%87%E8%80%83-02.%E5%81%A5%E5%BA%B7%E7%AE%A1%E7%90%86%E5%B8%88%2F%E5%81%A5%E5%BA%B7%E7%AE%A1%E7%90%86%E5%B8%88-%E9%87%8D%E7%82%B9%E8%AE%B0%E5%BF%86%E7%9F%A5%E8%AF%86%E7%82%B9%2F</url>
    <content type="text"><![CDATA[重点指标： 正常成年人膳食中蛋白质、脂肪、碳水化合物适宜的供能比分别为：10%-15%：20%-30%：50%-65%。 尿液一般检测包括：①一般性状检测：尿量、气味、外观、比重、酸碱度等；②化学检测：尿蛋白、尿糖、尿酮体、尿胆原、尿胆红素等；③尿沉渣检测：细胞、管型、结晶体等。 血常规检测项目包括：红细胞计数、血红蛋白测定、白细胞计数及分类计数、红细胞平均值测定和红细胞形态检测，血小板平均值测定和血小板形态检测。 健康教育的核心任务就是促进个体或群体改变不健康的行为和生活方式，，形成有益于健康的行为。 健康管理服务的三个层次：提高认知水平、生活方式的改变、建立支持性环境。 糖尿病诊断标准： 2010年，ADA指南已将HbA1C≥6.5%作为糖尿病诊断标准之一。因为糖化血蛋白小于6.5%也不能除外糖尿病，需进一步行糖耐量检查。p73 WHO1999年标准为：糖尿病症状+任意时间血浆葡萄糖水平≥11.1mmol/L或;空腹（至少空腹8小时）血浆葡萄糖水平≥7.0mmol/L。正常标准： 空腹血糖 3.9-6.1mmol/L 且糖负荷后 2h 血糖＜7.8mmol/L。 | 测量时间 | 正常标准 | 糖尿病诊断标准 | | —————– | ————- | ————– | | 空腹（8小时）血糖 | 3.9-6.1mmol/L | ≥7.0mmol/L | | 餐后2小时血糖 | &lt;7.8mmol/L | ≥11.1mmol/L | 如果IGT（糖耐量损害）伴有以下因素，即原空腹血糖≥5.0mmol/L，餐后2小时血糖≥9.4mmol/L，BMI&gt;25，腹部肥胖和空腹胰岛素水平增加等，更易转化为糖尿病。 2型糖尿病的危险因素包括：①遗传因素；②肥胖（或超重）；③身体活动不足；④膳食因素；⑤早期营养；⑥糖耐量受损；⑦胰岛素抵抗；⑧高血压及其他易患因素。 年度评估包括糖尿病患者建档动态管理情况、糖尿病管理开展情况、糖尿病患者转入转出执行情况、疾病预防控制机构和综合医院对社区卫生服务机构业务指导和培训情况。 （2）阶段性评估(每3-5年进行1次)主要包括满意程度、糖尿病及其危险因素流行现状的了解情况。 高血压的干预侧重于疾病管理的策略，具体干预原则包括：①个体化；②综合化；③连续性；④参与性；⑤及时性。 临床上高血压诊断标准： 经非同日3次测量血压，收缩压≥140mmHg和（或）舒张压≥90mmHg。 白大衣高血压是指患者到医疗机构测量血压高于140/90mmHg，但动态血压24小时平均值&lt;130/80mmHg或家庭自测血压值&lt;135/85mmHg。 隐性高血压是指患者到医疗机构测量血压&lt;140/90mmHg，但动态血压24小时平均值高于130/80mmHg或家庭自测血压值高于135/85mmHg。 高血压发病的危险因素包括：①高钠、低钾饮食；②体重超重和肥胖；③饮酒；④其他危险因素：遗传、性别、年龄、工作压力过重、心理因素、高脂血症等。其中不可改变的因素为：年龄、性别、种族、遗传等。 高血压是冠心病、脑血管病、慢性肾脏疾病发生和死亡的最主要的危险因素。 类别 收缩压(高压)(mmHg) 舒张压(低压) 理想血压 &lt;120 &lt;80 正常血压 &lt;130 &lt;85 正常高值 130-139 85-89 1级高血压(轻度) 140-159 90-99 2级高血压(中度) 160-179 100-109 3级高血压(重度) &gt;180 &gt;110 单纯收缩期高血压 &gt;140 &lt;90 收缩和舒张等级不一致时，以高级别为准。 个体高血压干预的效果评估的分级分为优良、尚可、不良共3个等级（C、E错），①优良：全年累计有9个月以上的时间血压记录在140/90mmHg以下（A错）；②尚可：全年有6～9个月的时间血压记录在140/90mmHg以下（D对）；③不良：全年有不足6个月的时间血压记录在140/90mmHg以下（B错）。题干患者既往血压测量记录发现其有约1/3的时间血压水平是高于140/90mmHg的，即有8个月的时间血压水平是低于140/90mmHg的，可判定为尚可。 四种因素影响人们的卫生服务消费需求： 1患病率 2感知到的需要 3患者偏好 4健康因素以外的因素 脑卒中的危险因素包括：①高血压；②心脏病；③糖尿病；④血脂异常；⑤吸烟；⑥饮酒；⑦颈动脉狭窄；⑧肥胖；⑨其他危险因素：高同型半胱氨酸血症、代谢综合征、缺乏体育活动、饮食营养不合理、口服避孕药、促凝危险因素（血小板聚集率、纤维蛋白原、凝血因子Ⅶ等）。国内外几乎所有研究都证明，高血压史脑出血和脑梗死最重要的危险因素。 目前我国成人BMI的切点为：18.5≤BMI&lt;24为正常，24≤BMI&lt;27.9为超重，BMI≥28为肥胖。 腰臀比=腰围（cm)/臀围（cm)为最窄部位的腰围除以最宽部位的臀围，腰臀比男性&lt;1.0、女性&lt;0.85为正常，而腰臀比男性≥1.0、女性≥0.85为腹型肥胖。 发现甲类传染病（包括：霍乱，鼠疫）和乙类传染病中的肺炭疽，传染性非典型肺炎，埃博拉出血热，人感染禽流感寨卡病毒病，黄热病，拉沙热，列谷热，西尼罗病毒等应按有关要求与2小时内报告，发现其他乙类丙类传染病患者，疑似患者和规定报告的传染病病源携带者，应与24小时内报告。 必须氨基酸是体内无法合成的氨基酸：（携一两本（组）单色书来） 缬氨酸、异亮氨酸、亮氨酸、苯丙氨酸、组氨酸(婴儿)、蛋氨酸、色氨酸、苏氨酸、赖氨酸。 常量元素是指在人体内含量较多，大于体重的0.01%，每日膳食需要量都在100mg以上者，称为常量元素，有钙、镁、钾、钠、磷、硫、氯，共7种。 能量的统一单位，即焦耳（joule，J）或卡（calorie，cal）。 单位换算：1kcal=4.184kJ;1kJ=0.239kcal 每克脂肪可以释放9Kcal能量，每克酒精可以释放7Kcal能量，每克蛋白质和碳水化合物都可以产生4Kcal能量，每克膳食纤维可以释放2Kcal能量。 《中国居民平衡膳食宝塔》指导：第一层，谷薯类250~ 400g；第二层，蔬菜类300~ 500g/天，水果类200~ 350g/天；第三层，畜肉类40~ 75g/天；第四层，奶及奶制品300g/天；第五层，油25~ 30g/天。 《中国居民膳食指南》（2016）一般人群膳食指南包括：①食物多样，谷类为主；②吃动平衡，健康体重；③多吃蔬菜、奶类、大豆；④适量吃鱼、禽、蛋、瘦肉；⑤少油少盐，控糖限酒；⑥杜绝浪费，兴新食尚。 风险管理的方法包括：风险损失控制、风险补偿、风险规避、风险转移等，但不包括风险自留。 自我戒烟法可分为四个阶段，第一阶段：准备阶段（B错），第二阶段：行动阶段（D错），第三阶段：维持阶段（E对），第四阶段：随访 无内分泌疾病或找不出可能引起肥胖的特殊病因的肥胖症为单纯性肥胖（C对）。单纯性肥胖者占肥胖症总人数的95%以上 男性腰围≥90cm、女性≥85cm患肥胖相关疾病的危险性增加。黄女士，体重75kg，身高160cm，腰围88cm，体质指数BMI29.3kg/M 2为肥胖，则腰围（F对）、体重（D对）；黄女士空腹血糖6.5mmol/L，正常人的空腹血糖值为3.9-6.1mmol/L，则空腹血糖（B对）需进一步干预；黄女士总胆固醇4.4mmol/L，正常人血总胆固醇&lt;5.2mmol/L，在正常范围内（G错）；黄女士甘油三酯1.5mmol/L，甘油三酯的正常参考值：0.45～1.69mmol/L，在正常范围内（A错）；餐后2小时血糖7.4mmol/L（4.6～7.8mmol/L）（C错） 长期过量饮酒者指：每日饮酒量≥100ml且每周饮酒在4次以上 健康教育的干预策略包括： ①教育策略：登载健康信息（A对）、提高防病认知和技能（D对）、组织讲座（E对）、利用专题活动宣传健康知识（F对）； ②环境策略：改善食堂饮食状况，提倡减盐少油（B错）； ③政策策略。改善社会环境不属于企业健康教育的策略（C错）。。 健康管理师要对兰女士进行有效随访，应完成的步骤包括：①第1步：事前准备（E对）；②第2步：总结前一段阶段的进展（D对）；③第3步：确认对方目前的需求（A对），而非尽量满足她的要求（C错）；④第4步：达成共识（F对）；⑤第5步：安排下次随访的时间和方式（B对）。 测量身高注意事项：受试者应当空腹（D错）、脱鞋（G对）、只穿轻薄的衣服（C对）。测量身高的量尺（最小刻度为1mm）（E对）应与地面垂直固定或贴在墙上。受试者直立、两脚后跟并拢靠近量尺（F对），并将两肩及臂部也贴近量尺。测量人员用一个直角尺放在受试者的头顶，使直角的两个边一边靠紧量尺，另一边接近受试者的头皮，读取量尺上的读数，准确至1mm（B错）。每次测量身高最好连续测2次，间隔30秒（A对）。两次测量的结果应大致相同。身高计的误差不得超过0.5cm。故此题选择ACEFG选项。 张女士身高164cm，体重80kg，身高在165厘米以下者：标准体重（kg）=身高（cm）-105=59kg，标准体重正负10%为正常体重，标准体重正负10%~20%为体重过重或过轻，标准体重正负20%以上为肥胖或体重不足（A对，B错），经计算BMI=29.7kg/cm 2（G对，C错），BMI≥28kg/m 2为肥胖（F错）；男性≥90cm、女性≥85cm为中心性肥胖标准（DE错误）。故此题选择AG选项。 血清总胆固醇的正常参考值：2.85～5.69mmol/L（D错），甘油三脂标准理想范围为&lt;1.7mmol/L 目标包括总体目标和具体目标，具体目标包括行为目标、认知目标、健康目标。运动属于行为目标。 有关信息收集的内容应包括：①既往身体活动水平评价；②心脑血管疾病风险评价、运动风险测试与体适能水平；③兴趣；④运动禁忌证；⑤运动环境；⑥运动指导需求。 人际传播的应用。针对个体的传播材料的使用包括传单、折页、手册。针对群体的传播材料的使用包括宣传栏、招贴画/海报、标语/横幅、DVD。发放小册子是针对个体的传播材料，题目问的是在社 区里开展健康教育应该是针对群体，E不选 合并冠心病的高血压患者.原则上舒张压最好不低于60mmHg。 理想体重=身高(cm)-100(身高165cm以下者，则减105)，张女士理想体重为164-105=59kg 代谢当量，是指相对于安静休息时身体活动的能量代谢水平，单位梅脱（MET）。1梅脱相当于每分钟每千克体重消耗3.5ml的氧，或每千克体重消耗1.05kcal能量的活动强度。≥6METs为高等强度活动，3~5.9METs为中等强度活动，1.6~2.9METs为低强度活动。1MET相当于安静代谢率，可转换用于能量消耗的计算，可以代替心率控制运动的相对强度。 基本卫生保健的原则。预防为主：卫生保健的重点是预防和保健，是为促进健康服务，而不是单纯治疗疾病，医疗部门也应参与其中。 心理健康的标准一般都包括智力正常、情绪良好、人际和谐、社会适应、人格完整这五点。 空腹血糖反映的是胰岛β细胞的功能，一般代表胰岛素的基础分泌功能 健康相关行为改变的理论包括：知信行模式、健康信念模式、自我效能理论、行为改变阶段理论，PRECEDE模式理论属于需求评估内容（A错），7P理论属于健康管理服务营销内容（C错）。故此题选择BDEF选项。 身体活动可分为： - ①柔韧性活动：促进提高关节柔韧性和灵活性的活动； - ②强壮肌肉活动：保持肌肉力量、体积和耐力的运动； - ③平衡性活动：保持姿势的活动； - ④健骨运动； - ⑤高强度间歇性运动。所以体力活动干预可选择灵活性练习（A对）、柔韧性练习（C对）、耐力运动（B对）和肌肉力量（E对） 个人基本信息，在填写过程中请参照下面的“项目说明”来填写：①工作单位；②联系人姓名；③民族；④血型；⑤文化程度；⑥药物过敏史；⑦既往史；⑧家族史。没有列出的，在“其他”中写明（C对）。ABDE说法均不全面。故此题选择C选项。 健康教育的干预策略包括健康教育干预策略包括教育策略、环境策略、政策策略， - 教育策略包括：①通过电子媒介开展的大众传媒活动；②通过印刷媒介开展的活动；③人际传播活动包括讲课、同伴教育、演示与示范； - ④因地制宜的社区活动：墙体标语、板报、墙报、展览、义诊、评选示范户、知识竞赛、患者俱乐部等（E对）； - ⑤民俗、文体活动；环境策略作用对象是物质环境、条件，其目的是使人们采纳健康行为的意愿得以实现（D错）； - 政策策略通常从两方面作用于人群的健康行为：①政策可以支持并促使这些行为得以实现。②政策策略可以通过影响资源配置、环境改善，从而促进健康行为甚至健康（C错）。倡导、传播不属于健康干预策略（A、B错）。故此题选择E选项。 群体的传播材料：宣传栏、招贴画/海报、标语/横幅、DVD。大众传播材料或媒介：报刊/杂志、广播、电视等都属于。个体传播材料：传单、折页、小册子。 健康教育计划评价 健康教育计划评价的主要内容： ①过程评价：关注对健康教育/健康促进计划实施过程（A对）； ②效应评价：关注目标人群健康相关行为及其影响因素的变化（D对）； ③结局评价：关注目标人群健康状况乃至生活质量的变化（E对）。形成评价、需求评价、经验评价不用与健康干预计划的评价（B、C、F错）。故答案选ADE。信息反馈健康管理师完成信息录入、分析整理后，应及时地将结果按照规定的格式反馈给客户。这些信息的传通方法有：①通知客户到健康管理中心：以面对面的方式将结果告诉客户，最好也同时打印一份结果交给客户，同时进行相应的解释（C对）；②将打印的结果通过邮寄方式寄给客户，但可能会遇到特殊问题需要解释或者需要到健康管理中心进行复查后进一步解释诊断（B错）；③以电子邮件的形式将结果发送给客户，缺点与邮寄方法相同（E错）；④电话通知客户，电话比较直接，可以比较详细地解释一些结果。但是，电话方式往往由于语言表达等问题造成客户的错误理解（D错）；⑤短信通知客户，由于短信的描述比较简单，只能是一些不太重要的信息，或紧急需要联系客户的情况下才使用（A错）。故此题选择C选项。糖尿病 糖尿病常规管理临床监测指标： 血糖（每2周1次）（D错）； 糖化血红蛋白（每3个月1次或每12个月1次）（A对）； 血压（一般每3个月一次，高血压患者每周1次）（E错）； 其他如血脂、尿微量白蛋白、视网膜检查、心电图、神经病变和足部检查均每年1次（BC错）， 增殖期视网膜病变患者应随时就诊眼科，有病变的视病情严重程度加强随访。故此题选择A选项。 糖尿病干预的过程评估包括：年度评估和阶段性评估（每3-5年进行1次）。 ①年度评估指标包括：建档动态管理情况、管理开展情况、转入转出执行情况、疾病预防控制机构和综合医院对社区卫生服务机构业务指导和培训情况； ②阶段性评估指标：社区糖尿病及其危险因素流行现状了解情况、参与工作的人员对该项工作的满意情况、社会大众对政府部门工作的满意情况。 D选项“糖尿病患者规范管理情况”属于糖尿病干预的效果评估。“糖尿病患者规范接受药物治疗情况”属于糖尿病干预的年度效果评估指标。注意题目要求选不属于过程评估的是，效果评估不是过程评估，所以选D。 糖尿病的常见干预效果评价指标包括：空腹血浆葡萄糖水平、任意时间血浆葡萄糖水平、口服葡萄糖耐量试验、糖化血红蛋白，不包括“尿蛋白”。 肥胖 肥胖干预措施的行为疗法应注意： 建立节食意识，每餐不过饱（A对），尽量减少暴饮暴食的额度和程度。 注意挑选脂肪含量低的食物。细嚼慢咽以延长进食时间（C对），使在进餐尚未完毕以前即对大脑发出饱信号，有助于减少进食量。 另一种方法就是进食时使用较小的餐具（B对），使得中等量的食物看起来也不显得单薄；也可按计划用餐，即在进餐前将一餐的食物计划分装，自我限制进食量，使每餐达到七分饱（D错），也可使漏者不致在下一餐过量进食。 餐后加点儿水果可以满足进食欲望（E对）。故此题选择D选项。 高血压分级标准： 1级高血压：收缩压140～159mmHg和（或）舒张压90～99mmHg。当收缩压和舒张压分属于不同级别时，以较高的分级为准。刘女士体检血压150/85mmHg，所以属于高血压1级。 2级高血压：收缩压160～179和（或）舒张压100～109。 3级高血压：收缩压≥180和（或）舒张压≥110。该患者属于1级高血压。 高血压分级管理，风险一级：高血压1级，无其他心血管疾病危险因素，要求至少每3个月随访1次（B错）；风险二级：高血压2级或1～2级同时有1～2个其他心血管疾病危险因素，至少每2个月随访1次；风险三级：高血压3级或合并3个以上其他心血管疾病危险因素或合并靶器官损害或糖尿病或并存临床情况，至少每个月随访1次（A错）。王先生高血压1级，伴有其他心血管疾病危险因素，属于高血压风险二级，至少每2个月随访1次（E对）。在高血压分级管理中未提及半年（D错）和一年（C错）。故此题选择E选项。 超重/肥胖医学营养治疗指南,推 荐每日能量摄入平均降低30%～50%或降低500～1000 kcal， 或推荐每日能量摄入限制在男性为1200～1500kcal/d，女性 为1000～1200 kcal/d 血压血压至少测量2次，应间隔2分钟重复测量，取2次读数的平均值记录。如果2次测量的收缩压或舒张压读数相差＞5mmHg，则应相隔2分钟后再次测量（E对）。然后取3次读数或后2次读数相近的结果的平均数值。E对，ABCD错。故此题选择E选项。 戒烟 文化因素：不同的文化现象对于成瘾行为起到了社会润滑作用（A错）。情绪不稳和冲动性。（B错）。传播媒介因素：媒体宣传与广告效应在成瘾行为的形成中起到了不可低估的作用（D错）。家庭影响：吸烟和酗酒行为都有“家庭集聚现象”，即家庭成员在某健康相关行为上的相似程度显著大于非成员（E错）。社会心理因素：生活节奏的加快、激烈的竞争、生活紧张性刺激增多，使人们应激增加。工作压力属于是社会心理因素（C对）。故此题选择C选项。 健康管理师在创建无烟环境和倡导戒烟中起着重要作用，可提供的干预至少包括询问吸烟情况、劝阻吸烟、提供信息以帮助戒烟、参与戒烟项目和安排随访防止复吸等。 二手烟暴露能使非吸烟者的冠心病风险增加25%~30%，肺癌风险提高20%~30%，也会导致上呼吸道损伤，激发哮喘频繁发作，增加血液黏稠度，伤害血管内膜，引起冠状动脉供血不足，增加心脏病发作的危险等。二手烟可以导致新生儿猝死综合征、中耳炎、低出生体重等。不会增加2型糖尿病的风险。 戒烟后的恢复情况：停止吸烟 8小时左右体内的一氧化碳水平趋向正常（E对），体内的血氧水平趋向正常。 48小时左右手和脚的血液循环得到改善，嗅觉、味觉能力明显改善。 72小时左右呼吸较轻松，肺活量开始增加（C错）。 1.5～2周肺功能改善约30%（D错）。 1～9个月内咳嗽，鼻塞、疲劳和呼吸困难减少，肺内纤毛重生，控制黏液的能力增加，清理肺部，减少感染，总的体能水平增加。 5年后患癌症几率会大大降低（B错），患心脏病危险性显著下降（A错）。 10年后患肺癌的几率可下降至近于从不吸烟人群，癌前细胞被替代，其他与吸烟有关癌症的机会减少。故此题选择E选项。 处理戒断反应针对戒断症状的处理措施： 抑郁：打电话给亲朋好友，和别人一起看电影、逛街或参观展览，默念自己的戒烟决心（B错）； 失眠：下午6点以后不喝咖啡，睡前在床上阅读，睡前保持1～15分钟的安静时间（A对）； 注意力难以集中：停下来休息，注意力最难集中时去做些重要的事情，不要在同一个位置坐太久（C对）； 坐立不安：尝试捏皮球或其他“减压器”，嚼无糖口香糖，糖果、胡萝卜或剔牙，投入到业余爱好中（D对）； 简易应对方法：散步、刷牙、勤做深呼吸、洗澡、逛街、看报纸等（E对）。故此题选择B选项。 其他 缺血性心血管疾病的风险评估应考虑的因素包括：年龄、收缩压、体重指数、总胆固醇、吸烟和血糖水平。 社区慢性病管理所使用的健康调查表中个人基本信息包括个人基本信息，比如姓名、性别、出生日期（A错）、文化程度（B错）、血型（C错）、婚姻状况、医疗费用支付方式（E错）等；心率并不属于个人基本信息（选D）。故此题选择D选项。 常见的干预方法有电话咨询指导、邮寄健康教育资料或上网阅读或上门家访。低危组的管理对象可采用邮寄健康教育的文字材料或发送邮寄的干预方式，这种方式成本最低，但效果也较差。多数高血压患者的管理采用电话干预(包括短信通知)，电话干预的成本中等，效率高，干预效果中等。采用电话干预每个对象大约占时20分钟。上门家访的方式成本高，但干预效果好。 香烟中的一氧化碳使血液中的氧气含量减少，造成相关的高血压等疾病，吸烟使冠状动脉血管收缩，使供血量减少或阻塞，造成心肌梗塞。吸烟可使肾上腺素分泌增加，引起心跳加快，心脏负荷加重，影响血液循环而导致高血压、心脏病、中风等。所以吸烟最可能导致心血管疾病。 健康管理干预的模式包括1、契约式。2、自我管理式。3、家庭管理式。4、社区综合管理式。也可以采取公共区域禁烟模式。 群体肥胖干预的评估：①被管理(如某社区)人群肥胖知晓率，肥胖防治相关知识的知晓情况。②被管理人群中通过饮食控制，增加身体活动等方式达到减肥目标的比例。③被管理人群中肥胖者控制体重达标和未达标比例。④被管理人群心脑血管病发病、致残和死亡信息以及卫生经济学评价。不包括C被管理人群肥胖干预的满意情况。]]></content>
      <categories>
        <category>Tobeinsert</category>
      </categories>
      <tags>
        <tag>Tobeinsert</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Recommendations_for_Tumor_Mutational_Burden_Assay_Validation_and_Reporting]]></title>
    <url>%2F01.%E7%9F%A5%E8%AF%86-03.standard%2FRecommendations_for_Tumor_Mutational_Burden_Assay_Validation_and_Reporting%2F</url>
    <content type="text"><![CDATA[TMB概念起源于2013年Nature发表的一项研究(Signatures of mutational processes in human cancer)。在30个癌种7,000多个标本中，研究者通过全基因组测序（whole genome sequencing,WGS）和全外显子测序（whole exome sequencing, WES）技术分析了突变图谱，描述了不同癌种样本中每百万碱基（megabase, Mb）的突变数量。紧接着 2014年的一项黑色素瘤研究（Genetic Basis for Clinical Response to CTLA-4 Blockade in Melanoma）发现，免疫治疗的响应率与肿瘤突变数目有一定的相关性，通过WES检出错义突变数量大于100的患者接受免疫治疗后具有更长的总生存期（overall survival, OS），这是首个验证TMB和免疫治疗疗效相关性的研究，此后有一系列研究都不断证明TMB对免疫治疗疗效的预测作用，尤其是在非小细胞肺癌中出现大量研究成功。2017年，Genome Medicine发表的一项10万例实体瘤患者研究(Analysis of 100,000 human cancer genomes reveals the landscape of tumor mutational burden)，探索了靶向捕获测序panel与WES检测TMB的相关性，证实了panel检测TMB的可行性与可靠性。自此肿瘤突变负荷（TMB）已被认为是多种肿瘤类型免疫治疗反应的预测生物标志物，并且逐步成为国内大Panel/WES产品的标配。但不同实验室之间的 TMB 计算、报告和解释方式存在显着差异，所以自2020年相继出现一些专家共识。但目前尚未发布有关 TMB 验证和报告的指南。认识到当前临床 TMB 检测的挑战，分子病理学协会召集了一个由美国临床肿瘤学会、美国病理学家学会和癌症免疫治疗学会代表组成的多学科协作工作组，以审查围绕TMB 并根据调查数据、文献综述和专家共识制定 TMB 测试的分析验证和报告建议。这些建议涵盖 TMB 分析的分析前、分析和分析后因素，并且强调全面方法学描述的相关性，以实现分析之间的可比性。今天我们来看看 Recommendations for Tumor Mutational Burden Assay Validation and Reporting 指南的具体内容。 TMB 定义为已测序编码 DNA 每兆碱基 (Mb) 的非同义体细胞突变总数。据推测，高度突变的肿瘤会产生肿瘤特异性表位或新抗原，这些表位或新抗原更有可能被免疫系统识别为异体或外来物，因此被认为更适合 ICI 治疗。然而，目前 TMB 的计算、报告和解释方式存在差异。大部分差异源于实验室特定的检测特征，包括计算区域的基因组大小、测定，是否进行仅体细胞测序或配对肿瘤种系测序，生物信息学管道的算法组件和设置，在计算中包含或排除特定变异类型，以及调整或标准化数据的其他分析方法。 13多个分析前因素也可能影响 TMB 计算。于是工作组通过对已发表材料进行整理和核查以及对57名参与者进行调查。虽然整体会存在很多困难（文章方法学描述不完整，涉及性能验证的资料非常有限等等）但工作组仍提出了 13 项主题专家共识建议，解决了临床 TMB 测试的实验室相关验证、报告和发布注意事项 Recommendation no. 推荐编号 Related area 相关领域 Recommendation 推荐 1 Testing 测试 Laboratories should validate and report the enrichment method used in the TMB assay. 2 Testing 测试 Laboratories should validate and report the size and describe the genomic regions (ie, exons, introns, and intergenic regions) used for TMB calculation. 3 Testing 测试 Laboratories should validate TMB measurement against an orthogonal assay, and the method of TMB calculation used by the orthogonal comparison assay should be documented. 4 Testing 测试 Laboratories should include validation samples that reflect the intended use of the TMB assay with respect to both specimen type and representative tumor types. 5 Testing 测试 Laboratories may use reference materials to supplement but not supplant clinical samples for TMB assay validation. 6 Testing 测试 Laboratories may use in silico validation studies to supplement but not supplant a TMB assay wet laboratory validation. 7 Testing 测试 Laboratories should specify the sequencing mode (tumor-germline paired or somatic only) used by the TMB assay during TMB assay validation. If somatic-only sequencing is performed, filter settings used to remove common population variants should also be documented. 8 Testing 测试 Laboratories should establish the performance parameters of bioinformatic pipelines used for TMB calculation during validation. 9 Reporting 报告 Laboratories should report the assay name, version, and sequencing platform used for clinical TMB assays. 10 Reporting 报告 Laboratories should report the name, version, properties, and/or settings of bioinformatic pipeline software components used for TMB calculation. 11 Reporting 报告 Laboratories should report the specific types and/or categories of variants included in and omitted from the TMB calculation. 12 Reporting 报告 Laboratories should report the sequencing mode (tumor-germline paired or somatic only) used by the TMB assay. If somatic-only sequencing is performed, filter settings used to remove common population variants should be provided or made available on request. 13 Publication 出版物 Publications describing TMB assays intended for clinical applications, including description of clinical validation, should include performance characteristics that would facilitate methodological assessment. 建议1：实验室应验证和报告在TMB检测中使用的富集方法 临床实验室采用的TMB评估方法缺乏统一性。尽管基于扩增子法的测序也在使用，但最流行的还是杂交捕获法。不同的靶向富集方法之间存在差异性，会影响TMB的计算。 建议2：实验室应验证和报告用于TMB计算的基因组区域（即外显子、内含子和基因间隔区）的大小和描述 一般来说，TMB评估的准确性与基因组测序的panel大小直接相关，小panel导致的误差较多，而WES或WGS测序导致的误差较小。现在TMB检测绝大多数的panel大小在1~2Mb（&gt;300个基因）。 建议3：实验室应通过正交试验验证TMB检测结果，并应记录正交比较试验所使用的TMB计算方法 TMB检测涉及湿实验和干实验两部分，每个环节都可能会影响TMB的计算。因此，仅使用外部参考品来判断TMB检测的分析性能并不总是可行。TMB工作小组建议对所有TMB检测进行正交验证研究（如WES），以衡量其相对高质量对照的性能。 建议4：实验室应提供验证样本，以反映TMB检测在标本类型和代表性肿瘤类型方面的预期用途 在选择用于验证TMB检测的样本（例如组织FFPET-DNA或血液ctDNA）时，应考虑正在开发的检测类型，因为不同的检测具有不同的特性和样本要求。同时，考虑到不同肿瘤类型的TMB不同，实验室应将常见肿瘤类型纳入其TMB检测的分析性能评估中。 建议5：实验室可以使用参考品作为TMB检测验证的补充，但不能替代临床样本 根据实践调查结果，缺乏具有明确TMB的样本用于检测开发和验证是TMB检测实施的主要障碍。多个参考品来源（如市售质控品、细胞系、已知MSI阳性或POLE高突变样品）是可用于评估TMB准确度和精密度以及评估TMB检测分析测量范围和检测下限的。 建议6：实验室可以使用计算机验证研究来补充但不能取代TMB检测的湿实验验证 进行计算机正交验证（虚拟验证）实验具有诸多优势，包括减少验证时间和成本以及能够以独立方式验证检测的生物信息学部分。由于计算机验证方法是总TMB分析的一部分，因此应将其视为包括湿实验和生信性能评估的验证补充，而不是替代。 建议7：实验室应在TMB检测验证期间指定TMB检测使用的测序模式（肿瘤体系-胚系配对或仅肿瘤体系）；如果进行仅肿瘤体系测序，用于去除常见群体变异的过滤算法设置也应记录在案 根据实践调查结果，约有一半的实验室是进行肿瘤体系-胚系配对测序，有一半实验室是进行仅肿瘤体系测序。由于只有体系变异才有能力产生肿瘤新生抗原，因此在TMB评估分析之前去除胚系变异非常重要。如果仅进行肿瘤体系变异检测，则应适当验证用于胚系变异过滤的具体方法及其局限性。 建议8：实验室应在验证过程中建立用于TMB计算的生物信息学流程的性能参数 不同NGS检测和不同实验室的变异检出算法策略有所不同，因此确定特定变异调用流程非常重要。比如，实验室通常不会分析报告同义突变，但对于包含同义突变类型的TMB算法，需要对流程进行评估。 建议9：实验室应报告临床TMB检测所用的检测名称、版本和测序平台 随着癌症诊断中重要基因和基因组变异的开发和识别，实验室检测panel可能会更新升级。此外，即使在同一实验室，分析流程和版本不同，TMB值也不一定具有可比性 建议10：实验室应报告用于TMB计算的生物信息学流程软件组件的名称、版本、属性和/或设置 变异调用的算法策略因NGS检测而异，一些实验室可能会使用多个调用程序，并采用变异调用的并集进行TMB分析，应关注其生信流程名称、版本和属性等因素。 建议11：实验室应报告TMB计算中包含和排除的变异具体类型和/或类别 目前，已发表的文献中没有足够的证据来具体推荐哪些变异类型应纳入TMB计算。在缺乏技术标准化的情况下，为了能够对TMB检测进行比较，需要将此信息作为报告的一部分进行传达，报告应清楚地指出TMB计算中包含和排除的表一类型。鉴于在不同检测中所查询的基因组区域存在差异，TMB工作组建议将TMB报告为每Mb中的突变数值，而不是NGS检测中发现的突变总数，以便于解释TMB报告并实现检测结果之间的比较。 建议12：实验室应报告TMB检测所使用的测序模式（肿瘤体系-胚系配对或仅肿瘤体系）；如果进行仅肿瘤体系测序，应提供用于去除常见群体变异的过滤算法设置，或应要求提供 检测样本和测序模式应在TMB报告中明确说明。如果存在用于实施TMB计算算法的验证研究，则应引用该验证研究。 建议13：描述用于临床应用的TMB检测出版物（包括临床验证描述）应包括有助于方法学评估的性能特征 改进TMB出版物的报道具有巨大的潜力，可以规范化临床TMB检测开发、识别影响检测性能的变量，并促进TMB检测实施。]]></content>
      <categories>
        <category>NGS</category>
        <category>标准化</category>
      </categories>
      <tags>
        <tag>AMP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pipeline-frameworks-框架语言概览]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-04.pipeline%2Fpipeline-frameworks-%E6%A1%86%E6%9E%B6%E8%AF%AD%E8%A8%80%E6%A6%82%E8%A7%88%2F</url>
    <content type="text"><![CDATA[​目前生物信息领域游非常多的生物信息流程开发框架 Snakemake、Ruffus、WDL、CWL、nextflow 等等。每种框架都有自己的优缺点。因为各种不同的业务需求，自己也接触和落地了其中一些框架的流程开发和产品落地。所以开一个系列来记录以下各种不同流程框架的概念和应用流程开发的原理和逻辑，更主要的也是借机整理自己之前进行相关产品开发过程中随手记录的笔记。希望对后来的开发人员能有所帮助。截至目前，其实我自己主要深度使用过的框架主要是 Snakemake 和 WDL。 流程构建逻辑snakemake 和 wdl 两个框架其实在底层逻辑上其实是相反的：snakemake 是由果及因的框架结构，在snakemake中，首先需要知道最终想要得到的结果文件Z（第一个rule的输出结果），然后我们梳理流程逻辑发现执行ruleA得到Z，那么我们首先需要获得ruleA的输入文件X和Y； 而此时 X和Y 成为我们想要的结果，然后我们需要梳理流程逻辑，确定我们需要得到X和Y，那么我们首先需要获得A和B然后执行ruleB。 依次类推，直到我们需要的文件都已经存在，然后从我们已经拥有的文件开始执行流程，依次生成X和Y，再根据X和Y 生成Z。 所以snakemake 的流程是从结果开始,逐步反推，直到需要的信息已经完备。 wdl 是由因及果的框架结构，在wdl中，我们首先知道我们要执行任务taskA，然后检查执行taskA所需的文件 A和B是否存在，执行taskA运行得到C，然后发现我们需要执行 taskB,发现taskB需要的文件是否依赖taskA，如果依赖就等待taskA结束，如果不依赖就直接执行。然后执行taskB 获得结果文件 D ,以此类推，检查下一个task。另外wdl中文件依赖关系是继承自流程的，会直接基于每个task的output属性获得结果并在下游通过对应的具体属性进行文件命名或其他变量的继承。 执行的依赖 WDLwdl的执行依赖于womtools（进行校验和输入文件的生成）和cromwell（进行业务的执行）。 snakemakeSnakemake 本身就是其执行的依赖软件命名，任务的执行也是基于Snakemake。 流程语言的支持特性 框架 docker singularity conda SGE 配置文件 Snakemake √ √ √ √ yaml WDL √ √ √ √ json 对应关系 - WDL Snakemake 最小任务逻辑单元 task rule 引用 import include 执行逻辑 call 基于rule的IO自构建 资源配置 runtime resources 配置文件 json yaml，命令行 -C 指定 数据处理 wdl自建语法逻辑 支持python语法 任务的投递 框架 中断位置续跑 中间启动分析 单独跑某一部 配置文件 Snakemake √ √ √ yaml WDL √ × × json 目前的一些流程开发框架： Snakemake Ruffus WDL CWL nextflow]]></content>
      <categories>
        <category>pipeline</category>
        <category>framework</category>
      </categories>
      <tags>
        <tag>流程开发</tag>
        <tag>培训</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pipeline-frameworks-WDL-0.常见问题]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-04.pipeline%2Fpipeline-frameworks-WDL-0.%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[Cannot mix leading whitespace characters in command: [“ “, “\r”]]]></content>
      <categories>
        <category>Tobeinsert</category>
      </categories>
      <tags>
        <tag>Tobeinsert</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux-常用shell命令-set]]></title>
    <url>%2F03.%E5%BA%94%E7%94%A8-02.Linux%E5%92%8Cshell%2FLinux-%E5%B8%B8%E7%94%A8shell%E5%91%BD%E4%BB%A4-set%2F</url>
    <content type="text"><![CDATA[Linux set命令用于设置shell。set指令能设置所使用shell的执行方式，可依照不同的需求来做设置。set所有的参数说明参考官网文档set12345678910111213141516171819-a 标示已修改的变量，以供输出至环境变量。-b 使被中止的后台程序立刻回报执行状态。-C 转向所产生的文件无法覆盖已存在的文件。-d Shell预设会用杂凑表记忆使用过的指令，以加速指令的执行。使用-d参数可取消。-e 若指令传回值不等于0，则立即退出shell。-f 取消使用通配符。-h 自动记录函数的所在位置。-H Shell 可利用"!"加&lt;指令编号&gt;的方式来执行history中记录的指令。-k 指令所给的参数都会被视为此指令的环境变量。-l 记录for循环的变量名称。-m 使用监视模式。-n 只读取指令，而不实际执行。-p 启动优先顺序模式。-P 启动-P参数后，执行指令时，会以实际的文件或目录来取代符号连接。-t 执行完随后的指令，即退出shell。-u 当执行时使用到未定义过的变量，则显示错误信息。-v 显示shell所读取的输入值。-x 执行指令后，会先显示该指令及所下的参数。+&lt;参数&gt; 取消某个set曾启动的参数。 set -o我们可以看到，在上述参数中，并没有列出 -o,因为上述参数都是布尔型的参数来确定是否开启相关功能，而 -o需要提供对应的参数。使用方法如下：12345678910111213141516set -o &lt;option-name&gt; # 其中 option-name 是系列预定义的命令选项，其中有些命令选项和开关参数是类似的，在此不进行罗列。开关参数未涉及的命令选项如下： emacs Use an emacs-style line editing interface (see Command Line Editing). This also affects the editing interface used for read -e. history Enable command history, as described in Bash History Facilities. This option is on by default in interactive shells. ignoreeof An interactive shell will not exit upon reading EOF. nolog Currently ignored. pipefail If set, the return value of a pipeline is the value of the last (rightmost) command to exit with a non-zero status, or zero if all commands in the pipeline exit successfully. This option is disabled by default. posix Change the behavior of Bash where the default operation differs from the POSIX standard to match the standard (see Bash POSIX Mode). This is intended to make Bash behave as a strict superset of that standard. vi Use a vi-style line editing interface. This also affects the editing interface used for read -e. set -e先说说set -e，这个参数的含义是，当命令发生错误的时候，停止脚本的执行。这是一种替代 &amp;&amp; 的相对优雅的解决方案。 比如一个shell种，我们需要有序执行一系列任务，如果其中一个任务失败，则直接推出。起始shell本身是不会监控中间任务的，我们可能会习惯于使用&amp;&amp;来实现这样的功能，比如：12#!/bin/bashecho 111 &amp;&amp; rm qzcsbj.txt &amp;&amp; echo 2222 但是这样的写法会有几个问题，一方面是代码的可读性会非常差，示例只是简单的任务可能相对还好，但是有些单个任务带着参数可能都要上百个字符串甚至更多，而且一个shell中可能会涉及非常多任务，这样写代码可读性会非常差，使用&quot;\&quot;进行人工换行，也会极大的增加编码人员的思考了；另一方面这样的写法过程中，是不能进行代码注释的。而使用 set -e 命令，可以设置脚本遇到错误就退出。123456#!/bin/bashset -eecho 111rm qzcsbj.txtecho 2222 set -x说完了-e，继续说说-x。-x参数的作用，是把将要运行的命令用一个+标记之后显示出来。 还是拿上面这个脚本举个例子，这次加上-x：123456#!/bin/bashset -execho 111rm qzcsbj.txtecho 2222 运行后，会用 + 号标记并打印出来每行所执行的具体shell命令，如下：1234+ echo 111111+ rm qzcsbj.txtrm: cannot remove 'qzcsbj.txt': No such file or directory 注意第一行和第三行前面那个+，这就是-x参数的作用。 注意点shell 默认只能捕获 1~255的错误码，所以当错误码大于255时，会有捕获问题，而例如python的错误码会存在大于255的情况，所以需要注意进行必要的错误码转换，将错误码转换为1~255的范围。]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Shell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[健康管理师-专业技能-章节习题-03_04]]></title>
    <url>%2F05.%E5%A4%87%E8%80%83-02.%E5%81%A5%E5%BA%B7%E7%AE%A1%E7%90%86%E5%B8%88%2F%E5%81%A5%E5%BA%B7%E7%AE%A1%E7%90%86%E5%B8%88-%E4%B8%93%E4%B8%9A%E6%8A%80%E8%83%BD-%E7%AB%A0%E8%8A%82%E4%B9%A0%E9%A2%98-03_04%2F</url>
    <content type="text"><![CDATA[第三章 健康指导第一节 健康教育 生活方式指导：营养指导（膳食指南）、身体活动指导（心率、代谢当量、自我感知运动强度）、控烟指导（成瘾行为形成、影响因素、吸烟矫正、预防、健康促进）。 消极性反馈：作出不赞同、不拥护、不支持或反对的反应，是一种消极性反馈，如说“不行”、“不对”、“我不同意”等，或以摇头、皱眉等表情或动作来表示。 积极性反馈：作出理解、赞同、支持的反应，是一种积极性反馈，如“我认为你说的对”、“好”、“对”等，或以点头、伸大拇指等体语来表达。 健康教育干预策略包括教育策略、环境策略、政策策略。 教育策略包括：①通过电子媒介开展的大众传媒活动;②通过印刷媒介开展的活动;③人际传播活动;④因地制宜的社区活动：墙体标语、板报、墙报、展览、义诊、评选示范户、知识竞赛、患者俱乐部等。因此C对。营养指导 升糖指数全称为“血糖生成指数”，是指含50克碳水化合物的食物引起血糖上升所产生的血糖时间曲线下面积和标准物质(一般为葡萄糖)所产生的血糖时间下面积之比值再乘以100，它反映了某种食物与葡萄糖相比升高血糖的速度和能力。 《中国居民膳食指南》（2016）一般人群膳食指南包括以下6条：①食物多样，谷类为主；②吃动平衡，健康体重；③多吃蔬菜、奶类、大豆；④适量吃鱼、禽、蛋、瘦肉；⑤少盐少油，控糖限酒（成人烹调油控制在25～30g，对于血压高的人群，每日食盐用量应控制到5g以下，男性饮酒量应控制在25g酒精以下）；⑥杜绝浪费，新兴时尚。 营养素的分类： 宏量营养素:人体需要量多，在膳食中所占的比重大，包括:碳水化合物、蛋白质、脂类。 微量营养素:人体需要量相对较少，在膳食中所占比重也小较，包括:矿物质和维生素。 矿物质:常7，微8 常量元素:钠、磷、镁、钾、钙、硫、氯;大于体重的0.01%，每日膳食需要量都在100mg以上者。(记忆：钙镁钾钠磷硫氯) 微量元素:钼、铜、碘、铬、硒、铁、锌、钴。指体内含量小于体重的0.01%，每日膳食需要量为微克至毫克的矿物质。 (记忆:牧童点歌媳妇，贴心股) 维生素: 脂溶性维生素:维生素A（夜盲症）、维生素D（佝偻病）、维生素E、维生素K（凝血功能下降）; 水溶性维生素:维生素C、维生素B1（脚气病）、B2、B6、B12、烟酸、泛酸、叶酸、胆碱、生物素。 碳水化合物（4kcal)、蛋白质（4kcal)和脂类(9kcal)的主要作用是提供能量来满足人体的需要!也被称为产能营养素。 1kcal=4.184kJ。其他物质能量，酒精是 7kcal、 膳食纤维是 2kcal 碳水化合物是人体的主要能量来源（50%~65%），人体消耗的能量的20%~30% 来自脂肪。 成人每日烹调油的摄入量不超过30g， 建议25～30g。 身体活动指导 绝对强度:根据身体活动的绝对物理负荷量测定的强度水平常用指标为代谢当量(梅脱，METs)代谢当量:相对于安静休息时身体活动的能量代谢水平。1MET相当于每分钟每公斤体重消耗3.5ml的氧 或 每小时每公斤体重消耗1.05千卡(4.4千焦耳)能量的活动强度， 相对强度:根据生理反应测定的强度水平。 主观性疲劳感: 指标为自觉运动强度量表(即伯格Borgs)量表，也称为RPE分级表。 客观心率水平、耗氧量等:常用指标为:最大心率百分比、最大耗氧量百分比、靶心率。运动时的心率作为训练时运动强度的监测指标称为目标心率或称靶心率。 相对强度的衡量:最大心率百分比/伯格量表 最大心率百分比，一般认为心率达到最大心率的60%~75%时身体活动水平达到了中等强度。粗略计算，最大心率=220-年龄。适用于所有年龄段和体适能水平的成年人群的最大心率计算公式为:207-0.7x年龄。 伯格量表: 按照主观疲劳程度分级(6-20分级)中等强度通常在11-14区间内（身体感觉稍累）。将主观疲劳程度6作为最低水平，20作为最高水平: 运动中的心率可以通过颈动脉或四肢动脉触摸直接测量，测量时间可以为10秒，更方便的方法是采用有线和无线仪器设备监测心率。 大多数与运动有关的意外伤害都受到身体的内在承受能力与外部体力负荷量两方面因素的影响。 适度的体力负荷，通过耐力、肌肉力量、身体平衡协调能力和关节灵活柔韧性的锻炼，增加了身体抵御骨关节系统伤害的能力。 控烟指导(1)成瘾行为的概念、形成过程及影响因素1)成瘾行为的概念:吸烟和酗酒是典型的成瘾行为。致瘾原能使成瘾者产生强烈的欣快感和满足感。 成瘾行为的特征: ① 生理性依赖:在体内形成包括循环、呼吸等生理基础 ② 心理性依赖:心理一直想 ③ 社会性依赖:如吸烟成瘾者假如不先吸烟就无法完成开会人际交往、做报告等社会活动。进入某个环境就会出现(某个时间想) ④ 戒断症状:一旦终止成瘾物质的使用，会出现空虚，无聊，无助、不安等心理异常，同时会出现嗜睡、流涎、恶心等躯体异常症状。 成瘾行为的形成过程:诱导阶段、形成阶段、巩固阶段、衰竭阶段。 ① 诱导阶段: 人与致瘾原偶尔接触，初步尝到“甜头”。但终止后不会有明显的戒断症状。 ② 形成阶段: 初期成瘾者常有羞耻感、畏惧感和自责心理，宜于及时矫治。一旦依赖建立，矫治难度将增加。 ③ 巩固阶段: 成瘾行为已经巩固，并整合为生命活动的一部分成瘾者此阶段对各种促使其戒断的措施有强烈的心理抵抗，瘾的发作可使成瘾者宁可不吃、不喝、不睡，甚至明知后果严重也要接触成瘾物质。 ④ 衰竭阶段: 由于成瘾行为使躯体和心理受到严重损害，社会功能也会发生不同程度的缺失。如酒精依赖和酒精中毒者出现酒精性肝硬化症状。 成瘾行为的人格特征有：被动依赖、过度敏感、性格内向、高级意向减退或不稳定、情绪不稳和冲动性。 成瘾行为的影响因素包括：人格特征、社会环境因素、社会心理因素、文化因素、传播媒介因素、团体效应、家庭影响。 健康教育的制定 健康教育计划设计原则 (1) 目标原则 : 健康教育计划应有明确的总体目标(或称远期目标)+切实可行的具体目标(或称近期目标)。 (2) 前瞻性原则 : 计划的制订和执行:要考虑长远的发展和要求。 (3) 弹性原则 : 在制订计划时要尽可能预计到在实施过程中可能发生的变故，要留有余地并预先制订应变对策。 (4) 从实际出发原则 (5) 参与性原则 : 健康教育计划应该是健康管理师与服务对象共同制订的， 确定健康教育干预策略: (1)教育策略： ①通过电子媒介开展的大众传媒活动:电视节目、广播节目; ②通过印刷媒介开展的活动:手册、小折页、挂图、招贴画、日历、卡片、传单等; ③人际传播活动:讲座/讲课、小组讨论、个别咨询、示范、人户指导、观摩学习、同伴教育等; ④因地制宜的社区活动:墙体标语、板报、墙报、展览、义诊,评选示范户、知识竞赛、患者俱乐部等; ⑤民俗、文体活动:相声、戏曲、民歌、庙会、赶集等。 (2)环境策酪: 和工作环境有关 如在某企业职工预防心脑血管病的健康教育中食堂提供低脂、低盐的食物，在工作场所为职工提供一些锻炼设施等。 (3) 政策策略： 和规章制度有关 某企业创立无烟单位，规定全企业办公区一律禁止吸烟，制订有关工间操制度、轮班制度等。 健康教育计划各目标的内涵。具体分为认知目标、行为目标和健康目标三大类。 开放型问题给对方以思考和判断的余地的问话。 倾向性问题指把重要人物、团体或自己的观点加在问话里，暗示或诱导对方做出答案的问题。 在人际交流中有3种反馈形式：语言反馈、体语反馈和书面反馈。 动态体语：以点头表示肯定，以摇头表示否定，微笑、握手表示友好，用亲切的目光注视对方表示尊重。 针对个体的传播材料包括：传单、折页、手册、手机信息。 第二节 跟踪随访 人际沟通技巧是指在人际沟通活动中为有效地达到预期目的而采用的语言和非语言的方法。在人际交流活动中，人际沟通技巧都与人的“传播器官”有关，包括语言器官（口）、听觉器官（耳）、视觉器官（眼）。用说、听、看、问、答、表情、动作等方式来传达信息是人际沟通的基本方式，每一种方式的运用都有一定的技巧，它直接影响交流的效果。 人际沟通的技巧包括：说话的技巧、倾听的技巧、提问的技巧、观察的技巧、反馈的技巧、非语言传播技巧。 说话的技巧可以概括为： ①用听者熟悉、能懂的语言； ②口气和蔼可亲； ③简化速度适中，避免过快和过慢； ④声音应该有高低起伏，不要平铺直叙； ⑤发音吐词要清晰，要让对方能够听清楚； ⑥讲话的语气要生动； ⑦适当重复重要的和不易被理解的话； ⑧在与对方交谈时说话要有停顿，避免长时间自己一个人说话； ⑨尽量避免使用专业词汇，尽量用通俗语言代替专业术语； ⑩恰当地运用举例引证、示范与演示的技巧。 倾听的技巧包括： ①尽可能地多听，留意听，努力发现对方对某一问题的了解程度和看法； ②不轻易打断对方的讲话，耐心地等对方讲完； ③始终保持友好和礼貌，利用各种语言和非语言的方式表示在认真听，使对方感到轻松和受到尊重； ④不急于表达自己的观点，不轻易对对方的话作出评论； ⑤不应在听对方讲话时被其他事情干扰； ⑥对敏感的问题更要善于听出话外音，以捕捉真实的信息。 提问技巧包括： (1)提问题时要注意对方的表情和感受，应创造轻松愉快的交流气氛，不要一个问题紧接一个问题地问。 (2)要设法使服务对象感到所提问题与自己利益相关，才能吸引对方的注意并促使其回答问题。 (3)对敏感问题的提问形式尤要注意，可以先问一般性问题再逐步深入询问，不要单刀直入，还要注意选择适宜的交谈环境、时间和地点。也可以采用试探型提问方式。 (4)要了解对方的态度、观点等方面的信息，应该使用开放型问题(问答题)，避免使用封闭型问题(选择题)。 (5)探究型问题时应特别注意口气缓和、态度轻松，不可用质问的语气。 (6)要想收集到真实信息，不能用诱导型提问 (7)问题尽量简练、明确，不提复合型问题。 观察技巧，有时通过观察所获得的信息比用耳朵获取的信息更有价值“耳听为虚，眼见为实”就是这个意思。 反馈技巧 在人际交流中有3种反馈形式:语言反馈、体语反馈和书面反馈。体语反馈:用动作、表情等“身体语言”给予反馈。书面反馈:利用书面上的文字或符号作出反应。 非语言传播技巧,运用身体语言、类语言和时空语言的传播技巧，即借助视听、触觉等感官分享信息，增进交流效果的一些技巧。包括：动态体语（点头微笑）、静态体语（服饰仪表）、类语言（声调和笑声调节氛围）、时间语（提前到达，准时赴约）、空间语(安静整洁的环境）。 依据性质的不同，反馈也可分为3种: ①积极性反馈:如“我认为你说对。”!“好!”“对”等;或以点头、伸大拇指等体语来表达， ②消极性反馈:如说“不行”“不对”“我不同意”等，或以摇头、皱眉等表情或动作来表示。 ③模糊性反馈:作出没有明确态度和立场的反应，如“哦!”“是吗?”以及不置可否的表情等, 在人际交流中，健康管理师应该掌握的技巧: (1) 在听对方的陈述时，要集中注意力，并随时用表情、体语来表示自己对对方谈话的兴趣，如微笑、点头等，以支持对方把交流进行下去(运用积极性反馈技巧) (2) 恰当运用体语，比如点头、摇头、伸大拇指等(运用积极性、消极性反馈技巧): (3) 支持对方的正确观点和行为要态度鲜明。 (4) 纠正对方错误观点和行为要和缓、婉转、耐心。 (5) 对有些敏感问题和难于回答的问题可以暂时回避，不作正面解答。 (6) 对于知识性问题或决策性问题，不要给对方似是而非、含糊不清的回答;搞清对方问题的核心，不要答非所问;了解对方的意图，针对问题的实质给予解答。 (7) 对于不同人提出的同样的问题，回答可以因人而异。根据当事人的背景、性别、年龄、文化程度、宗教信仰、性格等情况给子恰当的回答。 第四章 健康危险因素干预第一节 干预方案的实施 &amp; 第二节 干预效果监测 健康危险因素干预是指应用临床医学、预防医学、行为医学心理学、营养学和其他健康相关学科的理论和方法对个体和群体的健康危险因素进行控制和干预，预防疾病、促进健康，是在了解管理对象健康状况，并进行健康及疾病风险评估的基础上，以多种形式来帮助和指导管理对象采取行动，纠正不良的生活方式和习惯，控制健康危险因素，实现健康管理的目标。 健康管理的干预模式：契约式；自我管理式；家庭管理式；社区综合管理式。 高血压干预 高血压的干预原则包括：个体化、综合性、连续性、参与性、及时性。 高血压干预的目标人群包括:一般人群、高血压高危人群、高血压患者。符合下列任意一项者即为高血压高危人群：收缩压为120-139mmhg和（或）舒张压为80-89mmhg；有高血压家族史者；超重和肥胖者；长期过量饮酒者；长期高盐膳食者。 高血压的干预程序包括：筛查和确诊高血压患者、高血压患者的危险分层、制定干预计划、定时随访、评价管理工作和评价管理效果。 高血压的干预策略是非药物治疗(健康生活方式调整)和药物治疗相结合，两手一起抓，两手都要硬。 高血压管理的工作指标包括: 高血压规范治疗百分比、高血压双向转诊百分比、高血压建档合格百分比、高血压规范管理百分比 。 在开展生活方式干预之后的一定期间，应对其实际效果进行评估，一般以2个月为宜。 高血压患者的非药物治疗包括：提倡健康饮食；戒烟；限制饮酒和戒酒；增加身体活动；管理体重；高血压健康教育；保持良好的心理状态；食盐摄入小于5g。WHO和我国均建议每人每天的钠盐摄入量为6g以下，高血压患者应尽量达到5g以下的限制标准，在保证人体日常基本钠离子需要的基础上越低越好。 高血压生活方式干预开展之后，一方面应询问管理对象生活习惯的改善情况，另一方面检查其血压、血脂、血糖、体重的变化，并与第一次相关检查结果进行比较分析。 高血压患者治疗情况：规范治疗百分比＝（每年社区能按照医嘱接受规范治疗的高血压患者人数）/（当年社区中全部高血压患者人数）×100% 高血压干预的4要3补： 要在医师指导下锻炼: 要运动量、运动强度、运动时间逐渐增加。 要不拘泥于形式和时间: 要以有氧运动为主; 不要做动作过猛、体位幅度变化过大、屏息时间过长的动作 高血压患者应节制饮酒，一般建议男性将饮酒量控制在30ml/d，大约相当于酒精25g。女性不超过1.5g，孕妇不饮酒。 为确定随访的频率、干预的方式和干预的强度，将人力、物力用在危险度高、自我管理意识差的人群上，应将预备管理的高血压患者按照血压水平、伴随危险因素和并发症情况进行分层。 血压管理分組，应该是低血压、正常血压、正常高值血压、高血压再分三级(高血压一级；高血压二级；高血压三级). 高血压的危险因素包括∶高钠、低钾膳食;超重和肥胖;饮酒、其他危险因素。其中高钠低钾膳食是高血压病最重要的饮食危险因素。 糖尿病干预 糖尿病的干预策略包括：糖尿病教育和自我管理,自我血糖监测，随访管理，糖尿病的药物治疗，糖尿病的非药物治疗。 糖尿病干预步骤：筛查和确诊糖尿病患者;糖尿病患者的危险分类;制订干预计划;执行干预计划;定时随访和评价管理工作;评价管理效果。 糖尿病的常见干预效果评价指标包括：空腹血浆葡萄糖水平（价格最低廉）、任意时间血浆葡萄糖水平、餐后2小时血糖值、糖化血红蛋白。P129 任意时间血浆葡萄糖水平的测量的取血成分是静脉血浆。 餐后2小时血糖计时应该从进食第一口饭开始。 糖化血红蛋白作为筛查糖尿病高危人群和诊断糖尿病的一种方法，其结果稳定，不受进食时间及短期生活方式改变的影响，变异性小，检查不受时间限制，患者依从性好。 糖尿病干预的评估:过程评估和效果评估。 均包括年度评估、阶段性评估(3-5年进行一次)， 评估内容包括效果评估和过程评估 糖尿病干预的效果评估：被管理人群糖尿病知晓比例、糖尿病防治相关知识的知晓情况；被管理人群中糖尿病患者降糖达标和未达标比例；被管理人群心脑血管、糖尿病肾病、糖尿病神经病变、糖尿病足、视网膜病变等并发症的发生、致残和死亡信息，以及卫生经济学评价。技能115页，我们可以发现，B不良生活方式改变情况属于个体糖尿病干预的效果评估，不属于群体的，所以选B。 糖尿病干预的过程评估：年度评估（糖尿病患者建档动态管理情况、糖尿病管理开展情况、糖尿病患者转入转出执行情况、疾病预防控制机构和综合医院对社区卫生服务机构业务指导和培训情况）；阶段性评估（社区糖尿病及其危险因素流行现状了解情况、参与工作的人员对该项工作的满意情况，以及社会大众对政府部门工作的满意情况）。 糖尿病随访内容包括：了解患者病情、评估治疗情况；了解行为改变情况、调整非药物治疗方案、教会患者改变或消除行为危险因素的技能；了解患者药物就诊和药物使用情况、评价药物治疗效果、指导患者正确使用管理手册，对于治疗效果不佳的患者，应督促其到综合医院调整药物治疗方案；根据糖尿病分类管理要求，督促患者检查血糖、血压、糖化血红蛋白及相关并发症；有针对性地进行健康教育。 糖尿病患者随访管理内容应包括：了解与评估；非药物治疗；药物治疗；监测检查指标；健康教育；患者自我管理技能。 糖尿病随访方式包括：门诊随访、家庭随访、电话随访、集体随访。 糖尿病强化管理对象：已有早期并发症；自我管理能力差；血糖控制情况差；其他包括妊娠、围手术期、1型糖尿病等特殊情况；治疗上有积极要求；相对年轻且病程短。 糖尿病强化管理与常规管理基本相同，但随访频度要求至少每年达到12次。 糖尿病高危人群： 糖尿病前期(IFG和IGT) 有糖尿病家族史(双亲或同胞患有糖尿病) 肥胖和超重者(BMI&gt;24kg/m)，男性腰围&gt;90cm，女性腰围&gt;85cm 妊娠糖尿病患者或曾经分娩巨大儿(出生体重&gt;4kg)的妇女 高血压患者(血压&gt;140/90mmHg)和(或)心脑血管病变者 高密度脂蛋白胆固醇降低[≤0.9mmol/L(35mg/dl)]和(或)高甘油三酯[&gt;22mmo/L200mg/dl)]者 年龄在40岁以上，且常年身体活动不足者 有一过性类固醇诱导性糖尿病病史者 BMI&gt; 30kg/m的多囊卵巢综合征患者 严重精神病和(或)长期接受抗抑郁药物治疗者 糖尿病的治疗干预：非药物治疗（自我监测、合理膳食、增加身体活动）；药物治疗（口服降糖药、胰岛素）；强调糖尿病的健康教育。 中国2型糖尿病的控制目标包括：血糖空腹4.4-7.0，非空腹&lt;10.0；糖化血红蛋白&lt;7.0；血压&lt;130/80mmHg；男性高密度脂蛋白&gt;1.0mmol/L；女性高密度脂蛋白&gt;1.3mmol/L；低密度脂蛋白合并冠心病&lt;1.8mol/L。低密度脂蛋白未合并冠心病&lt;2.6mmol/L， 自我检测内容：体重、血糖、血压、尿中酮体和戒烟。超重患者减重速度要适当（每年5%~10%）。血压降至130/80mmHg。 合理膳食参照《中国糖尿病膳食指南》(2017) 推荐一:吃、动平衡，合理用药，控制血糖，达到或维持健康体重 推荐二:主食定量，粗细搭配，全谷物、杂豆类占1/3; 推荐三:多吃蔬菜、水果适量，种类、颜色要多样; 推荐四:常吃鱼禽，蛋类和畜肉适量，限制加工肉类; 推荐五:奶类豆类天天有，零食加餐合理选择; 推荐六:清淡饮食，足量饮水，限制饮酒; 推荐七:定时定量，细嚼慢咽;注意进餐顺序; 推荐八:注重自我管理，定期接受个体化营养指导。 糖尿病患者的运动治疗包括经常性的中高强度有氧动，尤其是抗阻力练习，以增加肌肉体积，促进血糖代谢。具体推荐中等强度有氧运动每周至少3天，每周至少150分钟，连续间断不超过2天抗阻运动每周至少2次鼓励各种肌肉力量训练，!负荷和重复数逐渐增加。同时务必限制糖尿病患者的持续久坐时间，每次不超过30分钟。 糖尿病患者应预防低血糖，早起空腹跑步易引起低血糖。 肥胖干预 肥胖干预的原则： 1.必须坚持以预防为主，从儿童、青青少年开始，从预防超重入手，并须终生坚持。 2.采取综合措施预防和控制肥胖，积极改变人们的生活方式，包括改变膳食、增加身体活动、矫正引起过度进食或活动不足的行为和习惯。 3.鼓励摄入低能量、低脂肪、适当蛋白质和碳水化合物、富含微量元素和维生素的膳食。 4.控制膳食与增强运动相结合 5.长期坚持减重计划，速度不宜过快，不可急于求成。 6.同时防治与肥胖相关的疾病、 7.树立健康体重的概念，防止为美丽而减肥的误区。 肥胖的干预措施：包括控制总能量摄取、增加身体活动量、行为疗法、必要时使用药物。 肥胖的干预步骤: 筛查和确诊肥胖患者并确定管理级别; 制订肥胖干预计划; 执行干预计划; 定时随访并进行效果评价。 肥胖干预的评估: 个体肥胖干预的评估:原因是否认清、减肥方法是否列出、是否尝试、评估中长期减肥效果等 群体肥胖干预的评估:肥胖知晓率、肥胖防治相关知识知晓率达成减肥目标的人群比例、肥胖者心脑血管发病、致残、死亡等信息。 分对于超重和肥胖的糖尿病患者，体重减少的速度要适当，每年减轻体重约5～10%为佳，不提倡短期内大幅度降低体重。 在肥胖干预过程中要注意结合管理对象的个人实际情况，综合运用多种手段管理体重。既要注重控制体重的短期目标，更应该关注管理对象养成自觉主动遵循健康生活方式的长期目标，矫正引起过量进食或身体活动不足的行为和习惯。 吸烟干预 针对群体的烟草干预措施有：拒吸第一支烟；加强健康教育，普及烟草危害知识；限制吸烟和劝阻别人吸烟；研究和推广有效的戒烟方法和戒烟产品；建立行为危险因素检测系统。 评价吸烟习惯，我们需要了解的内容包括：开始吸烟的年龄、每日吸烟量、吸烟年限。 由于戒断症状在戒烟后的头3个星期，尤其是第一周最为严重，并在随后的几个月仍可能出现。因此，通常推荐最佳的随访计划应安排在开始戒烟后的1周、1个月和3个月。 拒吸第一支烟对控制吸烟率是最为重要的。重点干预人群是青少年人群。 戒烟维持阶段包括：认真对待戒断反应；尽量避免和吸烟的人在一起；减少自己的空闲时间；积极参加体育运动和健康有益的公益活动；多做放松技术；多想自己戒烟的原因；调整膳食，适当多吃碱性食品；多向心理医师或戒烟门诊咨询。 戒断症状包括：抑郁；失眠；暴躁；焦虑；注意力难以集中；食欲或体重增加；坐立不安。 戒烟准备阶段：做出戒烟决定，牢记戒烟的原因；制定详细的戒烟计划；记录1周的吸烟行为；了解关于吸烟的医学知识；了解戒烟过程是考验一个人的毅力、信念、品质的过程。树立戒烟必定能够成功地信心；保持愉快的心情和良好的精神状态；寻求家人、朋友和同事的支持和鼓励。 在戒烟过程中，复吸是较为常见的。偶尔复吸别紧张，分析复习原因，想好对策，避免因为同样的诱因导致复吸，戒烟不是个容易的过程，需要坚定的毅力、适当的技巧和专业人员的指导。 许多研究发现，诱发青少年吸烟的主要原因是环境的影响，如烟草的易获得性、对成人吸烟的模仿、为获得社会认可的社交需求、广告或名人在文艺影视作品中的模范效应等。 吸烟成瘾行为的影响因素包括：社会环境因素、社会心理因素、文化因素、传播媒介因素、团体因素和家庭因素。 健康管理师在创建无烟环境和倡导戒烟中起着重要作用，可提供的干预至少包括询问吸烟情况、劝阻吸烟、提供信息以帮助戒烟、参与戒烟项目和安排随访防止复吸等。 烟草中已知有害的常见物质有焦油、尼古丁和一氧化碳等。 吸烟是心血管病大三大经典危险因素之一，吸烟可促进动脉粥样硬化，进而明显增加心脑血管病的发病和死亡，此外吸烟也是恶性肿瘤和慢性阻塞性肺部疾病等其他多种慢性病的危险因素，吸烟也是哮喘恶化和发作的常见诱因。 5A戒烟干预模型: ask - 询问(吸烟情况); advise - 建议(戒烟) assess - 评估(戒烟意愿); assist - 帮助(戒烟); arrange follow up - 安排随访(防止复吸) 实习-健康管理案例 测血压前受试者应至少坐位安静休息 5 分钟，30分钟内禁止吸烟或饮咖啡，排空膀胱。 老年人、糖尿病患者及出现直立性低血压情况者，应加测站立位血压。站立位血压应在卧位改为站立位后1分钟和5分钟时测量。 高血压危险因素： BMI≥24kg/m2者患高血压的危险是体重正常者的3—4倍，患糖尿病的危险是体重正常者的2—3 倍，具有两项及两项以上危险因素的患高血压及糖尿病危险是体重正常者的3—4倍。 男性腰围≥85cm、女性≥80cm者患高血压的危险为腰围低于此界线者的3.5倍，其患糖尿病的危险为腰围低于此界线者的2.5倍，其中有两项及两项以上危险因素聚集者的高血压及糖尿病患病危险为正常体重的4倍以上。 按每周至少饮酒一次为饮酒计算，我国中年男性人群饮酒率约30%~66%，女性为2%~7%。男性持续饮酒者比不饮酒者4年内高血压发生危险增加40%。每天平均饮酒&gt;3个标准杯(1个标准杯相当于12g酒精，约合360g啤酒，或100g葡萄酒，或30g白酒)，收缩压与舒张压分别平均升高3.5mmHg与2.1mmHg，且血压上升幅度随着饮酒量增加而增大。 其他危险因素，高血压的其他危险因素还有:遗传、性别、年龄、工作压力过重心理因素、高脂血症等。大量的临床资料证明高血压与遗传因素有关。如父母均患高血压其子女的高血压发生率可达46%;父母中一人患高血压，子女高血压发生率为28%;父母血压正常，子女高血压发生率仅为3%。女性在更年期以前患高血压的比例较男性略低，但更年期后则与男性患病率无明显差别甚至高于男性。 测量血压时，将袖带紧贴缚在被测者的上臂，袖带的下缘应在肘弯上 2.5cm 松紧合适，可插人1~2指为宜。]]></content>
      <categories>
        <category>考试资料,题库</category>
      </categories>
      <tags>
        <tag>健康管理师</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[健康管理师-专业技能-章节习题-01_02]]></title>
    <url>%2F05.%E5%A4%87%E8%80%83-02.%E5%81%A5%E5%BA%B7%E7%AE%A1%E7%90%86%E5%B8%88%2F%E5%81%A5%E5%BA%B7%E7%AE%A1%E7%90%86%E5%B8%88-%E4%B8%93%E4%B8%9A%E6%8A%80%E8%83%BD-%E7%AB%A0%E8%8A%82%E4%B9%A0%E9%A2%98-01_02%2F</url>
    <content type="text"><![CDATA[第一章 健康监测 测量血压时，至少测量2次，应间隔2分钟重复测量，取2次读数的平均值记录。如果2次测量的收缩压或舒张压读数相差&gt;5mmHg,则应相隔2分钟后再次测量。然后取3次读数或后2次读数相近的结果的平均数值。 BMI是评价18岁以上成人群体营养状况的常用指标。 中国成人判断超重和肥胖程度的界限值为：BMI&lt;18.5kg/m2为体重过低，18.5≤BMI&lt;24kg/m2为正常体重范围，24kg/m2≤BMI&lt;28kg/m2为超重，BMI≥28kg/m2为肥胖。因此钱女士的体重最好在正常的体重范围，也就是BMI至少控制在23kg/㎡，24为超重。答案选择C选项 在测量体重时需要注意的是：C 记录应精确到小数点后1位、D 测量误差不得超过0.1kg、F 测量体重时应穿薄衣服，赤足，全身放松。 若要了解钱女士的腹部肥胖情况，应测量腰围。 男性≥90cm,女性≥85cm患肥胖相关疾病的危险性增加。 腰围的测量方法：让受试者直立，两脚分开30-40cm,用一个没有弹性、最小刻度为1mm的软尺放在右侧腋中线胯骨上缘与第十二胁骨下缘连线的中点（通常是腰部的天然最窄部位），沿水平方向围绕腹部一周，紧贴而不压迫皮肤，在正常呼气末测量腰围的长度，读数准确至1mm。 体检信息不属于生活方式。 钱女士，45岁，从事办公室工作，BMI25.5kg/m2，不爱运动。询问钱女士的生活方式信息，不包括 A 膳食情况、B 身体活动、C 饮酒情况、D 烟草使用、E 体检信息。 选 E。 胰岛素水平需要相关检查来评测，不能自行监测。 建立居民个人健康档案，常用的调查表包括行为因素调查表；健康体检表；基本信息表格；疾病管理随访表。 个人基本信息表中包括个人基本信息；药物过敏史；家族史；既往史等。经济收入不包括在内。 心血管健康体检项目中包括肝肾功、血糖、血脂等生化检查；动态血压监测；颈动脉超声检查；头颅CT；眼底动脉检查等。 保持坐着所花费的时间，是想了解其静态习惯。 血压分为收缩压和舒张压。当心脏收缩时，动脉内的压力**最高，此时压力称为收缩压，也称高压；心脏舒张时，动脉弹性回缩产生的压力称为舒张压，也称低压**。收缩压和舒张压只差称为脉压。 血压：有动脉血压、毛细血管压和静脉压，血压是动脉血压，用毫米汞柱表示，也可用千帕（kPa）1mmHg＝0.133kpa或7.5mmHg＝1kpa, 我国成年人血压标准是：1.正常血压：收缩压小于120,舒张压小于80；2.正常高值：收缩压120-139,舒张压80-89；3.高血压：收缩压大于等于140,舒张压大于等于90。 对于同时患有高血压的糖尿病患者，血压应在患者能耐受的情况下酌情降到最佳结果是：130/80mmHg。 健康信息记录表填写过程中，如果数字填错，在原数码上打双横线，并在其上方工整填写正确数码。 年龄组别体重主要应用于0-6岁儿童。以实测体重与同年龄组的标准体重相比较，也应在均值的2个标准差之内。 某健康管理师对辖区内人群的肥胖进行健康管理，并对相关指标进行监测。开展流行病学调查时较为实用的人体测量学指标包括：A 腰围、B 皮褶厚度、C 身高组别体重、E 体质指数。 采集健康信息需要考虑的是：A 如果张先生同意接受后期的健康管理，则需要采集行为危险因素的信息、B 如果张先生有主要的慢性病如高血压、糖尿病，还需要选择疾病管理随访表、D 如果张先生只做体检，则使用健康体检表、E 家族史是指与自己有直接亲属关系的人的患病情况。 第二节 信息管理 录入员培训:在数据录入前要对录入员进行培训，掌握录入要求。录入与培训内容包括数据库结构，调查表的编码，逻辑差错的设置要求，数据库文件的保存。 信息清理的方法：主要包括三种：1.双录入法指双份独立双录入（不是双份平行双录入）；2.直接审阅数据库文件，通过专人目测检查数据库文件中的记录是否存在相同的格式，是否有空白数据；3.计算机查错：a数据库设计合理编码：在健康信息录入前的数据库设计阶段，确定每一个变量特定范围内的编码来确认其属性，以规定所要接受的合理编码b逻辑差错。 最基本的体检项目必选项目不包括：CT。心脏及血管的超声检查属于专项体检项目 1234567基础体检项目，主要分三类：1、体格检查：包括一般检查和物理检查两个部分。1）一般检查：身高、体重、腰围、臀围、血压、脉搏；2）物理检查：内科、外科、眼科检查、耳鼻咽喉科、口腔科、妇科等。能发现常见内外科、五官科或妇科等疾病的重要线索，或初步排除一些常见疾病。2、实验室检查：含血尿粪常规、肝功能、肾功能、血脂、血糖、尿酸、病原微生物感染、TCT（液基薄层细胞检测）等。3、辅助检查：包括心电图检查、X线检查、超声检查三个部分。能通过特定的检查方法查看肝、胆、胰、脾、双肾、甲状腺、乳腺、妇科等状况（有无肿块、结石等），以及评估血流血供、鉴别良恶性病变等。根据年龄、高危因素，选择专项体检项目（个性化体检项目），包括：慢性病早期风险筛查项目，包括：心脑血管病（高血压、冠心病、脑卒中、外周血管病）、糖尿病、慢阻肺（COPD）、慢性肾脏疾病、部分恶性肿瘤（食道癌、胃癌、直结肠癌、肺癌、乳腺癌、宫颈癌、前列腺癌）等。需要根据年龄、性别、职业、生活习惯或家族史、既往史、用药史等情况选择专项项目。 为调查企业职工高血压的可改变风险因素，需要采集的信息包括：B 企业食堂的配餐情况、C 职工饮酒嗜好、D 职工运动习惯、E 高血压职工个人饮食习惯。 职工性别和 高血压家族史 属于不可改变因素。 2型糖尿病筛查体检项目有空腹血糖，OGTT血糖两次，糖化血红蛋白，尿糖，尿酮体，尿微量蛋白等；高血压风险筛检体检项目有动态血压监测，血管超声，眼底检查，空腹血糖，OGTT血糖两次，糖化血红蛋白，尿糖，尿酮体，尿微量蛋白等。内脏脂肪检测要选的，案例通过BMI值计算现在超重状态，但是BMI值存在局限性，结合腰围或内脏脂肪测定能更好评估肥胖状态。 长期患有高血压或糖尿病的患者需要定期检查眼底。 健康信息收集：①收集资料前的准备；②明确调查对象；③签署知情同意书；④开始调查：填写个人基本信息时请参照项目说明：工作单位、联系人姓名、民族、血型、文化程度、药物过敏史、既往史、家族史。使用生活方式信息记录表收集信息相关内容包括：烟草使用、饮酒情况、膳食情况以及身体活动；⑤记录表的核查；⑥结束访谈，致谢；⑦资料保存。 收集严先生的健康危险因素资料，可采用的收集方法包括：通过体格检查；膳食问卷调查；体力活动问卷调查；生活方式问卷调查；家族史、个人史问卷调查、实验室检查。 收集居民健康信息基本步骤:（1）收集信息前接受调查员培训；（2）正式调查前向居民说明调查的意义；（3）受调查员培训调查开始时让居民签署知情同意书；（4）按照问卷项目说明正确填写收集信息前接；（5）完成问卷调查后及时核查收集前熟悉所要使用的健康信息记录表的每项内容；工作单位、联系人姓名、民族、血型、文化程度、药物过敏史、既往史、家族史。使用生活方式信息记录表收集信息相关内容包括：烟草使用、饮酒情况、膳食情况以及身体活动；（6）结束访谈，致谢；（7）资料保存。 社区卫生中心拟通过健康体检、问卷调查的方式收集居民相关信息，筛查出糖尿病患者。应该选择的表格包括：A 健康体检信息记录表、C 个人基本信息表、D 行为危险因素调查表、F 体格测量记录表。 饮酒量的评判标准：一标准杯等于1易拉罐啤酒；一标准杯等于一两半黄酒；一标准杯等于半两白酒；一标准杯等于三两葡萄酒；一标准杯等于一两低度白酒。 高血压患者随访内容包括：居民健康档案，个人基本信息，健康体检表，高血压患者随访服务调查表。 鉴别和核实健康信息的原则包括：检查核实数据编码是否正确；问题到编码的转换是否正确；录入是否正确。 信息录入容易产生的错误种类包括：重复录入数据;遗漏数据;读不懂手写文字;编码错误;错误的答案。 健康信息的保存：1.健康信息的保存包括计算录入的数据库文件的存档和调查问卷文件的保管和存放；2.数据库文件保存数据库在录入和清理完成后要进行双备份，分别保存在不同的计算机相应文件夹里；3.调查问卷的保存要有指定专职人员进行管理完整安全存放方便查阅。 信息清理的方法主要包括三种：1.双录入法；2.直接审阅数据库文件，通过专人目测检查数据库文件中的记录是否存在相同的格式，是否有空白数据；3.计算机查错：a数据库设计合理编码、b逻辑差错。 第二章 健康风险评估和分析第一节 健康风险识别 健康风险评估原理：是通过广大基础的流行病学数据与个人数据比较以推测个人患病或死亡风险。 健康风险评估定义：是对个人的健康状况及未来患病和（或）死亡危险性的量化估计。评估对象是个人不是人群。 健康风险评估的研究看起来健康而且没有任何疾病症状的人，其可能具有未来发生某种疾病或导致死亡的潜在风险。 风险是人们生活中经常经历的一种状况，使用“风险”来描述不确定的状况。 健康风险评估的研究目的：研究看起来健康而且没有任何疾病症状的人，其可能具有未来发生某种疾病或导致死亡的潜在风险；研究如何能将导致风险危险因素识别出来；研究如何消灭或控制这些能够预防或减弱疾病的致病因素，达到预防惊的或延迟疾病发生的目的。在于估计特定疾病发生的可能性，而不在于做出明确诊断。也不能治疗疾病。健康风险评估的内容一般不包括疾病并发症的评估。 健康风险评估的步骤中一般包括：A 信息录入、B 报告解读、个人健康信息采集、D 进行有关医学检查。 跟踪指导不属于健康风险评估。 健康风险评估是一种分析方法或工具，用于描述、和估计某一个体未来可能发生的某种特定疾病或因为某种特定疾病导致死亡的可能性。这种分析的目的在于估计特定事件发生的可能性，而不在于做出明确诊断。 常见健康风险评估报告：个人健康信息汇总报告；疾病风险评估报告；健康促进与指导信息。 个人报告一般包括健康风险评估的结果和健康教育信息。评估结果是健康风险评估的主要内容，应包含个人患病风险、人群风险，以及个人可降低风险。 个人健康风险评估报告一般包括：健康风险评估的结果和健康教育信息。 不良生活方式和行为对健康的直接或间接影响巨大。 相对危险性表示的具有某些共同暴露因素的个体与人群平均水平相比，危险度的升高或降低。 相对危险性表示的是与人群平均水平相比，危险度的升高或降低。 理想危险度表示的是健康风险降低的空间，将所有先兆因素修正到目标水平计算出来的危险度。 健康风险的表示方法：死亡率和发病率、危险度、健康年龄、评估分值。 疾病的预测模型中比较成熟、准确的是对常见慢性病的预测，如缺血性心脏病的预测，糖尿病的预测和脑卒中的预测等，有很大的参考价值。 当一个人的膳食由动物性食物为主转为以植物性食物为主的时候，会发生的变化为：其胆固醇水平会大幅度下降。膳食胆固醇的来源仅限于肉、蛋或乳制品等动物性食品，而甘油三酯的来源既可以是动物性食物，也可以是植物油。 问卷是健康风险评估进行信息收集的一个重要手段，通常所需的信息包括：：①生理、生化数据，如身高、体重、血压、血脂等；②生活方式数据，如吸烟、膳食与运动习惯等；③个人或家族健康史；④其他危险因素，如精神压力；⑤态度和知识方面的信息。 生活方式相关的危险因素归纳起来主要有：生活方式/行为因素、环境因素、生物学因素、健康服务因素四大类。 健康风险评估的基本模块包括：问卷（问卷内容包含体检结果）、危险度计算、评估报告。 问卷：包括生理生化指标、生活方式数据、个人或家族健康史、其他危险因素、态度和知识方面的信息。 危险度计算（相对危险度、绝对危险度）。 评估报告：包括个人报告和人群报告，评估结果是健康风险评估报告的主要内容。 第二节 健康风险分析 健康风险评估风险计算最常用的两种统计方法是:单因素加权法 和 多因素模型法。 健康年龄是指具有相同评估总分值的男性或女性人群的平均年龄。为得到健康年龄，受评估者的评估危险度要和同年龄同性别人群的平均危险度相比较。如果某个人的评估危险度与人群平均危险度相等，则他的健康年龄就是其自然年龄如果某人的评估危险度高于人群平均危险度，则他的健康年龄大于其自然年龄;反之，若评估危险度低于人群平均危险度，则其健康年龄小于自然年龄。理想健康年龄表示的是该个体可以修正的危险度与人群平均危险度之间的差距。 风险等级（相对危险性）：相对危险性反映相对于一般人群危险性的增减量。 膳食处方：根据被管理对象的个人基本信息、疾病史、体格信息及医学指标的不同，针对性地为其制订个性化膳食处方，并提供特定能量级别和膳食营养特点的食谱以及食物交换。食谱并非菜谱，概念要明确。 对高度危险的服务对象的随访时间一般为每三个月一次，中度危险的服务对象的随访时间为每六个月一次，低度危险服务对象的随访时间为每年一次。 康风险评估与临床诊断的目的不同之处在于：临床诊断的最终目的是为了对症治疗，而健康评估的最终目的是根据评估结果进行健康干预。 收集健康信息是健康风险评估的第一步，同样是与评估报告的质量最相关的因素。 健康风险评估中最主要的局限性是： 不能作为诊断的依据。 行为危险因素包括：不合理饮食、缺乏体育锻炼、吸烟、酗酒和滥用药物等。 可改变的危险因素包括：心理不健康、不良生活方式、腰围超标、血脂异常、血糖/血压/血尿酸偏高等。 健康相关危险因素也称健康危险因素，是指机体内外存在的使疾病发生和死亡概率增加的诱发因素，包括个人特征、环境因素、生理参数、疾病或亚临床疾病状态等。 ①个人特征包括不良的行为(如吸烟、身体运动不足、膳食不平衡、酗酒、吸毒、迷信、破坏生物节律等) :疾病家族史、职业等; ②环境因素包括暴露于不良的生活环境和生产环境等; ③生理参数包括有关实验室检查结果(如血脂素乱)、体型测量(如超重)和其他资料(如心电图异常)等。 4疾病或亚临床疾病状态 体检中常用的健康风险评估指标（掌握） 体重与体质指数 (BMI)。超重(肥胖)的人罹患高血压、高血胆固醇或其他脂质代谢素乱2型糖尿病、心脏病、脑卒中和某些癌症的危险性较大。计算方法:BMI=体重(Kg)÷身高(m)2，单位是Kg/m2。正常范围:18.5≤BMI&lt;24; 24≤BMI&lt;28为超重 ;BMI&gt;28为肥胖 血压。中国目前有将近2亿高血压患者。高血压是一种严重疾病，它会导致脑卒中、心脏病、肾衰和其他疾病, 高血压:收缩压&gt;140mmHg和(或)舒张压&gt;90mmHg。 总胆固醇（TC），是 LDL、HDL、VLDL 的总和。 高胆固醇与心脏病有高度联系。总胆固醇的正常值是 3-5.2mmo/L，超过为胆固醇高，胆固醇高的危害主要有：心脏病发病率增加。 高密度脂蛋白胆固醇（HDL-C）和低密度脂蛋白胆固醇（LDL-C） 。 HDL-C 正常值 0.8-1.8mmol/L，是运输内源性胆固醇到肝脏处理的载体，故有抗动脉粥样硬化作用。HDL-C 越低，动脉粥样硬化的危险性越大。降低见于急性感染、糖尿病、慢性肾功能衰竭、肾病综合征。LDL-C 正常值 1.9-3.3mmol/L，向组织及细胞内运送胆固醇，直接导致动脉粥样硬化，为判断心脏病的良好指标。升高见于高脂饮食、运动少、年龄增大、遗传性高脂蛋白血症、甲减、肾病综合征、慢性肾功能衰竭、梗阻性黄疸、精神紧张等；降低见于β- 脂蛋白血症、甲亢、消化吸收不良、肝硬化、恶性肿瘤。 总胆固醇 / HDL 之比。 用于衡量发生心血管疾病的危险性， 比值越高，说明罹患心血管疾病的危险性越大。 甘油三酯 。 正常值 0.5-1.7mmol/L，甘油三酯水平高的人罹患心脏病的危险性就大。 激素替代疗法。 这种药物治疗方法是要为已绝经妇女补充不再能自行合成的雌激素 左心室肥大（LVH）。 长期高血压容易导致的后果。 脂蛋白 (a)。 心脏病和脑卒中的危险因素。 前列腺抗原（ATG,PSA）。 前列腺癌的筛查指标。 前列腺肥大。 对老年男性来说，随着年龄的上升，前列腺也在逐步增大。对某些人来说，若是前列腺增长过快，以致于引起排尿困难，就称之为前列腺良性肥大。导致进行性的排尿困难。]]></content>
      <categories>
        <category>考试资料,题库</category>
      </categories>
      <tags>
        <tag>健康管理师</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[健康管理师-基础知识-章节习题-13_14]]></title>
    <url>%2F05.%E5%A4%87%E8%80%83-02.%E5%81%A5%E5%BA%B7%E7%AE%A1%E7%90%86%E5%B8%88%2F%E5%81%A5%E5%BA%B7%E7%AE%A1%E7%90%86%E5%B8%88-%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86-%E7%AB%A0%E8%8A%82%E4%B9%A0%E9%A2%98-13_16%2F</url>
    <content type="text"><![CDATA[第十三、十四、十五、十六章：医学伦理与职业道德、健康保险与健康管理、健康管理服务营销、健康管理法律、法规 分值占比：2～3% 难度：★ 这几个章节所涉及的内容对于健康管理工作来讲学科专业性程度会有所下降，不会着重讲健康管理方面的基础知识和概念，但是在开展健康管理服务过程中这些内容也是最基本的知识储备，所以他们的分值权重就会有所下降，可能每年就考1~2个小题。 第十三章 医学伦理与职业道德第一节、第二节 医学伦理与健康管理伦理的定义和基本原则、健康管理伦理规范及权利、义务 医学伦理学的基本原则是防病治病，救死扶伤，实行人道主义，全心全意为人民健康服务。 健康管理的伦理关系是一种双向的、特定的、动态的关系。 知情同意的四个要素是:信息的告知、信息的理解、同意的能力、自由地同意。 在基本伦理原则中，实行知情同意的前提是同意的能力。 健康管理伦理的基本原则：①以人为本，以健康为中心的原则 ②公平、合理的原则 ③保守秘密的原则 ④有利和主体原则 ⑤优质服务的原则 有利和主体原则:让健康管理“花钱少，获益大”；维护服务对象利益，并使之利益最大化；健康为主，效果明显；争取服务对象的配合，发挥服务对象的主体作用。 保守秘密的原则:保守秘密是医务人员对患者应景的责任，健康管理工作中也要坚守这一原则。 优质服务原则中包括了解、发现服务对象健康需求等。 服务对象在健康管理中的权利：①合理的、平等的健康保健权 ②知晓健康管理相关措施及进程的权利 ③保护自身正当利益的权利 ④要求保护秘密和隐私的权利 ⑤要求赔偿健康损害的权利。 职业道德的基本要求：①爱岗敬业、②诚实守信、③办事公道、④服务群众、⑤奉献社会。 生命伦理学的三大基本原则是尊重人、不伤害、有利、公正，其中的“尊重”包括:：以人为本，尊重人的自主性（一个人按自己的计划决定自己行为的一种理性能力）、遵守知情同意的原则、对自主性差的个人可能简要额外的保护（监护人、家庭、社区）。 健康管理师应始终严格遵守保密原则，具体措施如下:(1)健康管理师有责任向个人或群体说明健康管理工作的相关保密原则，以及应用这一原则时的限度。 (2) 在健康管理工作中,一旦发现个人或群体有危害自身或他人的情况，必须采取必要的措施,防止意外事件发生(必要时应通知有关部门或家属),应将有关保密的信息暴露限制在最低范围之内。(3)健康管理工作中的有关信息，包括个案记录、检查资料、信件、录音、录像和其他资料，均属专业信息,应在严格保密的情况下进行保存,不|得泄露。(4)健康管理师只有在个体同意的情况下才能对工作或危险因素干预过程进行录音、录像。在因专业需要进行案例讨论,或采用案例进行教学、科研、写作等工作时应隐去可能会据此辨认出个体的有关信息。 第三节 健康管理师职业道德 职业道德是用来调整职业个人、职业主体和社会成员之间关系的行为准则和行为规范。 健康管理师在日常工作中贯彻以人为本，以健康为中心的原则，：①了解、热爱服务对象 ②尊重服务对象 ③面向社区各个层次提供不同服务 ④正确判断，及时处理服务对象的相关健康问题 《中华人民共和国公民道德建设实施纲要》中明确指出：“要大力提倡以爱岗敬业、诚实守信、办事公道、服务群众、奉献社会为主要内容的职业道德，鼓励人们在工作中做一个好建设者。” 当继续保密涉及到对公众福利的危害时、继续保密会危及确定的第三方生命或重大健康风险时医生可以不保密。 职业道德是一切符合职业要求的心理意识、行为准则、行为规范的总和。 职业道德体现的社会关系三大要素是：职责、职权、利益。 健康管理消费行为特征包括：习惯型、经济型、理智型、盲目型、躲闪型。 第十四章-健康保险与健康管理第一节、第二节 健康保险概述、健康管理在健康保险中的应用 商业健康保险风险控制的传统方法如下：①条款设计时的风险控制;②核保时的风险控制;③理赔时的风险控制;④对风险转移的方法一再保险 我国2006年出台的《健康保险管理办法》将健康保险分为疾病保险、医疗保险、失能收入保险和护理保险四大类，针对不同的需要和损失进行给付和补偿。 健康保险行业中健康管理的分类包括健康指导和诊疗干预两类。健康指导类包括：健康咨询和健康维护。其中诊疗干预类包括：就诊服务和诊疗保障。 当被保险人更清楚地认识到保险对他利弊大小而做出选择时产生的一种行为称为逆向选择。 健康保险风险控制的新方法包括：对医疗服务过程的控制；医疗服务补偿方式；无赔款优待和其他利润分享措施；健康管理机制；管理式医疗。 健康风险控制中的专业风险控制环节包括：目标设定，风险识别，风险评价，识别和评价可选方案，选择方案，实施方案，和监督管理等环节。 健康保险产品设计中直接关系到最终保险产品质量的最重要内容是保险责任。 健康保险的概念：以人的身体健康为目标，是对因疾病或意外伤害所发生的医疗费用或因疾病或意外失能所致收入损失的保险，同时健康保险还包括因年老、疾病、或伤残需要长期护理而给予经济补偿的保险。 保险消费意识与健康保险发展的关联极为显著。保险消费意识反映了人们对于保险作用的认知程度，保险消费意识越强，对于健康保险产品的需求就越大，市场潜力就越大。 健康保险需求的影响因素包括：保险产品购买力、保险的消费意识、医疗费用的增长、人口老龄化。 疾病保险是指以约定疾病的发生为给付保险金条件的人身保险。 对社会基本医疗保险费用补偿不足部分进行二次补偿的健康保险是补充型医疗保险。 等价和公平是健康保险费率制定的两大基本原则。 按照保险性质不同，健康保险可分为社会医疗保险和商业健康保险。 护理保险特点包括：①主要形式是长期护理保险以50岁以上中老年的人为主要群体；②护理保险需要制定理赔判别标准表；③长期护理保险具有多种形式的保险责任；④长期护理保险的受益人可以享受税收的优惠待遇。 健康保险是以人的身体健康为目标，是对因疾病或意外伤害所发生的医疗费用或因疾病或意外失能所致收入损失的保险，同时健康保险还包括因年老、疾病、或伤残需要长期护理而给予经济补偿的保险。 社会医疗保险对医疗机构的费用支付的最大特点是第三方支付。 社会医疗保险是国家实施的基本医疗保障制度。 健康保险的基本原理是保费收入等于赔款支出。 健康保险的风险除了风险理论上的一般特征：如风险存在的客观性、普遍性、社会性、可变性等外，还具有不确定性、多发性、长期性。 健康保险的风险特点：①不确定性；②多发性；③长期性。 健康保险的内在风险因素主要是指因为保险公司企业经理管理不规范、不严格所带来的的风险。 健康险经营管理的基础工作之一是精算工作，主要分为费率制定、赔付率计算和准备金提取三大部分，其中主要任务就是费率制定。 医疗保险有如下特点：①医疗保险的保险金的给付条件是以医疗行为的发生或医疗费用支出作为依据，与疾病诊断不直接相关；②医疗保险产品具有不同的分类方法，按照保险金的给付性质可分为费用补偿型医疗保险和定额给付型医疗保险；③医疗保险风险因素多，经营管理复杂。 失能收入保险主要是满足被保险人因暂时或永久丧失工作能力后的基本生活需求，而不是承诺保证以往的生活方式，其中赔付比例的设定是为了控制到的风险。失能收入保险的给付期可长可短，短期为1~5年，长期的通常给付至被保险人65周岁或70周岁。 失能收入损失保险的特点包括：①界定的核心包括两点，一为工作能力丧失，二为失能导致收入损失；②主要是满足被保险人因暂时或永久丧失工作能力后的基本生活需求，而不是承诺保证以往的生活方式；③给付期可长可短；④保险合同中通常设有免责期条款；⑤在实际操作中，最大的困难和风险是判断被保险人是否持续满足赔付条件，并在被保险人恢复工作能力的情况下及时终止保险金给付；⑥特殊条款，保险合同中常常提供保费豁免。 我国健康保险发展形势是：我国健康保险市场已经初步形成了比较完善的市场体系。 道德风险和逆向选择始终是困扰健康保险的两大难题。 健康保险行业引入健康管理服务与技术是为了从根本上降低赔付风险、保障经营效益。 在健康保险行业中应用健康管理的其主要目的是：提供健康服务与控制诊疗风险，因此可以将其分为健康指导和诊疗干预两类。 健康保险是以经营健康风险为核心内容的金融服务行业。 我国健康保险发展面临的问题包括：低价竞争、非专业化发展、产品期限短、缺乏合作机制、高赔付率。 风险管理的方法包括损失控制;风险补偿;风险规避;风险转移。 健康保险对健康管理的意义有;1.健康保险促进健康管理的资源配置与整合2.健康保险可作为健康管理的战略性市场渠道3.健康保险能够加强健康管理的良好认同度。P311 健康保险与健康管理的合作可分为三种不同模式：服务完全外包模式、自行提供服务模式、共同投资模式。 健康保险发展影响的因素包括：健康保险信息的非对称性（道德风险、逆向选择）、健康保险需求的特殊性、疾病风险的高度相关性。 健康管理的具体内容包括定期体检、健身计划、预约专家、设立健康热线、开办保健知识讲座、编印健康知识手册等。 疾病保险的特点包括：①保险金的给付条件只依据疾病诊断结果，不与治疗行为的发生或医疗费用相关；②疾病保险的主要产品类型是重大疾病保险；③为了防止被保险人带病投保，降低逆选择的风险，疾病保险合同通常设有等待期。 第十五章 健康管理服务营销第一节、第二节、第三节、第四节、第五节 健康管理服务概述、健康管理服务消费行为分析、健康管理服务营销、健康管理相关产品、健康管理服务案例 健康管理服务营销分析评价需求的方法包括：①医院体检中心：通过体检后健康风险评估来细分客户需求；②企业工作场所：通过健康体检、健康评估、人群风险分组确定干预对象等方式来导入目标管理人群。 健康管理服务营销过程包括：确定目标客户、分析评价需求、选择和利用资源、确定产品价值、促进客户购买、实现客户价值。 健康管理服务特性包括：无形性、不可分割性、不稳定性、易逝性、客户的满意标准、客户的参与程度。 健康监测设备中的一般检查监测设备包括：身高体重仪、血压计、血糖仪、血氧仪、计步器、体温计、人体脂肪分析仪、BMI监测仪、皮褶计、胎心监测仪、心率监测仪、肌肉测定仪、脊柱电子测量仪等。超声骨密度仪属于特殊检查设备。 根据不同需求的客户人群提供不同的营销方案，资源配置，称为差异营销。 健康管理师所提供服务的每一步都会影响客户对服务质量的总体印象，这被称作“瞬间真实”。 健康管理师协调服务的内容包括：调整处方、协调转诊、与全科医生协调服务。 市场营销中，取得市场长期成功的关键是提供使顾客满意的服务。 消费者的购买决策过程包括：①识别需求；②搜索信息；③备选方案评估；④选择购买；⑤购买后评估。 健康管理的行业本质就是“管理”两字。 PDCA循环的含义是将质量管理分为四个阶段，即计划（Plan）、执行（Do）、检查（Check）、处理（Action）。 健康管理需求特征包括：需求的被动性、需求的不确定性、需求的差异性、需求的发展性、需求的外部关联性、需求的广泛性、需求的超前性与滞后性、需求的重复性。 直接影响健康管理服务质量的因素包括：服务提供者的可靠度（技术、产品）、对客户的敏感度（找到客户的真正需求）以及对客户的承诺，敬业程度以及整体外观、环境的专业性等表明了健康服务的质量。 区别竞争对手可通过：服务地点、价格、核心技术、服务治疗等。 健康管理服务主要包括三个层次：提高健康认知水平、生活方式的改变、建立支持性环境。 第十六章 健康管理相关法律、法规知识 劳动合同期限三个月以上不满一年的，试用期不得超过一个月。 劳动合同期限一年以上不满三年的，试用期不得超过两个月。 医师在执业活动中履行下列义务：（1）遵守法律法规，遵守技术操作规范；(2)树立敬业精神，遵守职业道德，履行医师职责，尽职尽责为患者服务；（3）关心、爱护、尊重患者，保护患者隐私；（4）努力钻研业务，更新知识，提高专业技术水平；（5）宣传卫生保健知识，对患者进行健康教育。 劳动者有下列情形之一的，用人单位可以解除劳动合同：（1）在试用期间被证明不符合录用条件的；（2）严重违反用人单位的规章制度的；（3）严重失职，营私舞弊，给用人单位造成重大损害的；（4）劳动者同时与其他用人单位建立劳动关系，对完成本单位的工作任务造成严重影响，或者经用人单位提出，拒不改正的；（5）以欺诈、胁迫的手段或者乘人之危，使对方在违背真实意思的情况下订立或者变更劳动合同的情形致使劳动合同无效的；（6）被依法追究刑事责任的。 劳动合同应当具备以下条款：（1）为用人单位的名称、住所和法定代表人或者主要负责人；(2)劳动者的姓名、住址和居民身份证或其他有效身份证件号码;(3)劳动合同期限；（4）工作内容和工作地点；（5）工作时间和休息休假；（6）劳动报酬；（7）社会保险；（8）劳动保护、劳动条件和职业危害防护；（9）法律、法规规定应当纳入劳动合同的其他事项。 患有痢疾、伤寒病毒性肝炎等消化道传染病的人员，以及患有活动性肺结核、化脓性或者渗出性皮肤病等有碍食品安全的疾病的人员,不得从事接触直接入口食品的工作。食品生产经营人员每年应当进行健康检查，取得健康证明后方可参加工作。 医师资格考试由省级以上人民政府卫生行政部门组织实施。 医师考试内容和办法由国务院卫生行政部门另行制定。 裁减人员时，应当优先留用下列人员：①与本单位订立较长期限的固定期限劳动合同的；②与本单位订立无固定期限劳动合同的；③家庭无其他就业人员，有需要抚养的老人或者未成年人的。 劳动合同分为固定期限劳动合同、无固定期限劳动合同和以完成一定工作任务为期限的劳动合同。 无固定期限劳动合同签订条件包括：①劳动者在该用人单位连续工作满十年的；②用人单位初次实行劳动合同制度或者国有企业改制重新订立劳动合同时，劳动者在该用人单位连续工作满十年且距法定退休年龄不足十年的；③连续订立两次固定期限劳动合同，且劳动者没有本法第三十九条和第四十条第一项、第二项规定的情形，续订劳动合同的。 消费者和经营者发生消费者权益争议的，可以通过下列途径解决：与经营者协商和解；请求消费者协会调解；向有关行政部门申诉；根据与经营者达成的仲裁协议提请仲裁机构仲裁；向人民法院提起诉讼。 《中华人民共和国劳动合同法》的立法宗旨：完善劳动合同制度，明确劳动合同双方当事人的权利和义务，保护劳动者的合法权益，构建和发展和谐稳定的劳动关系。 下列劳动合同无效或部分无效：（1）以欺诈、胁迫的手段或者乘人之危，使对方在违背真实意思的情况下订立或者变更劳动合同的；（2）用人单位免除自己的法定责任、排除劳动者的权利的；（3）违反法律、行政法规强制性规定的。 劳动者有以下情形之一的，用人单位不得解除劳动合同：（1）从事接触职业病危害作业的劳动者未进行离岗前职业健康检查，或者疑似职业病患者在诊断或者医学观察期间的；（2）在本单位患患职业病或者因公负伤并被确认丧失或者部分丧失劳动能力的；（3）患病或者非因工负伤，在规定的医疗期内的；（4）女职工在孕期、产期、哺乳期的；（5）在本单位连续工作满十五年，且距法定退休年龄不足五年的；（6）法律、行政法规规定的其他情形。 有下列情形之一的，劳动合同终止:(一)劳动合同期满的;(二)劳动者开始依法享受基本养老保险待遇的;(三)劳动者死亡，或者被人民法院宣告死亡或者宣告失踪的;(四)用人单位被依法宣告破产的;(五)用单位被吊销营业执照.责令关闭、撒销或者用人单位决定提前解散(六)法律、行政法规规定的其他情形。 消费者享有的权利正确的是：（1）消费者在购买商品和接受服务时享有人身、财产安全不受损害的权利；(2) 消费者享有知悉其购买、使用的商品或者接受的服务的真实情况的权利；（3）消费者享有自主选择商品或者服务的权利；（4）消费者享有公平交易的权利；（5）消费者因购买、使用商品或者接受服务受到人身、财产损害的，享有依法获得赔偿的权利；（6）消费者享有依法成立维护自身合法权益的社会团体的权利；（7）消费者享有获得有关消费和消费者权益保护方面的知识的权利；（8）消费者在购买、使用商品和接受服务时，享有其人格尊严、民族风俗习惯得到尊重的权利；（9）消费者享有对商品和服务以及保护消费者权益工作进行监督的权利。]]></content>
      <categories>
        <category>考试资料,题库</category>
      </categories>
      <tags>
        <tag>健康管理师</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[健康管理师-基础知识-章节习题-11_12]]></title>
    <url>%2F05.%E5%A4%87%E8%80%83-02.%E5%81%A5%E5%BA%B7%E7%AE%A1%E7%90%86%E5%B8%88%2F%E5%81%A5%E5%BA%B7%E7%AE%A1%E7%90%86%E5%B8%88-%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86-%E7%AB%A0%E8%8A%82%E4%B9%A0%E9%A2%98-11_12%2F</url>
    <content type="text"><![CDATA[第十一章 康复医学基础知识第一节 现代康复医学的兴起与发展 1984年“七五”重点工程期间，国家卫生部陆续在河北省立医院、北京小汤山、辽宁汤岗子、广东从化等地设立了4个康复医学试点，逐步开始了现代化康复服务的尝试。 1984年，国家“七五”重点工程—中国康复研究中心开工建设，标志着现代康复医学正式引入中国。 三级康复服务网络正在逐步形成：早期康复、后期康复、社区康复。 第二节 康复医学基本概念 根据工作内容和服务方式不同，康复可分为五个方面：医学康复、教育康复、职业康复、社会康复和康复工程。 医学康复是指通过应用医学的方法和手段帮助病伤残者实现全面康复的目标，包括药物、手术、物理疗法等治疗手段，是康复的首要内容和基础。 目前康复医学采取的模式是生物-心理-社会康复模式。 世界卫生组织将机体无器质性病变，但是有一些功能改变的状态称为“第三状态”，我国称为“亚健康状态”。 康复医学旨在预防和改善服务对象的功能障碍，提高生活质量，回归家庭、社会、学习、工作。 不包括治愈疾病。 第三节、第四节 康复医学的基本内容、康复治疗技术 康复医学的工作内容包括康复预防、康复功能评定和康复治疗三部分。 中国传统康复疗法：整理、发掘、研究、总结用中国传统医学的伦理和方法解决康复医学中所面临问题的医学方法，包括按摩、太极拳、针灸、气功、推拿等。 1981年世界卫生组织（WHO）对康复的最新定义是：综合地、协调地应用医学的、教育的、社会的、职业的各种方法，使病、伤、残者（包括先天性残疾）已经丧失的功能尽快地、最大可能地得到恢复和重建，使他们在体格上、精神上、社会上和经济上的能力得到尽可能的恢复，重新走向生活、工作和社会。 听力残疾是指因各种原因导致双耳不同程度的永久性听力障碍，听不到或听不清周围环境声及言语声，以致影响其日常生活和社会参与。 残疾三级预防的措施包括：防止残疾变成残疾障碍或降低残疾影响的各种措施，如通过各种康复治疗、安装假肢、训练等，对残疾者直接干预，以改善或提高躯体和心理功能；通过职业咨询和训练，提高生活自理能力，恢复或增强工作和学习能力；通过改变雇主和社会公众的态度和行为、保险等，促使残疾者重返家庭和社会。 康复医学的内容包括：康复基础学、康复评定学、康复治疗学、康复临床学和社区康复学等。 康复治疗由多学科的专业人员组成康复治疗小组共同进行。由康复医师召集物理治疗师、作业治疗师、言语治疗师、康复护师、心理医生、假肢及矫形器技师、社会工作者、营养师以及相关科室医生等出席康复评定会，确认患者的功能障碍、制定康复目标并制定、修正系统康复计划等。 康复治疗是康复医学日常工作的基本内容，最常用的康复治疗手段有：物理治疗、作业疗法、言语治疗、心理治疗、康复护理、康复工程、中国传统康复疗法、社会工作。 运动疗法是物理疗法的核心部分，主要是通过运动（力学方法）对身体的功能障碍和功能低下进行预防、改善和功能恢复的治疗方法。 作业治疗是针对病、伤、残者的功能障碍，指导患者参与选择性、功能性活动的治疗方法。 康复治疗是为帮助患者获得知识和技能，最大限度获得躯体、精神和社会功能的一个主动的、动态的过程。 各类残疾按残疾程度分为四级残疾，残疾一级、残疾二级、残疾三级和残疾四级。残疾一级为极重度残疾，残疾二级为重度残疾，残疾三级为中度残疾，残疾四级为轻度残疾。 2001年世界卫生组织修订通过了“国际功能、残疾、健康分类（ICF）”。用身体功能、个体功能、社会功能来表示健康功能状态。 ICF包括三个关键部分：①身体功能和结构；②活动；③参与。 根据残疾的性质和特点可以分为：视力残疾、听力残疾、言语残疾、肢体残疾、智力残疾、精神残疾和多重残疾。 根据工作内容和服务方式不同，康复可分为五个方面：医学康复、教育康复、职业康复、社会康复和康复工程。 第十二章：健康信息学 3～5% ★★ 健康信息学所占比重为3%~5%，所占的比重也较低，考题比较简单。 第一节 信息学概述 信息除具有物质的属性（如客观性、普遍性、有用性）外，还具有：①可识别性；②可储存性；③可扩充性；④可共享性；⑤可传递性；⑥可转换性；⑦可再生性；⑧时效性和时滞性。 信息一般有4种形态：数据、文本、声音、图像。 数据是载荷或记录信息的按一定规则排列组合的物理符号。 定量数据：反映事物数量特征的数据，如长度、面积、体积等几何量或重量、速度等物理量。 信息论的创始人香农认为：“信息是能够用来消除不确定性的东西”。邓宇等人2002年提出的“信息”概念与定义：“信息是事物现象及其属性标识的合集”。在管理信息系统领域，一种被普遍接受的观点认为，“信息是经过加工过的数据，它对接收者有用，对决策或行为有现实的、潜在的价值”。 但是信息不一定正确。 信息从不同角度可分为：①战略信息、战术信息和作业信息；②管理信息、社会信息、科技信息和军事信息；③一次信息、二次信息和三次信息等；④数字信息、图像信息和声音信息等；⑤语法信息、语义信息和语用信息；⑥实在信息、先验信息和实得信息；⑦有用信息、辅助信息、无用信息和有害信息；⑧前馈信息和反馈信息。 按信息的加工顺序可将信息分为一次信息、二次信息和三系信息。 按信息的反映形式可将信息分为数字信息、图像信息和声音信息。 按信息的作用可将信息分为有用信息、辅助信息、无用信息和有害信息。 数据的种类很多，按性质分为：定位数据、定性数据、定量数据、定时数据。 按表现形式可将数据分为：数字数据、模拟数据。模拟数据又分为图形数据、符号数据、文字数据、图像数据等。 第二节 健康信息收集、分析与利用 健康信息包括：健康相关信息、疾病相关信息、健康素质能力、健康寿命等信息。 信息收集原则包括：计划性、系统性、针对性、及时性、完整性、真实性。 健康管理者通过对群体健康信息科学、客观的分析、汇总和评估，作出社区诊断，分析主要健康问题危险因素和目标人群，为制订干预计划提供依据，为企业、机关、团体提供群体健康的指导建议和相关的健康需求参考资料，通过讲座、咨询、个别重点对象的针对性指导、服务等方式，切实落实有效的干预措施，达到最大的防治疾病和健康改善的效果。 电子病历（计算机化病案记录），是居民个人在医疗机构历次就诊过程中产生和被记录的完整、详细的临床信息资源，它是用电子设备保存、管理、传输和重现的数字化的患者的医疗记录，取代手写纸张病历，它的内容包括纸张病历的所有信息，具有超越纸张病历的机能。 个人健康档案是指一个人从出生到死亡的整个过程中，其健康状况的发展变化情况以及所接受的各项卫生服务记录的总和。 健康档案是以个人健康为核心，动态测量和收集生命全过程的各种健康相关信息，满足居民个人和健康管理需要建立的健康信息资源库；是社区顺利开展各项卫生保健工作，满足社区居民的预防、医疗、保健、康复、健康教育、生育指导等“六位一体”的卫生服务需求及提供经济、有效、综合、连续的基层卫生服务的重要保证。 电子健康档案是以电子化方式管理的有关全人全程健康状态和医疗保健行为的信息档案。 高级应用：由于计算机网络技术的发展，可以把健康档案中的信息通过互联网来传送，从而达到远程会诊的目的，建设以居民健康档案、电子病历为基础的区域卫生信息平台，实现健康信息资源共享。 初级应用：可首先利用计算机实现一些简单的信息管理。即利用计算机管理软件，对个人、家庭、社区健康档案中的各种文字资料进行记录、查询、检索。 信息收集的方法包括：访谈法、实地观察法、问卷法。 访谈法是以谈话为主要方式了解某人、某事、某种行为或态度的一种调查方法。 实地观察法是由调查员到现场对观察对象进行直接观察、检查、测量或技术而取得资料。 与健康管理相关的卫生服务记录表单主要有以下六个部分：基本信息、儿童保健、妇女保健、疾病控制、疾病管理、医疗服务。 信息收集原则包括：计划性、系统性、针对性、及时性、完整性、真实性。 信息整理一般可分为三步：①进行信息分类；②进行资料汇编；③进行资料分析。 健康信息包括：健康相关信息、疾病相关信息、健康素质能力、健康寿命等。p273 健康相关信息包括：①生理、心理社会适应；②营养与环境；③运动与生活方式。 区域卫生信息系统包括电子政务、医保互通、社区服务、网络转诊、居民健康档案、远程医疗、网络健康教育与咨询、农村合作医疗等。 个人健康信息可用于分析、评价其健康状况和健康危险因素，据此，制订有针对性的个人健康管理计划，提出具体的健康改善目标和健康管理指导方案，并针对健康危险因素的发展趋势进行相应的生活行为方式干预指导。 第三节 居民健康档案概述 建立健康档案的基本要求包括：①资料的真实性；②资料的科学性；③资料的完整性；④资料的连续性；⑤资料的可用性。 在我国，一般将居民健康档案分成三个部分，即个人健康档案、家庭健康档案和社区健康档案。 社区居民健康状况包括：社区的人口学资料、社区居民健康问题的分布及严重程度、社区居民健康危险因素评估、社区人群的发病率、患病率及疾病构成、病死率及疾病率、社区疾病谱及死因谱等。 建立健康档案的主体是乡镇卫生院、村卫生室和社区卫生服务中心。 社区动员潜力是指社区内可被动员起来参与和支持社区居民健康服务活动的人力、物力和财力资源。 社区卫生服务资源包括：社区卫生服务机构和社区卫生人力资源状况两部分。社区卫生人力资源指在社区中各类医务人员及卫生相关人员的数量、年龄结构、职称结构和专业结构等。 家系图是以绘图的方式来描述家庭结构、医疗史、家庭成员间的遗传联系、家庭关系及家庭重要事件等。 SOAP记录形式中的四个字母分别代表不同的含义：S(subjective data)代表服务对象主观资料；O(objective data)代表客观资料；A(assessment)代表对健康问题评估；P(plan)代表对问题的处理计划。 POMR模式是指以问题/患者为导向的记录方式。 社区卫生服务的主体是全科医生。 健康档案管理的基本原则包括：①自愿为主，多种方式相结合；②体现健康管理和连续性服务的特点；③科学性与灵活性相结合。 社区居民健康状况资料包括：①社区人口学资料；②社区患病资料；③社区死亡资料；④危险因素调查、评估与干预。 社区人群患病资料包括：社区人群的发病率、患病率、病残率、社区疾病谱等。 社区人口学基本资料包括：社区的总人口数、出生率、死亡率、人口自然增长率、平均寿命、负担人口比例，以及年龄、性别结构、职业、教育程度、文化、婚姻、种族等人口学因素构成比例。 社区健康档案中的社区基本资料包括：社区的自然环境（地理位置、自然气候、环境状况、卫生条件、宗教、传统习俗等）、社区的经济和组织情况、社区动员潜力。 社区健康档案一般包括社区基本资料、社区卫生服务资源、社区卫生服务状况、社区居民健康状况等。 家庭基本资料包括：家庭各成员的基本资料、家庭类型、内在结构、居住环境。 家庭健康档案的主要内容包括：家庭基本资料、家系图、家庭评估资料、家庭主要问题目录、问题描述、家庭各成员的个人健康记录和家庭生活周期健康维护记录。 健康检查计划应根据不同性别、年龄、职业和健康状况决定。还应依据社区人群健康资料及疾病发生的高危因素决定。 健康监测是指系统地、连续地收集与健康状况和影响健康状况的各种因素相关的资料后，进行归纳、整理和分析，得出健康有关的信息，并对个体或群体进行传播健康知识，以指导疾病预防、控制和促进健康。 个人健康管理档案的基础资料包括：①个人的人口学资料(A对)，如年龄、性别、受教育程度、职业、婚姻状况、种族、社会经济状况、家庭状况及家庭重大事件;②健康行为资料(B对)，如吸烟、酗酒、运动、饮食习惯、就医行为等;③临床资料(D对)，如患者的主诉过去史、家族史、个人史(药物过敏史、月经史、生育史等)、各种检查及结果、心理精神评估资料(E对，这也就是指的心理评价资料）等。使用费用的资料不需要记入个人基础资料(C错)。 以预防为向导的健康记录包括：预防接种、健康体检、危险因素筛查及评价等。 个人健康档案以问题/患者为向导的记录主要包括：个人基础资料、问题描述、健康问题随访记录、转会诊记录。 建立健康档案的基本要求包括：①资料的真实性；②资料的科学性；③资料的完整性；④资料的连续性；⑤资料的可用性。 在我国，一般将居民健康档案分成三个部分，即个人健康档案、家庭健康档案和社区健康档案。]]></content>
      <categories>
        <category>考试资料,题库</category>
      </categories>
      <tags>
        <tag>健康管理师</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[健康管理师-基础知识-章节习题-09_10]]></title>
    <url>%2F05.%E5%A4%87%E8%80%83-02.%E5%81%A5%E5%BA%B7%E7%AE%A1%E7%90%86%E5%B8%88%2F%E5%81%A5%E5%BA%B7%E7%AE%A1%E7%90%86%E5%B8%88-%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86-%E7%AB%A0%E8%8A%82%E4%B9%A0%E9%A2%98-09_10%2F</url>
    <content type="text"><![CDATA[第九章：心理健康 3～5% ★★ 这一章分值权重有所下降，会出到三个小题左右， 基本上单纯在基础知识这门考试中考核。 第一节 心理健康与心理卫生 人格倾向性主要包括：需要、动机、兴趣、理想、信念和世界观等心理活动。 人格特征主要包括：能力、气质和性格。 能力是人在生理、心理发育成熟后，从事生产劳动的本事。 气质又叫秉性，是一个人内在的个性本性，主要指大脑皮质神经细胞的特性类型，如稳定或者不稳定；反映的速度：是灵敏还是迟钝，是兴奋型还是抑制型；因此它是性格的内在基础，是决定个性类型的基础。 性格是个性的外显表现，是显露的气质的外形，是在社会实践中对外界现实的基本态度和习惯的行为方式。 共情也叫同理心、同感、共感，它是一种设身处地从别人的角度去体会并理解别人的情绪、需要与意图的能力。 沟通是指信息的传递和交流的过程，包括人际沟通和大众沟通。 认知、情感和意志行为三者共同构成心理活动的完整过程，整个心理现象是在时间上展开的。 第二节 心理健康与心理发展 心理健康的内容包括：智力正常、情绪健康、意志健全、行为协调、人际关系适应、反应适度、心理特点符合年龄。 心理健康的标准包括：智力正常、情绪稳定与愉快、良好的人际关系、良好的适应能力。 共情也叫同理心、同感、共感，它是一种设身处地从别人的角度去体会并理解别人的情绪、需要与意图的能力。简言之，就是换位思考的能力。共情既是一种态度，也是一种能力。作为态度，它表现为对他人的关心、接受、理解、珍惜和尊重。作为一种能力，它表现为能充分理解别人，并把这种理解以关切、温暖、得体、尊重的方式表达出来。按照我们常人的说法就是换位思考、将心比心。 第三节、第四节 常见心理行为问题、常见心理障碍 焦虑是指个体因预感到某种不利情况出现时而产生的一种担忧、紧张、不安、恐惧、不愉快等综合情绪体验。 抑郁或抑郁障碍是指由各种原因引起的心境低落为主的精神状态。 恐惧是人的一种情绪，因周围不可预料或者不确定因素而导致的无所适从的心理或生理的强烈反应，或因受到威胁而产生并伴随着逃避愿望的情绪反应。 强迫症状包括强迫观念、强迫动作、强迫意向、强迫情绪。强迫洗涤属于强迫动作，指反复多次洗手或者洗物件，心中总摆脱不了“感到脏”，明知己洗干净了，却不能自制而非洗不可。强迫计数属于强迫动作，指不可控制地数台阶、电线杆，做一定次数的某个动作，否则感到不安，若漏掉了要重新数起。 当个体觉察应激源的威胁后，就会通过心理和生理的中介机制产生心理、生理反应，这种变化称为应激反应。 常见心理问题包括：焦虑与焦虑症、抑郁与抑郁症、恐惧与恐惧症、强迫症、疑病症、网络成瘾、性心理问题、婚恋问题、社会适应问题、心理应激障碍。 抑郁的症状表现：①一天中的多数时候情绪沮丧；②对日常生活丧失兴趣，无愉快感；③精力明显减退，无原因的持续疲乏感；④自信心下降或自卑，或有内疚感；⑤失眠、早醒或睡眠过多；⑥明显的体重减轻或者增加，或明显的食欲减退或者增加；⑦有自杀或自杀的观念或行为；⑧性欲明显减退；⑨注意力集中困难或下降；⑩联想困难，自觉思考能力显著下降；⑪一天中情绪有较大波动，常以早上最重，然后逐渐减轻，到晚上最轻。 心理问题的应对策略包括：调节自我认知、调节自我情绪选择适当行为、寻求专业的心理帮助。 调节自我认知的策略有：①校正自我认知；②建立合理的自我期望；③纠正归因偏见；④多用积极的思维方式。 第五节 心理健康的维护与促进 心理健康维护与促进的基本原则：1）理想与现实相结合的原则；2）躯体与心理相结合的原则；3）科学与具体相结合的原则；4）整体与差异相结合的原则；5）指导与主体相结合的原则；6）发展与矫治相结合的原则。6个结合 心理健康维护与促进的实施措施：树立社会主义的人生观和价值观；保持与社会发展同步的生活节奏；培养良好的心理素质与健全的人格；规律生活，有效应对；积极锻炼，合理兴趣；自我察觉，善交朋友；释放压力，定期放松。 第十章中医养生学基础知识 分值占比：2～4% 难度：★★ 与心理健康所占权重类似，会出1~2个单选题。 第一节、第二节 概述、常用养生保健方法 中医理论的主要特点是：整体观和恒动观。 中医基础理论包括：阴阳学说、五行学说、藏象学说、经络学说、气血津液、发病与病因、辨证论治。整体学说不是中医基础理论。 针刺保健的应用：调虚实、和阴阳、通经络、调气血； 疾病治疗：治面瘫 具有代表性的道家健身功法，如五禽戏、马王堆出土的“导引图”、胎息经、八段锦、太极拳等。易筋经为佛家创立 中医五劳包括：久卧伤气、久坐伤肉、久立伤骨、久行伤筋、久视伤血。 起居作息养生包括：和谐自然、起居有常、劳逸适度。不妄劳作属于劳逸适度范围内容。 中医认为，人体疾病的发生和早衰的根本原因，在于机体正气的虚衰。 形指形体，即肌肉、血脉、紧固、脏腑等；神指情志、意识、思维。神以形为基础，精、气、营、卫、血、津、液等是“神”活动的基础物质，强调养生要形神共养。归纳起来即为“守神全形”和“保形全神”。 “治未病”源于《黄帝内经》，包括“未病先防”、“已病防变”、“瘥后防复”三方面内容，而在《金匮要略》中，“治未病”强调的是“已病防变”。 临床常用的辩证方法主要有八纲辨证、气血津液辨证、脏腑辨证、六经辨证、卫气营血辨证、三焦辨证、经络辨证。 黄苔主热证、里证。 中医认为脾为后天之本。 五脏包括：肝、心、脾、肺、肾。 五味是指：辛、甘、苦、酸、咸五种基的滋味。此外，还有涩味和淡味。但一般统称为五味。 六腑包括：胆、小肠、胃、大肠、膀胱、三焦。 阴经包括：手太阴肺经、手厥阴心包经、手少阴心经、足太阴脾经、足厥阴肝经、足少阴肾经。 阳经包括：手阳明大肠经、手少阳三焦经、手太阳小肠经、足阳明胃经、足少阳胆经、足太阳膀胱经。 足三阳经：足阳明胃经、足少阳胆经、足太阳膀胱经；足三阴：足太阴脾经、足厥阴肝经、足少阴肾经。 未完]]></content>
      <categories>
        <category>考试资料,题库</category>
      </categories>
      <tags>
        <tag>健康管理师</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[健康管理师-基础知识-章节习题-07_08]]></title>
    <url>%2F05.%E5%A4%87%E8%80%83-02.%E5%81%A5%E5%BA%B7%E7%AE%A1%E7%90%86%E5%B8%88%2F%E5%81%A5%E5%BA%B7%E7%AE%A1%E7%90%86%E5%B8%88-%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86-%E7%AB%A0%E8%8A%82%E4%B9%A0%E9%A2%98-07_08%2F</url>
    <content type="text"><![CDATA[第七章：营养与食品安全 分值占比：4～6% 难度：★★★ 每年的健康管理师考试中都会出现营养与食品安全相关的内容，一般来讲，有关于营养素的知识会直接出现在《基础知识》的教材里，另外在《专业技能》中案例分析的过程可能会出现相关考题，比如“每天摄入多少碳水化合物?”“摄入多少矿物质”等等，然后让考生去剖析这样的摄入有何不合理之处，然后让考生提出合理的改进建议，这一章相对来说也比较重要。 第一节 营养学基础 营养：是机体通过摄取食物，经过体内消化吸收代谢，利用食物中对身体有益的物质作为构建机体组织活动、满足生理功能和体力活动需要的生物学过程。 营养素：指食物中所含的营养成分。营养素是机体为了维持生存、生长发育、体力活动和健康以食物的形式摄入的必需物质。人体所需的营养素有碳水化合物、脂类、蛋白质、矿物质、维生素、水和膳食纤维。 矿物质和维生素因需要量相对较少，在膳食中所占比重较小，成为“微量营养素”。 产能营养素主要包括：碳水化合物是人体的主要能量来源 （50% ~ 65%）；剩余 脂肪占比20% ~ 30% 人在一般情况下主要是利用碳水化合物和脂肪氧化功能。；蛋白质占比10%~15%。 膳食营养素参考摄入量包括：平均需要量、推荐摄入量、适宜摄入量、可耐受最高摄入量、宏量营养素可接受范围、预防非传染性慢性病的建议摄入量及特定建议值。 蛋白质互补作用：两种或两种以上食物蛋白质混合食用，其中所含有的必需氨基酸取长补短，相互补充，达到较好的比例，从而提高蛋白质利用率。 蛋白质的互补作用应遵循三个原则：食物的生物学种属愈远愈好；搭配的种类愈多愈好；使用时间愈近愈好，最好同时食用。 中国营养学会根据目前我国居民膳食碳水化合物的实际摄入量和国际粮农组织和世界卫生组织的建议，建议中国居民膳食碳水化合物的参考摄入量为占总能量摄入量的50%~65%。 必须氨基酸是体内无法合成的氨基酸：（携一两本（组）单色书来） 缬氨酸、异亮氨酸、亮氨酸、苯丙氨酸、组氨酸(婴儿)、蛋氨酸、色氨酸、苏氨酸、赖氨酸。 非必须氨基酸并非体内不需要，只是可在体内合成，食物中缺少了也无妨。非必需氨基酸包括：天冬氨酸、天冬酰胺、谷氨酸、谷氨酰胺、甘氨酸、脯氨酸、丝氨酸、精氨酸、胱氨酸及丙氨酸。 常量元素是指在人体内含量较多，大于体重的0.01%，每日膳食需要量都在100mg以上者，称为常量元素，有钙、镁、钾、钠、磷、硫、氯，共7种。 微量元素是指体内含量小于体重的0.01%，每日膳食需要量为微克至毫克的矿物质，人体必需的微量元素包括铁、碘、锌、硒、铜、钼、铬、钴，共8种。此外，氟属于可能必需的微量元素。 蔬菜中辣椒维生素C含量最高。 钾最好的食物来源是 蔬菜和水果。 粮谷类食物含碳水化合物最多，谷类含碳水化合物60%~ 80%，豆类为40%~ 60%，薯类含量为15%~ 30%。 维生素B6又称吡哆醇、抗皮炎维生素； 维生素D又称钙化醇、抗佝偻病维生素 ；维生素B1又称硫胺素、抗脚气病维生素 ； “能”在自然界有多种形式，如太阳能、化学能、机械能、电能，它们之间可以相互转换。为了计量上的方便，国际上制订统一的单位，即焦耳（joule，J）或卡（calorie，cal）。 单位换算：1kcal=4.184kJ;1kJ=0.239kcal;1MJ=239kcal;1000kcal=4.184MJ. 每克脂肪可以释放9Kcal能量，每克酒精可以释放7Kcal能量，每克蛋白质和碳水化合物都可以产生4Kcal能量，每克膳食纤维可以释放2Kcal能量。 能量系数：指每克产能营养素在体内氧化所产生的能量值。 血糖生成指数（GI）：简称血糖指数，指分别摄入某种食物与等量葡萄糖2小时后血浆葡萄糖曲线下面积比。GI=（某食物在食后2小时血糖曲线下面积/相当含量葡萄糖在食后2小时血糖曲线下面积）×100 备孕妇女膳食指南：调整孕前体重至适宜水平、常吃含铁丰富的食物，选用碘盐，孕前三个月开始补充叶酸、禁烟酒，保持健康生活方式。P171 水溶性维生素主要有B族维生素和维生素C。 水溶只有B和C 脂溶性维生素包括：维生素A、维生素D、维生素E、维生素K。 脂溶A、D、E和K。 膳食纤维的功能有：①有利于食物的消化过程；②降低血清胆固醇，预防冠心病；③预防胆石形成；④促进结肠 功能，预防结肠癌；⑤防止能量过剩和超重与肥胖；⑥维持血糖正常平衡，防治糖尿病。 蛋白质的生理功能有：①构成身体组织；②调节生理功能；③供给能量。 蛋白质按营养价值分类包括：完全蛋白；不完全蛋白；半完全蛋白。 类脂主要包括：磷脂、糖脂、固醇及类固醇等。 类脂的生理功能包括：①供给能量；②促进脂溶性维生素吸收；③维持体温、保护脏器；④增加饱腹感；⑤提高膳食感官性状；⑥构成身体组织和一些重要的生理活性物质。 必须脂肪酸在体内有多钟生理功能，主要有：①构成线粒体和细胞膜的重要组成成分；②合成前列腺素的前体；③参与胆固醇代谢；④参与精子的形成；⑤维护视力。 按脂肪酸饱和度可分为：饱和脂肪酸、单不饱和脂肪酸、多不饱和脂肪酸。 按脂肪酸链长度可分为：长链脂肪酸、中链脂肪酸、短链脂肪酸。 生长发育阶段如果得不到充分的营养保证，可以引起种种不良后果，可能会引起蛋白质-能量营养不良或其他营养缺乏病，并导致体格发育障碍、身高体重低下。 第二节 平衡膳食 中国居民膳食中脂肪的参考摄入量为占总能量的20%~30%。 《中国居民平衡膳食宝塔》指导：第一层，谷薯类250~ 400g；第二层，蔬菜类300~ 500g/天，水果类200~ 350g/天；第三层，畜肉类40~ 75g/天；第四层，奶及奶制品300g/天；第五层，油25~ 30g/天。 每克脂肪可以释放9Kcal能量，每克蛋白质和碳水化合物都可以产生4Kcal能量，每克酒精可以释放7Kcal能量，每克膳食纤维可以释放2Kcal能量。人体储存能量最多的营养物质是：脂肪。 痛风患者不宜食用嘌呤含量高的食物，同时需要控制体重。 特殊人群膳食指南包括：备孕妇女膳食指南；孕期妇女膳食指南；哺乳期妇女膳食指南；6月龄内婴儿母乳喂养指南；7-24月龄婴幼儿喂养指南；学龄前儿童膳食指南（2-5岁）；学龄前儿童膳食指南（6-17岁）；中国老年人膳食指南。 6月龄内婴儿母乳喂养指南中包括：产后尽早开奶，坚持新生儿第一口食物是母乳；坚持6月龄内纯母乳喂养；顺应喂养，培养良好生活习惯；出生后数日开始补充维生素D，不需补钙；婴儿配方奶是不能纯母乳喂养时的无奈选择；监测体格指标，保持健康生长。 6〜12月婴儿喂养指南的内容包括:奶类优先，继续母乳喂养;及时合理添加辅食;尝试多种多样的食物，膳食少糖、无盐、不加调味品；逐渐让婴儿自己进食，培养良好的进食行为；定期监测生长发育状况；注意饮食卫生 中国老年人膳食指南包括：少量多餐细软；预防营养缺乏；主动足量饮水；积极户外活动；延缓肌肉衰减；维持适宜体重。 《中国居民膳食指南》（2016）一般人群膳食指南包括：①食物多样，谷类为主；②吃动平衡，健康体重；③多吃蔬菜、奶类、大豆；④适量吃鱼、禽、蛋、瘦肉；⑤少油少盐，控糖限酒；⑥杜绝浪费，兴新食尚。 第三节 保健食品 购买和食用保健品，应注意如下哪几项：①保健食品不是药品，不要相信“疗效”、“速效”的字样；②选择保健食品，必须针对自己的身体状况；③学会理性购买保健食品；④购买保健食品要认准蓝色草帽样标志和批准文号，一定要到正规的经销场所购买；⑤从科学的角度讲，平时注意营养合理的平衡膳食、有规律的生活习惯、适时适量的运动、保持开朗的性格才是身体健康的根本保证。 第四节 食品安全 根据我国食源性疾病监测网的资料，导致细菌性食物中毒的病原体依次为沙门菌属、变形杆菌、葡萄球菌肠毒素、副溶血弧菌、其他细菌或细菌毒素。 食源性疾病按致病因子分为：细菌性食源性疾病、食源性病毒感染、食源性寄生虫感染、食源性化学性中毒、食源性真菌毒素中毒、动物性毒素中毒和植物性毒素中毒。 食物中毒的特点包括：①季节性：食物中毒的季节性与与食物中毒的种类有关，细菌性食物中毒多发生在夏季，化学性食物中毒全年均可发生；②暴发性：发病潜伏期短，来势急剧，短时间内可能有多人发病，发病曲线呈突然上升趋势；③相似性：患者有食用同一食物史，临床表现基本相似，以恶心、呕吐、腹痛、腹泻为主要症状。④非传染性：流行波及范围与污染食物供应范围相一致，停止污染食物供应后，流行即告终止，人与人之间无直接传染。 食物中毒按病原分为：细菌性食物中毒、真菌及其毒素食物中毒、动物性食物中毒、有毒植物中毒、化学性食物中毒。 第八章：身体活动基本知识 分值占比：4～6% 难度：★★★ 在基础知识中会经常出现“怎么计算心率”“怎么计算能量代谢率”等问题，另外在专业技能考试中也经常会考核到身体活动的基础知识，需要考生为案例中的个体量身定做运动处方的相关建议。 第一节 身体活动及其健康益处 身体活动(physicalactivity，PA)指由于骨骼肌收缩引起机体能量消耗增加的所有活动。身体活动包括 频率(Freguency)、强度(Intensity)、时间(Timing)和类型(Type) 四个基本要素，也就是FITT原则。另外还有身体活动量(Volume)和进度(Progress)，统称为FITT-VP原则。 WHO在2004年发布了《饮食、身体活动与健康全球战略》，呼吁所有成员国将促进身体活动作为重要的国家公共卫生干预政策。 WHO于2010年出台了《关于有益健康的身体活动全球建议》，针对不同年龄人群的身体活动进行了原则性的建议。 我国于2011年出台了《中国成人身体活动指南》（试行）。 体力活动程度是影响成年人热能消耗的主要因素。 有氧运动是指：躯干、四肢等大肌肉群参与为主的、有节律、较长时间、能够维持在一个稳定状态、以有氧代谢为主要供能途径的运动形式，也叫耐力运动，如马拉松、 打太极拳、步行、慢跑、游泳等。 无氧运动是指：以无氧代谢为主要供能途径的运动形式，一般为肌肉的强力收缩活动，因此，不能维持一个稳定的状态。如100米短跑、举重等。 第二节 现有身体活动指南要点 身体活动分类， 按日常活动分类：根据日常生活中身体活动的目的和时间分配，可分为职业性身体活动（指有劳动收入(如工资)的活动，包括家政服务等职业行为。）、交通往来身体活动、家务性身体活动和 业余休闲身体活动(上述三类目的之外的时间里从事的活动) 四类。 按能量代谢分类：身体活动的本质是肌肉收缩做功，运动强度不同稳定维持在这强度的运动时间也不同，同时决定了肌肉活动的能量来自于无氧代谢、有氧代谢或有氧与无氧混合代谢。身体活动因此可分为有氧代谢运动和无氧代谢运动，简称有氧运动和无氧运动。 根据生理功能和运动方式，身体活动还可以有以下类别：1.柔韧性活动(伸展性活动)、2.强壮肌肉活动、3.平衡性活动、4.健骨运动、5.高强度间歇训练** 长时间运动血糖下降时首先影响脑。 身体活动强度分为绝对强度(也称“物理强度”)和相对强度也称“生理强度”)两类指标。同一种运动的绝对强度是一致的，而不同生理状态个体的疲劳感等相对强度可能存在较大差异。 绝对强度，据身体活动的绝对物理负荷量测定的强度水平，通常为普通健康成年人的某种运动测定结果。常用指标为代谢当量(Metabolismequivalent,METS，也称梅脱)。代谢当量是指相对于安静休息时运动的能量代谢水平1MET相当于每分钟每公斤体重消耗3.5ml的氧。或每公斤体重每小时消耗1.05千卡(4.4千焦耳)能量的活动强度。 中等强度对应 3~5.0梅脱。 中等强度(3~5.9梅脱)身体活动，如47km/h的快走和低于7km/h的慢跑，可以降低心血管病、糖尿病、结肠癌和乳腺癌等慢性病的风险和病死率。强度大于或等于7梅脱的活动具有更强的促进和预防疾病作用;强度小于3梅脱的活动可以增加能量消耗，有助于体重控制。 1个千步当量相当于普通人中等速度（4千步/h）步行10分钟（约1 千步）；1个千步当量相当于8km/h跑步3分钟；1个千步当量相当于16km/h骑行7分钟；1个千步当量相当整理床铺10分钟；1个千步当量约相当于慢跑4分钟。 相对强度，则根据生理反应情况测定的强度水平，包括: (1)主观性的疲劳感，常用指标为自觉运动强度量表(即伯格(Borgs)表Borg量表，也称为RPE量)等级可以分为轻、中、重二个水乎 (2)客观的心率水平、耗氧量等。常用指标为最大心率百分比(%HRmax) 和 最大耗氧量百分比(%V0,max)、靶心率等运动时的心率作为训练时运动强度的监测指标称为目标心率或称靶心率。 最大心率测算方式 最大心率 HRmax=207-0.7x年龄（岁）。中等强度运动对应最大心率的60%~75%。 测量运动强度最直接、简单、易行且准确的指标是心率。 运动时的心率作为训练时运动强度的监测指标，称为目标心率或称靶心率。 正常人安静状态下每分钟心跳的次数一般为60～100次/分。 人体身体活动过程中的三个关键环节是：疲劳、恢复、适应。 普通健康成年人有氧活动每次至少持续10分钟。 建议成年人每天进行累计相当于6000步以上的身体活动，如果条件允许，最好每天进行30分钟中等强度的身体运动。每周150分钟中等强度或75分钟高强度(约每周8-10梅脱·小时)身体活动总量可以增进心肺功能、降低血压和血糖、改善血糖、血脂代谢、调节内分泌系统、提高骨密度、保持或增加瘦体重、减少体内脂肪蓄积、控制不健康的体重增加等。可以使冠心病、脑卒中、2型糖尿病、乳腺癌和结肠癌的发病风险降20%30%。身体活动量增加到每周300分钟中等强度或150分钟高强度(总量20梅脱·小时)，可以获得更多的健康效益。 依据WHO《有益健康的身体活动建议》，对于5-17岁儿童和青少年进行身体活动的推荐要点为：①每天应当至少进行60分钟中等强度到高强度身体活动；②每天身体活动超过60分钟将可获得额外的健康效益；③每周应当包括至少三次加强肌肉和骨骼的活动。 锻炼中应注意：①量力而行、循序渐进、并采取必要的保护措施；②学习安全注自我监测运动中不适症状；③掌握发生意外时的应急处置技能；④平常很少活动的人、中老年人、患者和有潜在疾患的个体，在开始锻炼和增加活动量应进行必要的健康筛查和运动能力评估；⑤较大强度身体活动对心肺功能有更好的改善作用,但也易引起运动伤害，因此更应合理安排运动量。 运动处方的制定包括：运动前的常规体检、健康筛查与评估、运动测试（必要时进行）、制定运动量目标和内容、运动训练的医学监督和运动计划调整、运动伤害6个方面。其中具体内容还应包括：运动的形式、强度、时间及活动进度等。 合理选择有益健康的身体活动量，应遵循“动则有益、贵在坚持、多动更好、适度量力”的4项基本原则。 中等强度活动的自我感觉有：心跳和呼吸加快，用力但不吃力，可以随着呼吸的节奏连续说话，但不能放声唱歌，如尽力快走时的感觉。 第三节 慢性病与身体活动 严重骨质疏松患者运动过程中应注意避免在凹凸不平的场地运动，不宜进行高冲击性的活动，并注意运动与日光照射相结合。需要注意： A 避免运动速度过快、B 避免在凹凸不平的场地运动、C 避免游泳、D 运动与日光照射结合、E 禁忌跳绳运动 。 不能参加跳绳、跳高和举重等，可以散步，打太极。 安静时血压未能很好控制或超过180/110mmHg的患者暂时禁止中度及以上的运动，高血压危象、高血压合并不稳定心绞痛、高血压合并视网膜等禁止运动疗法。 糖尿病患者运动时应注意：①血糖&gt;16.7mmo/L应禁忌大强度耐力运动；②出现严重或增生性视网膜病变时，应避免大强度耐力活动、中高负荷抗阻力运动、冲击用力和暴发用力；③出现血糖控制不稳定、血糖&gt;16.7mmo/L合并酮症、合并视网膜出血或感染、不稳定心绞痛时应禁忌各种运动；④预防低血糖；⑤增加运动量时的进度安排；⑥运动时的足部保护。]]></content>
      <categories>
        <category>考试资料,题库</category>
      </categories>
      <tags>
        <tag>健康管理师</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[健康管理师-基础知识-章节习题-05_06]]></title>
    <url>%2F05.%E5%A4%87%E8%80%83-02.%E5%81%A5%E5%BA%B7%E7%AE%A1%E7%90%86%E5%B8%88%2F%E5%81%A5%E5%BA%B7%E7%AE%A1%E7%90%86%E5%B8%88-%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86-%E7%AB%A0%E8%8A%82%E4%B9%A0%E9%A2%98-05_06%2F</url>
    <content type="text"><![CDATA[第五章 流行病学和医学统计学基本知识 在此章节中会涉及到许多的专业术语和名词，比如比率、病例对照组等等，还有一系列的计算公式和方式，考生在考试中要结合案例分析名词和术语说代表的含义，所以这一章也是相对来说比较重要的章节，需要考生牢记公式和专业名词。 第一节 流行病学的基本知识 流行病学第一阶段的任务是揭示现象,流行病学第二阶段的任务是找出原因、影响或决定因素,流行病学第三阶段的任务是提供措施。 流行病学研究，分为 分析性研究、描述性研究 和实验性研究。 观察性研究包括：横断面研究、比例死亡比研究、生态学研究、病例对照研究和队列研究。 分析性研究包括：病例对照研究和队列研究等。 描述性研究包括：比例死亡比研究、生态学研究、横断面研究等。 实验性研究的特点包括：①属于前瞻性研究；②随机分组；③设立对照组；④有干预措施。如果某种疾病的危险因子分布广泛，不易确定高危人群时，也需要采用社区试验。 流行病学是研究疾病、健康状态和事件在人群中的分布、影响和决定因素，用以预防和控制疾病，促进健康的学科。 为了避免各种偏倚影响临床试验结果，随机同期对照效果最佳. 临床试验是运用随机分配的原则将试验对象（者）分为试验组和对照组。 病例对照研究的特点有：①在疾病发生后进行，研究开始是已有一批可供选择的病例；②研究对象按发病与否分成病例组与对照组；③被研究因素的暴露状况是通过回顾调查或信息收集获得的；④若按因果关系进行分析，结果已发生，是由果及因的推理顺序；⑤经两组暴露率或暴露水平的比较，分析暴露与疾病的联系。 病对照研究所需样本量小,病例易获取,因此工作量相对小,所需物力、人力较少,易于进行,出结果快;可以同时对一种疾病的多种病因进行研究;适合于对病因复杂、发病率低、潜伏期长的疾病进行研究;在某些情况下,还可以对治疗措施的疗效与副作用作初步评价。但是,由于受回忆偏倚的影响,选择合理的对照又较困难,因此结果的可靠性不如队列研究。此外,不能计算暴露与无暴露人群的发病率及相对危险度(RR),只能计算比值比(OR)。P104 病例对照研究为选择一组患所研究疾病的患者与一组无此病的对照组，调查其发病前对某个（些）因素的暴露状况，比较两组中暴露率和暴露水平的差异，以研究该疾病与这个（些）因素的关系。假定夫妻不和睦的男司机会有较高的事故发生率，如用病例对照研究来检验此假设，合适的对照为： 未出事故的男司机. 队列研究是将特定的人群按其是否暴露于某因素或按不同暴露水平分为n个群组或队列，追踪观察一定时间，比较两组或各组发病率或死亡率的差异，以检验该因素与某疾病有无因果关系及联系强度大小的一种观察性研究方法。 队列研究的特点有：①在时序上是由前向后的，在疾病发生前开始进行，故属于前瞻性队列研究；②属于观察性对比研究，暴露与否是自然存在于研究人群，而不是人为给予的；③研究对象根据暴露与否分组，这与实验性研究的随机分型不同；④是从“因”到“果”的研究；⑤追踪观察的是两组间的发病率或死亡率差异。 队列研究是从“因”到“果”的研究，包括：①前瞻性队列研究：研究对象的确定与分组由研究开始时是否暴露来决定，研究结局需随访观察一段时间才能得到；②历史性（回顾性）队列研究：研究工作现在开始，而研究对象是过去某个时间进入队列的；③双向性队列研究：根据历史档案确定暴露与否，随访至将来的某个时间确定结局。 队列研究的缺点: 队列研究在研究罕见病时，需要大量研究对象，因而不易收集到完整可靠的资料，故不适用于罕见病的研究。 现况调查的目的包括：①描述疾病或健康状况的分布；②发现病因线索；③适用于疾病的二级预防；④评价疾病的防治效果；⑤疾病监测；⑥其他。 计量资料为定量测定的结果，通常用专用仪器测量，并有计量单位。具有连续性的特点。 阳性预测值是指试验阳性结果中真正患病的比例。患病率相同时，特异度越高阳性预测值越好，临床医生越有理由判断阳性结果为患者。 试验真实性的评价指标包括：灵敏度、特异度、假阴性率、假阳性率及正确诊断指数。 相对危险度（RR）是指暴露组发病率（Ie）与非暴露组发病率（I0）之比，它反映了暴露与疾病的关联强度。相对危险度（RR）无单位，比值范围在0至∞之间。 RR=1表明暴露与疾病无关联； RR&lt;1，表明存在负联系（提示暴露是保护因子）； RR&gt;1时，表明两者存在正联系（提示暴露是危险因子） 发病率是指在特定时期内某人群某病新病例发生的频率，为动态指标，是一种真正的率。发病率是一个重要和常用的指标，对于传染病以及死亡率极低或不至死的疾病尤为重要，反映患该病的风险。常用来描述疾病的分布，探讨发病因素，提出病因假设和评价防治措施的效果。 患病率是指在特定时间点一定人群中某病新病例和旧病例的人数总共所占的比例，是一种静态指标，其本质上是一种比例，不是一种真正的率。 患病率=（特定时间点某人群中某病新旧病例数/同期观察人口数）×k。 病死率表示一定时期内患某病的全部患者中因该病而死亡的比例。病死率通常用于病程短的急性病，以衡量疾病对人生命威胁的程度。 死亡率 =（某人群某年总死亡人数/该人群同年平均人口数）× k。 比值比（OR）指病例组中暴露人数与非暴露人数的比值除以对照组中暴露人数与非暴露人数的比值。 可进行筛检的疾病包括：①筛检的疾病应是当地当前对公众危害大的疾病或缺陷，如发病率或死亡率高，易致伤残的疾病；②筛检的疾病应具有可识别的潜伏期或早期症状；③对疾病的自然史，包括从潜伏期发展到临床期、疾病结局的过程应有足够的了解；④对被筛检和诊断出来的病例应有有效而易被群众接受的治疗方法。 诊断试验的评价标准包括：①同金标准诊断方法进行同步盲法比较；②研究对象的代表性；③要有足够的样本量；④诊断界值的确定要合理；⑤不仅评价真实性，也评价可靠性；⑥试验的方法和步骤要具体，有可操作性。 筛检的主要用途包括： ①筛检最初用于早期发现那些处于临床前期或临床初期的可疑患者，以进行早期诊断和早期治疗，提高治愈率或延缓疾病的发展和改善预后； ②近年来，筛检试验越来越多地应用于发现某些疾病的高危个体，以预防疾病的发生； ③开展流行病学监测，了解疾病的患病率及其趋势，为公共卫生决策提供科学依据； ④了解疾病的自然史。 第二节 医学统计学的基本知识 医学统计工作的基本步骤是： 研究设计、收集资料、整理资料、分析资料。 医学统计学的主要内容：统计设计、统计描述和统计推断。 医学统计学的研究对象是：具有不确定的医学数据。 统计工作推断是用样本信息推断总体特征，包括总体参数的估计和假设检验（显著性检验），它是统计学的核心内容，也是实际应用最广的内容。 统计工作推断是用样本信息推断总体特征，包括总体参数的估计和假设检验，它是统计学的核心内容。 分析资料主要包括： ①用一些统计指标、统计图表等方式表达和描述资料的数量特征和分布规律，不涉及有样本推论总体的问题； ②对样本统计指标作参数估计和假设检验，并结合专业知识解释分析结果，目的是用样本信息推断总体特征。 计算统计指标时，各个反应变量可以进一步划分为：计量资料、计数资料、等级资料： 计量资料：亦称数值变量，为定量测量的结果，通常用专用仪器测量，并有计量单位，如身高(cm)、体重(kg)等。计量资料有连续性的特点，如身高可以是175cm、175.1cm、175.11cm等: 计数资料：计数资料是定性观察的结果。有二分类和多分类两种情况。多分类的定性观察结果有两种以上互不包含的属性，如新生儿出生缺陷、某病患者的死亡原因等。这类资料之所以称为计数资料，因为在统计时通常将各种观察结果按属性分类计数，如阳性人数、阴性人数、死于某病的人数等。 等级资料：介于定量测量和定性观察之间的半定性观察结果，通常有两个以上等级，如阴性、阳性、强阳性，治愈、好转、有效、无效等。等级资料与计数资料又可统称为分类变量(categorical variable)它们的区别在于，等级资料虽然也是多分类资料，但各个类别间还存在大小或程度上的差别。 制表的基本要求： ①标题：概括说明表的内容，简明扼要，位于表的上方； ②标目：用以指明表内数字含义，横标目为主语，表示被研究事物，纵标目为谓语，表示被研究事物的各项统计指标； ③线条：除必须的顶线、底线、标目线以外，应尽量减少其他不必要的线条，不使用竖线、斜线； ④数字：一律使用阿拉伯数字，应准确无误，同一指标的数字的小数位应一致，位次对齐。 制作统计图的基本要求有： ①根据资料的性质和分析目的，选择合适的图形； ②统计图要有标题，位于图体下方的中央位置； ③绘制有坐标轴的图形，纵、横轴要有标目，标注原点、尺度、单位等，纵横轴的比例以5 : 7为宜； ④同一张图内比较不同事物时，需用不同颜色或样式的线条区别表示，并附图说明。 参数是指总体指标，如总体均数、总体率、总体标准差等。 标准差是描述对称分布资料离散趋势的重要指标，标准差的数值越大，说明观测值的变异度越大，即离散程度越大，此时的数据就会越分散，均数的代表性越差。因此，变量值之间的差异越大，其标准差也越大。 变异系数即标准差与均数之比用百分数表示。当进行两个或多个资料变异程度的比较时，如果度量单位与平均数相同，可以直接利用标准差来比较。当单位和（或）平均数不同时，比较其变异程度就不能采用标准差，而需采用标准差与平均数的比值（相对值），即变异系数来比较。 几何均数是描述偏态分布资料集中趋势的一种重要指标。它尤其适用于描述以下两类资料的集中趋势：①等比资料，如医学上的血清抗体滴度、人口几何增长资料等；②对数正态分布资料（有些正偏态分布的资料，原始数据经过对数转换后服从正态分布），如正常成人血铅值或某些疾病的潜伏期等。 中位数具有不受两端特大或特小值影响的特点，当资料的一端或两端无确定数值时，算术均数不能计算，而中位数却可以。 概率是对总体而言，概率指某随机事件发生的可能性大小的数值，常用符号P来表示。随机事件的概率在0与1之间。统计中的许多结论都是带有概率性的。 频率是对样本而言，频率指一次实验结果计算得到的样本率。 描述集中趋势的指标包括：算数均数（简称均数）、中位数及几何均数。 总体是根据研究目的的确定的同质观察单位的全体，更确切的说，是同质的所有观察单位某种变量值的集合。 一般常将P≤0.05或P≤0.01称为小概率事件，表示某时间发生的可能性很小。 第六章：健康教育学 5～8% ★★★ 健康教育是健康管理师在服务受众时需要灌输和渗透的内容，健康管理师在服务中如何能将这种健康管理的概念传播给受众和社会也是考试中重点考察的内容，所以这一章节也占有很大比重。 第一节 健康教育与健康促进概述 健康教育是通过信息传播和行为干预的手段，帮助个人和群体掌握卫生保健知识、树立健康观念，自愿采纳有利的健康行为和生活方式的教育活动与过程，其目的是消除或减轻影响健康的危险因素，预防疾病，促进健康和提高生活质量 健康教育的着眼点是促进个人或群体改变不良的行为与生活方式。 行为的改变以知识、信念、健康观的改变为基础，因此首先要使个体或群体掌握卫生保健知识，提高认知水平和技能，建立起追求健康的理念，并为此自觉自愿地，而不是勉强地来改善自己的行为与生活方式。 健康促进的核心策略是： 社会动员 健康促进涉及的主要活动领域包括：建立促进健康的公共政策、创造健康支持环境、增强社区的能力、发展个人技能及调整卫生服务方向。 社区健康促进的目标人群描述最准确的是： 全部社区居民。 1986年，首届国际健康促进大会通过了《渥太华宣言》。 健康相关行为是与健康和/或疾病有关的行为。 第二节 健康相关行为改变的理论 健康信念模式理论强调感知在决策中的重要性，影响感知的因素很多，是运用社会心理学方法解释健康相关行为的理论模式。该理论认为信念是人们采纳有利于健康的行为的基础，人们如果具有与疾病、健康相关的信念，他们就会采纳健康行为，改变危险行为。 健康管理师在第1、2阶段，应重点促使人们进行思考，认识到危险行为的危害，权衡改变行为的利弊，从而产生改变行为的意向、动机。 健康管理师在第3阶段，应促使他们做出决策，尽快开始改变危害健康的行为。 健康管理师在第4、5阶段，应改变环境来消除或减少诱惑，通过自我强化和学会信任来支持行为改变。 对于成瘾行为来说，在一般行为改变5个阶段外，还须增加的第6个阶段是： 终止阶段。 自我效能指个体对自己组织、执行某特定行为并达到预期结果的能力的主观判断。即个体对自己有能力控制内、外因素而成功采纳健康行为并取得期望结果的自信心、自我控制能力。 自我效能是人类行为动机、健康和个体成就的基础，是决定人们能否产生行为动机和产生行为的一个重要因素。 艾滋病的传播是靠体液传播的，携带艾滋病毒的血液、汗液、乳汁、唾液、精液、等。最常见的传染方式有三种：1.性传播 2.母婴传播 3.血传播（共用针具）。 知—信—行模式（KABP或KAP），是知识（基础）、信念（动力）和行为（目标）的简称。健康教育的“知—信—行”模式实质上是认知理论在健康教育中的应用。知信行模式认为：卫生保健知识和信息是建立积极、正确的信念与态度，进而改变健康相关行为的基础，而信念和态度则是行为改变的动力。 健康信念模式（HBM）：是否采纳有利于健康的行为与下列因素有关：感知疾病的威胁、感知健康行为的益处和障碍、自我效能、提示因素及社会人口学因素。个体对健康行为益处的感知越强，采纳健康行为的障碍越小，个体采纳健康行为的可能性越大。 产生和提高自我效能的途径包括：自己成功完成过某行为、他人间接的经验、口头劝说及情感激发。 行为改变的阶段理论包括：没有打算阶段、打算阶段、准备阶段、行动阶段及维持阶段。对于成瘾行为来说，还有第6个阶段即终止阶段。 属于危害健康的团体行为是：大吃大喝现象泛滥、生产劣质食品的厂家、 生产假药的厂家、 厂矿生产过程排放污气污水。 是否及时就诊是个人行为，不属于集体行为。 不良行为生活方式包括潜伏期长、特异性差、协同作用强、变异性大、广泛存在。 促进健康行为的基本特征有规律性 、和谐性 、一致性、有利性等。 团体健康相关行为的干预策略和方法有： （1）开发领导，政策倡导； （2）应用竞争机制； （3）利用评价和激励手段； （4）动员广泛参与； （5）利用舆论与规范的力量。 第三节 健康传播 员工每天上午10点集体做工间操，属于团体，集体行为。 家人之间的影响是最直接，影响最深的，无论是行为还是态度，是好是坏。 吸烟、酗酒是个体的不良生活行为。 人际传播也称人际交流，是指人与人之间进行直接信息沟通的一类交流活动。人际传播可以分成个人之间、个人与群体之间、群体与群体之间三种形式。 人际传播的特点包括： ①直接的人际传播不需要任何非自然的媒介； ②就传播活动中信息的发出者和接受者而言，在同一次人际传播活动中交流的双方可以互为传播者和受传者； ③由于人际传播重点反馈及时，所以双方的交流也就容易充分； ④人际传播的信息量比较少，覆盖的范围比较小，传播的速度也比较慢； ⑤在人际传播活动中，特别是在多级的人际传播活动中，信息容易走样。 开场与结束技巧、提问技巧、谈话技巧、反馈技巧、观察技巧、倾听技巧等都是人际传播的基本技巧。 传播材料制作应遵循以下六个程序：①分析需求和确定信息；②制订计划；③形成初稿；④传播材料预试验；⑤材料的生产发放与使用；⑥监测与评价。 制订计划应包括：确定目标人群、材料种类、数量、使用范围、发放渠道、使用方法、预试验与评价方案、经费预算、时间进度等。 预试验的收集反馈内容包括：了解目标人群是否理解材料传播的信息内容，是否喜欢材料的表现形式和视觉舒适度，以及讯息的易读性、实用性、可接受性、趣味性等。 大众传播的特点包括： ①传播者是职业性的传播机构和人员，并需要借助非自然的特定传播技术手段； ②大众传播的信息是公开的、公共的，面向全社会人群； ③大众传播信息扩散距离远，覆盖区域广泛，速度非常快； ④大众传播对象虽然为数众多，分散广发，互不联系，但从总体上来说是大体确定的； ⑤大众传播是单向的，很难互换传授角色，信息反馈速度缓慢而且缺乏自发性。 针对个体传播的传播材料包括：传单、折页、小册子。 针对群体传播的传播材料包括：宣传栏、招贴画或海报，标语和横幅、DVD。 针对大众的传播媒介包括：报纸/杂志、广播/电视。 健康管理互动平台的系统构架包括：使用者操作页面、健康档案管理模块、健康风险评估模块、智能化膳食、运动管理数据库、个人健康教育资料库及依从性提醒及互动功能。 第四节 健康教育计划的设计、实施与评价 PRECEDE-PROCEED模式是健康教育领域应用最广、最具权威性的模式。 健康教育需求评估又称为健康教育诊断，根据PRECEDE-PROCEED模式,健康教育诊断包括如下内容：社会诊断、流行病学诊断、行为与环境诊断、教育与组织诊断及管理与政策诊断。 针对降低5岁以下儿童急性感染死亡率的健康教育一级目标人群是婴幼儿亲属。 在PRECEDE-PROCEED模式中，影响健康相关行为的因素分三大类：倾向因素、促成因素和强化因素。 倾向因素：先于行为，又被称为动因因素或前置因素，是产生某种行为的动机、愿望，或是诱发某行为的因素。倾向因素包括知识、态度、信念和价值观、行为动机与意向等，也包括个人技能。 促成因素包括保健设施、医务人员、诊所、医疗费用、交通工具、个人保健技术，行政的重视与支持，法律政策等也可归结为促成因素。 强化因素又称加强因素，是激励行为维持、发展或减弱的因素。强化因素可以分为躯体因素，心理因素，经济因素和社会因素，如社会支持、同伴鼓励、父母的劝告等。 经过一段时间60%员工做到了戒烟的行为，属于行为目标。 控烟计划一年后，80%的青少年掌握了三项以上吸烟对健康危害的知识，这属于教育目标。 健康教育计划的具体目标需要包含具体的、量化的、可测量的指标。 健康教育计划的过程评价指对健康教育/健康促进计划实施过程进行的评价，起始于计划实施开始之时，贯穿计划实施的全过程。过程评价着重关注项目是否按计划的数量和质量执行，包括项目计划执行涉及的各个方面。 环境策略作用对象是影响行为的促成因素，即物质环境、条件，从而使人们采纳健康行为的意愿得以实现。 在健康教育中，效应评价用来评估健康教育/健康促进项目导致的目标人群健康相关行为及其影响因素的变化。所采用的指标包括卫生知识均分、卫生知识知晓率（正确率）、健康信念持有率、行为流行率、行为改变率等。 一个描述传播行为的简便方法，就是回答下列5个问题：①谁（who）？②说了什么（says what）？③通过什么渠道（through what channel）？④对谁（to whom）？⑤取得什么效果（with what effect）？”这就是拉斯韦尔五因素传播模式（又称5W模式）。]]></content>
      <categories>
        <category>考试资料,题库</category>
      </categories>
      <tags>
        <tag>健康管理师</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[遗传病-先天性肾上腺皮质增生]]></title>
    <url>%2F01.%E7%9F%A5%E8%AF%86-11.%E9%81%97%E4%BC%A0%E7%97%85%2F%E9%81%97%E4%BC%A0%E7%97%85-%E5%85%88%E5%A4%A9%E6%80%A7%E8%82%BE%E4%B8%8A%E8%85%BA%E7%9A%AE%E8%B4%A8%E5%A2%9E%E7%94%9F%2F</url>
    <content type="text"><![CDATA[背景介绍先天性肾上腺皮质增生症（CAH）是一种最常见的常染色体隐性遗传病，其经典（严重）形式可能危及生命，而非经典（轻微）形式可能无症状或导致女性不孕。CAH最常见的类型是21-羟化酶缺乏症；CAH是一种多种激素失衡的疾病。CYP21A2基因（编码21-羟化酶，一种细胞色素P-450酶）的突变导致缺乏21-羟化酶，而这种酶是肾上腺皮质中皮质醇和醛固酮生产所必需的。这种酶的缺乏会连锁反应。皮质醇的减少导致垂体促肾上腺皮质激素过度分泌，刺激皮质醇前体的积累，以及随后通过类固醇途径的转移，这些途径产生肾上腺雄激素。如今，经典形式是46,XX新生儿非典型生殖器官最常见的原因，也是儿童时期原发性肾上腺功能不足的主要原因。 CAH的疾病表型范围通常与CYP21A2基因型和每种基因型预期的残余21-羟化酶活性相关。基于激素的、拯救生命的新生儿筛查，针对经典形式的筛查始于1977年的阿拉斯加，现在已在美国所有50个州和全球40多个国家实施。 根据全球数百万新生儿的筛查数据，经典CAH的发生率在1/10,000到1/20,000的活产婴儿中。非经典CAH最早在1957年由法国生物化学家Jacques Decourt及其同事发现，而在20年后，对经典CAH患者的亲属的研究揭示了一些具有生化和遗传非经典CAH的人是无症状的，不需要治疗。非经典CAH在全球范围内很常见，估计的患病率在200人中有1例到1000人中有1例。与治疗其他形式的肾上腺功能不足的方法不同，CAH的治疗目标是双重的：首先，替代缺乏的激素；其次，减少过多的雄激素水平。尽管由于遗传学、代谢组学和治疗策略的进步，绝大多数CAH患者能够存活下来，但现有的治疗未能预防多种并存状况，肾上腺危象导致的死亡仍然发生。$\sqrt{a}$ CYP21A2定位于第6号染色体（6p21.3），位于主要组织相容性复合体中的一个低拷贝重复序列位点，包括真基因和假基因（图1A）。已知有200多种CYP21A2突变；然而，大多数突变涉及10个有害变异，这些变异来源于非功能性的CYP21A1P，并通过同源重组在减数分裂期间的错位和基因转换产生。大约20%到30%的CYP21A2经典突变是30-kb的缺失，通常与空缺突变相关联。 然而，连接位点在临床上可能是相关的，因为大约3%的缺失由于连接位点的位置而保留了部分21-羟化酶活性，并与较温和的表型相关联17（图1B）。大多数患者是复合杂合，每个等位基因上有不同的突变，表型对应于较温和的基因缺陷。大约10%的CAH患者有CAH-X综合征，其特征是CAH的特征与高流动性型埃勒斯-当洛斯综合征的特征相结合，这是由于连续基因缺失破坏了CYP21A2和TNXB，通过基因分型诊断。TNXB编码的tenascin X是一种大型胞外基质蛋白，参与胶原沉积。大多数CAH-X综合征等位基因是由于单等位基因存在一个非功能性的TNXA/TNXB嵌合基因；双等位基因遗传导致更严重的症状。CAH-X的临床表型包括关节过度活动、关节痛、关节脱位、疝气和中线缺陷，其中包括心脏结构异常。 类固醇激素测量是诊断CAH的标准。由于CYP21A2位点的复杂性，二代测序不作为一线诊断测试。基因重复和缺失、CYP21A1P假基因以及一些等位基因中的多个突变使得很难将患者与携带者区分开来，通常需要父母的基因分型来确认基因型。 基因信息CYP21A2 检测过程中的常见干扰就是来自其假基因 CYP21A1P 。12&gt;NM_000500.9 CYP21A2 [organism=Homo sapiens] [GeneID=1589] [transcript=1]&gt;NR_040090.1 CYP21A1P[organism=Homo sapiens] [GeneID=1590] 基因信息可以直接NCBI上查询下载，在这里为了更方便的查看两个基因序列的相似度，在这贴一下两个基因多重比对的结果，如下：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122NM_000500.9_CYP21A2_[organism=Ho ---------------------------------------------GTCTcGCCATGCTGCNR_040090.1_CYP21A1P[organism=Ho ctggggctcttgagctataagtggcacctcagggccctgacgggcGTCTtGCCATGCTGCNM_000500.9_CYP21A2_[organism=Ho TCCTGGGCCTGCTGCTGCTGCTGCCCCTGCTGGCTGGCGCCCGCCTGCTGTGGAACTGGTNR_040090.1_CYP21A1P[organism=Ho TCCTGGGCCTGCTGCTGCTGCTGCCCCTGCTGGCTGGCGCCCGCCTGCTGTGGAACTGGTNM_000500.9_CYP21A2_[organism=Ho GGAAGCTCCGGAGCCTCCACCTCCcGCCTCTTGCCCCGGGCTTCTTGCACCTGCTGCAGCNR_040090.1_CYP21A1P[organism=Ho GGAAGCTCCGGAGCCTCCACCTCCtGCCTCTTGCCCCGGGCTTCTTGCACCTGCTGCAGCNM_000500.9_CYP21A2_[organism=Ho CCGACCTCCCCATCTATCTGCTTGGCCTGACTCAGAAATTCGGGCCCATCTACAGGCTCCNR_040090.1_CYP21A1P[organism=Ho CCGACCTCCCCATCTATCTGCTTGGCCTGACTCAGAAATTCGGGCCCATCTACAGGCTCCNM_000500.9_CYP21A2_[organism=Ho ACCTTGGGCTGCA-----------------------------------------------NR_040090.1_CYP21A1P[organism=Ho ACCTTGGGCTGCAaggtgagaggctgatctcgctctggccctcaccataggagggggcggNM_000500.9_CYP21A2_[organism=Ho --------------------------------------------------AGATGTGGTGNR_040090.1_CYP21A1P[organism=Ho aggtgacggagagggtcctctctccgctgacgctgctttggctgtctcccAGATGTGGTGNM_000500.9_CYP21A2_[organism=Ho GTGCTGAACTCCAAGAGGACCATTGAGGAAGCCATGGTCAAAAAGTGGGCAGACTTTGCTNR_040090.1_CYP21A1P[organism=Ho GTGCTGAACTCCAAGAGGACCATTGAGGAAGCCATGGTCAAAAAGTGGGCAGACTTTGCTNM_000500.9_CYP21A2_[organism=Ho GGCAGACCTGAGCCACTTACCT--------------------------------------NR_040090.1_CYP21A1P[organism=Ho GGCAGACCTGAGCCACTTACCTgtaagggccgggggcattttttctttcttaaacaaattNM_000500.9_CYP21A2_[organism=Ho ------------------------------------------------------------NR_040090.1_CYP21A1P[organism=Ho ttttttttgttagagatggggtcttgctatgttgcccaggctggtcttgaattcctggtcNM_000500.9_CYP21A2_[organism=Ho ------------------------------------------------------------NR_040090.1_CYP21A1P[organism=Ho tcaagtgatcctcccacctcggcctcaagtgggagccaccttcgggggcttccccaatccNM_000500.9_CYP21A2_[organism=Ho ------------------------------------------------------------NR_040090.1_CYP21A1P[organism=Ho tccaggtcactggaagctcttggggggcatatcttcaggagaagaagcaggtgttgaggaNM_000500.9_CYP21A2_[organism=Ho ------------------------------------------------------------NR_040090.1_CYP21A1P[organism=Ho ggcagaagaaggtcaggccctcggcttccttggtcagttcccaccctccagcccccagctNM_000500.9_CYP21A2_[organism=Ho ----------ACAAGCTGGTGTCTAgGAACTACCCGGACCTGTCcTTGGgagactacTCcNR_040090.1_CYP21A1P[organism=Ho cctcctgcagACAAGCTGGTGTCTAaGAACTACCCGGACCTGTCgTTGG--------TCtNM_000500.9_CYP21A2_[organism=Ho CTGCTCTGGAAAGCCCACAAGAAGCTCACCCGCTCAGCCCTGCTGCTGGGCATCCGTGACNR_040090.1_CYP21A1P[organism=Ho CTGCTCTGGAAAGCCCACAAGAAGCTCACCCGCTCAGCCCTGCTGCTGGGCATCCGTGACNM_000500.9_CYP21A2_[organism=Ho TCCATGGAGCCAGTGGTGGAGCAGCTGACCCAGGAGTTCTGTGAGCGCATGAGAGCCCAGNR_040090.1_CYP21A1P[organism=Ho TCCATGGAGCCAGTGGTGGAGCAGCTGACCCAGGAGTTCTGTGAGCGCATGAGAGCCCAGNM_000500.9_CYP21A2_[organism=Ho CCCGGCACCCCTGTGGCCATTGAGGAGGAATTCTCTCTCCTCACCTGCAGCATCAtCTGTNR_040090.1_CYP21A1P[organism=Ho CCCGGCACCCCTGTGGCCATTGAGGAGGAATTCTCTCTCCTCACCTGCAGCATCAaCTGTNM_000500.9_CYP21A2_[organism=Ho TACCTCACCTTCGGAGACAAGATCAAGGAcGACAACTTAATGCCTGCCTATTACAAATGTNR_040090.1_CYP21A1P[organism=Ho TACCTCACCTTCGGAGACAAGATCAAGGAgGACAACTTAATGCCTGCCTATTACAAATGTNM_000500.9_CYP21A2_[organism=Ho ATCCAGGAGGTGTTAAAAACCTGGAGCCACTGGTCCATCCAAATTGTGGACGTGATTCCCNR_040090.1_CYP21A1P[organism=Ho ATCCAGGAGGTGTTAAAAACCTGGAGCCACTGGTCCATCCAAATTGTGGACGTGATTCCCNM_000500.9_CYP21A2_[organism=Ho TTTCTCAGGTTCTTCCCCAATCCAGGTCTCCGGAGGCTGAAGCAGGCCATAGAGAAGAGGNR_040090.1_CYP21A1P[organism=Ho TTTCTCAGGTTCTTCCCCAATCCAGGTCTCCGGAGGCTGAAGCAGGCCATAGAGAAGAGGNM_000500.9_CYP21A2_[organism=Ho GAtCACAtCGtGGAGAtGCAGCTGAGGCAGCACAAGGAGAGCCTcGTGGCAGGCCAGTGGNR_040090.1_CYP21A1P[organism=Ho GAcCACAaCGaGGAGAaGCAGCTGAGGCAGCACAAGGAGAGCCTgGTGGCAGGCCAGTGGNM_000500.9_CYP21A2_[organism=Ho AGGGACATGATGGACTACATGCTCCAAGGGGTGGCGCAGCCGAGCATGGAAGAGGGCTCTNR_040090.1_CYP21A1P[organism=Ho AGGGACATGATGGACTACATGCTCCAAGGGGTGGCGCAGCCGAGCATGGAAGAGGGCTCTNM_000500.9_CYP21A2_[organism=Ho GGACAGCTCCTGGAAGGGCACgTGCACATGGCTGCAGTGGACCTCCTGATCGGTGGCACTNR_040090.1_CYP21A1P[organism=Ho GGACAGCTCCTGGAAGGGCACtTGCACATGGCTGCAGTGGACCTCCTGATCGGTGGCACTNM_000500.9_CYP21A2_[organism=Ho GAGACCACAGCAAACACCCTCTCCTGGGCCGTGG-TTTTTTTGCTTCACCACCCTGAGATNR_040090.1_CYP21A1P[organism=Ho GAGACCACAGCAAACACCCTCTCCTGGGCCGTGGtTTTTTTTGCTTCACCACCCTGAGATNM_000500.9_CYP21A2_[organism=Ho TCAGCAGCGACTGcAGGAGGAGCTAGACCACGAACTGGGCCCTGGTGCCTCCAGCTCCCGNR_040090.1_CYP21A1P[organism=Ho TCAGCAGCGACTGtAGGAGGAGCTAGACCACGAACTGGGCCCTGGTGCCTCCAGCTCCCGNM_000500.9_CYP21A2_[organism=Ho GGTCCCCTACAAGGACCGTGCACGGCTGCCCTTGCTCAATGCCACCATCGCCGAGGTGCTNR_040090.1_CYP21A1P[organism=Ho GGTCCCCTACAAGGACCGTGCACGGCTGCCCTTGCTCAATGCCACCATCGCCGAGGTGCTNM_000500.9_CYP21A2_[organism=Ho GCGCCTGcGGCCCGTTGTGCCCTTAGCCTTGCCCCACCGCACCACACGGCCCAGCAGCATNR_040090.1_CYP21A1P[organism=Ho GCGCCTGtGGCCCGTTGTGCCCTTAGCCTTGCCCCACCGCACCACACGGCCCAGCAGCATNM_000500.9_CYP21A2_[organism=Ho CTCCGGCTACGACATCCCTGAGGGCACAGTCATCATTCCGAACCTCCAAGGCGCCCACCTNR_040090.1_CYP21A1P[organism=Ho CTCCGGCTACGACATCCCTGAGGGCACAGTCATCATTCCGAACCTCCAAGGCGCCCACCTNM_000500.9_CYP21A2_[organism=Ho GGATGAGACGGTCTGGGAGAGGCCACATGAGTTCTGGCCTGATCGCTTCCTGGAGCCAGGNR_040090.1_CYP21A1P[organism=Ho GGATGAGACGGTCTGGGAGAGGCCACATGAGTTCTGGCCTGATCGCTTCCTGGAGCCAGGNM_000500.9_CYP21A2_[organism=Ho CAAGAACTCCAGAGCTCTGGCCTTCGGCTGCGGTGCCCGCGTGTGCCTGGGCGAGCCGCTNR_040090.1_CYP21A1P[organism=Ho CAAGAACTCCAGAGCTCTGGCCTTCGGCTGCGGTGCCCGCGTGTGCCTGGGCGAGCCGCTNM_000500.9_CYP21A2_[organism=Ho GGCGCGCCTGGAGCTCTTCGTGGTGCTGACCCGACTGCTGCAGGCCTTCACGCTGCTGCCNR_040090.1_CYP21A1P[organism=Ho GGCGCGCCTGGAGCTCTTCGTGGTGCTGACCCGACTGCTGCAGGCCTTCACGCTGCTGCCNM_000500.9_CYP21A2_[organism=Ho CTCCGGGGACGCCCTGCCCTCCCTGCAGCCCCTGCCCCACTGCAGTGTCATCCTCAAGATNR_040090.1_CYP21A1P[organism=Ho CTCCGGGGACGCCCTGCCCTCCCTGCAGCCCCTGCCCCACTGCAGTGTCATCCTCAAGATNM_000500.9_CYP21A2_[organism=Ho GCAGCCTTTCCAAGTGCGGCTGCAGCCCCGGGGGATGGGGGCCCACAGCCCgGGCCAGAgNR_040090.1_CYP21A1P[organism=Ho GCAGCCTTTCCAAGTGCGGCTGCAGCCCCGGGGGATGGGGGCCCACAGCCCaGGCCAGAaNM_000500.9_CYP21A2_[organism=Ho CCAGTGATGGGGCAGGACCGATGCCAGCCGGGTACCTCAGTTTCTCCTTTATTGCTCCCGNR_040090.1_CYP21A1P[organism=Ho CCAGTGATGGGGCAGGACCGATGCCAGCCGGGTACCTCAGTTTCTCCTTTATTGCTCCCGNM_000500.9_CYP21A2_[organism=Ho TACGAACCCCTCCCCTCCCCCCTGTAAACACAGTGCTGCGAGATCGCTGGCAGAGAAGGCNR_040090.1_CYP21A1P[organism=Ho TACGAACCCCTCCCCTCCCCCCTGTAAACACAGTGCTGCGAGATCGCTGGCAGAGAAGGCNM_000500.9_CYP21A2_[organism=Ho TTCCTCCAGCGGCTGGGTGGTGAAGGACCCTGGCTCTTCTCTCGGGGCGACCCCTCAGTGNR_040090.1_CYP21A1P[organism=Ho TTCCTCCAGCGGCTGGGTGGTGAAGGACCCTGGCTCTTCTCTCGGGGCGACCCCTCAGTGNM_000500.9_CYP21A2_[organism=Ho CTCGGCAGTCATACTGGGGTGCGAGAGAGGTGGGCAGCAGCTCAGCCTCCCCCCGCTGGGNR_040090.1_CYP21A1P[organism=Ho CTCGGCAGTCATACTGGGGTGCGAGAGAGGTGGGCAGCAGCTCAGCCTCCCCCCGCTGGGNM_000500.9_CYP21A2_[organism=Ho GAGCGAAAGTTTCTTGGTCTCAGCTTCATTTCCGTGAAGGGCACCGAGAACTCGAAGCCCNR_040090.1_CYP21A1P[organism=Ho GAGCGAAAGTTTCTTGGTCTCAGCTTCATTTCCGTGAAGGGCACCGAGAACTCGAAGCCCNM_000500.9_CYP21A2_[organism=Ho TTCCAGTGGTACCAGCTCACTCCCTGGGAAAGGGGTTGTCAAGAGAGAGTCAAAGCCGGANR_040090.1_CYP21A1P[organism=Ho TTCCAGTGGTACCAGCTCACTCCCTGGGAAAGGGGTTGTCAAGAGAGAGTCAAAGCCGGANM_000500.9_CYP21A2_[organism=Ho TGTCCCATCTGCTCtTCCCGTTCCCCTTAAGGAGGTaGCTCCCAGCACTCAACCAACCTCNR_040090.1_CYP21A1P[organism=Ho TGTCCCATCTGCTCcTCCCGTTCCCCTTAAGGAGGTgGCTCCCAGCACTCAACCAACCTCNM_000500.9_CYP21A2_[organism=Ho CCCGCAGAGCTCCCTTCCTGACCCTCcGCtGCAGAGGATTGAGGCTTAATtCTGAGCTGGNR_040090.1_CYP21A1P[organism=Ho CCCGCAGAGCTCCCTTCCTGACCCTCtGCcGCAGAGGATTGAGGCTTAATcCTGAGCTGGNM_000500.9_CYP21A2_[organism=Ho cCCTTTCCAGCCAATAAATCAACTCCAGCTCCCTCTGNR_040090.1_CYP21A1P[organism=Ho tCCTTTCCAGCCAATAAATCAACTCCAGCTCCCTCTG references[1]. Congenital Adrenal Hyperplasia due to 21-Hydroxylase Deficiency]]></content>
      <categories>
        <category>遗传病</category>
        <category>chr6</category>
        <category>常隐</category>
      </categories>
      <tags>
        <tag>遗传病</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[git submodule的场景和应用]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-09.%E7%89%88%E6%9C%AC%E6%8E%A7%E5%88%B6%2FGit-6.%E5%AD%90%E6%A8%A1%E5%9D%97%E7%AE%A1%E7%90%86%E4%BB%93%E5%BA%93%E5%BC%95%E7%94%A8%2F</url>
    <content type="text"><![CDATA[随着git的使用，我们开始习惯性的用git进行项目的管理，但是作为天选牛马，我们往往需要参与不同的项目，这些项目中的某些功能是相同的。这时出于解耦和模块代码独立运维管理的需求，我们一般会针对一些功能实现进行独立的子仓库管理，当然除了自己的，我们可能也会用到一些第三方的仓库。 他们的一个共同特点是有一个独立的仓库进行管理，同时作为一个相对独立的功能实现在我们的仓库中被使用。 丑陋一点的方法把所需模块的源代码都拷贝到主项目里，这样主项目就变成一个完整的项目了。但是显而易见的，如果子仓库有一些独立的优化或性能升级，那么在我们主项目进行同步，或者我们的一些开发需要同步到子项目这些工作都会非常麻烦，甚至随着项目增加而成为灾难。 显然，我们需要一个更好的解决方案。而git submodule 模块就是为了解决这个问题而生的，submodule 让我们可以将一个 Git 仓库作为另一个 Git 仓库的子目录。 它能让你将另一个仓库克隆到自己的项目中，同时还保持提交的独立。 配置首先介绍一些配置可以在后续使用中减少键盘的敲击~~， 因为后续可能是复用比较多的配置代码，所以卸载最前面12345678910111213# 使用git status 直接显示你的子模块的更改摘要git config status.submodulesummary 1 # git diff 显示子模块的区别，不需要额外输入 --submodulegit config --global diff.submodule log# 推送主项目更改时，会自动检查所有子仓库的更改是否已经推送，并自动尝试推送未推送的子项目仓库git config push.recurseSubmodules on-demand # 配置子模块的仓库分支git config -f .gitmodules submodule.DbConnector.branch stable# 让 Git 为每个拥有 --recurse-submodules 选项的命令（除了 git clone） 总是递归地在子模块中执行。git config submodule.recurse true 使用添加子模块我们首先将一个已存在的 Git 仓库添加为正在工作的仓库的子模块。 你可以通过在 git submodule add 命令后面加上想要跟踪的项目的相对或绝对 URL 来添加新的子模块。 在本例中，我们将会添加一个名为 “DbConnector” 的库。1234567$ git submodule add https://github.com/chaconinc/DbConnectorCloning into 'DbConnector'...remote: Counting objects: 11, done.remote: Compressing objects: 100% (10/10), done.remote: Total 11 (delta 0), reused 11 (delta 0)Unpacking objects: 100% (11/11), done.Checking connectivity... done. 添加子模块并指定目录默认情况下，子模块会将子项目放到一个与仓库同名的目录中，本例中是 “DbConnector”。 如果你想要放到其他地方，那么可以在命令结尾添加一个不同的路径(eg:scripts/DbConnector)。1git submodule add https://github.com/chaconinc/DbConnector scripts/DbConnector 如果这时运行 git status，你会注意到几件事。123456789$ git statusOn branch masterYour branch is up-to-date with 'origin/master'.Changes to be committed: (use "git reset HEAD &lt;file&gt;..." to unstage) new file: .gitmodules new file: DbConnector .gitmodules我们通过 git submodule add 拉取子仓库后，会增加了一个 .gitmodules 文件。 该配置文件保存了项目 URL 与已经拉取的本地目录之间的映射：123[submodule &quot;DbConnector&quot;] path = DbConnector url = https://github.com/chaconinc/DbConnector 如果有多个子模块，该文件中就会有多条记录。 要重点注意的是，该文件也像 .gitignore 文件一样受到版本控制。 它会和该项目的其他部分一同被拉取推送。 这就是克隆该项目的人知道去哪获得子模块的原因。 变更子仓库的配置 修改 .gitmodules 文件中对应模块的url属性; 使用 git submodule sync 命令，将新的URL更新到文件.git/config；再使用命令初始化子模块：git submodule init最后使用命令更新子模块：git submodule update 子仓库信息在 git status 输出中列出的另一个是项目文件夹记录。 如果你运行 git diff，会看到类似下面的信息：12345678$ git diff --cached DbConnectordiff --git a/DbConnector b/DbConnectornew file mode 160000index 0000000..c3f01dc--- /dev/null+++ b/DbConnector@@ -0,0 +1 @@+Subproject commit c3f01dc8862123d317dd46284b05b6892c7b29bc 虽然 DbConnector 是工作目录中的一个子目录，但 Git 还是会将它视作一个子模块。当你不在那个目录中时，Git 并不会跟踪它的内容， 而是将它看作子模块仓库中的某个具体的提交。 如果你想看到更漂亮的差异输出，可以给 git diff 传递 --submodule 选项，可以在diff的时候，展示更详细的模块信息（远程仓库和版本变动）。 1234567891011$ git diff --cached --submodulediff --git a/.gitmodules b/.gitmodulesnew file mode 100644index 0000000..71fc376--- /dev/null+++ b/.gitmodules@@ -0,0 +1,3 @@+[submodule "DbConnector"]+ path = DbConnector+ url = https://github.com/chaconinc/DbConnectorSubmodule DbConnector 0000000...c3f01dc (new submodule) 提交整体而言，提交带子仓库的模块和普通的提交推送所需要的操作是一样的.123456789# 提交时，会看到类似下面的信息：$ git commit -am 'added DbConnector module'[master fb9093c] added DbConnector module 2 files changed, 4 insertions(+) create mode 100644 .gitmodules create mode 160000 DbConnector#最后，推送这些更改：$ git push origin master 只不过注意看的化会发现 DbConnector 记录的文件类型是 160000 模式。 这是 Git 中的一种特殊模式，它本质上意味着你是将一次提交记作一项目录记录的，而非将它记录成一个子目录或者一个文件。 子项目获取直接初始化并拉取子项目这是一个更为简单的方式， 在执行 git clone 命令时，传递 –recurse-submodules 选项，它就会自动初始化并更新仓库中的每一个子模块， 包括可能存在的嵌套子模块。123456789101112131415$ git clone --recurse-submodules https://github.com/chaconinc/MainProjectCloning into 'MainProject'...remote: Counting objects: 14, done.remote: Compressing objects: 100% (13/13), done.remote: Total 14 (delta 1), reused 13 (delta 0)Unpacking objects: 100% (14/14), done.Checking connectivity... done.Submodule 'DbConnector' (https://github.com/chaconinc/DbConnector) registered for path 'DbConnector'Cloning into 'DbConnector'...remote: Counting objects: 11, done.remote: Compressing objects: 100% (10/10), done.remote: Total 11 (delta 0), reused 11 (delta 0)Unpacking objects: 100% (11/11), done.Checking connectivity... done.Submodule path 'DbConnector': checked out 'c3f01dc8862123d317dd46284b05b6892c7b29bc' 单独拉取子项目如果准备单独拉取子项目内容，那么我们开始的初始项目克隆和正常项目是一致的，只不过我们拉取下来的项目中，子项目只会是一个空目录，里面没有完整的子项目内容。我们需要运行两个命令：git submodule init 用来初始化本地配置文件,git submodule update 则从该项目中抓取所有数据并检出父项目中列出的合适的提交。或者我们也可以使用 git submodule update --init 命令一次性完成初始化并更新操作。如果还要初始化、抓取并检出任何嵌套的子模块， 请使用简明的git submodule update --init --recursive。 12345678910$ git submodule initSubmodule 'DbConnector' (https://github.com/chaconinc/DbConnector) registered for path 'DbConnector'$ git submodule updateCloning into 'DbConnector'...remote: Counting objects: 11, done.remote: Compressing objects: 100% (10/10), done.remote: Total 11 (delta 0), reused 11 (delta 0)Unpacking objects: 100% (11/11), done.Checking connectivity... done.Submodule path 'DbConnector': checked out 'c3f01dc8862123d317dd46284b05b6892c7b29bc' 现在 DbConnector 子目录是处在和之前提交时相同的状态了。 子项目同步我们有一份包含子模块的项目时，我们将会同时在主项目和子模块项目上与队员协作。 从远程仓库中同步子项目项目中使用子模块的最简模型，就是只使用子项目并不时地获取更新，而并不在你的检出中进行任何更改。 我们来看一个简单的例子。 直接操作子仓库如果想要在子模块中查看新工作，可以进入到目录中运行 git fetch 与 git merge，合并上游分支来更新本地代码。这和我们在主项目中运行 fetch 与 merge 的方式完全相同。 通过submodule管理你不想在子目录中手动抓取与合并，那么还有种更容易的方式。 运行 git submodule update --remote，Git 将会进入子模块然后抓取并更新。默认会更新所有的子模块。1234567# 更新指定子模块，默认使用 master 分枝$ git submodule update --remote DbConnector # DbConnector 子模块跟踪仓库的 “stable” 分支，git config submodule.DbConnector.branch stable # 指定远程仓库的分枝，仅为自己配置，不会推送相关分值信息git config -f .gitmodules submodule.DbConnector.branch stable # 指定远程仓库的分枝，相关分支信息会推送的远程同步给其他用户。git submodule update --remote # 更新子仓库内容 子仓库冲突处理你和其他人同时改动了一个子模块引用，那么可能会遇到一些问题。 也就是说，如果子模块的历史已经分叉并且在父项目中分别提交到了分叉的分支上，那么你需要做一些工作来修复它。123456789101112$ git pullremote: Counting objects: 2, done.remote: Compressing objects: 100% (1/1), done.remote: Total 2 (delta 1), reused 2 (delta 1)Unpacking objects: 100% (2/2), done.From https://github.com/chaconinc/MainProject 9a377d1..eb974f8 master -&gt; origin/masterFetching submodule DbConnectorwarning: Failed to merge submodule DbConnector (merge following commits not found)Auto-merging DbConnectorCONFLICT (submodule): Merge conflict in DbConnectorAutomatic merge failed; fix conflicts and then commit the result. Git 在这里指出了子模块历史中的两个分支记录点，并且不能自动进行合并。这是你可以通过 git diff 查看具体的差异12345$ git diffdiff --cc DbConnectorindex eb41d76,c771610..0000000--- a/DbConnector+++ b/DbConnector eb41d76 是我们的子模块中大家共有的提交，而 c771610 是上游子仓库进行的提交。这时候我们需要单独进行子仓库的冲突处理（处理方式和正常仓库的处理一样，只是我们需要进入到子仓库进行操作。123456789101112# 进入子仓库的目录，后续相关操作都是针对子仓库的$ cd DbConnector# 将子仓库回退到父项目记录的节点，$ git rev-parse HEADeb41d764bccf88be77aced643c13a7fa86714135# 拉取子项目的远程最新的代码，并创建一个分支 try-merge$ git branch try-merge c771610# 进行代码的合并，将远程新代码合并到本地主分支上，(DbConnector) $ git merge try-merge 当然可能在merge的时候，会存在一些文件冲突，这时候，就是正常的处理冲突文件，提交推送即可。123$ vim $conflict_File$ git add $conflict_File$ git commit -am 'merged our changes' 父项目同步拉取1234567891011121314151617181920212223242526272829303132# 默认拉取父项目$ git pullFrom https://github.com/chaconinc/MainProject fb9093c..0a24cfc master -&gt; origin/masterFetching submodule DbConnectorFrom https://github.com/chaconinc/DbConnector c3f01dc..c87d55d stable -&gt; origin/stableUpdating fb9093c..0a24cfcFast-forward .gitmodules | 2 +- DbConnector | 2 +- 2 files changed, 2 insertions(+), 2 deletions(-)# 会显示其中子项目的更改，但是不会自动同步子项目中的更改$ git status On branch masterYour branch is up-to-date with 'origin/master'.Changes not staged for commit: (use "git add &lt;file&gt;..." to update what will be committed) (use "git checkout -- &lt;file&gt;..." to discard changes in working directory) modified: DbConnector (new commits)Submodules changed but not updated:* DbConnector c87d55d...c3f01dc (4): &lt; catch non-null terminated lines &lt; more robust error handling &lt; more efficient db routine &lt; better connection routineno changes added to commit (use "git add" and/or "git commit -a") 默认情况下，git pull 命令会递归地抓取子模块的更改，如上面第一个命令的输出所示。 然而，它不会 更新 子模块。这点可通过 git status 命令看到，它会显示子模块“已修改”，且“有新的提交”。 此外，左边的尖括号（&lt;）指出了新的提交，表示这些提交已在 MainProject 中记录，但尚未在本地的 DbConnector 中检出。 为了完成更新，你需要运行 git submodule update：1234567$ git submodule update --init --recursiveSubmodule path 'vendor/plugins/demo': checked out '48679c6302815f6c76f1fe30625d795d9e55fc56'$ git status On branch masterYour branch is up-to-date with 'origin/master'.nothing to commit, working tree clean 推送如果我们的子模块目录中有一些改动。如果我们针对父项目进行提交并推送但并不推送子模块上的改动，其他人因为他们无法得到依赖的子模块改动，他们在执行项目的时候会遇到麻烦，为了避免这个问题，可以让 Git 在推送到主项目前检查所有子模块是否已推送。 git push 命令接受可以设置为 check（检查是否推送） 或 on-demand（自动推送未推送的子模块） 的 --recurse-submodules 参数。 如果任何提交的子模块改动没有推送那么 check 选项会直接使 push 操作失败。on-demand会尝试推送子模块1234# 推送主项目更改时，会自动检查所有子仓库的更改是否已经推送git push --recurse-submodules=checkgit push --recurse-submodules=on-demand 子模块的技巧性操作遍历有一个 foreach 子模块命令，它能在每一个子模块中运行任意命令。 如果项目中包含了大量子模块，这会非常有用。git submodule foreach &quot;$git_command&quot; 可以让我们对任何一个子模块执行某个相同的命令。1234567891011121314151617# 将所有子模块的更改进行暂存$ git submodule foreach 'git stash'Entering 'CryptoLibrary'No local changes to saveEntering 'DbConnector'Saved working directory and index state WIP on stable: 82d2ad3 Merge from origin/stableHEAD is now at 82d2ad3 Merge from origin/stable# 对所有子模块都创建一个新的分支，并切换过去。$ git submodule foreach 'git checkout -b featureA'Entering 'CryptoLibrary'Switched to a new branch 'featureA'Entering 'DbConnector'Switched to a new branch 'featureA'# 查看所有子项目的改动git submodule foreach 'git diff' 别名可以看到很多命令的参数非常长，尤其是在使用子模块后，额外增加了一些参数，但是 git 并不支持将这些选项作为它们的默认选项。我们可以通过为这些命令设置别名来简化我们的操作命令，这里有一些例子。123$ git config alias.sdiff &apos;!&apos;&quot;git diff &amp;&amp; git submodule foreach &apos;git diff&apos;&quot;$ git config alias.spush &apos;push --recurse-submodules=on-demand&apos;$ git config alias.supdate &apos;submodule update --remote --merge&apos; 从子目录切换到子模块有时候我们想把项目中的一部分工作或者功能，独立抽象出来，形成一个单独的模块（改成子仓库单独保存），那么我们需要先取消暂存对应子模块代码所在的目录（否则会由于目录以存在产生冲突）。 然后才可以添加子模块12$ git rm -r subFunction$ git submodule add https://github.com/****/subFunction 现在假设你在一个分支下做了这样的工作。 当你切换到其他分支（文件还在子目录而非子模块中）时——你会得到这个错误，所以在切换时，需要进行强制（-f）切换分支 ：12345678910# 正常切换由于模块目录在目标分支是父项目的一个目录存在冲突会报错$ git checkout mastererror: The following untracked working tree files would be overwritten by checkout: CryptoLibrary/Makefile CryptoLibrary/includes/crypto.h ...Please move or remove them before you can switch branches.Aborting# 强制切换分支git checkout -f master 子模块的注意事项切换分支在切换分支使用 git checkout 命令时，最好添加 --recurse-submodules(git ≥2.14),来确保父项目分支切换后，子项目会和分支实际情况保持一致。通过 git config submodule.recurse true 设置 submodule.recurse 选项， 告诉 Git（&gt;=2.14）总是使用 --recurse-submodules。 如上所述，这也会让 Git 为每个拥有 --recurse-submodules 选项的命令（除了 git clone） 总是递归地在子模块中执行。如果使用了子模块，直接进行相关的设置吧，旧版本的一些潜在风险笔记多，但是我们似乎在这也不需要再去过度纠结旧版本的问题了，毕竟此刻，刚开始的你和我可以直接使用新版本。 参考起始就是边看边抄了一遍 git doc:git-submodule]]></content>
      <categories>
        <category>编程拾慧</category>
        <category>Git</category>
      </categories>
      <tags>
        <tag>培训</tag>
        <tag>Git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pipeline-container-docker-demo01.md]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-04.pipeline%2Fpipeline-container-docker-demo.%E6%90%BA%E5%B8%A6%E8%80%85%2F</url>
    <content type="text"><![CDATA[12345678910111213141516FROM condaforge/mambaforgeMAINTAINER liubo (liubo4@genomics.com)# conda configureRUN conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/ &amp;&amp;\ conda config --add channels defaults &amp;&amp;\ conda config --add channels bioconda &amp;&amp;\ # install python conda install python=3.9 pandas numpy pyvcf3 pyfaidx pysam pyyaml openpyxl xlrd scikit-allel scikit-learn matplotlib seaborn joblib tqdm -y &amp;&amp;\ # install vep=110 &amp; perl library conda install -c bioconda ensembl-vep==110 -y &amp;&amp;\ conda install perl-Excel-Writer-XLSX perl-Spreadsheet-XLSX perl-Spreadsheet-ParseExcel -y &amp;&amp;\ perl -MCPAN -e "install Spreadsheet::XLSX" &amp;&amp;\ conda update -c conda-forge perl-compress-raw-zlib -y &amp;&amp; \ # install R &amp; library conda install -c conda-forge r-base &amp;&amp; r-ggplot2 -y]]></content>
      <categories>
        <category>pipeline</category>
        <category>镜像</category>
        <category>container</category>
      </categories>
      <tags>
        <tag>流程开发</tag>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python-装饰器-0.常见装饰器汇总.md]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-02.python%2FPython-%E8%A3%85%E9%A5%B0%E5%99%A8-0.%E5%B8%B8%E8%A7%81%E8%A3%85%E9%A5%B0%E5%99%A8%E6%B1%87%E6%80%BB%2F</url>
    <content type="text"><![CDATA[什么是装饰器装饰器其实就是一个以函数或对象作为参数并返回一个替换函数或对象的可执行函数，即装饰器是一个函数，它以函数作为参数，返回另一个函数。 可以先来看一个装饰器修饰函数的例子，理解装饰器的作用。1234567891011121314151617# 定义一个装饰器函数，虽然没有任何用处，但是它确实是一个最简单的装饰器，并且可以用的很好。def warp(obj): return obj# 使用装饰器的方法1：@warp # 等价于 foo = warp(foo)def foo(): print('hello decorator!') foo() # =&gt; hello decorator!# 使用装饰器的方法2：def foo(): print('hello decorator!') foo = warp(foo)foo() # =&gt; hello decorator! 再函数定义前添加 @warp 和将 foo = warp(foo) 一个使用装饰器函数处理后的新函数重新定义到原命名是一样的。通过最简单的代码示例，我们可以理解，装饰器其实就是接受了一个函数（对象），并且返回了一个函数（对象）的函数（可调用对象）。 上面是函数的例子，接下来我们可以看一个用装饰器修饰类的例子：123456789101112# 定义一个装饰器def warp(obj): obj.name = 'python' return obj# 使用装饰器装饰一个类（Bar）：@warp # =&gt; Bar = warp(Bar)class Bar(object): def __init__(self): pass print(Bar.name) # =&gt; python 在这个示例中，我们通过一个装饰器，对一个类的属性进行了默认配置。 通过上述两个示例，我们可以理解装饰器的作用，它可以让我们再不用对原类/函数进行任何修改的情况下，给类增加新的功能。 函数相关装饰器@mcp.tool()`@mcp.tool()` 装饰器，用于将一个函数包装成MCP服务工具，用于在LLM中配置并提供相应的服务功能。123456789101112131415161718192021222324from mcp.server.fastmcp import FastMCP# 初始化MCP服务实例mcp = FastMCP("My_Mcp_Server")@mcp.tool()def sum(a: int,b: int) -&gt; int: """ 两个数相加求和 Args: a (int): 整数a b (int): 整数b Returns: int: 两个数的和 """ return a+bif __name__ == '__main__': # 启动MCP服务，使用标准输入输出作为传输方式 mcp.settings.host = "0.0.0.0" mcp.settings.port = 8000 mcp.run(transport='stdio') print("MCP服务已启动，等待工具调用...") @atexit.register`@atexit.register `装饰器用于注册要在程序终止时执行的函数。该函数可用于在程序即将退出时执行任何任务，无论是由于正常执行还是意外错误。1234567891011121314import atexit# Register the exit_handler function@atexit.registerdef exit_handler(): print("退出程序。可以在此处执行清理任务。")# Rest of the programdef main(): print("main函数内部。") # Your program logic goes here.if __name__ == "__main__": main() @enum.unique@enum.unique 装饰器位于 enum 模块中，用于确保枚举的所有成员的值是唯一的。这有助于防止意外创建具有相同值的多个枚举成员，这可能会导致混乱和错误。如果发现重复值，则会引发 ValueError。1234567891011121314151617181920from enum import Enum, unique@uniqueclass VehicleType(Enum): CAR = 1 TRUCK = 2 MOTORCYCLE = 3 BUS = 4# Attempting to create an enumeration with a duplicate value will raise a ValueErrortry: @unique class DuplicateVehicleType(Enum): CAR = 1 TRUCK = 2 MOTORCYCLE = 3 # BUS and MOTORCYCLE have duplicate values BUS = 3except ValueError as e: print(f"Error: &#123;e&#125;") 在上面的实现中，“BUS”和“MOTORCYCLE”具有相同的值“3”。结果，@unique 装饰器引发 ValueError 并显示一条消息，指示已找到重复值。我们不能多次使用相同的 key，也不能将相同的值分配给不同的成员。通过这种方式，它有助于防止多个枚举成员出现重复值。 @singledisptach@singledisptach 装饰器用于创建通用函数。它允许我们定义具有相同名称但不同参数类型的函数的不同实现。当我们希望代码针对不同的数据类型表现不同时，它特别有用。 1234567891011121314151617181920212223242526272829from functools import singledispatch# Decorator@singledispatchdef display_info(arg): print(f"Generic: &#123;arg&#125;")# Registering specialized implementations for different types@display_info.register(int)def display_int(arg): print(f"Received an integer: &#123;arg&#125;")@display_info.register(float)def display_float(arg): print(f"Received a float: &#123;arg&#125;")@display_info.register(str)def display_str(arg): print(f"Received a string: &#123;arg&#125;")@display_info.register(list)def display_sequence(arg): print(f"Received a sequence: &#123;arg&#125;")# Using the generic function with different typesdisplay_info(39)display_info(3.19)display_info("Hello World!")display_info([2, 4, 6]) 1234Received an integer: 39Received a float: 3.19Received a string: Hello World!Received a sequence: [2, 4, 6] 在上面的实现中，我们首先使用@singledisptach装饰器开发了通用函数display_info()，然后分别注册了其int、float、string和list的实现。输出显示了不同数据类型的 display_info() 的工作情况。 类相关装饰器@property可以将对象内定义的方法转换为属性（不可变更值）,同时调用的方法发生变化（方法的调用需要后面添加”()”,属性不能添加） 12345678910111213141516171819202122232425DataSet(object): @property def method_with_property(self): ##含有@property return 15 def method_without_property(self): ##不含@property return 15l = DataSet()print(l.method_with_property) # 加了@property后，可以用调用属性的形式来调用方法,后面不需要加（）。print(l.method_without_property()) #没有加@property , 必须使用正常的调用方法的形式，即在后面加()class DataSet(object): def __init__(self): self._images = 1 self._labels = 2 #定义属性的名称 @property def images(self): #方法加入@property后，这个方法相当于一个属性，这个属性可以让用户进行使用，而且用户有没办法随意修改。 return self._images @property def labels(self): return self._labelsl = DataSet()#用户进行属性调用的时候，直接调用images即可，而不用知道属性名_images，因此用户无法更改属性，从而保护了类的属性。print(l.images) # 加了@property后，可以用调用属性的形式来调用方法,后面不需要加（）。 @classmethodclassmethod 修饰符对应的函数不需要实例化，不需要 self 参数，但第一个参数需要是表示自身类的 cls 参数，可以来调用类的属性，类的方法，实例化对象等。1234567891011class A(object): bar = 1 def func1(self): print ('foo') @classmethod def func2(cls): print ('func2') print (cls.bar) cls().func1() # 调用 foo 方法A.func2() # 不需要实例化 @staticmethodpython staticmethod 返回函数的静态方法(该方法不强制要求传递参数)。如下的代码，实例声明了静态方法 *toDashDate()* ，从而可以不实例化调用该方法 Dates.toDashDate()，当然也可以实现实例化使用 Dates().toDashDate()。 静态方法的用例有限，因为与类方法或类中的任何其他方法一样，它们无法访问类本身的属性。但下述事一种情况。 12345678910111213141516171819class Dates: def __init__(self, date): self.date = date def getDate(self): return self.date @staticmethod def toDashDate(date): return date.replace("/", "-")date = Dates("15-12-2016")dateFromDB = "15/12/2016"dateWithDash = Dates.toDashDate(dateFromDB) # 不需要实例化，直接调用类函数if(date.getDate() == dateWithDash): print("Equal")else: print("Unequal") 在这里，我们有一个 Dates 类，它仅适用于带破折号的日期。然而，在我们之前的数据库中，所有日期都以斜杠表示。为了将斜杠日期转换为破折号日期，我们在 Dates 中创建了一个实用函数 toDashDate 。它是一个静态方法，因为它不需要访问 Dates 本身的任何属性，只需要参数。我们还可以在类外部创建 toDashDate ，但由于它仅适用于日期，因此将其保留在 Dates 类中是合乎逻辑的。 Function相关的装饰器检测函数性能返回函数运行的时间1234567891011121314151617from functools import wrapsimport timedef function_timer(fn): """璁＄??????????淇??楗板????@20220511 By liubo""" @wraps(fn) def measure_time(*args, **kwargs): t1 = time.time() result = fn(*args, **kwargs) t2 = time.time() print(f"@timefn: &#123;fn.__name__&#125; took &#123;t2 - t1: .5f&#125; s") return result return measure_time@function_timerdef start(self): ****]]></content>
      <categories>
        <category>编程拾慧</category>
        <category>python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python-进阶-错误和异常]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-02.python%2FPython-%E8%BF%9B%E9%98%B6-%E9%94%99%E8%AF%AF%E5%92%8C%E5%BC%82%E5%B8%B8%2F</url>
    <content type="text"><![CDATA[这里的错误和异常不是特指语法错误，而是指程序在运行时发生的错误。即使语句或表达式在语法上是正确的，当尝试执行它时也可能会导致错误。这个错误可能是由于输入数据的问题，运行环境的问题，还可能是参数配置的问题等等，随着代码编写的多了，应用环境的变化，各种各样的奇奇怪怪问题都会出现，我们会开始意识到，错误处理的重要性。毕竟比运行中断更可怕的，任务以错误的方式运行完了。 随着处理的问题情况增加，我们必须要开始有意识的对代码进行一些错误处理。来增强代码的鲁棒性/异常识别能力。 try except基础版本使用 try except 我们更多的可能是类似if else的用法，重点关注try的代码能不能成功运行，但是实际上except 是可以识别不同的异常类型的，对应的我们也可以进行不同的处理。 12345678910111213import systry: f = open('myfile.txt') s = f.readline() i = int(s.strip())except OSError as err: # 文件读写错误 print("OS error:", err)except ValueError: # 值转换错误 print("Could not convert data to an integer.")except Exception as err: # 捕获其他的异常 print(f"Unexpected &#123;err=&#125;, &#123;type(err)=&#125;") raise python的标准异常类型： 异常名称 描述 BaseException 所有异常的基类 SystemExit 解释器请求退出 KeyboardInterrupt 用户中断执行(通常是输入^C) Exception 常规错误的基类 StopIteration 迭代器没有更多的值 GeneratorExit 生成器(generator)发生异常来通知退出 StandardError 所有的内建标准异常的基类 ArithmeticError 所有数值计算错误的基类 FloatingPointError 浮点计算错误 OverflowError 数值运算超出最大限制 ZeroDivisionError 除(或取模)零 (所有数据类型) AssertionError 断言语句失败 AttributeError 对象没有这个属性 EOFError 没有内建输入,到达EOF 标记 EnvironmentError 操作系统错误的基类 IOError 输入/输出操作失败 OSError 操作系统错误 WindowsError 系统调用失败 ImportError 导入模块/对象失败 LookupError 无效数据查询的基类 IndexError 序列中没有此索引(index) KeyError 映射中没有这个键 MemoryError 内存溢出错误(对于Python 解释器不是致命的) NameError 未声明/初始化对象 (没有属性) UnboundLocalError 访问未初始化的本地变量 ReferenceError 弱引用(Weak reference)试图访问已经垃圾回收了的对象 RuntimeError 一般的运行时错误 NotImplementedError 尚未实现的方法 SyntaxError Python 语法错误 IndentationError 缩进错误 TabError Tab 和空格混用 SystemError 一般的解释器系统错误 TypeError 对类型无效的操作 ValueError 传入无效的参数 UnicodeError Unicode 相关的错误 UnicodeDecodeError Unicode 解码时的错误 UnicodeEncodeError Unicode 编码时错误 UnicodeTranslateError Unicode 转换时错误 Warning 警告的基类 DeprecationWarning 关于被弃用的特征的警告 FutureWarning 关于构造将来语义会有改变的警告 OverflowWarning 旧的关于自动提升为长整型(long)的警告 PendingDeprecationWarning 关于特性将会被废弃的警告 RuntimeWarning 可疑的运行时行为(runtime behavior)的警告 SyntaxWarning 可疑的语法的警告 UserWarning 用户代码生成的警告 raise(引发异常) raise 语句允许程序员强制发生指定的异常。 raise 的唯一参数指示要引发的异常。这必须是异常实例或异常类（从 BaseException 派生的类，例如 Exception 或其子类之一）。如果传递异常类，它将通过不带参数调用其构造函数来隐式实例化：例如： 12raise ValueError() # 不传递参数的异常类，会进行隐式实例化raise NameError('HiThere') # 传递参数将异常类进行实例化。 定义清理操作(final)try 语句还有另一个可选子句，旨在定义在所有情况下都必须执行的清理操作。如果存在 finally 子句，则 finally 子句将作为 try 语句完成之前的最后一个任务执行。无论 try 语句是否产生异常， finally 子句都会运行。在实际应用程序中， finally 子句对于释放外部资源（例如文件或网络连接）非常有用，无论资源的使用是否成功。示例如下： 123456789101112131415161718192021222324def divide(x, y): try: result = x / y except ZeroDivisionError: print("division by zero!") else: print("result is", result) finally: print("executing finally clause")&gt;&gt;&gt; divide(2, 1)result is 2.0executing finally clause&gt;&gt;&gt; divide(2, 0)division by zero!executing finally clause&gt;&gt;&gt; divide("2", "1")executing finally clauseTraceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt; File "&lt;stdin&gt;", line 3, in divideTypeError: unsupported operand type(s) for /: 'str' and 'str' 以下几点讨论发生异常时更复杂的情况： 如果在执行 try 子句期间发生异常，则该异常可以由 except 子句处理。如果 except 子句未处理异常，则在执行 finally 子句后将重新引发异常。 执行 except 或 else 子句期间可能会发生异常。同样，在执行 finally 子句后会重新引发异常。 如果 finally 子句执行 break 、 continue 或 return 语句，则不会重新引发异常。 如果 try 语句到达 break 、 continue 或 return 语句，则 finally 子句将在之前执行到 break 、 continue 或 return 语句的执行。 如果 finally 子句包含 return 语句，则返回值将是 finally 子句的 return 语句中的值，而不是值来自 try 子句的 return 语句。 丰富异常当创建异常以便引发时，通常使用描述已发生的错误的信息对其进行初始化。在某些情况下，在捕获异常后添加信息很有用。为此，异常有一个方法 add_note(note) ，它接受一个字符串并将其添加到异常的注释列表中。标准回溯渲染包括异常之后的所有注释（按添加顺序）123456789101112try: raise TypeError('bad type')except Exception as e: e.add_note('Add some information') e.add_note('Add some more information') raiseTraceback (most recent call last): File "&lt;stdin&gt;", line 2, in &lt;module&gt;TypeError: bad typeAdd some informationAdd some more information]]></content>
      <categories>
        <category>编程拾慧</category>
        <category>python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python-进阶-class]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-02.python%2FPython-%E8%BF%9B%E9%98%B6-class%2F</url>
    <content type="text"><![CDATA[class中的function类型在Python中，存在三种不同的方法类型：静态方法、类方法和实例方法。每一种都有不同的特点，应该在不同的情况下使用。 静态方法（Static Methods）Python 中的静态方法必须通过用 @staticmethod 修饰来创建。这让 Python 知道该方法应该是静态的。静态方法的主要特点是无需实例化类即可调用它们。这些方法是独立的，这意味着它们无法访问该类中的任何其他属性或调用任何其他方法。适用于不需要类中的其他信息（attribute 或function）可以独立完成功能的场景。 例如，如果您有一个名为 Math 的类，并且有一个名为 factorial 的方法，那么您可能不需要特定的实例来调用该方法。因此，您可以使用静态方法。12345678910class Math: @staticmethod def factorial(number): if number == 0: return 1 else: return number * MethodTypes.factorial(number - 1) factorial = MethodTypes.factorial(5)print(factorial) 类方法（Class Method）类方法必须使用装饰器 @classmethod 创建，并且这些方法与静态方法共享一个特征，无需实例化类,即可以在没有类实例的情况下调用它们。差异取决于访问其他方法和类属性但不访问实例属性的能力。 实例方法(Instance Methods)仅当类已实​​例化时才能调用此方法。一旦创建了该类的对象，就可以调用实例方法，并可以通过保留字self访问该类的所有属性。实例方法能够创建、获取和设置新的实例属性以及调用其他实例、类和静态方法。]]></content>
      <categories>
        <category>编程拾慧</category>
        <category>python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据预处理方法-局部加权回归(LOWESS)]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-20.MachineLearn%2F1005.%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86-%E5%B1%80%E9%83%A8%E5%8A%A0%E6%9D%83%E5%9B%9E%E5%BD%92LOWESS%2F</url>
    <content type="text"><![CDATA[趋势预测：对于预测问题，回归中最简单的线性回归，是以线性的方法拟合出数据的趋势。但是对于有周期性，波动性的数据，并不能简单以线性的方式拟合，否则模型会偏差较大，而局部加权回归（lowess）能较好的处理这种问题。可以拟合出一条符合整体趋势的线，进而做预测。 平滑问题：局部加权回归（lowess）也能较好的解决平滑问题。在做数据平滑的时候，会有遇到有趋势或者季节性的数据，对于这样的数据，我们不能使用简单的均值正负3倍标准差以外做异常值剔除，需要考虑到趋势性等条件。使用局部加权回归，可以拟合一条趋势线，将该线作为基线，偏离基线距离较远的则是真正的异常值点。实际上，局部加权回归（Lowess）主要还是处理平滑问题的多，因为预测问题，可以有更多模型做的更精确。但就平滑来说，Lowess很直观而且很有说服力。 算法讲解2.1 算法思想局部加权回归（Lowess）的大致思路是：以一个点 x 为中心，向前后截取一段长度（步长）为 frac 的数据，对于该段数据用权值函数 w 做一个加权的线性回归，记 ( x , y^ ) 为该回归线的中心值，其中 y^ ​为拟合后曲线对应值。对于所有的 n 个数据点则可以做出 n 条加权回归线，每条回归线的中心值的连线则为这段数据的Lowess曲线。 2.2 参数讲解在这个思路中，能提取出的可调参数则是： 长度 frac ，应该截取多长的作为局部处理，frac 为原数据量的比例； 权值函数 w ，使用什么样的权值函数 w 合适； 迭代次数 it ，在进行一次局部回归后，是否需要迭代，再次做回归； delta回归间隔，是否真的每个点都需要算一次加权回归，能否隔 delta 距离算一次，中间没算的用插值替换即可。 在了解了算法算法的大致思想和可调参数以后，你可以马上上手使用statsmodels.api.nonparametric.lowess了。使用方法如下： 123import statsmodels.api as smlowess = sm.nonparametric.lowessresult = lowess(y, x, frac=0.2, it=3, delta=0.0) 但是，在statsmodels中，你会发现：1、权值 w 函数你是不可调的；2、在用了 delta 之后，插值函数你是不可调的。总之就是，黑盒子，很多你都不可调的。而且它还有一个非常严重的问题，具体可看本文3.3效果对比。接下来就是看相关文档，了解思路，之后，你可以自己写一个lowess，而且速度不会慢。 相关文档，statsmodels就给出了比较权威的参考：《Cleveland, W.S. (1979) “Robust Locally Weighted Regression and Smoothing Scatterplots”. Journal of the American Statistical Association 74 (368): 829-836.》。文章是《鲁棒性的加权回归》，即原始加权基础上迭代，增加鲁棒性。网上还有一些其他的lowess讲解，我看了，和这个不太一样，可以选择性阅读。 2.3 权值函数理解了lowess之后，可以明白，其实权值函数并不是固定的，只要满足一定的规则条件即可（当然并也非强制），条件如下： W(x) &gt; 0 for ∣x∣&lt;1 ; W(−x)=W(x); W(x) is a nonincreasing function for x ⩾ 0 ; W(x) = 0 for ∣x∣⩾1 选择该类函数大致思路是：希望 W(x) 大于0，且作用域为[-1,1]，且为对称函数，该函数对于中间(0处)的值较大，两边(-1\1)处值较小。选择思路是，中间的权值较高，对于加权回归的影响较大；[-1,1]的原因是，对于任意不规则的数据段，可以压缩映射到[-1,1]，方便处理。 权值函数如，B函数（二次函数）：B(x) = ${ (1− x^2)^2 , for ∣x∣ &lt; 1 \choose 0 , for ∣x∣⩾ 1}$ W函数（三次函数）：W(x) = ${ (1−|x∣^3 )^3, for ∣x∣&lt;1 \choose 0 , for ∣x∣⩾ 1}$ 二次与三次函数的区别在于，三次函数对于周围权值降速更快，在平滑最初时候效果好，且适用于大多数分布，但增加了残差的方差。对于权值函数选取，第一次迭代适用W函数（三次函数），之后迭代使用B函数（二次函数）。权值函数的使用： 使用权值函数 W(x)； 数据段 [d1​,d2​]，映射成 [−1,1] 对应的坐标； 带入函数 W(x)，计算出每个点对应的 $w_i$​； 使用加权回归得出模型：$ \hat{Y}=X(X^{T}w X)^{-1}X^{T}wY$ 2.4 回归迭代上面讲了权值函数的选取和使用，提到了迭代，这里讲解怎么迭代。首先，原值为 y，预测值为 $\hat{y} $​，残差为$ e=y-\hat{y} $​，记 s 为 $|e_{i}|$的中位数。鲁棒性的权值调整附加值 $ \delta_{k}=W(\frac{e_{k}}{6s})$ ，修正后的权值为 $ \delta_{k}w_{k} $​。迭代过程为：1.使用W函数（三次函数）作为权值函数，求出 $w_{i} $​。2.将 $ w_{i} $​带入加权回归计算出 $ \hat{y}$​3.求出$ e=y-\hat{y}$​和 $ s$4.以B函数作为修正权值函数，求出$ \delta_{k}=B(\frac{e_{k}}{6s})$，计算出$ \delta_{k}w_{k} $5.将 $\delta_{k}w_{k} $​作为修正权值，重复2、3、4步骤 该迭代没有明确的终止条件，据大量实验得知，原文中提到是2次迭代就基本收敛了，我做实验的时候，3次左右基本收敛，根文中描述差不多。 2.5 间隔回归，中间插值在使用局部加权回归的时候，如果每个点都使用一次加权回归，则会比较耗时，所以有了，对于部分点使用加权回归，而未使用加权回归的点采用插值法处理，速度会增快很多，同时不会影响太大效果。可以每间隔$ delta $个点使用一次加权回归，中间点采用：线性插值、二次插值、三次插值等方法。statsmodels推荐当数据点$ N &gt; 5000 $的时候，选择 $ delta = 0.01 * N $而我一般是设置 $ delta=4 $，同时，我的插值函数是线性插值，因为我不希望插值出负数，大家可以根据自己需求选择。 2.6 其他参数长度 $ frac $比例，文章给出的适用比例是：0.49，statsmodels默认的是：0.666。同时，文章中还有一个参数是，使用的是多项式加权回归进行拟合，最高次是 $ d $，而进行讨论给出的结论是 $ d=1 $时候，即为线性回归，适合基本大多数场景。所以本博客一开始就使用线性加权回归介绍，感兴趣的可以去看看原文。 三、实验效果3.1 效果上面讲了整个思路，和详细的参数意义等，看了之后写出代码应该不难。这里作个伪代码总结：12345678910data_x # x轴数据data_y # y轴数据map(x in data_x): # 对每个x x_list = getRange(x, data_x, frac) # 以x为中心，按照frac的比例截取数据 w = calFuncW(x_list) # 以w函数计算权值函数 y_hat = weightRegression(x_list, data_y[x_list], w) # 计算y_hat for it in iters: # 迭代iters次 err, s, new_w = calNewWeight(y_hat, data_y[x_list], B) # 使用B函数计算,迭代的new_w y_hat = weightRegression(x_list, data_y[x_list], new_w) # 使用new_w计算y_hat result = y_hat[x] # 取结果中心点 3.2 效率上面自写的lowess，在spark上运行，executors15g4cores，计算30W8数据，每条数据90的数据条长度，10分钟即可完成（包括数据读写），个人觉得效率已经挺高的。 3.3 效果对比与statsmodels进行对比，右图为使用statsmodels的函数的结果，左图为自己根据文章写的运行结果，都是同一数据。 可以看出，对于一些特殊数据，直接使用statsmodels会有非常意外的结果，而且随着迭代进行，依旧不会收敛。 有的甚至会出现平滑结果很意外，如右图绿线最低部。 随着迭代进行，产生了更加剧烈的震荡。所以有时候，成熟的代码块可能会给我们节省时间，但是也会把我们莫名其妙的带到沟里？ 参考资料[1] 局部加权回归]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>Machine-Learning</tag>
        <tag>数据预处理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[配置go test]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-08.Go%2FGO-05.go_test%2F</url>
    <content type="text"><![CDATA[当我们完成我们的正常代码逻辑编写以后，后续会涉及到各种需求的剔除，解决这些需求，我们就无法避免需要进行代码的更改，因此为了确保后续更改不会发生一些不可预料的问题，所以添加一些测试，从而保证代码的稳定性是必要的。之前用python、perl的时候，经常都是自己凑数据，直接运行代码，也许作用类似，但是不可否认手动测试非常的繁琐，所以慢慢的向自动化测试的方向靠拢。在这里记录一下 使用go单元测试的效果。go的单元测试有自己的命名约定（*_test.go） 和对应的 Go 的 testing 包和go test 命令，您可以快速编写和执行测试。当然前提时我们已经针对我们的业务需求，完成了相关代码的开发工作。 这里我们以 max.go 为例，代码如下：123456789package main/* max函数返回两个数的最大值 */func max(num1, num2 int) int &#123; if num1 &gt; num2 &#123; return num1 &#125; else &#123; return num2 &#125;&#125; 创建测试代码接下来我们构建我们的测试代码 首先我们在项目代码目录下，创建对应的测试代码max_test.go(测试代码文件名必须使用 _test.go 为后缀才能被 go test 识别，前缀不一定和go编码文件名一致） 测试代码文件和项目代码文件在同一个目录下，应该属于相同的包。 撰写我们的测试函数，需要注意测试函数的命名规则，测试函数必须以 Test 开头，后面接测试函数名。（非TestName 格式的函数不会被执行） 代码中，我们可以构建相应的输入文件，执行对应的函数，然后对函数返回值进行校验，确定什么样的返回值测试通过(t.Log,或不做任何处理），什么样的返回值测试不通过（t.Error)。 测试代码示例（clib_test.go）如下： 12345678910111213package main // 注意这里需要和项目代码包名一致import ( "testing")// 自定义的测试函数 函数名称必须以 Test 开头func TestClib(t *testing.T) &#123; if max(1, 2) == 2 &#123; t.Log("max(1,2)==2 pass") &#125; else &#123; t.Error("max(1,2)==2 fail") &#125;&#125; 完成测试代码编写后，我们使用 go test 命令来运行我们的测试代码，测试代码分析结果符合预期，返回结果如下 1234567PS D:\Git_Repo\Work_temp\Go-test&gt; go test -v=== RUN TestClib clib_test.go:9: max(1,2)==2 pass--- PASS: TestClib (0.00s)PASSok github.com/test 1.175sPS D:\Git_Repo\Work_temp\Go-test&gt; 如果测试失败，示例如下（这里我们把测试代码的 max(1,2)==2 改成了 max(1,2)==1 12345678PS D:\Git_Repo\Work_temp\Go-test&gt; go test -v=== RUN TestClib clib_test.go:11: max(1,2)==1 fail--- FAIL: TestClib (0.00s)FAILexit status 1FAIL github.com/test 0.431sPS D:\Git_Repo\Work_temp\Go-test&gt; 这样完成单元测试以后，后续我们需要对我们的代码进行验证时，就可以通过构建 *test.go 文件进行代码的单元测试啦。 ReferenceGO Documentation]]></content>
      <categories>
        <category>编程拾慧</category>
        <category>GO</category>
      </categories>
      <tags>
        <tag>GO</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Database-HGMD]]></title>
    <url>%2F01.%E7%9F%A5%E8%AF%86-04.Database%2FDatabase-HGMD%2F</url>
    <content type="text"><![CDATA[HGMD突变分类 疾病引起突变（DM）：至少在一篇文献中被报道为导致疾病的病理性疾病突变。 疑似疾病引起突变（DM？）：在文献中被报道为致病突变，但作者表示存疑或文献之后的证据显露突变的损害性存疑。 疾病关联功能多态性（DFP）：人群中等位基因频率&gt;1%的突变，且同某类疾病或表型具有重要关联性，并被证实具有功能型影响。 疾病关联多态性（DP）：人群中等位基因频率&gt;1%的突变，且同某类疾病或表型具有重要关联性，但并未被证实具有功能型影响。 体内或体外功能多态性（FP）：在文献中被报道能够影响结构，功能，基因表达或基因产物，但并未被报道和疾病具有关联性。]]></content>
      <categories>
        <category>知识沉淀</category>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>数据库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[健康管理师-基础知识-章节习题-03_04]]></title>
    <url>%2F05.%E5%A4%87%E8%80%83-02.%E5%81%A5%E5%BA%B7%E7%AE%A1%E7%90%86%E5%B8%88%2F%E5%81%A5%E5%BA%B7%E7%AE%A1%E7%90%86%E5%B8%88-%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86-%E7%AB%A0%E8%8A%82%E4%B9%A0%E9%A2%98-03_04%2F</url>
    <content type="text"><![CDATA[第三章 预防医学基础知识 分值占比：5%~6% 难度：★★★ 健康管理从本质上来讲遵循的就是一种预防医学的思路，在疾病尚未发生之前提前做好干预措施使得中断疾病的发展、发生或蔓延，这一章的内容需要考生详细的掌握。 第一节 预防医学概述 一级预防包括针对健康个体的措施和针对整个公众的社会措施， 针对健康个体的措施主要有：①个人的健康教育；②有组织地进行预防接种；③做好婚前检查；④做好妊娠和儿童期的卫生保健；⑤某些疾病的高危个体服用药物来预防疾病的发生，即化学预防。 第一级预防（又称为病因预防），是在疾病尚未发生时针对致病因素(或危险因素)采取措施，也是预防疾病和消灭疾病的根本措施。 人类健康四大基石“合理膳食、适量运动、戒烟限酒、心理平衡”是一级预防的基本原则。其主要措施有：健康教育、保护环境、合理营养、良好生活方式和体育锻炼、特殊保护。 任何疾病，无论致病因子是否明确，都应强调第一级预防。 二级预防：发现疾病通过普查，筛检，定期健康检查，高危人群重点项目检查及设立专科门诊。 其主要措施有：普查、自我检查、定期体检、密切观察癌前期病变。产前检查属于二级预防 第二级预防(又称“三早”预防)，即早发现、早诊断、早治疗。是在疾病早期，症状体征尚未表现出来或难以觉察，及早发现、诊断疾病，及时给予适当的治疗，有更大的机会实现治愈。或者如果疾病无法治愈，可以通过治疗阻止疾病发展到更严重的阶段或至少减缓发展进程，减少对更复杂的治疗措施的需要。 达到三早的根本办法是宣传，提高医务人员诊断水平和建立社会性高灵敏而可靠的疾病监测系统。 三级预防：对已患某些疾病的人，采取及时的，有效的治疗措施，防止病情恶化，预防并发症和伤残。 主要措施有：对症治疗缓解症状;预防疾病进一步恶化；预防急性事件的发生和复发；预防合并症和残疾；加强康复措施。 三级预防（又称临床预防或疾病管理），三级预防可以防止伤残和促进功能恢复，提高生存质量，延长寿命，降低病死率。主要是对症治疗和康复治疗措施， 预防医学是医学的一门应用学科，它以个体和确定的群体为工作对象，目的是保护、促进和维护健康，预防疾病、失能和早逝。特点是工作对象包括个体和确定的群体，研究方法上注重微观和宏观，更重视促进人群健康的社会性措施，相比临床医学具有更大的人群健康效益。研究的对象是：健康和无症状的患者。 临床预防服务中无症状的“患者”是指:因某一较轻的疾病来看病，但存在将来有可能发生严重疾病危险因素的就医患者。 社区公共卫生是人群健康策略和原则在社区水平上的具体应用。 健康影响因素包括：①环境因素（社会经济环境：社会制度与政策、文化背景和社会支持网络、个人收入和社会地位、物质环境）；②行为和生活方式；③生物遗传因素；④卫生服务。 社会环境因素包括： 1.个人收入与社会地位、2.文化背景和社会支持网络、3.社会制度与政策、4.就业和工作条件； 物质环境因素有：生产环境中产生的有害物质 工业生产中燃料如煤炭和石油的燃烧是最重要的大气污染来源。 社区是指若干社会群体或社会组织聚集在某一地域里所形成的一个生活上相互关联的大集体；社区的边界是比较模糊的；以社区为范围开展健康促进和疾病防治有非常明确的针对性； 行政区域是人为设定的，其边界是固定的。 行政区和社区在地域上是不重叠的。 预防医学的学科体系可分为：流行病学；医学统计学；环境卫生科学；社会与行为科学；卫生管理学。其中预防医学学科的基础方法学有：流行病学，医学统计学。基础教材45页 第二节 临床预防服务 临床预防服务的内容通常有求医者的健康咨询，健康筛检，化学预防，预防接种.p51 卫生计生监督协管服务包括：食源性疾病及相关信息报告,饮用水卫生安全巡查，学校卫生服务，非法行医和非法采供血信息报告，计划生育相关信息报告。 有关健康筛查： 建议对65岁以上老年人进行青光眼筛查 超重者加上男性腰围≥90cm、女性腰≥85cm，则肥胖并发症的危险性增加 建议35-65岁男性定期测定血胆固醇 建议40岁以上的妇女每年接受1次乳房临床物理检查 定期测量血压:社区医生对辖区内35岁以上常住居民，每年免费测量一次血压。 临床预防服务的内容包括:健康咨询,健康筛检,化学预防,预防接种。 卫生监督协管服务包括：（一）食源性疾病巡查及信息报告。（二）饮用水卫生巡查及信息报告。（三）学校卫生巡查及信息报告。（四）无证行医和非法采供血巡查及信息报告。（五）计划生育卫生巡查及信息报告。（六）职业卫生巡查及信息报告。 第三节 公共卫生服务 在重点地区对高危人群实施肺炭疽疫苗，钩端螺旋体疫苗应急接种。根据传染病控制需要，开展乙型肝炎，麻疹，脊髓灰质炎等疫苗强化免疫或补充免疫，群体性接种工作和应急接种工作。 职业病的五个特点：病因明确，病因与疾病之间一般存在剂量一反应关系，群体发病重点在治疗群体，早期诊断 发现越早 疗效越好，重在预防。 预防接种的目的：提高人群免疫水平。 社区公共卫生实施的原则包括：以健康为中心，以人群为对象，以需求为导向，多部门合作，人人参与。 发现甲类传染病（包括：霍乱，鼠疫）和乙类传染病中的肺炭疽，传染性非典型肺炎，埃博拉出血热，人感染禽流感寨卡病毒病，黄热病，拉沙热，列谷热，西尼罗病毒等应按有关要求与2小时内报告，发现其他乙类丙类传染病患者，疑似患者和规定报告的传染病病源携带者，应与24小时内报告。 医疗机构发现甲类传染病时，按照有关规定要求，对传染病患者，疑似患者采取隔离，医学观察等措施。 城乡居民健康档案管理的重点人群以0-6岁儿童，孕产妇，慢性病患者，严重精神障碍患者，肺结核患者等人群为重点。不含青少年。 国家基本公共卫生服务项目(第三版)包括内容有： 健康教育 预防接种 孕产妇健康管理 高血压患者健康管理 健康档案管理 老年人健康管理 2型糖尿病患者健康管理 严重精神障碍患者管理 肺结核患者健康管理 中医药健康管理等。 0~6岁儿童健康管理 我国职业病分为10大类132个病种包括：职业性尘肺病，职业性皮肤病，职业性眼病，职业性耳鼻喉口腔疾病，职业性化学中毒，物理因素所致职业病，职业性放射性疾病，职业性传染病，职业性肿瘤，其他。 我国已将石棉所致肺癌、间皮瘤苯所致白血病，砷所致肺癌、皮肤癌等明确为职业性恶性肿瘤。 国家基本公共卫生服务的内容： 城乡居民健康档案管理 孕产妇健康管理服务 高血压患者健康管理服务 2型糖尿病患者健康管理服务，1型糖尿病患者健康管理不属于基本公共卫生服务 老年人健康管理服务 工作有关疾病的概念：如果职业因素不是疾病发生和发展的唯一直接因素，而是诸多因素之一，并且职业因素影响了健康，通过控制有关职业因素可使所患疾病得到控制和缓解。 高血压患者的健康管理服务： 辖区内35岁及以上人群进行高血压筛查，35岁及以上原发性高血压患者每年至少提供4次面对面随访，根据患者情况进行分类干预，对原发性高血压患者结合随访每年进行1次健康检查。 突发公共卫生事件的叙述正确的是：突然发生、重大食物中毒、重大传染病疫情、群体性不明原因疾病。一般影响范围广，严重程度大。 第四章 常见慢性病 在常见慢性病的管理中，高血压、冠心病、慢性阻塞性肺疾病等的考点每年都会涉及到。另外在专业技能考试案例中，案例的主人公往往也都有着慢性疾病困扰，所以考生要对此章节非常熟悉。比如高血压的发病原因、高血压的后果、高血压的衡量标准等重要知识点一定能要掌握，基本上是每年必考的考点。这一章非常重要，需要考生集中精力的复习。 第一节 概述 慢性非传染性疾病简称“慢性病”，指一类病程漫长，无传染性，不能自愈，目前也几乎不能被治愈的疾病，其主要特点包括：①病因复杂，其发病与不良行为和生活方式密切相关；②起病隐匿，潜伏期较长，没有明确的起病时间；③病程较长，随着疾病的发展，表现为功能进行性受损或失能；④难以治愈，疾病一旦发生，表现为不可逆转，很难彻底治愈；⑤预后较差，疾病后期致残率和致死率高。 慢性病的主要危险因素包括：吸烟；过量饮酒；不合理膳食；缺乏身体活动、病原体感染；⑤其他因素：包括不良心理精神因素、自然环境和社会环境因素等，如空气污染。遗传因素也是慢性病的危险因素，属于不可改变的。“最”主要因素，不选其他的。 慢性病主要的社会危害有两点：①慢性病严重危害居民健康；②慢性病不断加重经济负担。 第二节 恶性肿瘤 我国恶性肿瘤的主要危险因素依次为吸烟、HBV感染、膳食不合理及职业危害等。其中吸烟是多种恶性肿瘤主要或重要的危险因素。 流行病学及实验研究资料表明，乙型肝炎病毒与肝癌发病相关。 我国目前进行的肿瘤早期筛查有：胃癌筛查、肺癌筛查、女性两癌筛查（乳腺癌、宫颈癌）、大肠癌。有研究提示，对乙型肝炎病毒感染者，恰当使用甲胎蛋白测定，有可能降低肝癌死亡率，因此可考虑在相应的高发区特定的人群中测定甲胎蛋白筛查肝癌。 第三节 高血压 高血压是一种以动脉血压持续升高为特征的进行性心血管损害性疾病，是全球人类最常见的慢性病，是冠心病、脑血管病、慢性肾脏疾病发生和死亡的最主要的危险因素。但它并不是由不良生活方式导致的最终结果，而是中间危险因素。 临床上高血压诊断标准： 经非同日3次测量血压，收缩压≥140mmHg和（或）舒张压≥90mmHg。 白大衣高血压是指患者到医疗机构测量血压高于140/90mmHg，但动态血压24小时平均值&lt;130/80mmHg或家庭自测血压值&lt;135/85mmHg。 隐性高血压是指患者到医疗机构测量血压&lt;140/90mmHg，但动态血压24小时平均值高于130/80mmHg或家庭自测血压值高于135/85mmHg。 诊室血压是指患者在医疗单位由医护人员测量的血压。目前，高血压诊断一般以诊室血压为准。 国内外几乎所有研究均证实，高血压是脑出血和脑梗死最重要的危险因素。高血压病最常见的并发症是：导致脑卒中。 高血压发病的危险因素包括：①高钠、低钾饮食；②体重超重和肥胖；③饮酒；④其他危险因素：遗传、性别、年龄、工作压力过重、心理因素、高脂血症等。其中不可改变的因素为：年龄、性别、种族、遗传等。 高血压是冠心病、脑血管病、慢性肾脏疾病发生和死亡的最主要的危险因素。 类别 收缩压(高压)(mmHg) 舒张压(低压) 理想血压 &lt;120 &lt;80 正常血压 &lt;130 &lt;85 正常高值 130-139 85-89 级高血压(轻度) 140-159 90-99 2级高血压(中度) 160-179 100-109 3级高血压(重度) &gt;180 &gt;110 单纯收缩期高血压 &gt;140 &lt;90 收缩和舒张等级不一致时，以高级别为准。 第四节 2型糖尿病 糖尿病是由多种病因引起的代谢紊乱，其特点是慢性高血糖，伴有胰岛素分泌不足和（或）作用障碍，导致碳水化合物、脂肪、蛋白质代谢紊乱，造成多种器官的慢性损伤、功能障碍甚至衰竭。如：冠心病、高血压、肾衰竭、糖尿病足及致盲等。 2型糖尿病主要是由遗传因素和环境因素引起外周组织胰岛素抵抗和胰岛素分泌缺陷，导致机体胰岛素相对不足或绝对不足，使葡萄糖摄取利用减少，从而引发高血糖，导致糖尿病。 糖尿病诊断标准： 2010年，ADA指南已将HbA1C≥6.5%作为糖尿病诊断标准之一。因为糖化血蛋白小于6.5%也不能除外糖尿病，需进一步行糖耐量检查。p73 WHO1999年标准为：糖尿病症状+任意时间血浆葡萄糖水平≥11.1mmol/L或;空腹（至少空腹8小时）血浆葡萄糖水平≥7.0mmol/L或；OGTT试验中，餐后两小时血浆葡萄糖水平≥11.1mmol/L。正常标准： 空腹血糖 3.9-6.1mmol/L 且糖负荷后 2h 血糖＜7.8mmol/L。 2型糖尿病具有更强的遗传倾向，中国人2型糖尿病的遗传度为51.2% ~ 73.8%，一般高于60%，而1型糖尿病的遗传度为44.4%~53.7%，低于60%，可见两型的遗传是各自独立的。 2型糖尿病的危险因素包括：①遗传因素；②肥胖（或超重）；③身体活动不足；④膳食因素；⑤早期营养；⑥糖耐量受损；⑦胰岛素抵抗；⑧高血压及其他易患因素。 按照世界卫生组织及国际糖尿病联盟专家组的建议，糖尿病可分为1型、2型、其他特殊类型及妊娠糖尿病4种。 糖化血红蛋白作为筛查糖尿病高危人群和诊断糖尿病的一种方法，其结果稳定，不受进食时间及短期生活方式改变的影响，变异性小，检查不受时间限制，患者依从性好。但是检测费用不低 胰岛素抵抗是指机体对一定量的胰岛素的生物学反应低于预期正常水平的一种现象，常伴有高胰岛素血症。胰岛素抵抗是2型糖尿病高危人群的重要特征之一。空腹胰岛素水平高的人更容易发展为IGT或2型糖尿病。 如果IGT（糖耐量损害）伴有以下因素，即原空腹血糖≥5.0mmol/L，餐后2小时血糖≥9.4mmol/L，BMI&gt;25，腹部肥胖和空腹胰岛素水平增加等，更易转化为糖尿病。 年度评估包括糖尿病患者建档动态管理情况、糖尿病管理开展情况、糖尿病患者转入转出执行情况、疾病预防控制机构和综合医院对社区卫生服务机构业务指导和培训情况。 （2）阶段性评估(每3-5年进行1次)主要包括满意程度、糖尿病及其危险因素流行现状的了解情况。 第五节 冠状动脉粥样硬化性心脏病 冠状动脉粥样硬化性心脏病，简称冠心病，又称缺血性心脏病，是由于冠状动脉发生严重粥样硬化性狭窄或阻塞，或在此基础上合并痉挛，以及血栓形成，引起冠状动脉供血不足、心肌缺血或梗死的一种心脏病。1979年，世界卫生组织将冠心病分为5型：无症状性心肌缺血；心绞痛；心肌梗死；缺血性心肌病；猝死。 冠心病的危险因素包括：①高血压；②血脂异常和高胆固醇血症；③超重和肥胖；④糖尿病；⑤生活方式：吸烟、饮食、身体活动；⑥多种危险因素的联合作用；⑦其他：家族史等。 近10余年来趋于将冠心病分为急性冠脉综合征和慢性冠脉病两大类。 典型心绞痛的特点有： 诱因： 常由于身体活动、情绪激动、饱餐、寒冷或心动过速而诱发，也可发于夜间； 典型部位：为胸骨体上中段的后方，也可在心前区，常放射至左肩，内侧臂至小指及无名指，或至颈部、咽部、下颌骨，少数可放射于其他不典型部位或放射部位疼痛更显著。心前区疼痛范围如手掌大小、界限不清； 性质：压迫、紧缩或发闷，有时有窒息和濒死感，疼痛可轻可重，重者伴焦虑、冷汗；④ 持续时间即环节：疼痛出现后，常逐渐加重，1~5分钟而自行缓解，偶尔可长达15分钟，休息或舌下含化硝酸甘油而缓解。 心肌梗死的临床特定： 急性心肌梗死临床症状差异极大，有1/3的患者，发病急骤，极为严重，未及医院就已死于院外；另有1/4～1/3的患者无自觉症状或症状很轻未就诊。其部位及放射部位与心绞痛相同，持续时间持久，多在半小时至几个小时或更长，休息和含化硝酸甘油不能缓解，常需要使用麻醉性镇痛剂。 其诊断依据为典型的临床表现、特征性心电图改变和血清酶学的升高。 目前，诊断冠状动脉狭窄的金标准，仍为冠状动脉造影检查。 缺乏身体活动的人患冠心病的危险是正常活动量者的1.5~2.4倍。 发病率是指一定时期内特定人群中某病新病例出现的频率。新发的冠状动脉事件包括急性心肌梗死和冠心病。 第六节 脑卒中(中风) 脑卒中是指一组发病急骤的脑血管病，后者含义更广，包括中枢神经系统的所有动脉和静脉系统的病变。 脑卒中的危险因素包括：①高血压；②心脏病；③糖尿病；④血脂异常；⑤吸烟；⑥饮酒；⑦颈动脉狭窄；⑧肥胖；⑨其他危险因素：高同型半胱氨酸血症、代谢综合征、缺乏体育活动、饮食营养不合理、口服避孕药、促凝危险因素（血小板聚集率、纤维蛋白原、凝血因子Ⅶ等）。国内外几乎所有研究都证明，高血压史脑出血和脑梗死最重要的危险因素。 定义:脑卒中(Stroke)是脑中风的学名，是一种突然起病的脑血液循环障碍性疾病。又叫脑血管意外。是指在脑血管疾病的病人，因各种诱发因素引起脑内动脉狭窄，闭塞或破裂，而造成急性脑血液循环障碍，临床上表现为一过性或永久性脑功能障碍的症状和体征。 中国多中心脑卒中亚型调查结果表明:蛛网膜下腔出血占18%，脑出血17.1%~39.4%，脑梗死45.5%~75.9%。 脑梗死也称缺血性脑卒中，脑部血液循环障碍，缺血、缺氧引起局部脑组织的缺血性坏死或软化，出现相应的神经功能缺损。通常分为脑血栓形成、脑栓塞和腔隙性脑梗死。 脑出血 蛛网膜下腔出血占 脑梗死的临床特征主要有：①多数在安静时急性起病，活动时起病者以心源性脑梗死多见，部分病例在发病前可有短暂性脑缺血发作；②病情多在几小时或几天内达到高峰，脑栓塞起病尤为急骤，一般数秒至几分钟内达到高峰，部分患者症状可进行性加重或波动；③临床表现决定于梗死灶的大小和部位，主要为局灶神经功能缺损的症状和体征。 脑出血的临床特点为：①多在情绪激动或活动时急性起病；②突发局灶性神经功能缺损症状，常伴有头痛、呕吐，可伴有血压增高、意识障碍和脑膜刺激征。 脑栓塞起病尤为急骤，一般数秒至几分钟内达到高峰。 第七节 慢性阻塞性肺疾病 慢性阻塞性肺疾病的主要症状有：①慢性咳嗽；②咳痰；③气短或呼吸困难：这是COPD的标志性症状；④喘息和胸闷；⑤全身性症状：如体重下降、食欲减退、外周肌肉萎缩和功能障碍、精神抑郁和（或）焦虑。 肺功能测定指标是诊断COPD（慢性阻塞性肺疾病）的金标准。 慢性阻塞性肺疾病的环境危险因素包括：吸烟、职业性粉尘和化学物质、空气污染及感染。吸烟为CPOD重要发病因素。 慢性阻塞性肺疾病的特点是：气道受阻、不完全可逆、进行性发展、与慢性支气管炎、肺气肿密切相关、患病率高。 慢性阻塞性肺疾病的主要症状有：①慢性咳嗽；②咳痰；③气短或呼吸困难；④喘息和胸闷；⑤全身性症状：如体重下降、食欲减退、外周肌肉萎缩和功能障碍、精神抑郁和（或）焦虑。 慢性阻塞性肺疾病（COPD）是一种以气流受限为特征的疾病，其气流受限不完全可逆、进行性发展，与肺部对香烟烟雾的等有害气体或有害颗粒的异常炎症反应有关。COPD与慢性支气管炎和肺气肿密切相关。 第八节 其他常见慢性病 导致肥胖的主要原因有：遗传因素、饮食因素、活动因素及其他因素。目前我国成人BMI≥28kg/m2为肥胖。 目前我国成人BMI的切点为：18.5≤BMI&lt;24为正常，24≤BMI&lt;27.9为超重，BMI≥28为肥胖。 腰臀比=腰围（cm)/臀围（cm)为最窄部位的腰围除以最宽部位的臀围，腰臀比男性&lt;1.0、女性&lt;0.85为正常，而腰臀比男性≥1.0、女性≥0.85为腹型肥胖。 导致肥胖的主要危险因素包括：①遗传因素；②饮食因素；③活动因素；其他因素：心理因素、社会因素、经济因素。 口腔健康已具备以下要素：没有任何疼痛和不适；良好的功能心理方面，外观正常、不影响自尊、个人满意；社会方面，不影响社会交流。]]></content>
      <categories>
        <category>考试资料,题库</category>
      </categories>
      <tags>
        <tag>健康管理师</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[健康管理师-07-营养与食品安全]]></title>
    <url>%2F05.%E5%A4%87%E8%80%83-02.%E5%81%A5%E5%BA%B7%E7%AE%A1%E7%90%86%E5%B8%88%2F%E5%81%A5%E5%BA%B7%E7%AE%A1%E7%90%86%E5%B8%88-%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86-%E7%9F%A5%E8%AF%86%E7%82%B9-07%2F</url>
    <content type="text"><![CDATA[主要来源 粮谷类食物一般含碳水化合物 60%~80%， 豆类 40%~60% 薯类含量 15%~30%， 脂肪（占比95%） 构成：由中性脂肪、甘油三酯、一分子甘油和三分子脂肪酸构成，占脂类95%。 脂肪酸 按饱和程度分类：饱和脂肪酸、单不饱和脂肪酸；多不饱和脂肪酸 按空间结构： 顺式脂肪酸 反式脂肪酸：摄入过多会导致心血管疾病风险增加。 按脂肪酸碳链长度分：长链、中链、短链脂肪酸。 蛋白质 能量系数：产能 16.7KJ = 4kcal/g ； 必需氨基酸 必须从食物中供给的氨基酸：携一两本（组）单色书来 缬氨酸、异亮氨酸、亮氨酸、苯丙氨酸、组氨酸(婴儿)、蛋氨酸、色氨酸、苏氨酸、赖氨酸。 脂溶性 维生素A：缺失导致夜盲症 维生素D：缺乏导致佝偻病 维生素E 维生素K 水溶性 维生素C B1：缺乏导致脚气病 B2 B6 B12 烟酸 泛酸 叶酸 胆碱 生物素]]></content>
      <categories>
        <category>考试资料</category>
      </categories>
      <tags>
        <tag>健康管理师</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[健康管理师-基础知识-章节习题-01_02]]></title>
    <url>%2F05.%E5%A4%87%E8%80%83-02.%E5%81%A5%E5%BA%B7%E7%AE%A1%E7%90%86%E5%B8%88%2F%E5%81%A5%E5%BA%B7%E7%AE%A1%E7%90%86%E5%B8%88-%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86-%E7%AB%A0%E8%8A%82%E4%B9%A0%E9%A2%98-01_02%2F</url>
    <content type="text"><![CDATA[第一章 健康管理概论 （分值占比：1%~2% 难度：★） 在全书中起到提纲挈领的作用，它向我们阐述了有关健康管理这门学科大致的一些概念以及它的发展趋势，在考核中占到1%~2%的权重，即大致上会出1到2个题，可能是一个单选一个多选或者两个多选。 第一节 概述 一般来说健康管理有以下三个基本步骤：①了解和掌握你的健康；②关心和评价你的健康；③改善和促进你的健康。 健康管理新的医学模式是： 生理-心理-社会 模式。 健康管理的目标包括：①完善健康和福利；②减少健康危险因素；③预防疾病高危人群患病；④易患疾病的早期诊断；⑤增加临床效用、效率；⑥避免可预防的疾病相关并发症的发生；⑦消除或减少无效或不必要的医疗服务；⑧对疾病结局作出度量并提供持续的评估和改进。不属于健康管理目标的： 提供健康产品的最佳选择、定制健康计划 健康管理的常用服务流程由以下五个部分组成：健康调查与健康体检、健康评估、个人健康咨询、个人健康管理后续服务、专项的健康及疾病管理服务。 不包括 对疾病进行早诊断、早治疗、早预防 健康管理师是从事对人群或个人健康和疾病的监测、分析、评估以及健康维护和健康促进的专业人员。其工作内容包括：采集和管理个人或群体的健康信息、评估个人或群体的健康和疾病危险性、进行个人或群体的健康咨询与指导、制定个人或群体的健康促进计划、对个人或群体进行健康教育和推广、进行健康管理相关技术的研究与开发、进行健康管理技术应用的成效评估等。不涉及 进行疾病治疗 1948年WHO宪章中首次提出三维的健康概念：“健康不仅仅是没有疾病和虚弱，而是一种身体、心理和社会的完好状态”。1989年WHO进一步完善了健康概念，指出健康应是“生理、心理、社会适应和道德方面的良好状态”。健康与疾病是一种动态平衡，是阶段性、连续性的过程。 健康行为改变的技术包括：教育、激励、训练、营销。不包括： 宣传 健康管理师是从事对人群或个人健康和疾病的监测、分析、评估以及健康维护和健康促进的专业人员。 三级健康管理师应至少具备21种能力。 我国慢性病相关危险因素流行日益严重，人群超重和肥胖患病率快速上升，膳食不合理、身体活动不足及吸烟是造成多种慢性病的三大行为危险因素。 健康管理研究与服务内容由最初单一的健康体检与生活方式指导，发展到目前的国家或国际组织全民健康促进战略规划的制定、个体或群体全面健康检测、健康风险评估与控制管理。 健康风险评估对个人的健康状况及未来患病/死亡危险性的量化评估。包括健康状态、未来患病/死亡危险量化评估3个关键词。 与一般健康教育和健康促进不同的是，健康管理过程中的健康干预是个性化的，即根据个体的健康危险因素，由健康管理师进行个体指导，设定个体目标，并动态追踪效果。 健康管理的公众理念是“病前主动防，病后科学管，跟踪服务不间断”。 健康管理服务的两大支撑点是信息技术和金融保险。 健康管理的客体是：健康人群、亚健康人群、慢性非传染性疾病早期或康复期人群。 健康管理的特点是: 标准化、足量化、个体化和系统化。 健康管理是以现代健康概念（生理、心理和社会适应能力）和新的医学模式（生理-心理-社会）以及中医治未病为指导，通过采用现代医学和现代管理学的理论、技术、方法和手段，对个体或群体整体健康状况及其影响健康的危险因素进行全面检测、评估、有效干预与连续跟踪服务的医学行为及过程。 健康管理的对象是：健康人群、亚健康人群、慢性非传染性疾病早期或康复期人群。健康管理是对健康资源进行计划、组织、指挥、协调和控制的过程，一般不涉及疾病的诊断和治疗过程，是一个提供综合性医疗卫生服务的模式，健康管理是集生命科学、管理科学和信息科学为一体的综合学科。 一般来说健康管理有以下三个基本步骤：①了解和掌握你的健康；②关心和评价你的健康；③改善和促进你的健康。 健康管理的常用服务流程由以下五个部分组成：健康调查与健康体检、健康评估、个人健康咨询、个人健康管理后续服务、专项的健康及疾病管理服务。 健康管理师三级的职业技能包括：健康监测、健康风险评估和分析、健康指导、健康危险因素干预。二级及以上还需要掌握业务指导、培训与研究。 第二节 健康管理的基本策略 健康管理的基本策略有以下六种：生活方式管理、需求管理、疾病管理、灾难性病伤管理、残疾管理和综合的群体健康管理。 需求管理的常用手段包括：寻求手术的替代疗法、帮助病人减少特定的危险因素、帮助病人采纳健康的生活方式、鼓励自我保健和干预等。 为患癌症病人提供各种服务，属于灾难性病伤管理。灾难性病伤管理是疾病管理的一个特殊类型，它关注的是“灾难性”的疾病或伤害。这里的“灾难性”是指对健康的危害十分严重，也可指其造成的医疗卫生花费巨大，常见于肿瘤、肾衰竭、严重外伤等情形。 在美国，雇主对员工实行需求管理；医疗保险机构和医疗服务机构开展疾病管理；大型企业开展残疾管理；人寿保险公司、雇主、社会福利机构提供灾难性病伤管理。 生活方式管理通过健康促进技术，如行为纠正和健康教育，来保护人们远离不良行为，减少危险因素对健康的损害，预防疾病，改善健康。与危害的严重性相对应，膳食、身体活动、吸烟、适度饮酒、精神压力等是目前对国人进行生活方式管理的重点。 残疾管理的具体目标包括：①防止残疾恶化；②注重功能性能力；③设定实际康复和返工的期望值；④详细说明限制事项和可行事项；⑤评估医学和社会心理学因素；⑥与病人和雇主进行有效沟通；⑦有需要时要考虑复职情况；⑧实行循环管理。 疾病管理的主要特点包括：①目标人群是患有特定疾病的个体；②不以单个病例和（或）其单次就诊事件为中心，而关注个体或群体连续性的健康状况与生活质量；③医疗卫生服务及干预措施的综合协调至关重要。 常见的需求管理方法包括：24小时电话就诊和健康咨询、转诊服务、基于互联网的卫生信息数据库、健康课堂、服务预约等。 影响人们的健康服务消费需求：患病率、感知到的需要、消费者选择偏好、健康因素以外的动机。需求管理实质上是通过帮助健康消费者维护自身健康和寻求恰当的健康服务，控制医疗成本，E是有关于控制医疗成本的，也会影响人们健康服务消费需求。 需求管理的目标是帮助人们维护自身健康，寻求适当卫生服务、控制费用，更有效地利用医疗保健服务。减少昂贵的、临床并非必需的医疗服务，同时改善人群的健康状况。 个人感知到的健康服务需要是影响服务利用的最重要的因素。有很多因素影响着人们感知到的需要，主要包括：个人关于疾病危险和卫生服务益处的知识、个人感知到的推荐疗法的疗效、个人评估疾病问题的能力、个人感知到的疾病的严重性、个人独立处理疾病问题的能力、以及个人对自己处理好疾病问题的信心等。 生活方式管理的特点包括：①以个体为中心，强调个体的健康责任和作用；②以预防为主，有效整合三级预防；3通常与其他健康管理策略联合进行。 第三节 健康管理的发展趋势 对生命投入的成本和效益比最高的是： 参加健康管理。 健康管理的目的是使有限的资源得到最大化的利用，即以最小的投入获得最大的健康效益。 对生命投入的成本和效益比最低的是把钱投入临终前的抢救，投入大量的金钱成本，获得相对较小的健康效益。 第四节 基本卫生保健 基本卫生保健的意义包括：充分享有健康权、促进社会经济发展、提高人人健康水平、提高精神文明水平。 基本卫生保健的原则包括：合理布局、社区参与、预防为主、适宜技术、综合利用。 基本卫生保健的特点包括：社会性、群众性、艰巨性、长期性。 基本卫生保健是指最基本的、人人都能得到的、体现社会平等权利的、人民群众和政府都能负担得起和全社会积极参与的卫生保健服务。 合理布局要求：人们接受卫生服务的机会必须均等，不能忽视边远山区、少数民族地区或城郊居民。 基本卫生保健工作的四个方面包括：促进健康、预防保健、合理治疗、社区康复。 基本卫生保健（初级卫生保健）的内容：①对当前主要卫生问题及其预防和制方法的健康教育；②改善食品供应和合理营养；③供应足够的安全卫生水平和基本环境卫生设施；④妇幼保健和计划生育；⑤主要传染病的预防接种；⑥预防和控制地方病； ⑦常见病和外伤的合理治疗；⑧提供基本药物；⑤使用一切可能的方法，通过影响生活方式、控制自然和社会心理环境，来预防和控制非传染疾病，促进精神卫生。” 第二章 临床医学基础知识 分值占比：5%~6% 难度：★★★ 健康管理在进行对象服务的时候一定会用到一些临床医学的基础概念和基础理念，所以这一章的篇幅很长，所占的分值权重也较高，有5%~6%即可能会考5~6个小题，一般情况下这几个小题会以单选题和多选题交叉分布的考核形式出现。 第一节 概述 临床医学是研究疾病的病因、诊断、治疗和预后，直接面对患者实施诊断和治疗的一组医学学科。现代临床医学的一个显著特征，是学科分科的不断细化，即专科化。研究的对象是人体。 循证医学通常的定义是“应用最多的有关信息，通过谨慎、明确和明智的确认和评估，做出医学决策的实践活动”。目前公认最为可靠的证据是来自随机对照试验的证据。 循证医学将 成本-效果分析 作为一个重要内容列入，要求对现有众多诊断、治疗或其他干预措施和临床决策，采用客观的证据予以卫生经济学评估，以尽可能少的投入满足医疗卫生保健需求，使卫生资源得到优化配置和利用。循证医学代表性的成果是大量“临床指南”的制订和实施。 随着医学研究的不断深入，医学学科也不断分化。由于人类的疾病繁多，诊断技术层出不穷，治疗方法也复杂多样。临床医生对日益增长的知识和复杂的技术，难以全面掌握，因此形成了各种临床专业学科。 临床医学分科大体上有五种建立方式：①按治疗手段建立的学科；②按治疗对象建立的学科；③按人体的系统或解剖部分建立的学科；④按病种建立的学科；⑤按诊断手段建立的学科。 临床医学的专科化发展，促进了诊断和治疗水平的提高，但也带来了一系列问题，如重治疗、轻预防，关注疾病而忽略患者，关注本专科的问题而忽略其他专科问题，难以提供连续性的照顾，以及医疗费用的急剧升高等。 自20世纪中期后，由于疾病谱的改变和人口老龄化，这些问题愈显突出，从而导致了“全科医学”和“家庭医学”的诞生。 临床医学得发展趋势主要有以下三点：①微观深入与宏观扩展；②学科体系分化与综合；③医学与高科技的结合日趋密切。 临床医学的主要特征有：临床医学研究和服务的对象是人、临床工作具有探索性、临床医学启动医学研究、临床医学检验医学成果。微观深入与宏观扩展是临床医学的发展趋势。 第二节 现代医学主要诊断方法和技术 现代医学最主要的治疗方法是： 药物治疗。药物治疗是最常用和最主要的治疗方法。 通过实验室检测下列哪项可以确定诊断：骨髓检查诊断白血病。 骨髓穿刺目前是可以诊断白血病的。单独发现肿瘤标志物升高，不能作为肿瘤诊断的依据。此外，肝功能测定、肾功能测定、大便潜血试验都不能作为肝病、肾病的诊断标准和结肠癌的诊断标准。 尿液一般检测包括：①一般性状检测：尿量、气味、外观、比重、酸碱度等；②化学检测：尿蛋白、尿糖、尿酮体、尿胆原、尿胆红素等；③尿沉渣检测：细胞、管型、结晶体等。 临床工作具有探索性：临床上面对患者，不可能在未知因素全部搞清楚后再去防治，只能探索性地最大限度缓解患者的痛苦，挽救和延长患者的生命。这是与许多应用科学的显著区别之一。 血常规检测项目包括：红细胞计数、血红蛋白测定、白细胞计数及分类计数、红细胞平均值测定和红细胞形态检测，血小板平均值测定和血小板形态检测。 体格检查是客观了解和评估患者身体状况的一系列最基本的检查方法。包括：视诊、触诊、叩诊、听诊 了解患者症状的最基本方法是： 问诊 血液学检验的目的：血液和造血组织的原发性血液病以及非造血细胞疾病所致的血液学变化检查、包括抗凝和纤溶功能的检验等，还有血型的鉴定和交叉配血试验。 目前呼吸系统、骨关节系统、消化系统疾病的首选影像学方法是： X线成像。X线成像是基于X线对人体组织的穿透性，以及不同组织由于厚度、密度差异，对X线吸收衰减不同而形成图像。 肾功能检测包括：肾小球滤过功能，常用的指标有血清肌酐测定、血尿素氮测定；肾小管重吸收、酸化、 肾小球滤过功能等功能。 诊断准确度：指某检验项目在实际使用中，所有检验结果中诊断准确结果的百分比。 诊断灵敏度：指某检验项目对某种疾病具有鉴别、确认的能力。 诊断特异性：指某检验项目确认无某种疾病的能力。 在诊断上，内镜应用最广者是消化道(消化系统) 和 支气管(呼吸系统) 的检查。 内镜是一种光学仪器，由体外经过人体自然腔道送入体内，对体内疾病进行检查。借助内镜可以直接观察到脏器内腔病变，确定其部位、范围，并可进行照相、活检及进行某些治疗。 心电图主要反映心脏激动的电学活动，因此对于各种心律失常和传导障碍的诊断分析具有肯定价值。另外，特征性的心电图改变和演变是诊断心肌梗死可靠而实用的方法。但是对于瓣膜活动、心音变化、心肌功能状态等，心电图不能提供直接判断。 超声检查的主要用途有：①检测实质性脏器的大小、形态及物理特性；②检测某些囊性器官（如胆囊、胆道）的形态、走向及功能状态；③检测心脏、大血管和外周血管的结构、功能及血流动力学状态；④检测脏器内各种占位性病变的物理特性；⑤检测积液的存在与否以及对积液量的多少作出估计；⑥产科上可确定妊娠，判断胎位、胎儿数量等；⑦在超声引导下进行穿刺做针吸细胞学或组织活检，或进行某些引流及药物注入治疗。 X线检查、CT检查、放射性核素显像均有射线的损害；MRI、超声检查图像无射线损害。 医学影像学检查包括：X线成像、CT检查、超声成像、磁共振成像（MRI）。 免疫学检验主要包括：免疫功能检查、临床血清学检查，以及肿瘤标志物等的临床免疫学检测检验。 肝功能试验包括:血清总蛋白和白蛋白/球蛋白比值测定、血清总胆红素测定、血清结合胆红素与非结合胆红素测定、血清丙氨酸氨基转移酶（ALT，旧称谷氨酸丙酮酸转移酶）和天门冬氨酸氨基转移酶（AST，旧称谷氨酸草酰乙酸转移酶）、碱性磷酸酶（ALP）、γ-谷氨酰转移酶等项目。 生化学检验是对组成机体的生理成分、代谢功能、重要脏器的生化功能、毒物分析及药物浓度监测等的临床生物化学检验。包括糖、脂肪、蛋白质及其代谢产物和衍生物的检验；血液和体液中电解质和微量元素的检验；血气分析和酸碱平衡的检验；临床酶学检验；激素和内分泌功能的检验；药物和毒物浓度检测等。尿蛋白的检验属于体液与排泄物检验。 临床实验室检查的主要内容有：①血液学检验；②体液与排泄物检验；③生化学检验；④免疫学检验；⑤病原学检验。 问诊的内容：①患者一般情况；②主诉：患者感受最主要的痛苦或最明显的症状；③现病史：此次患病后的全过程；④既往史；⑤个人史和家族史，女性还应包括月经史和生育史。 现在医学主要的诊断方法有：①问诊和病史采集；②体格检查；③实验诊断；④医学影像检查；⑤其他临床辅助检查（心电图检查、核医学检查、内镜检查）。 第三节 现代医学主要治疗方法 药物选择的原则：根据疾病的严重程度选择用药；根据药物药动学和药效学的特点选择药物；根据患者的个体差异来选择用药；根据药物的价格或效应来选择用药。 现在医学主要治疗方法包括：药物治疗、手术治疗、放射治疗、介入治疗、物理治疗和其他治疗（如生活方式干预治疗、心理治疗等）。 我国管理部门对药品的定义为：用于预防、治疗、诊断人的疾病，有目的地调节人的生理功能并规定有适应症或者功能主治、用法和用量的物质。 药物主要包括：中药材、中药饮片、中成药、化学原料药及其制剂、抗生素、生化药品、放射性药品、血清、疫苗、血液制品和诊断药品等。保健食品不属于药品。 要做到合理用药，首先要明确疾病地诊断，有选择的用药；其次，在初步确定使用哪一类药物后，要根据所选药物的药效动力学和药代动力学的特点制订合适的剂量、给药途径、疗程等。此外，要考虑可能出现的药物不良反应，最好达到个体化给药。同时也要兼顾避免发生药源性疾病。 物理疗法是应用自然界和人工的各种物理因子作用于机体，达到预防、治疗疾病和康复的方法。现代物理疗法的方法很多，包括：电疗、超声治疗、磁疗、生物反馈、音乐电疗、光疗、冷热治疗、水疗、高压氧疗法等。 药源性疾病分类包括:甲型、乙型、长期用药致病型、药后效应型。 手术后常见的并发症有：术后出血、切口感染、切口裂开、肺不张及感染、尿潴留及感染。 不涉及心理问题。 第四节 临床医学在健康管理中的应用 健康管理的学科基础涉及医学、管理学与生物信息学等领域，是相关学科专业基础知识在健康管理理论研究和实践中的应用概括，临床医学是健康管理的学科基础。 临床医学是以病人为中心，以疾病检查、诊断、治疗和康复为服务内容，以药品、诊疗设备和康复器械为服务手段,重点关注疾病的诊断和治疗。 健康管理服务的对象是健康人群、亚健康人群、慢性病风险人群和慢性病早期康复人群，而临床医学服务的对象为各种疾病的人群。 健康管理服务的主要模式是全面检测风险评估、有效干预和连续跟踪,而临床医学服务的主要模式是通过病史采集、体格检查和辅助检查确定诊断后，采用药物、手术、介入、放射和物理疗法等技术和手段实施治疗。 健康管理则是以健康为中心，以健康检测、健康评估、健康干预和健康跟踪为基本环节。]]></content>
      <categories>
        <category>考试资料,题库</category>
      </categories>
      <tags>
        <tag>健康管理师</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Office-excel-插件]]></title>
    <url>%2F03.%E5%BA%94%E7%94%A8-03.windows%2FOffice-excel-%E6%8F%92%E4%BB%B6%2F</url>
    <content type="text"><![CDATA[方方格子方方格子内置了 文本处理、批量录入、删除工具、合并转换、重复值工具、数据对比、高级排序等上百个实用功能。 比如： 聚光灯 批量删除、提取]]></content>
      <categories>
        <category>software</category>
        <category>Windows</category>
        <category>Office</category>
      </categories>
      <tags>
        <tag>环境配置</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[理解 Go 模块管理的两个文件 go.mod 与 go.sum]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-08.Go%2FGO-04.%E7%90%86%E8%A7%A3go.mod%E4%B8%8Ego.sum%2F</url>
    <content type="text"><![CDATA[1.Go Modules 的前世今生流行的现代编程语言一般都提供依赖库管理工具，如 Java 的 Maven 、Python 的 PIP、Node.js 的 NPM 和 Rust 的 Cargo 等。Go 最为一门新生代语言，自然也有其自己的库管理方式。 1.1 GOPATH在 Go 1.5 之前，Go 最原始的依赖管理使用的是 go get，执行命令后会拉取代码放入 GOPATH/src 下面。但是它是作为 GOPATH 下全局的依赖，并且 go get 还不能进行版本控制，以及隔离项目的包依赖。而随着 Go 生态圈的快速壮大，无法进行版本控制，会导致项目中的依赖库经常出现 API broken 的情况。因为依赖的库相关接口改变了，导致我们的项目更新了依赖库后编译不过，我们不得不需要修改自己的代码以便适应依赖库的最新版本。更困难的是，如果多个依赖库分别依赖第三个依赖库的第三个版本，版本冲突就出现了。依赖库冲突几乎每个编程语言都有这样的问题，甚至操作系统也有 DLL 地狱问题，所以各种编程语言都尝试使用自己的方式解决依赖库版本的问题。 1.2 Go vendorGo 1.5 版本推出了 vendor 机制。但是需要手动设置环境变量 GO15VENDOREXPERIMENT= 1，Go 编译器才能启用。从 Go1.6 起，默认开启 vendor 机制。所谓 vendor 机制，就是每个项目的根目录下可以有一个 vendor 目录，里面存放了该项目的依赖的 package。go build 的时候会先去 vendor 目录查找依赖，如果没有找到会再去 GOPATH 目录下查找。vendor 将原来放在 $GOPATH/src 的第三方包放到当前工程的 vendor 目录中进行管理。它为工程独立的管理自己所依赖第三方包提供了保证，多个工程独立地管理自己的第三方依赖包，它们之间不会相互影响。 vendor 将原来包共享模式转换为每个工程独立维护的模式，vendor的另一个好处是保证了工程目录下代码的完整性，将工程代码复制到其他Go编译环境，不需要再去下载第三方包，直接就能编译，这种隔离和解耦的设计思路是一大进步。但 vendor 也有缺点，那就是对外部依赖的第三方包的版本管理。我们通常使用 go get -u 更新第三方包。默认的是将工程的默认分支的最新版本拉取到本地，但并不能指定第三方包的版本。而在实际包升级过程中，如果发现新版本有问题，则不能很快回退，这是个问题。好在社区有很多优秀的第三方包管理工具可以解决此问题。 1.3 第三方管理工具在 Go 1.11 之前，很多优秀的第三方包管理工具起到了举足轻重的作用，弥补了 Go 在依赖管理方面的不足，比如 godep、govendor、glide、dep 等。其中 dep 拥趸众多，而且也得到了 Go 官方的支持，项目也放在 Golang 组织之下 golang/dep。但是蜜月期没有多久，2018 年 Russ Cox 经过深思熟虑以及一些早期的试验，决定 Go 库版本的方式需要从头再来，深度集成 Go 的各种工具（go get、go list等)，实现精巧的最小化版本选择算法，解决 broken API 共存等问题，所以 dep 就被废弃了，这件事还导致 dep 的作者相当的失望和数次争辩。随着历史车轮的滚滚向前，这些工具均淹没在历史长河之终，完成了它们的使命后，光荣地退出历史舞台。 1.4 Go Modules 横空出世从 2018 年 Go 1.11 开始，Go 官方推出了 Go Modules。为了保持向后兼容，Go 官方旧的依赖管理方式依然存在。启用 Go Modules 需要显示通过设置一个环境变量 GO111MODULE=on。在之后的 go 1.12 正式推出后，Go Modules 成为默认的依赖管理方式。先前，我们的库都是以 package 来组织的，package 以一个文件或者多个文件实现单一的功能。一个项目包含一个package 或者多个 package。Go modules 就是一个统一打版和发布的 package 的集合，在项目根文件下有 go.mod 文件定义 module path 和依赖库的版本，还有一个 go.sum 的文件，该文件包含特定依赖包的版本内容的散列哈希值。一般我们项目都是单 module 的形式，项目根目录下包含 go.mod 和 go.sum 文件，子文件夹定义 package，或者主文件夹也是一个 package。但是一个项目也可以包含多个 module，只不过这种方式不常用而已。 2.go.mod 文件go modules 最重要的是 go.mod 文件的定义，它用来标记一个 module 和它的依赖库以及依赖库的版本。会放在 module 的主文件夹下，一般以 go.mod 命名。一个 go.mod 内容类似下面的格式：1234567891011121314151617181920212223242526272829module github.com/dablelv/go-huge-utilgo 1.17replace github.com/coreos/bbolt =&gt; ../rrequire ( github.com/cenk/backoff v2.2.1+incompatible github.com/edwingeng/doublejump v0.0.0-20200330080233-e4ea8bd1cbed github.com/go-sql-driver/mysql v1.5.0 github.com/spf13/cast v1.4.1 github.com/stretchr/testify v1.7.0 golang.org/x/text v0.3.2)require ( github.com/davecgh/go-spew v1.1.1 // indirect github.com/pmezard/go-difflib v1.0.0 // indirect gopkg.in/yaml.v3 v3.0.0-20200313102051-9f266ea9e77c // indirect)exclude ( go.etcd.io/etcd/client/v2 v2.305.0-rc.0 go.etcd.io/etcd/client/v3 v3.5.0-rc.0)retract ( v1.0.0 // 废弃的版本，请使用v1.1.0) 虽然是一个简单的文件，但是里面的乾坤不少，让我们依次介绍它们。 2.1 module pathgo.mod 的第一行是 module path，一般采用“仓库+module name” 的方式定义。这样我们获取一个 module 的时候，就可以到它的仓库中去查询，或者让 go proxy 到仓库中去查询。1module github.com/dablelv/go-huge-util 如果你的版本已经 &gt;=2.0.0，按照 Go 的规范，你应该加上 major 的后缀，module path 改成下面的方式：123module github.com/dablelv/go-huge-util/v2module github.com/dablelv/go-huge-util/v3 而且引用代码的时候，也要加上v2、v3、vx后缀，以便和其它major版本进行区分。这是一个建议性的约定，带来的好处是你一个项目中可以使用依赖库的不同的 major 版本，它们可以共存。 2.2 go directive第二行是 go directive。格式是 go 1.xx，它并不是指你当前使用的 Go 版本，而是指名你的代码所需要的 Go 的最低版本。1go 1.17 因为 Go 的标准库在不断迭代，一些新的 API 会陆续被加进来。如果你的代码用到了这些新的 API，你可能需要指明它依赖的 Go 版本。 这一行不是必须的，你可以不写。 2.3 requirerequire段中列出了项目所需要的各个依赖库以及它们的版本，除了正规的v1.3.0这样的版本外，还有一些奇奇怪怪的版本和注释，那么它们又是什么意思呢？ 2.3.1 伪版本号1github.com/edwingeng/doublejump v0.0.0-20200330080233-e4ea8bd1cbed 上面这个库中的版本号就是一个伪版本号v0.0.0-20200330080233-e4ea8bd1cbed，这是 go module 为它生成的一个类似符合语义化版本 2.0.0 版本，实际这个库并没有发布这个版本。 正式因为这个依赖库没有发布版本，而 go module 需要指定这个库的一个确定的版本，所以才创建的这样一个伪版本号。 go module 的目的就是在 go.mod 中标记出这个项目所有的依赖以及它们确定的某个版本。 这里的 20200330080233 是这次提交的时间，格式是 yyyyMMddhhmmss, 而 e4ea8bd1cbed 就是这个版本的 commit id。通过这个字段，就可以确定这个库的特定的版本。 而前面的 v0.0.0 可能有多种生成方式，主要看你这个 commit 的 base version： vX.0.0-yyyymmddhhmmss-abcdefabcdef 如果没有 base version，那么就是 vX.0.0 的形式。 vX.Y.Z-pre.0.yyyymmddhhmmss-abcdefabcdef 如果 base version 是一个预发布的版本，比如 vX.Y.Z-pre，那么它就用 vX.Y.Z-pre.0 的形式。 vX.Y.(Z+1)-0.yyyymmddhhmmss-abcdefabcdef 如果 base version 是一个正式发布的版本，那么它就 patch 号加1，如 vX.Y.(Z+1)-0。 2.3.2 indirect 注释12345require ( github.com/davecgh/go-spew v1.1.1 // indirect github.com/pmezard/go-difflib v1.0.0 // indirect gopkg.in/yaml.v3 v3.0.0-20200313102051-9f266ea9e77c // indirect) 有些库后面加了 indirect 注释，这又是什么意思呢？如果用一句话总结，间接的使用了这个库，但是又没有被列到某个 go.mod 中，当然这句话也不算太准确，更精确的说法是下面的情况之一就会对这个库加 indirect 注释。 当前项目依赖 A，但是 A 的go.mod 遗漏了 B，那么就会在当前项目的 go.mod 中补充 B，加 indirect 注释； 当前项目依赖 A，但是 A 没有 go.mod，同样就会在当前项目的 go.mod 中补充 B，加 indirect 注释； 当前项目依赖 A，A 又依赖 B。当对 A 降级的时候，降级的 A 不再依赖 B，这个时候 B 就标记 indirect 注释。我们可以执行go mod tidy来清理不依赖的 module。 需要注意的是，从 go 1.17 开始，indirect 的 module 将被放在单独 require 块的，这样看起来更加清晰明了。 2.3.3 incompatible有些库后面加了 incompatible 后缀，但是你如果看这些项目，它们只是发布了 v2.2.1 的 tag，并没有 +incompatible 后缀。1github.com/cenk/backoff v2.2.1+incompatible 这些库采用了 go.mod 的管理，但是不幸的是，虽然这些库的版 major 版本已经 &gt;=2 了，但是他们的 module path 中依然没有添加 v2、v3 这样的后缀，不符合 Go 的 module 管理规范。所以 go module 把它们标记为 incompatible 的，虽然可以引用，但是实际它们是不符合规范的。 2.4 exclude如果你想在你的项目中跳过某个依赖库的某个版本，你就可以使用这个段。1234exclude ( go.etcd.io/etcd/client/v2 v2.305.0-rc.0 go.etcd.io/etcd/client/v3 v3.5.0-rc.0) 这样，Go 在版本选择的时候，就会主动跳过这些版本，比如你使用 go get -u ...... 或者 go get github.com/xxx/xxx@latest 等命令时，会执行version query 的动作，这些版本不在考虑的范围之内。 2.5 replacereplace 也是常用的一个手段，用来解决一些错误的依赖库的引用或者调试依赖库。123replace github.com/coreos/bbolt =&gt; go.etcd.io/bbolt v1.3.3replace github.com/panicthis/A v1.1.0 =&gt; github.com/panicthis/R v1.8.0replace github.com/coreos/bbolt =&gt; ../r 比如 etcd v3.3.x 的版本中错误地使用了github.com/coreos/bbolt作为 bbolt 的 module path，其实这个库在它自己的go.mod 中声明的 module path 是 go.etcd.io/bbolt。又比如 etcd 使用的 grpc 版本有问题，你也可以通过 replace 替换成所需的 grpc 版本。甚至你觉得某个依赖库有问题，自己 fork 到本地做修改，想调试一下，你也可以替换成本地的文件夹。replace 可以替换某个库的所有版本到另一个库的特定版本，也可以替换某个库的特定版本到另一个库的特定版本。 2.6 retractretract 是 go 1.16 中新增加的内容，借用学术界期刊撤稿的术语，宣布撤回库的某个版本。 如果你误发布了某个版本，或者事后发现某个版本不成熟，那么你可以推一个新的版本，在新的版本中，声明前面的某个版本被撤回，提示大家都不要用了。 撤回的版本tag依然还存在，go proxy也存在这个版本，所以你如果强制使用，还是可以使用的，否则这些版本就会被跳过。 和 exclude 的区别是 retract 是这个库的 owner 定义的， 而 exclude 是库的使用者在自己的 go.mod 中定义的。 2.7 语义化版本 2.0.0Go module 遵循语义化版本 2.0.0。语义化版本规范 2.0.0 规定了版本号的格式，每个字段的意义以及版本号比较的规则等等。如果你想为你的项目发版，你可以设置tag为上面的格式，比如 v1.3.0、v2.0.0-alpha.1 等等。metadata中在Go版本比较时是不参与运算的，只是一个辅助信息。 3.go.sum 文件上面我们说到，Go 在做依赖管理时会创建两个文件，go.mod 和 go.sum。 相比于go.mod，关于 go.sum 的资料明显少得多。自然，go.mod 的重要性不言而喻，这个文件几乎提供了依赖版本的全部信息。而 go.sum 则是记录了所有依赖的 module 的校验信息，以防下载的依赖被恶意篡改，主要用于安全校验。 每行的格式如下：12&lt;module&gt; &lt;version&gt; &lt;hash&gt;&lt;module&gt; &lt;version&gt;/go.mod &lt;hash&gt; 比如：12github.com/spf13/cast v1.4.1 h1:s0hze+J0196ZfEMTs80N7UlFt0BDuQ7Q+JDnHiMWKdA=github.com/spf13/cast v1.4.1/go.mod h1:Qx5cxh0v+4UWYiBimWS+eyWzqEqokIECu5etghLkUJE= 其中 module 是依赖的路径。version 是依赖的版本号。如果 version 后面跟/go.mod表示对哈希值是 module 的 go.mod文件；否则，哈希值是 module 的.zip文件。 hash 是以h1:开头的字符串，表示生成 checksum 的算法是第一版的 HASH 算法（SHA256）。如果将来在 SHA-256 中发现漏洞，将添加对另一种算法的支持，可能会命名为 h2。 参考资料 深入理解 Go Modules 的 go.mod 与 go.sum Go Modules Reference 深入Go Module之go.mod文件解析 Golang包管理，go module模式、go mod和go sum等文件介绍]]></content>
      <categories>
        <category>编程拾慧</category>
        <category>GO</category>
      </categories>
      <tags>
        <tag>GO</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[地中海贫血检测-HBA2融合基因检测]]></title>
    <url>%2F01.%E7%9F%A5%E8%AF%86-11.%E9%81%97%E4%BC%A0%E7%97%85%2F%E9%81%97%E4%BC%A0%E7%97%85-%E5%9C%B0%E4%B8%AD%E6%B5%B7%E8%B4%AB%E8%A1%80%E6%A3%80%E6%B5%8B-HBA2%E8%9E%8D%E5%90%88%E5%9F%BA%E5%9B%A0%E6%A3%80%E6%B5%8B%2F</url>
    <content type="text"><![CDATA[地中海贫血（以下简称“地贫”）是一种常见的溶血性单基因隐性遗传病，主要是由于珠蛋白基因缺陷导致珠蛋白链合成减少或缺如而引起的 。我国长江以南地区，特别是广西、广东、云南、海南是该病的高发区。同时在不同地区具有各地特异的分型，很多数据库也记录了这些分型信息，常用名称和具体变异的信息。比如：HbVar 记录了影响血红蛋白的变异和地贫相关的突变（截至20240325，地贫相关突变有541条记录， ithanet记录了大量突变，同事可以基于突变的蛋白链和地贫类型进行一些筛选，同时提供了一些common的名称，对于初接触的人可以从这里根据常用名称查找具体突变信息。比如常见的 -α3.7、-α4.2、–SEA、和–THAI,其实都是对应一种具体的突变， 目前临床实践中常规使用的检测试剂盒，主要是针对中国人群的常见基因突变。 导致地贫表型的 α２ 珠蛋白融合基因于 2013 年首次报道 ，其人群分布尚不清楚，常规试剂盒常不包含该检测位点，故此突变的临床检测是目前所面临的一个具体问题。[^1] α２ 珠蛋白融合基因（HBA1/HBA2) 是在配子生成过程中，α２ 珠蛋白基因与 Ψα１ 发生了片段重组, 改变了 α２ 珠蛋白基因的 ３’UTR，并引起了多聚腺苷酸信号突变，从而产生广泛的 α２ 珠蛋白基因转录本，引起 α＋－地贫。 此融合基因的序列结构是 α２珠蛋白基因中有一段序列与 Ψα１ 一致，涉及 ７ 个差异碱基位点，故据此 ７ 个差异位点，从而实现此基因的准确检测分析。虽然叫HBA的融合基因加测，但是显然，这并不是检测融合的思路（┓( ´∀` )┏。u 因为是，α２ 珠蛋白基因与 Ψα１ 发生了片段重组后，重组后的序列，仅影响了原，α２ 珠蛋白基因末端8个bp的碱基（实际一些临床应用可能都是重点关注前7个位点），所以检测的本质其实更多的是检测固定位点的SNV，如果检测到SNV信号就认为发生了HBA2基因和Ψα１ 的融合。针对 α２珠蛋白融合基因 检测，目前有一些已经发表的方法和专利，但是大部分都是基于实验方案进行检测，比如有： 基于融合前后基因结构变化，设计引物，通过琼脂糖凝胶电泳法即可检测α珠蛋白Fusion gene的 一种α-地中海贫血相关基因检测试剂盒 结合文献中提供的原始位点，基于序列信息，获得这7个位点在 HBA2基因上的位置，通过基因位置获得染色体位置，最终相关位置信息如下。方便追溯，记录回溯过程如下： 首先我们通过 NG_000006.1 获得文献中的nt位置对应在HBA2 基因上的位置, 下载 NG_000006.1的genebank文件,相关部分展示如下：12345678910111213141516gene 33739..34573 /gene=&quot;HBA2&quot; /gene_synonym=&quot;HBA-T2; HBH&quot; /note=&quot;hemoglobin subunit alpha 2&quot; /db_xref=&quot;GeneID:3040&quot; /db_xref=&quot;HGNC:HGNC:4824&quot; /db_xref=&quot;MIM:141850&quot;mRNA join(33739..33870,33988..34192,34335..34573) /gene=&quot;HBA2&quot; /gene_synonym=&quot;HBA-T2; HBH&quot; /product=&quot;hemoglobin subunit alpha 2&quot; /transcript_id=&quot;NM_000517.6&quot; /db_xref=&quot;GeneID:3040&quot; /db_xref=&quot;HGNC:HGNC:4824&quot; /db_xref=&quot;MIM:141850&quot;CDS join(33776..33870,33988..34192,34335..34463) 我们可以看到HBA2基因全长835bp，对应 NG_000006.1的 33739..34573 区段。 然后我们通过NCBI，获得HBA2基因的基因序列 12345678910111213&gt;NC_000016.9:222875-223709 HBA2 [organism=Homo sapiens] [GeneID=3040] [chromosome=16], GRCh37.p13 Primary AssemblyACTCTTCTGGTCCCCACAGACTCAGAGAGAACCCACCATGGTGCTGTCTCCTGCCGACAAGACCAACGTCAAGGCCGCCTGGGGTAAGGTCGGCGCGCACGCTGGCGAGTATGGTGCGGAGGCCCTGGAGAGGTGAGGCTCCCTCCCCTGCTCCGACCCGGGCTCCTCGCCCGCCCGGACCCACAGGCCACCCTCAACCGTCCTGGCCCCGGACCCAAACCCCACCCCTCACTCTGCTTCTCCCCGCAGGATGTTCCTGTCCTTCCCCACCACCAAGACCTACTTCCCGCACTTCGACCTGAGCCACGGCTCTGCCCAGGTTAAGGGCCACGGCAAGAAGGTGGCCGACGCGCTGACCAACGCCGTGGCGCACGTGGACGACATGCCCAACGCGCTGTCCGCCCTGAGCGACCTGCACGCGCACAAGCTTCGGGTGGACCCGGTCAACTTCAAGGTGAGCGGCGGGCCGGGAGCGATCTGGGTCGAGGGGCGAGATGGCGCCTTCCTCTCAGGGCAGAGGATCACGCGGGTTGCGGGAGGTGTAGCGCAGGCGGCGGCTGCGGGCCTGGGCCGCACTGACCCTCTTCTCTGCACAGCTCCTAAGCCACTGCCTGCTGGTGACCCTGGCCGCCCACCTCCCCGCCGAGTTCACCCCTGCGGTGCACGCCTCCCTGGACAAGTTCCTGGCTTCTGTGAGCACCGTGCTGACCTCCAAATACCGTTAAGCTGGAGCCTCGGTAGCCGTTCCTCCTGCCCGCTGGGCCTCCCAACGGGCCCTCCTCCCCTCCTTGCACCGGCCCTTCCTGGTCTTTGAATAAAGTCTGAGTGGGCAGCA 我们可以看到HBA2 基因的835bp 对应的染色体位置是 NC_000016.9(chr16):222875-223709, 所以借此，我们可以基于文献中提供的 nt34528T&gt;C 获得该位点在基因HBA2上的相对位置 789，然后根据HBA2的染色体位置获得对应的染色体位置 chr16:173764 (38基因组）和 chr16:223663（hg19基因组） 。 七个SNP位点最终的位置对应关系如下：| nt | HBA2 gene site | 染色体位置(38 NC_000016.10:172876-173710) | 染色体位置(hg19 NC_000016.9:222875-223709) || ———- | ——————– | —————————————– | —————————————— || nt34528T&gt;C | NM_000517.6:c.64T&gt;C | 173764 | 223663 || nt34532A&gt;C | NM_000517.6:c.68A&gt;C | 173768 | 223667 || nt34535G&gt;A | NM_000517.6:c.71G&gt;A | 173771 | 223670 || nt34538C&gt;A | NM_000517.6:c.74C&gt;A | 173774 | 223673 || nt34546G&gt;A | NM_000517.6:c.82G&gt;A | 173782 | 223681 || nt34556A&gt;G | NM_000517.6:c.92A&gt;G | 173792 | 223691 || nt34562T&gt;C | - | 173797 | 223696 || A&gt;G | - | 173855 | 223754 | 文章位点的最终确认同时我们也在另一篇文献[^2]中看到了研究这7个变异位点的文章，提供了比较长的序列,经过对比和我们最终得到的参考序列一致。123456789&gt;NC_000016.10:172876-173710 HBA2 的 769-835AACGGGCCCTCC TCCCCTCC\TT GCACCGGCCC TTCCTGGTCT TTGAATAAAG TCTGAGTGGG CAGCAC C A A A G C 3 3 3 3 3 3 3 4 4 4 4 4 4 4 5 5 5 5 5 5 5 2 3 3 3 4 5 6 8 2 5 8 6 6 2 同时，我们也查看了一些样本的数据，可以看到一些HBA2融合样本和非融合的样本在这些位点缺失有非常明显的差异。 我们可以看到，阳性样本和阴性样本在这几个位点存在明显的差异，用这7个位点进行HBA2融合基因的检测性能不会有什么瓶颈。 建立预测模型其实通过查看上面的截图，可以看到，其实两类数据的区分度非常明显。对手上的阴阳性数据也进行了一些简单的统计，发现除了nt34562位点外，其他的6个位点，进行任意的单一指标的统计量都存在明显的差异。 column cut neg_max pos_min nt34528 0.2551733780760626 0.2083333333333333 0.3020134228187919 nt34532 0.2622971285892634 0.2134831460674157 0.3111111111111111 nt34535 0.23971471471471467 0.1888888888888888 0.2905405405405405 nt34538 0.24000000000000002 0.2 0.28 nt34546 0.2242424242424242 0.1818181818181818 0.2666666666666666 nt34556 0.16107579053036636 0.127906976744186 0.1942446043165467 nt34562 1.0 1.0 1.0 所以在这里面，我们也不太有必要进行画蛇添足的复杂模型应用，首先我们剔除掉对最终判断没有太大帮助的nt34562位点，然后建立模型。模型也可以比较简单的对单个位点根据测试数据取得二分类的阈值（在这里面使用pos的最小值和neg的最大值的平均值，因为数据分离度很大，在这里面也没有使用ROC之类方法的必要性）。剔除一个位点后，我们还有6个位点用于判断，我们简单的指定个标准，有4个及以上位点的状态定义为样本的最终状态。 references[^1]: 2021.10 α－地中海贫血融合基因检测方法及应用评价[^2]: 2019 一个黎族α-地中海贫血融合基因遗传家系的鉴定_胡俊杰.pdf)]]></content>
      <categories>
        <category>遗传病</category>
        <category>chr11</category>
        <category>chr16</category>
        <category>常隐</category>
      </categories>
      <tags>
        <tag>遗传病</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[地中海贫血检测]]></title>
    <url>%2F01.%E7%9F%A5%E8%AF%86-11.%E9%81%97%E4%BC%A0%E7%97%85%2F%E9%81%97%E4%BC%A0%E7%97%85-%E5%9C%B0%E4%B8%AD%E6%B5%B7%E8%B4%AB%E8%A1%80%E6%A3%80%E6%B5%8B%2F</url>
    <content type="text"><![CDATA[背景介绍地中海贫血 (简称“地贫”) 是全球热带和亚热带地区高发的溶血性单基因隐性遗传病，在中国大陆地贫主要分布在长江流域以南广大省份，其中四川、重庆、湖南、江西和福建的人群携带率为6%-11%，云南、贵州、广西、广东和海南地区的人群携带率高达12%～25%[^2][^3][^4] ，按基因频率估算，中国南方每年出生的病例数为1.29万。由于该病高发区的人群发生率高，且治疗昂贵，$\color{green}地贫已经成为我国$南方控制出生缺陷的重点对象，通过人群筛查和产前诊断阻止患儿出生是目前国内外公认的地贫的首选预防措施。 地中海贫血的类型血红蛋白（红细胞中携带氧气的蛋白质）由两种蛋白质组成： α珠蛋白 和 β珠蛋白。地中海贫血主要是由于珠蛋白基因缺陷导致珠蛋白链合成减少或缺[^1]进而致使大量红细胞被破坏[^5]。 人体生长发育不同阶段对氧的需求不同导致血红蛋白类型不同。所以某结构的异常可能会在人体某个生长时期发病。 α地中海贫血 当与α珠蛋白相关的一个或多个基因缺失或改变（突变）时，就会发生α地中海贫血。 最常见于东南亚、中东、中国和非洲人后裔。 α地中海贫血的几种类型型[^6] 阿尔法地中海贫血沉默携带者(Alpha thalassemia silent carrier)。一个基因缺失或损坏，其他 3 个基因正常。血液检查通常是正常的。您的红细胞可能比正常值小。作为沉默携带者意味着您没有疾病迹象，但您可以将受损的基因遗传给您的孩子. α地中海贫血携带者(Alpha thalassemia carrier)。两个基因缺失。您可能患有轻度贫血。 血红蛋白 H 病(Hemoglobin H disease)。三个基因缺失。这样就只剩下 1 个工作基因了。您可能患有中度至重度贫血。发烧时症状可能会恶化。如果您接触某些药物、化学品或传染源，情况也会变得更糟。经常需要输血。您生下患有重型α地中海贫血的孩子的风险更大。 重型阿尔法地中海贫血(Alpha thalassemia major)。 4个基因全部缺失。这会导致严重贫血。在大多数情况下，患有这种疾病的婴儿会在出生前死亡。 β地中海贫血 当类似的基因缺陷影响β珠蛋白的产生时，就会发生β地中海贫血。 最常见于地中海血统的人。在较小程度上，中国人、其他亚洲人以及非裔美国人可能会受到影响。 β地中海贫血的几种类型[^7] 轻度地中海贫血（Thalassemia minima）：症状很少或没有。 中间型地中海贫血（Thalassemia intermedia）：这会导致中度至重度贫血。 重型β地中海贫血又称库利贫血（Beta thalassemia major (Cooley’s anemia)）。有两个基因受损。这是这种疾病最严重的形式。患有这种疾病的人需要经常输血。他们可能不会过正常的寿命。 地中海贫血相关的基因地贫表型相关的基因[^8] Location Phenotype Phenotype(MIM number) Inheritance Phenotype mapping key Gene/Locus Gene/Locus MIM number 16p13.3 Thalassemia, alpha- 604131 3 HBA2 141850 16p13.3 Thalassemias, alpha- 604131 3 HBA1 141800 相关文章 The Thalassemias: The Role of Molecular Genetics in an Evolving Global Health Problem* The role of molecular diagnostic testing for hemoglobinopathies and thalassemias Molecular basis of α-thalassemia 。文章提供了几个地贫相关的数据库，HbVar 记录了影响血红蛋白的变异和地贫相关的突变（截至20240325，地贫相关突变有541条记录， ithanet记录了大量突变，同事可以基于突变的蛋白链和地贫类型进行一些筛选，同时提供了一些common的名称，对于初接触的人可以从这里根据常用名称查找具体突变信息。比如常见的 -α3.7、-α4.2、–SEA、和–THAI,其实都是对应一种具体的突变，单纯的看名字可能不明所以，但其实他们就是一些特定区域的deletion，然后得到的别名。 HBA1/2α 样珠蛋白基因簇位于 135-155kb 富含 GC、Alu 重复密集和基因密集的基因组 DNA 区域，距 16 号染色体端粒 (16p13.3) 约 150kb。它包含三个功能性珠蛋白基因，即胚胎 z 基因 (HBZ) 和两个胎儿/成人 α 基因（HBA1 和 HBA2），三个假基因，即假 z (HBZps)、假 α1 (HBA1ps) 和假 α1 (HBA1ps)。 检测原理临床产品信息[^13] 圣湘生物： α-地中海贫血基因检测试剂盒[^9]. 可检测α-地贫(–SEA、-α3.7、-α4.2、–THAI)。 博奥晶典：地中海贫血基因检测试剂盒（微阵列芯片法）[^10]，检测类型如下： 相关专利 基于二代测序技术检测α和β地中海贫血点突变的方法及试剂盒[^11] 地中海贫血症相关基因突变的检测试剂盒、检测方法及其应用[^12] $\color{red}的 业务逻辑实现单分子检测地贫的业务逻辑资料 references[^1]: Viprakasit V, Ekwattanakit S. Clinical Classification, Screening and Diagnosis for Thalassemia. Hematol Oncol Clin North Am. 2018;32(2):193-211. doi:10.1016/j.hoc.2017.11.006[^2]: 李友琼，何 升，丘小霞，董柏青，陈碧艳，韦 慧，黄秀宁，赵 林，梁 亮，覃 婷，田 矛，黎君君.2010～2019年广西地中海贫血发生现状与防治策略[J].中国临床新医学,2020,13(10):955-959.[^3]: 揭秋玲,李崎,孙文页,周繇,陈宏健,黄慧敏,卢伟英,马燕琳海南地区地中海贫血筛检者的基因结果分析实用医学杂志202010.3969/j.issn.1006-5725.2020.08.021[^4]: 邹团标, 姚莉琴, 李智, 徐咏梅, 陆克文, 何磊, 周凤珍, 刘锦桃, 郭光萍. 中国云南省23个民族育龄人群地中海贫血基因检测与分析[J]. 中国公共卫生, 2019, 35(11): 1505-1510. doi: 10.11847/zgggws1117281[^5]: https://medlineplus.gov/ency/article/000587.htm[^6]: https://www.hopkinsmedicine.org/health/conditions-and-diseases/alpha-thalassemia[^7]: https://www.hopkinsmedicine.org/health/conditions-and-diseases/beta-thalassemia[^8]: https://www.omim.org/entry/604131[^9]:https://www.sansure.com.cn/sj/index.aspx?itemid1=11&amp;itemid=142[^10]: https://www.capitalbiotech.com/product/16/%E8%AF%95%E5%89%82%E7%9B%92/%E5%87%BA%E7%94%9F%E7%BC%BA%E9%99%B7.html[^11]: 基于二代测序技术检测α和β地中海贫血点突变的方法及试剂盒[^12]: 地中海贫血症相关基因突变的检测试剂盒、检测方法及其应用[^13]:一文读懂地中海贫血检测技术的来龙去脉 2021.10 α－地中海贫血融合基因检测方法及应用评价 2013.06 A novel fusion gene and a common α0-thalassemia deletion cause hemoglobin H disease in a Chinese family]]></content>
      <categories>
        <category>遗传病</category>
        <category>chr11</category>
        <category>chr16</category>
        <category>常隐</category>
      </categories>
      <tags>
        <tag>遗传病</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[生物学基础概念]]></title>
    <url>%2F01.%E7%9F%A5%E8%AF%86-01.NGS%2F%E6%A6%82%E5%BF%B5-%E7%94%9F%E7%89%A9%E5%AD%A6%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5%2F</url>
    <content type="text"><![CDATA[遗传基因型 杂合子(Heterozygous ): 一对同源染色体中，特定位点上具有两个不同的等位基因。 半合子(Hemizygous ) : 一对同源染色体中，特定位点具有不成对的等位基因。例如：雄性 X 染色体上的等位基因，或不成对的转基因等位基因。 纯合子(Homozygous ) : 一对同源染色体中，特定位点上具有两个相同的等位基因。]]></content>
      <categories>
        <category>NGS</category>
      </categories>
      <tags>
        <tag>基础概念</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[git remote远程仓库管理及多仓库配置]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-09.%E7%89%88%E6%9C%AC%E6%8E%A7%E5%88%B6%2FGit-5.git%20remote%E8%BF%9C%E7%A8%8B%E4%BB%93%E5%BA%93%E7%AE%A1%E7%90%86%E5%8F%8A%E5%A4%9A%E4%BB%93%E5%BA%93%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[由于各种原因（需要同时使用多个仓库平台，例如：github存放一些个人代码和企业gitlab存放公司业务代码； 或者公司内多个内网不同的gitlab平台需要实现项目代码同步），因此经常会遇到我们需要同时访问多个git托管平台。手动拷贝粘贴变动文件显然是不符合我们慵懒策略的，可以自动通过不同的域名（IP）调取对应的密钥进行数据的访问，git仓库实现多远程仓库配置实现本地单仓库管理多远程仓库的功能，极大的提高我们的效率。为了满足业务场景，查了一些方案，在此进行记录。 remote因为关注到remote，可能大多数人已经有了多远程仓库的配置需求，所以在这里先介绍多仓库的配置，在补充介绍其他相关命令 常用命令查看远程仓库配置123456$ git remote # 查看所有远程仓库的别名：origin # origin是默认命名，当我们clone一个仓库时，git会默认给它命名为origin。$ git remote -v # 查看所有远程仓库的别名和对应的URL：origin ssh://git@gitlab.xxx.cn/git-demo.git (fetch)origin ssh://git@gitlab.xxx.cn/git-demo.git (push) 添加远程仓库123456789101112# 添加远程仓库$ git remote add &lt;name&gt; &lt;url&gt;# name是我们为这个新仓库取的一个别名# url是这个新仓库的具体地址# 重命名远程仓库$ git remote rename &lt;old-name&gt; &lt;new-name&gt;# 删除远程仓库git remove &lt;name&gt;# 添加多个远程仓库 多仓库的配置因为本文主要是针对多仓库，所以全局配置显然已经不合适了，多仓库的账号往往不互通，所以首先我们再本地进行多个账号的配置。 配置多个账号 首先针对我们每个不同的托管平台账号生成各自的密钥信息。以gitlab &amp; github为例。 123ssh-keygen -t rsa -f ~/.ssh/id_rsa_gitlab -C "gitlab@account" ssh-keygen -t rsa -f ~/.ssh/id_rsa_github -C "github@account" 生成密钥后，在密钥存储目录创建~/.ssh/config文件并进行相关信息的记录，config文件格式如下 1234567891011121314151617# github# host 与 hostname 需要相同Host github.comHostName github.com# 你的github账号User github@account# github对应的rsa秘钥文件IdentityFile ~/.ssh/id_rsa_github# gitlab# host 与 hostname 需要相同Host gitlab.genomics.cnHostName gitlab.genomics.cn# 你的gitlab账号User gitlab@account# gitlab对应的rsa秘钥文件IdentityFile ~/.ssh/id_rsa_gitlab 完成上述步骤后，分别将不同托管平台对应生成的秘钥文件信息添加到对应平台中，例如：gitlab中添加方式: Edit profile ==&gt; SSH key ==&gt; Add an SSH key ；github中添加方式：Settings ==&gt; SSH and GPG keys ==&gt; New SSH key。然后就可以直接对多个不同托管平台的数据进行操作，而不用每个仓库单独配置账号和权限了。 仓库远程关联弱关联配置所谓弱关联，其实就是本地和远程代码不是强相关，可以指定代码推送到那个远程仓库，不同的远程仓库代码也可能存在版本差异，这样会有更多的可操作性，自由度和安全度也更高。 1234567891011121314151617181920$ git remote -v # 查看关联的远程仓库origin https://github.com/Ben-unbelieveable/Ben-unbelieveable.github.git (fetch)origin https://github.com/Ben-unbelieveable/Ben-unbelieveable.github.git (push)# git remote add &lt;name&gt; &lt;url&gt;添加一个远程仓库 name 值请确保唯一性,方便区分$ git remote add gitlab ssh://git@gitlab.********.cn:2200/li****/ben-unbelieveable.github.gitgit remote -v # 查看关联是否成功gitlab ssh://git@gitlab.genomics.cn:2200/liubo4/ben-unbelieveable.github.git (fetch)gitlab ssh://git@gitlab.genomics.cn:2200/liubo4/ben-unbelieveable.github.git (push)origin https://github.com/Ben-unbelieveable/Ben-unbelieveable.github.git (fetch)origin https://github.com/Ben-unbelieveable/Ben-unbelieveable.github.git (push)# 向特定仓库推送代码$ git push origin master$ git push gitlab master# 从特定仓库拉取代码$ git pull origin master$ git pull gitlab master 其实以前用git 经常看到origin，但是直到现在才知道origin 是默认的远程仓库别名，所以可以随意添加远程仓库并起别名进行区分，例如：gitlab。 强关联配置但是显而易见的，弱关联请胯下，我们每次 push 都需手动指定远程仓库,显得比较麻烦.有写项目，仓库我仅仅需要一次操作同步至 github 和 gitlab 即可, 只可以多仓库同步推送，但是不能多仓库同步拉取流程，当然也可能是为了规避可能的代码冲突。 1234567# git remote set-url --add &lt;name&gt; &lt;url&gt; 为当前仓库添加额外的远程地址$ git remote set-url --add origin ssh://git@gitlab.********.cn:2200/li****/ben-unbelieveable.github.git$ git remote -vorigin https://github.com/Alioth996/txt-reader.git (fetch) # git fetch/pull 时的默认仓库origin https://github.com/Alioth996/txt-reader.git (push)origin git@gitlab.com:alioth-code/txt-reader.git (push) 当然如果我们默认拉取的仓库不是origin的仓库，我们也可以更改remote的默认(origin)配置，默认每个remote的第一个配置的远程仓库是由拉取权限的，后续添加的仓库可以推送。]]></content>
      <categories>
        <category>编程拾慧</category>
        <category>Git</category>
      </categories>
      <tags>
        <tag>培训</tag>
        <tag>Git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[GO-发布package]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-08.Go%2FGO-04.%E5%8F%91%E5%B8%83package%2F</url>
    <content type="text"><![CDATA[创建github仓库因为go的代码库都是以包为单位的，依托于github，所以需要创建一个github仓库（也可以完成包内容撰写后关联仓库推送）。相应的，如果需要饮用go数据库的话，也是通过github的地址进行引用的。 留坑，如果后期有相关需求需求在进行补充，目前相关项目涉及的包暂存在 https://github.com/Ben-GO-package 下，后续有需求在展开]]></content>
      <categories>
        <category>编程拾慧</category>
        <category>GO</category>
      </categories>
      <tags>
        <tag>GO</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[GO-标准库]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-08.Go%2FGO-99.%E6%A0%87%E5%87%86%E5%BA%93%2F</url>
    <content type="text"><![CDATA[GO 语言相比其它语言，目前最大的区别可能就是库没有那么完善，在这里记录一下相关资料，方便后续使用。http://word.topgoer.com/ go标准库文档 https://gobyexample.com/ 一些库的调用测试案例]]></content>
      <categories>
        <category>编程拾慧</category>
        <category>GO</category>
      </categories>
      <tags>
        <tag>GO</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[GO-00.环境安装]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-08.Go%2FGO-00.%E7%8E%AF%E5%A2%83%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[GOJ解释器安装Go 语言支持以下系统： Linux FreeBSD Mac OS X（也称为 Darwin） Windows 安装包下载地址为：https://go.dev/dl/ or https://golang.google.cn/dl/ 。 GOPATH配置GOPATH是一个环境变量，用来表明你写的go项目的存放路径GOPATH路径最好只设置一个，所有的项目代码都放到GOPATH的src目录下。 一些建议的执行习惯：$GOPATH/src： 在进行Go语言开发的时候，保存我们的原始代码。$GOPATH/pkg： 通过go get，等命令，下载的第三方包的源文件$GOPATH/bin： 经过go build以后，生成的二进制可执行文件。在工程经过go build、go install或go get等指令后，会将下载的第三方包源代码文件放在$GOPATH/src目录下， 产生的二进制可执行文件放在 $GOPATH/bin目录下，生成的中间缓存文件会被保存在 $GOPATH/pkg 下。这样执行逻辑下，我们开发阶段，仅需要对src进行版本的管理和代码维护。但是代码相关依赖都在pkg目录下，同时可执行文件都在bin目录下。 学习资料和博客topGoer菜鸟教程https://books.studygolang.com/The-Golang-Standard-Library-by-Examplego标准库文档]]></content>
      <categories>
        <category>编程拾慧</category>
        <category>GO</category>
      </categories>
      <tags>
        <tag>GO</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[GO-代码结构]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-08.Go%2FGO-02.%E4%BB%A3%E7%A0%81%E7%BB%93%E6%9E%84%2F</url>
    <content type="text"><![CDATA[代码语言结构学习一门语言，我们先了解一下这门语言的基本结构，这可以帮助我们拥有快速看懂一门语言编码的能力。毕竟如果是从头造轮子，很多时候，我们可以用自己熟悉的编程语言，另外就是其实看一些优秀项目可以帮我们更快的学习一些使用的语言编写规范和技巧。本来先写01的简介，但是想想，还是专门记录一下GO的语言结构，以便自己查看新的go代码时，可以更快速的上手。GO代码的语言结构，我们以一个简单的 Hello，world 为例,代码由一下几个部分组成。 123456789101112package main // 包声明import "fmt" // 引入包func init()&#123; fmt.PrintLn("Prepare") // 进行项目执行前的准备工作。&#125;func main() &#123; // 定义函数 /* 这是我的第一个简单的程序 */ // 必要的注释内容 fmt.Println("Hello, World!") // 处理逻辑/语句&#125; 多文件包GO同级目录包可以直接引用和其他语言不通，在GO中，如果几个go项目的文件定义属于同一个包（首行的package),那么即便函数在不同的代码文件中，也可以直接调用互相调用。但是由于不同的go解释器本身差异，可能部分解释器不会把所有包一起编译/执行，需要在编译/执行 时，选择全部的go文件。那么就可以实现跨包内的多脚本，进行函数、变量、对象的调用 func.go(定义了函数) 示例 12345678910package mainfunc Fibonacci(n int, c chan int) &#123; x, y := 0, 1 for i := 0; i &lt; n; i++ &#123; c &lt;- x x, y = y, x+y &#125; close(c)&#125; main.go（主程序） 示例 12345678910111213141516171819202122package mainimport ( "fmt" "time" //"strconv")func main() &#123; c := make(chan int, 3) go Fibonacci(cap(c), c) // range 函数遍历每个从通道接收到的数据，因为 c 在发送完 10 个 // 数据之后就关闭了通道，所以这里我们 range 函数在接收到 10 个数据 // 之后就结束了。如果上面的 c 通道不关闭，那么 range 函数就不 // 会结束，从而在接收第 11 个数据的时候就阻塞了。 time.Sleep(1) fmt.Println(len(c)) for i := range c &#123; fmt.Println(i) fmt.Println() &#125;&#125; 在这种情况下，我们可以执行多包的编译，从而实现类似上文中的包的调用 unix机器12345go.exe run main.go func.go ## 但是显而易见的，当我们包的文件躲起来的时候，输入包的名字会成为巨大的负担# 所以在unix 机器上，我们可以使用通配符（这在windows上开发时不可行。）go run *.go windows机器在windows上，通配方案往往会出现问题，这个时候的一个可选的解决方案是，撰写一个 run.bat 文件放到目录中，然后每次执行这个文件123go.exe build.\&#123;project_name&#125;.exego.exe clean]]></content>
      <categories>
        <category>编程拾慧</category>
        <category>GO</category>
      </categories>
      <tags>
        <tag>GO</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python-装饰器-property]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-02.python%2FPython-%E8%A3%85%E9%A5%B0%E5%99%A8-1.property%2F</url>
    <content type="text"><![CDATA[对象\@property可以将对象内定义的方法转换为属性（不可变更值）,同时调用的方法发生变化（方法的调用需要后面添加”()”,属性不能添加）12345678910111213141516171819202122232425DataSet(object): @property def method_with_property(self): ##含有@property return 15 def method_without_property(self): ##不含@property return 15l = DataSet()print(l.method_with_property) # 加了@property后，可以用调用属性的形式来调用方法,后面不需要加（）。print(l.method_without_property()) #没有加@property , 必须使用正常的调用方法的形式，即在后面加()class DataSet(object): def __init__(self): self._images = 1 self._labels = 2 #定义属性的名称 @property def images(self): #方法加入@property后，这个方法相当于一个属性，这个属性可以让用户进行使用，而且用户有没办法随意修改。 return self._images @property def labels(self): return self._labelsl = DataSet()#用户进行属性调用的时候，直接调用images即可，而不用知道属性名_images，因此用户无法更改属性，从而保护了类的属性。print(l.images) # 加了@property后，可以用调用属性的形式来调用方法,后面不需要加（）。 方法]]></content>
      <categories>
        <category>编程拾慧</category>
        <category>python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Software-编程终端-VSCode-4.远程开发.md]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-00.%E7%BC%96%E7%A8%8B%E8%BE%85%E5%8A%A9%E5%B7%A5%E5%85%B7%2FSoftware-%E7%BC%96%E7%A8%8B%E7%BB%88%E7%AB%AF-VSCode-4.%E8%BF%9C%E7%A8%8B%E5%BC%80%E5%8F%91%2F</url>
    <content type="text"><![CDATA[NGS方向进行数据处理的时候，数据动不动就是GB级，所以很多开发工作其实很难再本地环境运行，但是同时另一方面，各类代码编辑器很少会有适配 commandLine 的功能。所以寻找到了一个解决方案，就是使用 VSCode 的 remote Explorer插件。远程链接服务器，通过vscode的IDE直接编辑服务器代码，并进行调试。这里记录下相关操作流程。 前置准备终端和服务器主机已经可以联通 已安装 SSH（Git 自带，最好直接安装 Git） 本地电脑已连接网络 本地电脑已连接 VPN（若远程服务器在内网下） 远程服务器已连接网络 本地安装 安装 vscode 安装 Remote-SSH 插件 远程安装这也是本文档记录的重点 查看 vscode commit id (vscode -&gt; help -&gt; about -&gt;commit ) 下载对应版本的 vscode-server现在链接的格式如下，将其中的 ${commit_id} 对应更换为你本地vscode的commitID。 1https://update.code.visualstudio.com/commit:$&#123;commit_id&#125;/server-linux-x64/stable 将 vscode-server 部署到远程服务器 1234567891011121314151617# 登陆远程服务器，在 ~ 目录下创建 .vscode-server/bin 目录：mkdir -p ~/.vscode-server/bin# 将下载得到的 vscode-server-linux-x64.tar.gz 文件上传至上述新建的 ~/.vscode-server/bin 目录：scp vscode-server-linux-x64.tar.gz user_name@server_ip:~/.vscode-server/bin# 登陆远程服务器，解压 vscode-server-linux-x64.tar.gz：cd ~/.vscode-server/bintar -zxvf vscode-server-linux-x64.tar.gz# 解压后将在 ~/.vscode-server/bin 目录下生成 vscode-server-linux-x64 目录，将其改名为上文中得到的 vscode 的 commit id，并删除 vscode-server-linux-x64.tar.gz：mv vscode-server-linux-x64 e18005f0f1b33c29e81d732535d8c0e47cafb0b5rm vscode-server-linux-x64.tar.gz# 在这个以 vscode commit id 命名的目录中创建名为 0 的文件：cd ~/.vscode-server/bin/e18005f0f1b33c29e81d732535d8c0e47cafb0b5touch 0 vscode-server 完成远程部署后不包含扩展，可将其它已安装有 vscode 的 Ubuntu 机器的 ~/.vscode 目录下的 extensions 目录放到上文中创建的远程服务器的 ~/.vscode-server 目录下，实现扩展离线安装，或通过下文中将提到的方法直接将本地扩展全部或部分在线安装到远程服务器。 referencehttps://zhuanlan.zhihu.com/p/493050003]]></content>
      <categories>
        <category>software</category>
        <category>Windows</category>
        <category>VSCode</category>
      </categories>
      <tags>
        <tag>环境配置</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python-性能调优-核心函数改用cpython.md]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-02.python%2Fpython-%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%98-%E6%A0%B8%E5%BF%83%E5%87%BD%E6%95%B0%E6%94%B9%E7%94%A8cpython%2F</url>
    <content type="text"></content>
      <categories>
        <category>Tobeinsert</category>
      </categories>
      <tags>
        <tag>Tobeinsert</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python-包-pydantic-数据模型定义和字段校验]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-02.python%2FPython-%E5%8C%85-pydantic-%E6%95%B0%E6%8D%AE%E6%A8%A1%E5%9E%8B%E5%AE%9A%E4%B9%89%E5%92%8C%E5%AD%97%E6%AE%B5%E6%A0%A1%E9%AA%8C%2F</url>
    <content type="text"><![CDATA[介绍Pydantic 可以帮助我们显示的定义数据集中每个字段的数据类型，并在数据导入时，对数据进行强制验证，以确保输出的数据满足预期，并在数据无效时提供用户友好的错误信息。pydantic主要是一个解析库，而不是验证库。验证是达到目的的一种手段：构建符合所提供的类型和约束的模型。换句话说，pydantic保证输出模型的类型和约束，而不是输入数据。官方文档 其实python对于变量本身是提供了类型限制的方案的,但由于约束不是强制的，所以可能我们会出现偷懒的情况~，也许大部分情况下可能只是影响我们二次阅读代码或者函数调用的成本（回溯代码核查输入输出格式），但是在一些特殊的情境下，可能会导致我们的结果出现一些意料之外的错误，示例如下：123456789101112# def sum(a,b):def sum_1(a: int, b: int): print(str(a) +"+" + str(b) + " = " + str(a+b)) return a + bdef sum_2(a, b): print(str(a) +"+" + str(b)+" = "+ str(a+b)) return a + ba=1;b=2; sum_1(a,b) # 1+2 = 3a=1;b=2; sum_2(a,b) # 1+2 = 3a=str(1);b=str(2); sum_2(a,b) # 1+2 = 12 我们相对两个数字求和，但是在定义函数的时候，忘记了对输入参数的类型进行限制，而某个调用的时候，可能会由于一些特殊的需求，我们把数字都格式成了字符串，这个时候我们如果函数使用sum_2的方式，那么就会出现一些无法预测的问题。当然上述示例，主要是对输入输出类型进行了限制，但是有时候，我们除了限制输入输出，还需要对数据的具体结构进行限制，比如某些函数我们处理转录本为 “NM“的数据，而不应该出现 “NR“格式的数据等。所以除了数据类型，有时候，我们还需要惊醒一些更复杂的数据格式校验，这个时候，我们就可以使用pydantic来帮我们进行。 主要功能Pydantic 的核心功能包括： 数据验证：确保输入数据符合预定义的类型和结构。 序列化 ：将复杂的数据结构转换为 Python 数据类型，便于处理和传输。 错误处理：提供详细的错误信息，帮助开发者快速定位和修复问题。 配置管理：支持通过环境变量等方式管理配置，提高应用的可配置性。 特性Pydantic 以其高效、灵活和易用性著称，以下是其主要特性： 类型注解支持：Pydantic 充分利用 Python 的类型注解，使得数据模型的定义简洁明了。 高性能：Pydantic 的核心验证逻辑是用 Rust 编写的，这使得它在数据验证方面表现出色，速度快于许多其他 Python 数据验证库。 JSON Schema 生成：Pydantic 模型可以自动生成 JSON Schema，便于与其他工具和系统集成。 严格模式和宽松模式：Pydantic 支持严格模式（strict=True）和宽松模式（strict=False），在严格模式下，数据不会被自动转换，而在宽松模式下，Pydantic 会尝试将数据转换为正确的类型。 数据类支持：Pydantic 支持标准库中的数据类（dataclasses）和 TypedDict，提供更灵活的数据结构定义。 自定义验证器和序列化器：Pydantic 允许开发者自定义验证器和序列化器，以满足特定的数据处理需求。 生态系统丰富：Pydantic 被广泛应用于各种项目中，包括 FastAPI、Hugging Face、Django Ninja、SQLModel 和 LangChain 等知名库。 经过实战测试：Pydantic 每月被下载超过 7000 万次，被所有 FAANG 公司和纳斯达克 25 家最大公司中的 20 家使用，证明了其可靠性和广泛的应用场景。 通过这些特性，Pydantic 为 Python 开发者提供了一个强大而灵活的工具，用于处理数据验证和解析，极大地简化了数据处理的复杂性。 pydantic的使用安装1234# pip 直接安装pip install pydantic# 基于conda安装conda install -c conda-forge pydantic Pydantic 可以可选的使用 Cython 进行编译，将会带来 30%-50%的性能提升。 PyPI可以为Linux、MacOS和64位Windows提供二进制文件。如果您是手动安装，请在安装pydantic之前安装 cython，这样编译就会自动进行。 要测试pydantic 是否已经编译，可以使用如下方法：12&gt;&gt;&gt; import pydantic&gt;&gt;&gt; print(pydantic.compiled) 使用定义数据模型在使用Pydantic进行数据验证之前，首先需要定义数据模型。Pydantic通过继承BaseModel类来实现这一点。以下是一个简单的示例，展示了如何定义一个包含基本数据类型的数据模型。 12345678from pydantic import BaseModelfrom typing import Listclass EmployeeModel(BaseModel): name: str username: str salary: int habits: List[str] 在这个示例中，我们定义了一个名为 EmployeeModel 的类，它继承自 BaseModel 。该类包含四个字段：name、username、salary和habits。每个字段都通过类型注解（type annotation）指定了其数据类型。 数据验证示例Pydantic的核心功能之一是数据验证。一旦定义了数据模型，Pydantic会自动验证输入数据是否符合模型定义的类型和结构。以下是一个验证示例：12employee = EmployeeModel(name='Bar', username='bar', salary=1000, habits=[])print(employee) 在这个示例中，我们创建了一个EmployeeModel的实例，并传入相应的字段值。Pydantic会自动验证这些值是否符合模型定义的类型。如果验证通过，实例将被成功创建。 如果输入数据不符合模型定义的类型，Pydantic会抛出 ValidationError 异常，并提供详细的错误信息。例如：1234try: employee = EmployeeModel(name='Bar', username='bar', salary='secret', habits=[])except ValidationError as e: print(e) 在这个示例中，我们将salary字段设置为字符串’secret’，这不符合模型定义的int类型。因此，Pydantic会抛出ValidationError异常，并显示详细的错误信息： 1231 validation error for EmployeeModelsalary value is not a valid integer (type=type_error.integer) 处理外部数据Pydantic 不仅可以在创建模型实例时进行数据验证，还可以处理外部数据，如JSON格式的数据。以下是一个处理外部数据的示例：12345678import json# 假设我们有一个JSON字符串json_data = '&#123;"name": "Bar", "username": "bar", "salary": "1000", "habits": []&#125;'# 将JSON字符串解析为Python字典data = json.loads(json_data)# 使用字典数据创建模型实例employee = EmployeeModel(**data)print(employee) 在这个示例中，我们首先将JSON字符串解析为Python字典，然后使用字典数据创建EmployeeModel的实例。Pydantic会自动验证字典中的数据是否符合模型定义的类型。即使salary字段在JSON字符串中是字符串类型，Pydantic也会尝试将其转换为整数类型。 通过这种方式，Pydantic可以方便地处理来自外部数据源的数据，并确保数据的完整性和一致性。 高级功能自定义验证器Pydantic 提供了强大的数据验证功能，其中自定义验证器是一个非常重要的特性。通过自定义验证器，开发者可以实现复杂的验证逻辑，确保数据的完整性和准确性。 使用 @validator 装饰器Pydantic 提供了 @validator 装饰器，用于在模型字段上定义自定义验证逻辑。以下是一个简单的示例： 12345678910111213141516171819202122from pydantic import BaseModel, validator, ValidationErrorclass UserModel(BaseModel): name: str age: int @validator('name') def name_must_contain_space(cls, v): if ' ' not in v: raise ValueError('must contain a space') return v.title() @validator('age') def age_must_be_positive(cls, v): if v &lt;= 0: raise ValueError('must be a positive integer') return vtry: user = UserModel(name="john doe", age=-1)except ValidationError as e: print(e) 在这个示例中，我们定义了两个验证器： name_must_contain_space：确保 name 字段包含空格。 age_must_be_positive：确保 age 字段是正整数。 验证器的参数@validator 装饰器支持多个参数，用于控制验证器的行为： pre: 如果设置为 True，验证器将在字段验证之前执行。 each_item: 如果设置为 True，验证器将应用于列表或字典中的每个元素。 always: 如果设置为 True，验证器将始终执行，即使字段没有值。1234567891011from pydantic import BaseModel, validatorclass UserModel(BaseModel): name: str hobbies: list @validator('hobbies', each_item=True) def check_hobbies_not_empty(cls, v): if not v: raise ValueError('hobby cannot be empty') return v 在这个示例中，check_hobbies_not_empty 验证器将应用于 hobbies 列表中的每个元素，确保每个爱好不为空。 嵌套模型Pydantic 支持嵌套模型，这使得定义复杂的数据结构变得非常方便。嵌套模型可以包含其他模型作为字段，从而实现层次化的数据结构。 定义嵌套模型以下是一个嵌套模型的示例：1234567891011121314from pydantic import BaseModelclass Address(BaseModel): street: str city: str state: str zip_code: strclass User(BaseModel): name: str address: Addressuser = User(name="John Doe", address=&#123;"street": "123 Elm St", "city": "Springfield", "state": "IL", "zip_code": "62704"&#125;)print(user) 在这个示例中，User 模型包含一个 Address 模型作为其 address 字段。 嵌套模型的验证嵌套模型的验证是自动进行的。如果嵌套模型的字段不符合定义的类型或验证规则，Pydantic 将抛出 ValidationError。1234try: user = User(name="John Doe", address=&#123;"street": "123 Elm St", "city": "Springfield", "state": "IL", "zip_code": "invalid"&#125;)except ValidationError as e: print(e) 在这个示例中，zip_code 字段不符合定义的类型，因此会抛出验证错误。 配置选项常用的配置选项 allow_mutation: 如果设置为 False，模型实例将是不可变的。 extra: 控制如何处理未定义的字段。可选值包括 allow、forbid 和 ignore。 validate_all: 如果设置为 True，所有字段在实例化时都将被验证。 error_msg_templates: 自定义错误消息模板。 pydantic:v112345678910111213141516from pydantic import BaseModelclass User(BaseModel): name: str age: int class Config: # pydantic v1版本方式，在v2版本不再支持config方式的配置 allow_mutation = False extra = 'forbid' validate_all = Trueuser = User(name="John Doe", age=30)try: user.age = 31 # 这将引发错误，因为 allow_mutation 设置为 Falseexcept TypeError as e: print(e) 上述是在 pydantic:v1 版本中的格式，在 pydantic:v2版本不再支持config方式的配置，使用示例如下：pydantic:v21234567891011121314from pydantic import BaseModelclass User(BaseModel): name: str age: int allow_mutation = False extra = 'forbid' validate_all = Trueuser = User(name="John Doe", age=30)try: user.age = 31 # 这将引发错误，因为 allow_mutation 设置为 Falseexcept TypeError as e: print(e) 在这个示例中，allow_mutation 设置为 False，因此 user 实例是不可变的。 自定义错误消息可以通过 error_msg_templates 自定义错误消息：123456789101112131415from pydantic import BaseModel, validatorclass User(BaseModel): name: str age: int class Config: error_msg_templates = &#123; 'value_error.any_str.max_length': '字符串长度不能超过 &#123;limit_value&#125; 个字符', 'value_error.any_str.min_length': '字符串长度不能少于 &#123;limit_value&#125; 个字符' &#125; @validator('name') def name_length(cls, v): if len(v) &gt; 3: raise ValueError('name too long') return v 在这个示例中，我们自定义了两个错误消息模板，用于处理字符串长度的错误。 通过这些高级功能，Pydantic 提供了强大的数据验证和自定义能力，使得开发者可以轻松处理复杂的数据结构和验证逻辑。 参考资料pydantic官方文档]]></content>
      <categories>
        <category>编程拾慧</category>
        <category>python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[临床下一代测序的自动化与常规化专家共识]]></title>
    <url>%2F01.%E7%9F%A5%E8%AF%86-03.standard%2F%E5%85%B1%E8%AF%86-%E4%B8%B4%E5%BA%8A%E4%B8%8B%E4%B8%80%E4%BB%A3%E6%B5%8B%E5%BA%8F%E7%9A%84%E8%87%AA%E5%8A%A8%E5%8C%96%E4%B8%8E%E5%B8%B8%E8%A7%84%E5%8C%96%E4%B8%93%E5%AE%B6%E5%85%B1%E8%AF%86%2F</url>
    <content type="text"><![CDATA[2023年12月底，由康熙雄教授领衔撰写的《临床下一代测序的自动化与常规化专家共识》发表，近几年，NGS凭借其高通量、低成本和靶标广泛的优势而快速发展。并在2011年开始应用于临床并逐步深入，NGS目前已经成为无创产前、新生儿筛查、病原体鉴定、肿瘤分型、肿瘤靶向药物筛查和伴随诊断等方面迅速发展。但是行业快速发展的同时，NGS检测的自动化和规范化建设仍有一定的滞后性。本共识也是少有的，针对自动化和规范化形成的专家共识，对相关从业者也可以提供一定的参考。 共识针对NGS检测的全流程（从采样到遗传咨询，甚至涵盖了医患培训和数据安全等内容），提供了非常完善的规范化参考。 共识 １ ：规范化采样规范化采样是临床 NGS 常规化的基础，实现自动化采样是重点，是持续降低采样时间和成本并提高用户体验的手段。 目前，居家自采样仅适用于表皮拭子和唾液采集；院内采样建议培训专门的医生、护士或遗传咨询师进行样本的规范采集、保存转运以及人类遗传资源的合规管理等操作。 共识 ２ ：自动化核酸提取纯化和建库目前核酸提取纯化和建库的自动化程度已相对较高，未来需进一步提高其便捷度和降低操作周期，以及进一步提高在质检、杂交和洗脱试剂及耗材准备等方面的自动化水平，提高一体化程度并拓展适用范围，进一步将临床基因检测各环节有机整合。 同时，还需要在不同建库流程间构建一致性评判标准。 共识 ３ ：更灵活的基因测序系统临床 NGS 应用实现常规化，亟需更灵活的基因测序系统，包括灵活的通量、更低的起始量、更短的检测周期以及不同工作流程模块的集成。同时，测序仪与自动化核酸提取纯化和建库设备的有机整合是未来临床 NGS 常规化的趋势和必备的硬件基础设施。 共识 ４：生物信息分析自动化及参考数据库建设物信息分析自动化需建立不同样本类型及分析场景的规范及标准，并扩展分析工具对不同数据类型的兼容，形成既可独立分析的管道，又可组装至自动化系统的分析流程；此外，需建立普遍适用且可定期更新的参考数据库并建立版本控制。 在对大规模数据分析时，需配备对应的网络传输工具以及数据管理架构，形成有效的协作空间。 共识 ５：临床 ＮＧＳ 数据解读和遗传咨询常规化临床 NGS 检测前／ 检测后的遗传咨询是有效利用 NGS 数据并服务患者的必要环节。 我国需要尽快健全遗传咨询师的培养、培训和认证体系，将临床 NGS 纳入医院信息系统，鼓励医院设置遗传咨询师岗位， 发挥遗传咨询师在科室团队中的作用。 同时，促进遗传咨询远程会诊平台的建设，推动开发权威的标准化在线决策支持平台，加强报告解读过程的自动化。 对于部分无需深度遗传咨询的疾病检测，可建立自动化报告系统产品来辅助医生提高效率。 共识 ６：整合临床 NGS 结果和表型信息至医疗信息化系统将临床 NGS 报告通过数字化平台分发，可提高诊疗协作效率，有利于数据报告的迭代优化，未来可辅以数据可视化及数据管理等功能；将临床 NGS 结果与 HIS 和 EHR 有机融合，提高诊断效率，提高临床科研便捷性，有助于遗传咨询体系的完善和个人及群体遗传资源的保护，从长远角度将更有利于推动建立真正的数字化医疗健康基础设施。 共识 ７：LDT&amp;IVD并存发展IVD 和 LDT 是未来临床 NGS 服务并存的两种方式，建议出台政策鼓励和加速不同病种的 IVD 产品研发；建议加速出台促进 LDT 的政策、规范及行业指南，鼓励基于 NGS 的创新应用， 推动更多有效的 NGS 技术普及临床，服务大众 共识 ８：降低和分摊NGS成本，促进其加速进入医保降低成本是关键。 建议通过提升测序技术和优化检测流程实现降低 NGS 的应用成本；与此同时，促进 NGS 进入医保，以及推动社保和商业保险参与，鼓励新药研发与临床诊疗结合，拓展分摊 NGS 成本的途径。 共识 ９：加强医患培训和 NGS 科普教育加强宣教是基础。 推进临床 NGS 的常规化，应将加强开展专业技术人员培训以及对患者及家属进行科普教育作为基础。 同时，加强相关人员对临床 NGS 结果和规范的理解和使用，这有助于缓解当前专业人员极度匮乏的现状。 共识 10：建立数据安全管理和利用的规范加强保护促进利用。 加强人类遗传资源的管理是保障临床 NGS 数据安全及合规利用的基础，需为此建立数据合规转化的系统及平台，并加以规范和试点。 使相关人群的重要且敏感的基因信息以及临床诊疗信息得以体系化管理和保护；与此同时，临床 NGS 数据有效且合规的利用对于科技突破、产业发展和临床服务具有战略价值。]]></content>
      <categories>
        <category>NGS</category>
        <category>共识</category>
      </categories>
      <tags>
        <tag>中国生物医学工程学报</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[常用数据库 - OMIM]]></title>
    <url>%2F01.%E7%9F%A5%E8%AF%86-04.Database%2FDatabase-OMIM%2F</url>
    <content type="text"><![CDATA[OMIM 全称叫做 Online Mendelian Inheritance in Man 是一个不断更新的人类孟德尔遗传病的数据库。数据库主要记录人类基因变异和表型性状之间的关系。每日更新，并免费对外提供。OMIM 由约翰·霍普金斯大学医学院 McKusick-Nathans 遗传医学研究所在 Ada Hamosh 博士的指导下撰写和编辑。 发展背景OMIM的发展历史悠久，最早由 Dr. Victor A. McKusick 在20世纪60年代早期发起，作为孟德尔性状和疾病 (Mendelian traits and disorders)的目录，命名为人类孟德尔遗传 (Mendelian Inheritance in Man，MIM)。 在1966年到1998年间，MIM出版了12个版本。在线版的OMIM是1985年由美国国家医学图书馆 (National Library of Medicine)和约翰·霍普金斯大学 (Johns Hopkins)威廉·H·韦尔奇医学图书馆 (William H. Welch Medical Library)合作创建的。从1987年开始在互联网上广泛使用。1995年，美国国家生物技术信息中心 (NCBI)参与了OMIM的万维网开发。 至今（2024.02）为止，已经收录了17，216个基因。 MIM Number Prefix Autosomal X Linked Y Linked Mitochondrial Totals Gene descriptio 16,359 769 51 37 17,216 Gene and phenotype, combined 21 0 0 0 21 Phenotype description, molecular basis known 6,380 386 5 34 6,805 Phenotype description or locus, molecular basis unknown 1,389 110 4 0 1,503 Other, mainly phenotypes with suspected mendelian basis 1,639 100 3 0 1,742 Totals 25,788 1,365 63 71 27,287 并仍然以每个月几十个基因的速度在进行增长，OMIM数据库记录了从1995年至今每个月的更新记录。 内容介绍MIM编号数据库中的每一条记录,都会有一个唯一的由6位数字组成的OMIM编号，不同数字开头的编号含义不同，其中： 数字1和2 开头的，代表常染色体上的相关位点和表型（1995年之前创建的） 数字3开头的，代表X染色体上的相关位点和表型 数字4开头的，代表Y染色体上的相关位点和表型 数字5开头的，代表线粒体上的相关位点和表型 数字6开头的，代表常染色体上的相关位点和表型（1995 年之后创建的）参考自MIM [FAQ](https://www.omim.org/help/faq) 对于等位基因，每一条记录都有一个唯一的MIM编号，由点号分隔的两部分构成，点号之前是突变位点对应的OMIM编号，点号之后是一个由4位数字构成的唯一ID, 用来记录等位基因。以EGFR为例，该基因对应的MIMID为131550 ，同时含有131550.0001、131550.0002、131550.0003、131550.0004、131550.0005、131550.0006、131550.0007 几个不同的等位基因型。 MIM编号的前缀含义 以 开头记录以””开头，主要记录基因的相关信息，比如 *131550 以 # 开头条目号之前的数字符号 (#)表示它是描述性条目，通常是表型，并不代表唯一的基因座。使用数字符号的原因在条目的第一段中给出。与表型相关的任何基因的讨论位于第一段中描述的另一个条目中。比如： #616069、#211980 以 + 开头条目编号前的加号 (+) 表示该条目包含已知序列和表型的基因的描述。 以 % 开头条目号之前的百分号 (%) 表示该条目描述了已确认的孟德尔表型或表型基因座，但其潜在分子基础尚不清楚。 以 ^ 开头条目号之前的插入符号 (^) 表示该条目不再存在，因为它已从数据库中删除或移至指示的另一个条目。 没有符号前缀条目号之前没有符号通常表示对表型的描述，尽管怀疑该表型的孟德尔基础，但尚未明确确定，或者该表型与另一个条目中的表型的区别尚不清楚。 MIM库内各字段内容介绍Inheritance 显示内容 含义 AR - Autosomal recessive 常染色体隐性遗传 AD - Autosomal dominant 常染色体显性遗传 DR - Digenic recessive 双基因隐性 IC - Isolated cases 孤立的案例 SMu - Somatic mutation 体细胞突变 Mu - Multifactorial 多因素 PD - Pseudoautosomal dominant 假常染色体显性遗传 PR - Pseudoautosomal recessive 假常染色体隐性遗传 DD - Digenic dominant 双基因显性 ICB - Inherited chromosomal imbalance 遗传性染色体失衡 Mi - Mitochondrial 线粒体 SMo - Somatic mosaicism 体细胞镶嵌 XL - X-linked X连锁 XLD - X-linked dominant X连锁显性 XLR - X-linked recessive X连锁隐形 YL - Y-linked Y连锁 Phenotype mapping key该字段记录了疾病的研究进展情况，仅通过连锁图谱定位，或是已经有充分的​分子基础进展等。在查询表格中，会标注数字编号（鼠标悬停会有提示），在这里列出可能出现的4种编号对应的含义 key 说明 1 该疾病已根据其与基因的关联被放置在图上，但潜在的缺陷尚不清楚。 2 该疾病已通过连锁被放置在图谱上；尚未发现突变 3 该疾病的分子基础已知；基因中发现了突变 4 多个基因缺失或重复综合征；多个基因被删除或重复导致表型。]]></content>
      <categories>
        <category>知识沉淀</category>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>数据库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[GO-代码打包]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-08.Go%2FGO-03.%E4%BB%A3%E7%A0%81%E6%89%93%E5%8C%85%2F</url>
    <content type="text"><![CDATA[Go ，编译型语言，性能很好，原生高并发，跨平台，语法简单，有自动GC，相对安全的指针操作。其编译特性在目前各种代码/数据保密需求的环境下同样也成为一种优势。 Python ，解释性语言，语法简单，更加贴近英语的书写习惯，不过性能不好，但又因为他的基于C解释器，很容易去和C的库进行通讯，因此也被称为“胶水语言”。 实例代码为了便于操作和理解，一下所有打包过程均基于如下示例代码（main.go)进行处理。1234567891011121314151617181920212223242526272829package mainimport "fmt"func main() &#123; /* 定义局部变量 */ var a int = 100 var b int = 200 var ret int /* 调用函数并返回最大值 */ ret = max(a, b) fmt.Printf( "最大值是 : %d\n", ret )&#125;/* 函数返回两个数的最大值 *///export maxfunc max(num1, num2 int) int &#123; /* 定义局部变量 */ var result int if (num1 &gt; num2) &#123; result = num1 &#125; else &#123; result = num2 &#125; return result&#125; 非常重要:，export 表示把go的函数映射到python的函数调用,如果没有export（格式必须是 //export func_name），那么就不能生成.h文件，python也就无法调用该函数同时有几个注意事项： //export add 在函数定义之前添加上注释来告诉编译器哪些定义可以被 C 引用，注意 // 和 export 之前不能有空格，否则会导出失败的。 main() main 函数一定不能少，即使没有任何一行代码也没事； import &quot;C&quot; 这个必须要加载 Go 源文件前，这一点必须做，应该就是告诉编译器我要即将编译的软件需要做为 C 的库而不直接是二进制。这个包也提供一些功能让 Go 去直接操作 C 的数据结构等等。 编译成可执行文件（直接运行）go本身支持对代码进行打包，所以可以直接通过build 将代码打包可执行文件。123go build main.go# 指定打包后文件名称，默认命名为go代码文件前缀go build -o max main.go 跨平台交叉编译和其他的编程语言不同，很多编程语言都是基于编译平台生成可执行的二进制文件，并不能跨平台生成可执行文件（比如没办法再windows下生成Linux可用的执行文件），但是再使用go进行编译的过程中发现，GO支持这个功能，可以实现windows进行代码开发，编译后直接再Linux上运行。 Mac 下编译 Linux 和 Windows 64位可执行程序1234# 编译成Linux可用CGO_ENABLED=0 GOOS=linux GOARCH=amd64 go build# 编译成Windows可用CGO_ENABLED=0 GOOS=windows GOARCH=amd64 go build Linux 下编译 Mac 和 Windows 64位可执行程序1234# 编译成Mac可用CGO_ENABLED=0 GOOS=darwin GOARCH=amd64 go build# 编译成Windows可用CGO_ENABLED=0 GOOS=windows GOARCH=amd64 go build Windows 下编译 Mac 和 Linux 64位可执行程序1234567891011#编译成Mac可用SET CGO_ENABLED=0SET GOOS=darwinSET GOARCH=amd64go build# 编译成Linux可用SET CGO_ENABLED=0SET GOOS=linuxSET GOARCH=amd64go build GOOS：目标平台的操作系统（darwin、freebsd、linux、windows） GOARCH：目标平台的体系架构（386、amd64、arm） 交叉编译不支持 CGO 所以要禁用它 编译成共享静态库（cython调用）编译成c可以调用的库，在go代码的导入阶段必须执行 import &quot;C&quot; 。1go build --buildmode=c-archive -o test.lib.a test.go 其中，–buildmode=c-archive 告诉 Go 来编译一个静态库，-o 是输出文件的名字，这里我们输出为 library.a。此时，目录下应该有一个 library.a 的文件和 library.h 的头文件。 打包成动态链接库（python调用）1go build --buildmode=c-shared -o library.so test.go 打包完成后会生成有两个文件 library.so 和 library.h 。然后我们就可以在python代码中调用相关的功能了1234567891011121314import ctypeslib = ctypes.cdll.LoadLibrary("library.so") # 对应生成的动态链接库代码# 简单的调用print(lib.max(1, 2)) # max就是对应打包代码中 export的函数# 系统的调用，需要对函数的输入和输出的数据类型进行声明GoInt64 = ctypes.c_int64GoInt = GoInt64add = lib.addadd.argtypes = [GoInt64, GoInt64]add.restype = GoInt64res = add(GoInt(1), GoInt(2)) 在整体使用的时候，需要注意相关变量的定义，这部分后续有具体业务使用时，在进行细化补充。其中各类数值类型在不同语言环境下的对应关系如下，| ctypes | Python | C | Go.h | Go || ———— | ————– | ————————————– | —————- | ——————— || c_bool | bool | _Bool | bool | bool || c_byte | int | char | GoInt8 | int8 || c_ubyte | unsigned char | int | GoUint8 | uint8 || c_short | short | int | GoInt16 | int16 || c_ushort | unsigned short | int | GoUint16 | uint16 || c_int | int | int | GoInt32 | int32 || c_uint | int | unsigned int | GoUint32 | uint32 || c_ulong | int | unsigned long | GoUint32 | uint32 || c_longlong | int | int64 or long long | GoInt64 or GoInt | int64 or int || c_ulonglong | int | unsigned int64 or unsigned long long | GoUint64 | uint64 || c_size_t | int | size_t SIZE_TYPE | GoUintptr | uintptr || c_ssize_t | int | ssize_t or Py_ssize_t | Go中无定义 | || c_float | float | float | GoFloat32 | float32 || c_double | float | double | GoFloat64 | float64 || c_longdouble | float | long double | GoFloat64 | float64 || 无定义 | float | float _Complex | GoComplex64 | complex64 || 无定义 | float | double _Complex | GoComplex128 | complex128 or complex | 其中各类字符串类型在不同语言环境下的对应关系如下，| ctypes | Python | C || ——— | ——————– | ————————- || c_char | 长度为1的bytes | char || c_wchar | 长度为1的string | wchar_t || c_char_p | bytes object or None | char (NUL terminated) || c_wchar_p | string or None | wchar_t (NUL terminated) || c_char | 长度为1的bytes | char | 除了上述内容，还需要注意一些结构体的对应、指针、切片、通道的处理等问题，后续可以更详细的研究后述的相关参考资料。 参考资料：https://zhuanlan.zhihu.com/p/518374146]]></content>
      <categories>
        <category>编程拾慧</category>
        <category>GO</category>
      </categories>
      <tags>
        <tag>GO</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[携带者筛查相关指南]]></title>
    <url>%2F01.%E7%9F%A5%E8%AF%86-11.%E9%81%97%E4%BC%A0%E7%97%85%2F%E6%8C%87%E5%8D%97%E5%85%B1%E8%AF%86-01.%E6%90%BA%E5%B8%A6%E8%80%85%E7%AD%9B%E6%9F%A5%E7%9B%B8%E5%85%B3%E6%8C%87%E5%8D%97%E6%B1%87%E6%80%BB%2F</url>
    <content type="text"><![CDATA[2020.04 - 《单基因隐性遗传病扩展性携带者筛查的遗传咨询》 2020.10 - 《关于胎儿染色体异常筛查指南》[^1][^1]:American College of Obstetricians and Gynecologists’ Committee on Practice Bulletins—Obstetrics; Committee on Genetics; Society for Maternal-Fetal Medicine. Screening for Fetal Chromosomal Abnormalities: ACOG Practice Bulletin, Number 226. Obstet Gynecol. 2020 Oct;136(4):e48-e69. doi: 10.1097/AOG.0000000000004084. PMID: 32804883. 2021.06 - 《基于DNA的健康人群携带者筛查声明》[^2][^2]:Murray MF, Giovanni MA, Doyle DL, Harrison SM, Lyon E, Manickam K, Monaghan KG, Rasmussen SA, Scheuner MT, Palomaki GE, Watson MS; ACMG Board of Directors. DNA-based screening and population health: a points to consider statement for programs and sponsoring organizations from the American College of Medical Genetics and Genomics (ACMG). Genet Med. 2021 Jun;23(6):989-995. doi: 10.1038/s41436-020-01082-w. Epub 2021 Mar 16. PMID: 33727704. 2021.07 - 《隐性遗传病携带者筛查指南》[^3][^3]:Gregg AR, Aarabi M, Klugman S, Leach NT, Bashford MT, Goldwaser T, Chen E, Sparks TN, Reddi HV, Rajkovic A, Dungan JS; ACMG Professional Practice and Guidelines Committee. Screening for autosomal recessive and X-linked conditions during pregnancy and preconception: a practice resource of the American College of Medical Genetics and Genomics (ACMG). Genet Med. 2021 Oct;23(10):1793-1806. doi: 10.1038/s41436-021-01203-z. Epub 2021 Jul 20. Erratum in: Genet Med. 2021 Aug 27;: PMID: 34285390; PMCID: PMC8488021. 2021年7月20日，美国医学遗传学和基因组学学会（the American College of Medical Genetics and Genomics ，ACMG，以下简称“ACMG”）更新了常染色体隐性和X连锁疾病携带者筛查指南，建议采用一种新的基因分级方法，对怀孕或孕前的所有个体进行常染色体隐性和X连锁疾病筛查。更新后的指南中，作者建议建立一个基于分级结构的携带者筛查系统。在该系统中，所有怀孕患者和计划怀孕患者都应接受三级携带者筛查。作者将第三级筛查定义为携带者筛查，筛查频率为1/200或更高（包括X连锁疾病）。ACMG现在已经明确列出113个基因，包括常染色体隐性基因和X连锁遗传基因（X-linked），这些基因应被视为对孕前和产前患者的标准筛查基因。 2022.03 -《生殖携带者筛查和产前诊断的基因检测》[^4][^4]: AIM CLINICAL APPROPRIATENESS GUIDELINES FOR GENETIC TESTING FOR REPRODUCTIVE CARRIER SCREENING AND PRENATAL DIAGNOSIS.2022 AIM Specialty Health GEN03-0322.1 2022年3月，AIM 发布生殖携带者筛查和产前诊断基因检测临床适用性指南，指南中涉及的检测包括家族突变和常见和种族遗传疾病的携带者检测；植入前筛查和诊断；产前筛查和诊断；并测试复发性流产和不孕症。 2024.01 - 《孕前及孕早期常见隐性单基因遗传病携带者筛查临床应用专家共识》 2024.02 - 《针对生育人群的携带者筛查实验室和临床实践专家共识》]]></content>
      <categories>
        <category>共识</category>
        <category>指南</category>
        <category>遗传病</category>
      </categories>
      <tags>
        <tag>遗传病</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[遗传病相关医学知识概述]]></title>
    <url>%2F01.%E7%9F%A5%E8%AF%86-11.%E9%81%97%E4%BC%A0%E7%97%85%2F0002.%E9%81%97%E4%BC%A0%E7%97%85%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[三级防控针对出生缺陷，我国采取三级防控体系： 一级防控：婚前、孕前 防止出生缺陷儿的发生 二级防控：孕期 防止出生缺陷儿的出生 三级防控：产后 出生缺陷儿出生后的在诊断早治疗 出生缺陷的类类型 根据遗传性疾病发生原因、部位，其遗传的方式可以分为五类。 单基因遗传病。由染色体上的单个基因突变而引起的遗传性疾病，包括分子病和遗传代谢病。分子病即人体内蛋白质分子结构异常所致的疾病，如血红蛋白病、某些凝血因子缺乏、补体系统缺陷及胶原蛋白异常等。遗传代谢病是因酶的先天缺陷所引起的疾病，如氨基酸、糖类及脂质代谢异常等。按遗传方式不同，单基因病还可分为常染色体显性遗传性疾病、常染色体隐性遗传性疾病、X连锁显性遗传性疾病、X连锁隐性遗传性疾病和Y连锁遗传性疾病。常见的单基因遗传病有多指(趾)、白化病、先天聋哑、小头白痴、血友病、色盲等。 多基因遗传病。由多对基因控制的遗传病。这些基因单独对遗传性状作用小，称为微效基因，几种微效基因累加起来，就产生明显的表型效应。多基因遗传病受遗传因素和环境因素的双重影响，具有家族聚集现象，但没有单基因病遗传中所见到的系谱特点。多基因病在群体中的发病率高达15%~20%，包括一些先天畸形和常见病(如原发性高血压、冠心病、糖尿病、无脑儿、先天性心脏病和精神分裂症等)。 染色体病。由染色体结构或数目异常引起的一类疾病。所谓数目、结构异常，包括染色体数目的增多或减少，染色体部分断裂后重排时出现的易位、缺失、倒位、重复等。染色体的各种畸变，使所载基因发生数量上或排列组合上的改变，遗传物质失去正常状态而引起疾病。由于每个染色体小片段中均含有多个基因，因而染色体畸变常涉及较多基因，致使机体多种器官结构和功能异常，故往往表现为综合征，其危害一般要比单基因病和多基因病严重。染色体病可分为两种：(1)常染色体病，如唐氏综合征、猫 叫综合征；(2)性染色体病，如先天性卵巢发育不全综合征、先天性睾丸发育不全综合征。 线粒体遗传病。是由于线粒体基因突变导致的疾病，伴随线粒体传递，呈细胞质遗传。由于精子的细胞质含量极少，受精卵的线粒体DNA几乎全部来自卵子，所以线粒体遗传病呈现出母系遗传的特点。现已发现100多种疾病与线粒体基因突变或结构异常有关，如糖尿病、帕金森病。 体细胞遗传病。由体细胞内遗传物质发生突变引起的疾病。这种遗传物质的突变仅仅发生在特定的体细胞内，不涉及生殖细胞，所以此类疾病一般不会遗传给后代，如肿瘤等。]]></content>
      <categories>
        <category>遗传病</category>
      </categories>
      <tags>
        <tag>遗传病</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Software-geogebra-数学可视化.md]]></title>
    <url>%2F03.%E5%BA%94%E7%94%A8-99.other%2FSoftware-geogebra-%E6%95%B0%E5%AD%A6%E5%8F%AF%E8%A7%86%E5%8C%96%2F</url>
    <content type="text"><![CDATA[geogebra 是一款免费的课堂活动数字工具, 用于绘图计算, 几何作图, 白板协作等等。而在我们工作中，有些涉及数学模拟的信息，当我们需要向其他人进行展示时，那么也可以使用这类格局，让听众更容易理解我们所要表达的内容。官方手册使用指南 使用说明 首先我们登录官网启动计算器启动计算器后，我们可以看到GeoGebra同时可以展示多种类型的数据 概率：展示常见概率分布，提供相关参数可以进行分布的调整 3D计算器： 绘图：针对一些个性化比较强，逻辑比较复杂的数据进行自定义绘图今天我们也主要介绍绘图这部分功能。 复现我们之前的图片因为也是第一次使用，所以在这里主要是​告诉大家如何复现我们上次的结果。其实绘制示意图，主要包括3个步骤：​图片的绘制分成几个部分： 我们需要确定我们要绘制的函数对应的​数学公式。有些数学分布​再geogebra中是内置的，比如我们用到的二项分布。除了二项分布，还支持正态分布、t分布等等， 粘贴分布后，会针对公式中使用的未定义变量生成一个滑轨（设置变量的取值范围和滑动步长）。每个分布或者公式的值，其实都可以作为变量再其他下游公式中进行引用。 调整坐标轴的展示范围，优化我们的观感 图形界面的操作，​文字描述确实很难说明清楚，可能大家看了还是不会画图，所以为了帮助理解，制作了一个动图，我们可以看下图尝试复现： 结果保存导出动图1ExportImage(&quot;filename&quot;, &quot;image.gif&quot;, &quot;type&quot;, &quot;gif&quot;, &quot;slider&quot;, depth, &quot;loop&quot;, true, &quot;time&quot;, 100, &quot;width&quot;, 2000) 其中参数是类似字典的格式，提供| 参数 | 说明 || ——– | —————— || filename | 输出文件名 || type | 文件格式 || slider | 滑动变量 || loop | 是否循环 || time | 两帧之间的时间间隔 || width | 图片宽度（单位像素 | 不过导出的时候，网页版导出遇到一些问题，我最后是安装的桌面版（免费哒），就可以正常导出了。就可以导出如下格式的gif动图了。]]></content>
      <categories>
        <category>可视化</category>
      </categories>
      <tags>
        <tag>Windows</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[携带者筛查的背景知识]]></title>
    <url>%2F01.%E7%9F%A5%E8%AF%86-01.NGS%2F2024-01-25.%E6%90%BA%E5%B8%A6%E8%80%85%E7%AD%9B%E6%9F%A5%E7%9A%84%E8%83%8C%E6%99%AF%E7%9F%A5%E8%AF%86%2F</url>
    <content type="text"><![CDATA[生育诊断生育方向的检测整体分为三级防控体系，综合实施三级防控有利于降低婴儿死亡率、提高人均预期寿命，有利于减少残疾、提高出生人口素质，有利于促进家庭幸福和社会和谐发展因此开展基于孕前-产前-出生后的出生缺陷三级防控体系尤其重要，三级防控分别是： 一级防控: 婚前、孕前（防止发生） 二级防控： 孕期（减少缺陷儿的出生） 三级防控：新生儿（缺陷患儿的早诊早治） 常见检测疾病 症状 治病类型 基因 携带率 发病率 表现 脊肌萎缩症（SMA） 常染色体隐性遗传 SMN1 功能缺失。 1/40-50 1/6000-10000 肌肉无力和肌萎缩，逐渐瘫痪，失去呼吸、吞咽等能力而死亡。 肌肉萎缩症（DMD） 性染色体隐性遗传 DMD基因功能缺失 1.6%（2/122） 1/3600（男） 一般 6 岁左右出现症状， 逐渐累及呼吸、 心脏及骨骼系统， 患者平均 19 岁死于心肺功能衰竭 珠蛋白生成障碍贫血 常染色体隐性遗传 chr6；chr11 地贫区域 4-24% 轻者终生没事，重者胎死，不同阶段发病。 检测类型]]></content>
      <categories>
        <category>知识沉淀</category>
        <category>生育</category>
      </categories>
      <tags>
        <tag>携带者筛查</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Software-代码可视化.md]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-00.%E7%BC%96%E7%A8%8B%E8%BE%85%E5%8A%A9%E5%B7%A5%E5%85%B7%2FSoftware-%E4%BB%A3%E7%A0%81%E5%8F%AF%E8%A7%86%E5%8C%96%2F</url>
    <content type="text"><![CDATA[最近一些工作需要，觉得最好的方案是对bwa进行源码层面的优化，但是之前虽然看过c，但是也仅仅停留在hello world的水平，所以要从头梳理bwa的代码，还是蛮大的工作量，所以想着有没有一些代码函数梳理及依赖关系可视化的工具，所以进行了一些调研。因为自己主要是用python，所以为了兼容性考虑（毕竟只支持c的话性价比略低），所以先和chatgpt咨询了一下，得到如下的一些工具推荐。一共5款 Doxygen： Doxygen 是一个通用的文档生成工具，支持多种编程语言，包括C、Perl和Python。通过适当的配置，你可以生成这三种语言的代码文档，并包括函数之间的关系。 Graphviz： Graphviz 是一个通用的图形可视化工具包，支持多种语言。你可以使用Graphviz创建调用图、依赖图等，同时适用于C、Perl和Python。 cppDepend： cppDepend 不仅支持C++，还可以用于C、Perl和Python的代码分析和可视化。它提供了交互式的依赖图，帮助你理解代码结构和函数之间的关系。 SonarQube： SonarQube 是一个用于管理代码质量的开源平台，支持多种编程语言，包括C、Perl和Python。你可以使用适当的插件进行深入的代码分析。 Understand： SciTools Understand 支持多种编程语言，包括C、Perl和Python。它提供了代码可视化、度量和分析功能，帮助你理解代码的结构和复杂性。 网上看了下，介绍 Understand 是初步看下来，评价比较好的一款，所以优先测试这个。 Understand软件破解 首先下载 Understand v6.4.1141 其他版本破解方式可能无效 打开HxD(好兄弟)软件，然后使用他打开刚刚复制的exe文件。好兄弟下载链接 使用Ctrl+F, 文本搜索 的方式搜索”areYouThere” ， 用”IamNotHere!” 替代。（都不带引号） 然后Ctrl+F, 字节序列 模式搜索”45 33 FF 41 0F B6 C6 48 3B DF 44 0F 4E F8”，替换为 “41 BF 01 00 00 00 90 90 90 90 90 90 90 90” 。 Ctrl+S保存退出，然后将刚刚修改的在桌面上的文件，拖到之前的bin文件替换原有的exe即可 简单应用 Show Graphic Views展示项目内各个代码和模块，也可以显示模块之间的关联关系。]]></content>
      <categories>
        <category>可视化</category>
      </categories>
      <tags>
        <tag>可视化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Database-ENCODE-人类基因组_黑名单]]></title>
    <url>%2F01.%E7%9F%A5%E8%AF%86-04.Database%2FDatabase-ENCODE-%E4%BA%BA%E7%B1%BB%E5%9F%BA%E5%9B%A0%E7%BB%84_%E9%BB%91%E5%90%8D%E5%8D%95%2F</url>
    <content type="text"><![CDATA[高通量测序检测基因组变异的过程依赖准确的基因组注释和绘图。而在组装困难的区域存在底层注释的不一致(T2T基因组的到来也许可以有效的缓解这个问题带来的影响）。但是在hg19和38的人类基因组中，相对于实际的基础基因组序列，参考序列中的重复区会由于区域的特殊性出现测序数据的富集或测序数据中模板的代表性不足导致整体组装结果出现噪音。这些有问题的区域其实没有得到足够的关注，通常被忽略或过滤掉。在ENCODE的项目中，使用 blacklist 来排除基因组组装结果中的错误信号和假阳性信号区域带来的影响。同时为了客观的方式生成黑名单，ENCODE 开发了一套程序来标记具有假阳性信号的区域。构建的方式是基于大量的样本。 构建方式参考github项目项目已经针对hg19和GRCh38基因组提供了构建好的黑名单信息。其中Hg19的黑名单说明 reference The ENCODE Blacklist: Identification of Problematic Regions of the Genomepdf下载]]></content>
      <categories>
        <category>知识沉淀</category>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>数据库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[NGS-Dev-使用UMI的低频变异检测的性能预估模型.md]]></title>
    <url>%2F01.%E7%9F%A5%E8%AF%86-01.NGS%2FNGS-Dev-%E4%BD%BF%E7%94%A8UMI%E7%9A%84%E4%BD%8E%E9%A2%91%E5%8F%98%E5%BC%82%E6%A3%80%E6%B5%8B%E7%9A%84%E6%80%A7%E8%83%BD%E9%A2%84%E4%BC%B0%E6%A8%A1%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[在我们之前的文章《深度和检测限测算》中，我们介绍了在进行一个产品开发的时候，我们应该如何比较合理的确定产品特定检测限下的预期测序深度。但是这个方法对于UMI的测序数据可能会没有那么友好，因为在使用UMI时，并不是单纯的要求深度高，而且对于Dup簇也有了一些额外的要求，比如一个Dup簇里面的read必须满足一定的标准我们才能认为这是一个有效的支持。所以直观的， 在没有UMI的时候，我们初始DNA投入量越高，其实越好，因为特定数据量下，初始投入越高，dup率越低，所以我们的有效深度越高。 在使用UMI的时候，如果我们的DNA投入量越高，最终的dup率越低，则会导致所有Dup簇都是单条reads支持，那么这时候，我们就不能发挥Dup簇内部矫正的优势，可能得不到满足特定要求的支持突变的Dup簇，反而会导致性能下降。 所以在此介绍一种最近考虑的性能测算方案。我们不在基于模型，而是充分发挥计算机的优势，对整个实验-测序的环节进行模拟。通过指定的实验指标，进行多次模拟寻找数据的性能表现。 相关代码实现，参考仓库： https://github.com/BGITumorBI/PerformanceMetricsModels]]></content>
      <categories>
        <category>NGS</category>
        <category>肿瘤检测</category>
        <category>模拟评估</category>
      </categories>
      <tags>
        <tag>基础概念</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习中的距离计算]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-20.MachineLearn%2F1101.%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E8%B7%9D%E7%A6%BB%E8%AE%A1%E7%AE%97-%E6%A6%82%E8%BF%B0%2F</url>
    <content type="text"><![CDATA[距离定义：基础知识在机器学习中，我们经常需要计算样本之间的差异，进而评价个体的相似性和类别等信息。特征空间中两个样本之间的距离就是两个样本相似性的一种反映。常见的分类和聚类算法，如k近邻、k-means、层次聚类等、等都会选择一种距离或相似性的度量方法。根据数据特性的不同，可以采用不同的度量方法。一般而言，定义一个函数 d ( x , y ) d(x,y) d(x,y), 若它是一种“距离度量”，则需要满足一些基本性质： 非负性： d ( x , y ) ≥ 0 两点之间的距离一定是非负的同一性： d ( x , y ) = 0 ⇔ x = y 当且仅当两个点是同一个点时，距离为0对称性： d ( x , y ) = d ( y , x ) 交换两个点的位置，不会对两点见的距离计算产生影响三角不等式： d ( x , y ) ≤ d ( x , z ) + d ( z , y ) 三角形定理任意两点距离之和大于第三个点 距离定义（一）：欧几里得距离（Euclidean Distance）欧几里得距离或欧几里得度量是欧几里得空间中两点间的即直线距离。使用这个距离，欧氏空间成为度量空间，相关联的范数称为欧几里得范数。$$d(x, y)=({\sum_{i=1}^n|x_i-y_i|^2})^{1/2} = \sqrt{(x_1-y_1)^2+(x_2-y_2)^2+…+(x_n-y_n)^2}$$ python 计算距离的代码12345def EuclideanDistance(x, y): import numpy as np x = np.array(x) y = np.array(y) return np.sqrt(np.sum(np.square(x-y))) 距离定义（二）：曼哈顿距离（Manhattan Distance）曼哈顿距离是种使用在几何度量空间的几何学用语，用以标明两个点在标准坐标系上的绝对轴距总和。 下图中红线代表曼哈顿距离，绿色代表欧氏距离，也就是直线距离，而蓝色和黄色代表等价的曼哈顿距离。 曼哈顿距离在2维平面是两点在纵轴上的距离加上在横轴上的距离，即： $$ d(x, y)=({\sum_{i=1}^n|x_i-y_i|^1})^{1/1}=∣ x 1 − y 1 ∣ + ∣ x 2 − y 2 ∣$$ 对于一个具有正南正北、正东正西方向规则布局的城镇街道（如：曼哈顿），从一点到达另一点的距离正是在南北方向上旅行的距离加上在东西方向上旅行的距离，因此，曼哈顿距离又称为出租车距离。曼哈顿距离不是距离不变量，当坐标轴变动时，点间的距离就会不同。曼哈顿距离示意图在早期的计算机图形学中，屏幕是由像素构成，是整数，点的坐标也一般是整数，原因是浮点运算很昂贵，很慢而且有误差，如果直接使用AB的欧氏距离（欧几里德距离：在二维和三维空间中的欧氏距离的就是两点之间的距离），则必须要进行浮点运算，如果使用AC和CB，则只要计算加减法即可，这就大大提高了运算速度，而且不管累计运算多少次，都不会有误差。 多维空间中的曼哈顿距离：$$d(x, y)=\sum_{i=1}^n|x_i-y_i|$$ python 计算距离的代码12345def ManhattanDistance(x, y): import numpy as np x = np.array(x) y = np.array(y) return np.sum(np.abs(x-y)) 距离定义（三）：闵可夫斯基距离（Minkowski Distance）其实细心的话，可以发现我们在介绍曼哈顿和欧几里得公式时候，有一个相似格式的前置公式。$$ d(x, y)=({\sum_{i=1}^n|x_i-y_i|^1})^{1/1} 和 d(x, y)=({\sum_{i=1}^n|x_i-y_i|^2})^{1/2} $$ 而 闵可夫斯基距离其实就是对这两个公式的一种抽象，所以闵可夫斯基距离本身不是一个具体的距离计算方式，而是一类特定格式的距离计算类型。$$ d(x, y)=({\sum_{i=1}^n|x_i-y_i|^p})^{1/p} $$而之前介绍的欧几里得距离（p=2）和曼哈顿距离（p=1）就是闵可夫斯基距离的两种最常用的特例。 为了帮助大家更感观的理解感受，从计算公式可以知道，坐标平面上到原点的欧氏距离(p=2)相等的所有点组成的形状是一个圆，比如距离为1，则为一个半径为1的圆。类似的当p=1时，形状为菱形，p趋于∞时，形状是正方形，如图2：而针对不同的p值，我们到原地距离相同的点所构成的同行分别示例如下： 注意，当p&lt;1时，闵可夫斯基距离不再满足直递性（即三角不等式），举个简单的反例：假设点xxi=(0,0)，xxj=(1,1)，和另一点xxu=(0,1)，当p&lt;1时，带入公式（1.1）计算xxi=(0,0)到xxj=(1,1)的距离等于(1+1)1/p&gt;2, 而xxu=(0,1)到这两个点的距离都是1。 python 计算距离的代码12345def MinkowskiDistance(x, y, p): import math import numpy as np zipped_coordinate = zip(x, y) return math.pow(np.sum([math.pow(np.abs(i[0]-i[1]), p) for i in zipped_coordinate]), 1/p) 距离定义（四）：切比雪夫距离（Chebyshev Distance）是闵可夫斯基距离的另一个特例 (p=+∞)，在p=+∞ 时候，我们可以对原公式进行如下转换$$ d(x, y)=({\sum_{i=1}^n|x_i-y_i|^{+∞}})^{1/+∞} $$$$= (|x_1-y_1|^{+∞}+|x_2-y_2|^{+∞}+…+|x_n-y_n|^{+∞})^{1/+∞} $$$$ ≈\max_{1&lt;=i&lt;=n}(|x_i-y_i|^{+∞})^{1/+∞} $$$$ ≈(\max_{1&lt;=i&lt;=n}(|x_i-y_i|)) $$基于上述结果，所以我们切比雪夫距离，实际计算的就是两点之间距离向量投影到所有维度后，投影距离最大的单维度上的距离。 国际象棋棋盘上二个位置间的切比雪夫距离是指王要从一个位子移至另一个位子需要走的步数。由于王可以往斜前或斜后方向移动一格，因此可以较有效率的到达目的的格子。下图是棋盘上所有位置距王位置的切比雪夫距离。 python 计算距离的代码12345def ChebyshevDistance(x, y): import numpy as np x = np.array(x) y = np.array(y) return np.max(np.abs(x-y)) 距离定义（五）：标准化的欧几里得距离（Standardized Euclidean Distance）其实在介绍切比雪夫距离时候，我们就可以发现一个问题，两个点在不同维度的距离上是可能是不一致的，虽然在欧几里得距离（p=2）的影响没有切比雪夫距离（p=+∞）大，但是在某些情况下，这个影响依然是无法忽略的，所以产生了标准化的欧几里得距离，和常规欧几里得距离的区别就是，我们在计算距离前，先对各个维度的距离进行标准化，将各个分量都“标准化”到均值、方差相等的区间。$$ X∗=(X−m​)/s $$X* 为标准化后的值， X 为原值， m 为分量的均值， s 为分量的标准差。所以 n n n维空间中标准化的欧几里得距离为：$$ d ( x , y ) = \sqrt{\sum_{i=1}^n (( x_i − y_i)/ s_i )^2} $$ python 计算距离的代码12345678def StandardizedEuclideanDistance(x, y): import numpy as np x = np.array(x) y = np.array(y) X = np.vstack([x,y]) sigma = np.var(X, axis=0, ddof=1) return np.sqrt(((x - y) ** 2 /sigma).sum()) 距离定义（六）：马氏距离（Mahalanobis Distance）距离定义（七）：兰氏距离（Lance and Williams Distance）/堪培拉距离（Canberra Distance）距离定义（八）：余弦距离（Cosine Distance）余弦距离是一种用于测量两个向量方向的相似性的度量方法，而不考虑它们的大小。对于两个向量 $𝑢u$ 和 $𝑣v$，余弦距离可以表示为它们的夹角的余弦值的补数：$$cos(\theta)=1-\frac{u·v}{||u|| · ||v||}$$其中 $u·v $表示向量 $u$ 和 $v$ 的点积，$||u|| · ||v||$ 表示向量 $u$ 和 $v$ 的范数。 距离定义（九）：测地距离（Geodesic Distance）距离定义（十）： 布雷柯蒂斯距离（Bray Curtis Distance）距离定义（十一）：汉明距离（Hamming Distance）距离定义（十二）：编辑距离（Edit Distance，Levenshtein Distance）距离定义（十三）：杰卡德距离（Jaccard Distance）和杰卡德相似系数（Jaccard Similarity Coefficient）距离定义（十四）：Ochiia系数（Ochiia Coefficient）距离定义（十五）：Dice系数（Dice Coefficient）距离定义（十六）：豪斯多夫距离（Hausdorff Distance）距离定义（十七）：皮尔逊相关系数（Pearson Correlation）距离定义（十八）：卡方距离（Chi-square Measure）距离定义（十九）：交叉熵（Cross Entropy）距离定义（二十）：相对熵（Relative Entropy）/KL散度（Kullback-Leibler Divergence）距离定义（二十一）：JS散度（Jensen–Shannon Divergence）距离定义（二十二）：海林格距离（Hellinger Distance）距离定义（二十三）：α-散度（α-Divergence）距离定义（二十四）：F-散度（F-Divergence）距离定义（二十五）：布雷格曼散度（Bregman Divergence）距离定义（二十六）：Wasserstein距离（Wasserstei Distance）/EM距离（Earth-Mover Distance）距离定义（二十七）：巴氏距离（Bhattacharyya Distance）距离定义（二十八）：最大均值差异（Maximum Mean Discrepancy, MMD）距离定义（二十九）：点间互信息（Pointwise Mutual Information, PMI）参考资料：blog]]></content>
      <categories>
        <category>machine_learning</category>
      </categories>
      <tags>
        <tag>machine_learning</tag>
        <tag>距离计算</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Windows下的Linux子系统 - 基于WSL]]></title>
    <url>%2F03.%E5%BA%94%E7%94%A8-03.windows%2FWindows%E4%B8%8B%E7%9A%84Linux%E5%AD%90%E7%B3%BB%E7%BB%9F%2F</url>
    <content type="text"><![CDATA[Windows Subsystem for Linux (WSL) ，是Windows原生的，让开发人员直接在 Windows 上运行 GNU/Linux 环境（包括大多数命令行工具、实用程序和应用程序）的子系统，无需修改，无需传统虚拟机或双启动设置的开销。 123wsl --install # 默认安装Ubuntu系统wsl --list --verbose # 查看可以安装的Linux系统]]></content>
      <categories>
        <category>环境配置</category>
      </categories>
      <tags>
        <tag>Windows</tag>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python-包-pyscafford.md]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-02.python%2FPython-%E5%8C%85-%E9%A1%B9%E7%9B%AE%E7%AE%A1%E7%90%86-pyscafford%2F</url>
    <content type="text"><![CDATA[pyscaffoldpyscaffold-github 安装并创建项目1234567891011# 安装pip install pyscaffoldpython3 -m pip install --upgrade setuptools wheel # 用于对代码进行打包python3 -m pip install twine # twine用于将代码上传到pypi# 创建项目putup project_demo # 会在项目下，生成整个项目的结构文件。# -n 项目名， -p 包名， --markdown 使用 md 格式， demo_project 项目路径putup -n CSSCAnnotationProcess -p CSSCAnnotationProcess --markdown CSSCAnnotationProcess 项目默认使用rst文件，如果使用–markdown，需要提前安装扩展 pip install pyscaffoldext-markdown 编辑项目内容设置项目文档为markdown格式 安装必须模块 1pip install recommonmark m2r2 在 docs/source/conf.py 文件中编辑或添加以下内容以支持Markdown： 12345678910111213from recommonmark.parser import CommonMarkParserfrom m2r2 import MdIncludesource_parsers = &#123; &apos;.md&apos;: CommonMarkParser,&#125;source_suffix = [&apos;.rst&apos;, &apos;.md&apos;]extensions = [ &apos;m2r2&apos;, # 其他可能已有的扩展...] 将你的Markdown文档放在 docs/source/ 目录下，并将原有的 .rst 文件替换为 .md 格式。 构建Markdown文档为HTML格式： 1make html 就可以在基于 PyScaffold 的项目中使用Markdown编写文档了。虽然创建项目时无法直接指明使用Markdown，但通过上述修改，可以方便地转换到Markdown并保持与 sphinx 文档生成系统的兼容性。 构建Markdown文档为HTML格式： 编辑指定构建项目的工具 pyproject.toml初次测试阶段，这部分可以不进行调整 1234567[build-system]requires = [&quot;setuptools&quot;, &quot;wheel&quot;]build-backend = &quot;setuptools.build_meta&quot;[tool.setuptools]name = &quot;your-package-name&quot;version = &quot;0.1.0&quot; 编辑说明项目的元信息 setup.cfgsetup.cfg是用于配置 setuptools 的文件。它可以包含一系列选项，用于定义项目的元数据、依赖关系、插件等。这部分信息对应着pypi页面的展示信息， 12345678910111213141516[metadata]name = your-package-nameversion = 0.1.0author = Your Namedescription = Your package descriptionurl = https://github.com/yourusername/your-packageclassifiers = Programming Language :: Python :: 3 License :: OSI Approved :: MIT License Operating System :: OS Independent[options]packages = find:install_requires = requests numpy 在这个例子中，metadata 部分定义了项目的元数据，而 options 部分定义了项目的选项，包括依赖关系和包的查找方式。 编辑软件包的版本名称12345setup( name=&apos;project_demo&apos;, # 此处软件名 version=&apos;0.1.0&apos;, # 此处为你的版本号 packages=[&apos;project_demo&apos;] # 此处包名 ) 所有要编辑的代码存储在project_demo/src/project_demo下面，在进行发版时，也会打包该目录下的代码。 默认进行包发布的命令123456# 构建项目# pip install toxtox -e build # 发布tox -e publish -- --repository pypi --verbose 其他项目管理包hatchhatch github]]></content>
      <categories>
        <category>Tobeinsert</category>
      </categories>
      <tags>
        <tag>Tobeinsert</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[GO-Start]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-08.Go%2FGO-01.%E5%9F%BA%E7%A1%80%E8%AF%AD%E6%B3%95%E7%AE%80%E4%BB%8B%2F</url>
    <content type="text"><![CDATA[初始入门可以参考菜鸟教程 ,整体比较系统，也是入门参考的文档，涉及的相关字面内容，在此不进行赘述。仅记录一些，相比python、perl而言的个性化特点。 基础语法记录用的比较多，教程中也没有展开介绍，列在这，后续有需要的情况下，酌情补充。 key_word 示例 说明 break default func interface select case defer go map struct chan else goto package switch const fallthrough if range type continue for import return var 变量声明 变量声明需要明确声明，这在perl、python中可能没有那么严格， 声明支持 var x types 或 x := &quot;&quot; 两种方式 常量有单独的声明, const identifier [type] = value ,用来声明不会进行改变的值（只支持布尔型、数字型和字符串型）。在使用常量时候，会生成一个自增长的特殊变量iota，会对未赋值的常量进行自动复制。这部分在使用中需要注意。 运算符常规运算符，这里不进行展开了，其中赋值运算符遇到两个之前没有遇到过的， &lt;&lt;= 和 &gt;&gt;= ，左移后复制，和右移后赋值。之前在其他语言没见过的。其实就是将对应的整数型数值转换成 2进制的值，然后对二进制的数据进行移动后得到一个新的二进制数据，然后解析成整数。 运算符 描述 实例 &lt;&lt;= 左移后赋值 C &lt;&lt;= 2 等于 C = C &lt;&lt; 2 &gt;&gt;= 右移后赋值 C &gt;&gt;= 2 等于 C = C &gt;&gt; 2 条件语句在 GO中，不支持三目运算符。所以不能使用类似python中的 maxNum = a if a &gt; b else b 函数哎，为啥所有编程语言的函数声明格式不能一致点呢，哪怕关键字统一统一也行呀。有 sub、def 在go里面的关键字是funcGo语言的函数定义示例如下：12345678910func function_name( [parameter list] ) [return_types] &#123; 函数体&#125;/*func：函数由 func 开始声明function_name：函数名称，参数列表和返回值类型构成了函数签名。parameter list：参数列表，参数就像一个占位符，当函数被调用时，你可以将值传递给参数，这个值被称为实际参数。参数列表指定的是参数类型、顺序、及参数个数。参数是可选的，也就是说函数也可以不包含参数。return_types：返回类型，函数返回一列值。return_types 是该列值的数据类型。有些功能不需要返回值，这种情况下 return_types 不是必须的。函数体：函数定义的代码集合。*/ 相比于其他的语言，比较特殊的是需要声明返回值的数据类型。如果函数有返回值，则该参数是必须的！如果返回多个值，则需要声明每个值的数据类型。1234567891011121314151617/* 函数返回两个数的最大值 */func max(num1, num2 int) int &#123; /* 声明局部变量 */ var result int if (num1 &gt; num2) &#123; result = num1 &#125; else &#123; result = num2 &#125; return result&#125;// 返回多个结果值时，函数声明的格式func swap(x, y string) (string, string) &#123; return y, x&#125; 在go语言中，同时支持值传递（传递数值，变更不会影响函数外的变量）和引用传递（传递地址，变更会影响函数外的变量）。 作用域这部分和其他编程语言基本一致。 数组这部分比较特殊的，就是在声明数组类型的时候，可以指定元素个数123456789101112//格式var arrayName [size]dataType//示例var balance [10]float32//初始化var numbers = [5]int&#123;1, 2, 3, 4, 5&#125;numbers := [5]int&#123;1, 2, 3, 4, 5&#125;// 将索引为 1 和 3 的元素初始化balance := [5]float32&#123;1:2.0,3:7.0&#125; 指针Go 语言的取地址符是 &amp;，放到一个变量前使用就会返回相应变量的内存地址。在指针类型前面加上 * 号（前缀）来获取指针所指向的内容。1234567891011121314151617181920var var_name *var-type// var-type 为指针类型，var_name 为指针变量名，* 号用于指定变量是作为一个指针。以下是有效的指针声明：var ip *int /* 指向整型*/var fp *float32 /* 指向浮点型 */// 获取指针，并通过指针访问值的示例func main() &#123; var a int= 20 /* 声明实际变量 */ var ip *int /* 声明指针变量 */ ip = &amp;a /* 指针变量的存储地址 */ fmt.Printf("a 变量的地址是: %x\n", &amp;a ) /* 指针变量的存储地址 */ fmt.Printf("ip 变量储存的指针地址: %x\n", ip ) /* 使用指针访问值 */ fmt.Printf("*ip 变量的值: %d\n", *ip )&#125;// 当一个指针被定义后没有分配到任何变量时，为空指针，它的值为 nilif(ptr != nil) /* ptr 不是空指针 */if(ptr == nil) /* ptr 是空指针 */ 结构体可以简单的理解成其他语言中的对象/类。结构体在定义完成后，就属于一种可以用于声明的变量。对应的结构体也可以作为函数的传入参数。123456789101112131415161718192021222324252627282930type Books struct &#123; title string author string subject string book_id int&#125;func main() &#123; //几种不同的赋值方式： // 创建一个新的结构体 var Book1 = Books&#123;"Go 语言", "www.runoob.com", "Go 语言教程", 6495407&#125; // 也可以使用 key =&gt; value 格式 var Book2 = Books&#123;title: "Go 语言", author: "www.runoob.com", subject: "Go 语言教程", book_id: 6495407&#125; fmt.Println(Book2) // 忽略的字段为 0 或 空 var Book3 = Books&#123;title: "Go 语言", author: "www.runoob.com"&#125; fmt.Println(Book3) // 声明 Book1 为 Books 类型,并单独赋值 var Book4 Books Book4.title = "Go 语言" Book4.author = "www.runoob.com" Book4.subject = "Go 语言教程" Book4.book_id = 6495407 // 访问结构体的内容，使用 "结构体.成员名" fmt.Printf( "Book 4 title : %s\n", Book1.title) fmt.Printf( "Book 4 author : %s\n", Book1.author) fmt.Printf( "Book 4 subject : %s\n", Book1.subject) fmt.Printf( "Book 4 book_id : %d\n", Book1.book_id)&#125; 切片切片和python的数组概念类似，go语言的数组更像是python中的元组。 切片的一些方法 方法 示例 说明 len() len(x) 获取切片的长度。 cap() cap(x) 可以测量切片最长可以达到多少 append() x = append(x, 1) 切片中追加一个元素 copy() copy(x,x1) 拷贝一个切片 集合和python的字典，perl的hash类似。不过不通过的是，Map 是引用类型，如果将一个 Map 传递给一个函数或赋值给另一个变量，它们都指向同一个底层数据结构，因此对 Map 的修改会影响到所有引用它的变量。1234567891011121314151617181920212223242526/* 使用 make 函数定义一个集合 */map_variable := make(map[KeyType]ValueType, initialCapacity)/*KeyType 是键的类型，ValueType 是值的类型，initialCapacity 是可选的参数，用于指定 Map 的初始容量。Map 的容量是指 Map 中可以保存的键值对的数量，当 Map 中的键值对数量达到容量时，Map 会自动扩容。*/// 创建并赋值，使用字面量创建 Mapm := map[string]int&#123; "apple": 1, "banana": 2, "orange": 3,&#125;// 获取键值对v1 := m["apple"]v2, ok := m["pear"] // 如果键不存在，ok 的值为 false，v2 的值为该类型的零值// 遍历 Mapfor k, v := range m &#123; fmt.Printf("key=%s, value=%d\n", k, v)&#125;// 删除键值对delete(m, "banana") 遍历语言范围，看下来其实相当于其他编程语言中的遍历，包括遍历数组，字典等结构。遍历返回值均为2个，第一项为索引，第二项为值。12345678910111213141516171819202122232425map1 := make(map[int]float32)map1[1] = 1.0map1[2] = 2.0map1[3] = 3.0map1[4] = 4.0// 遍历map类型的数据// 读取 key 和 valuefor key, value := range map1 &#123;fmt.Printf("key is: %d - value is: %f\n", key, value)&#125;// 读取 keyfor key := range map1 &#123;fmt.Printf("key is: %d\n", key)&#125;// 读取 valuefor _, value := range map1 &#123;fmt.Printf("value is: %f\n", value)// 遍历数组var pow = []int&#123;1, 2, 4, 8, 16, 32, 64, 128&#125;for i, v := range pow &#123; fmt.Printf("2**%d = %d\n", i, v)&#125;&#125; 递归Go 语言支持递归。但我们在使用递归时，需要设置退出条件，否则递归将陷入无限循环中。尤其注意的是，go是支持函数的递归的（函数中调用函数自身），这在其他的语言中其实很少见，会存在先有鸡还是先有蛋的逻辑悖论。递归的一个示例如下：12345678910111213141516171819202122232425262728293031323334353637//通过 Go 语言的递归函数实例阶乘func Factorial(n uint64)(result uint64) &#123; if (n &gt; 0) &#123; result = n * Factorial(n-1) return result &#125; return 1&#125;func main() &#123; var i int = 15 fmt.Printf("%d 的阶乘是 %d\n", i, Factorial(uint64(i)))&#125;// 通过 Go 语言使用递归方法实现求平方根的代码：func sqrtRecursive(x, guess, prevGuess, epsilon float64) float64 &#123; if diff := guess*guess - x; diff &lt; epsilon &amp;&amp; -diff &lt; epsilon &#123; return guess &#125; newGuess := (guess + x/guess) / 2 if newGuess == prevGuess &#123; return guess &#125; return sqrtRecursive(x, newGuess, guess, epsilon)&#125;func sqrt(x float64) float64 &#123; return sqrtRecursive(x, 1.0, 0.0, 1e-9)&#125;func main() &#123; x := 25.0 result := sqrt(x) fmt.Printf("%.2f 的平方根为 %.6f\n", x, result)&#125; 前面的阶乘等方法可能本身不能凸显递归本身的优势，但是从求指定精度的平方根来看，递归确实有其独特的数学应用基础。 语言接口Go 语言提供了另外一种数据类型即接口，它把所有的具有共性的方法定义在一起，任何其他类型只要实现了这些方法就是实现了这个接口。看下来，和 python中对象的方法比较类似，不是一个独立的函数，而是依托于对应的数据类型的方法。 go并发这可能是go之所以能脱颖而出的重要优势。Go 语言支持并发，我们只需要通过 go 关键字来开启 goroutine 即可。goroutine 是轻量级线程，goroutine 的调度是由 Golang 运行时进行管理的。在使用go进行多线程分析的时候，可以使用直接在对应的函数前添加go即可。（go中的多线程是无序的，所以go中的程序执行顺序必须是不影响业务逻辑的） 123456789101112131415package main​import ( &quot;fmt&quot; &quot;time&quot;)​func hello() &#123; fmt.Println(&quot;Hello world goroutine&quot;)&#125;func main() &#123; go hello() time.Sleep(1 * time.Second) fmt.Println(&quot;main function&quot;)&#125; Goroutine的规则：当新的Goroutine开始时，Goroutine调用立即返回。与函数不同，go不等待Goroutine执行结束。当Goroutine调用，并且Goroutine的任何返回值被忽略之后，go立即执行到下一行代码。main的Goroutine应该为其他的Goroutines执行。如果main的Goroutine终止了，程序将被终止，而其他Goroutine将不会运行。 go 通道通道（channel）是用来传递数据的一个数据结构。通道可用于两个 goroutine 之间通过传递一个指定类型的值来同步运行和通讯。操作符 &lt;- 用于指定通道的方向，发送或接收。如果未指定方向，则为双向通道。go语言中，除了直接共享内存，还可以在多个goriutine之间完成数据通信，实现数据共享。 12345678// 创建通道 chan关键字，后面是缓冲区的数据类型ch := make(chan int)// 创建带缓冲区的通道ch := make(chan int, 100)// 通过通道发送和接受数据ch &lt;- v // 把 v 发送到通道 chv := &lt;-ch // 从 ch 接收数据，并把值赋给 v 注意： 默认情况下，通道是不带缓冲区的。发送端发送数据，同时必须有接收端相应的接收数据。带缓冲区的通道允许发送端的数据发送和接收端的数据获取处于异步状态，就是说发送端发送的数据可以放在缓冲区里面，可以等待接收端去获取数据，而不是立刻需要接收端去获取数据。不过由于缓冲区的大小是有限的，所以还是必须有接收端来接收数据的，否则缓冲区一满，数据发送端就无法再发送数据了。注意： 如果通道不带缓冲，发送方会阻塞直到接收方从通道中接收了值。如果通道带缓冲，发送方则会阻塞直到发送的值被拷贝到缓冲区内；如果缓冲区已满，则意味着需要等待直到某个接收方获取到一个值。接收方在有值可以接收之前会一直阻塞。 123456789101112131415161718func sum(s []int, c chan int) &#123; sum := 0 for _, v := range s &#123; sum += v &#125; c &lt;- sum // 把 sum 发送到通道 c&#125;func main() &#123; s := []int&#123;7, 2, 8, -9, 4, 0&#125; c := make(chan int) go sum(s[:len(s)/2], c) go sum(s[len(s)/2:], c) x, y := &lt;-c, &lt;-c // 从通道 c 中接收 fmt.Println(x, y, x+y)&#125; 遍历和关闭通道Go 通过 range 关键字来实现遍历读取到的数据，类似于与数组或切片。格式如下：v, ok := &lt;-ch, 不过为了区分通道是在等待还是已经终止存储，需要在通道存储数据结束后，对通道进行关闭 close(ch) 。通道除了作为数据的通信，还可以用来确保对应的子线程执行完成（利用通道的阻塞功能）。 错误处理]]></content>
      <categories>
        <category>编程拾慧</category>
        <category>GO</category>
      </categories>
      <tags>
        <tag>GO</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[容器服务]]></title>
    <url>%2F01.%E7%9F%A5%E8%AF%86-06.cloud_computing%2F%E4%BA%91%E8%AE%A1%E7%AE%97-%E6%A6%82%E5%BF%B5-%E5%AE%B9%E5%99%A8%E6%9C%8D%E5%8A%A1%2F</url>
    <content type="text"><![CDATA[容器服务节点池节点池是集群中一个或一组节点的逻辑集合，一个集群中可以创建多个不同配置和类型的节点池。节点池的配置包含节点的属性，例如节点规格、可用区、标签、污点等。这些属性可以在创建节点池时指定，也可以在创建完成后进行编辑修改。 注意事项 在删除节点池前，您需清空节点池内所有节点。 只能在创建节点池的时候开启自动弹性伸缩功能。开启了自动弹性伸缩功能的节点池有以下特性： 不支持手动扩容。 付费类型支持抢占式实例。 弹性模式上除了普通CPU实例，还支持GPU实例和GPU共享实例。 允许您关闭弹性伸缩配置，将弹性节点池切换为节点池 （反向操作不允许）。]]></content>
      <categories>
        <category>云计算</category>
      </categories>
      <tags>
        <tag>基础概念</tag>
        <tag>阿里云</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据模拟软件 VarBen]]></title>
    <url>%2F03.%E5%BA%94%E7%94%A8-01.software_in_NGS%2FSoftware-%E6%95%B0%E6%8D%AE%E6%A8%A1%E6%8B%9F-VarBen%2F</url>
    <content type="text"><![CDATA[随着NGS在分子诊断中变异（体系和胚系）检测的应用普及。临床NGS检测产品的检测范围越来越大，带来的一个优势是可以识别更多基因中的变异；但是同时也使性能验证面临巨大的挑战。临床方法学验证过程中很难获得真实样本，能同时具有大量关注且能够被检测到频率也合适的变异。所以大多数实验室都是通过对具有已知变异的标准品或细胞系进行测序分析，并依靠测序指标（即具有足够覆盖率的目标部分）来推断其他区域的性能。 在这个大背景下，很多实验室会开始考虑使用模拟数据进行性能评估，使用数据模拟的方法在临床实验室中具有非常高的实用性，可作为临床检测验证的辅助工具。通过模拟数据可以便捷的构建出真实样本中难以获取的变异数据，从而使实验室能够更高效经济同时准确地测试生物信息学流程的性能（不同频率梯度、流程灵敏度，针对不同长度插入/缺失的性能边界）。从而进行更全面的方法学验证和生信流程验证，同时模拟数据也已经成为AMP的建议方法，同时针对不同的检测目的提供了不同的模拟方案，可以参考文章。同时文章中也提供了非常多的数据模拟软件和方案，可以参考模拟软件参考列表 而今天我们介绍的是VarBen，一款由中检院开发的可以模拟4中变异（SNV、InDel、CNV、SV）的软件。也许整体性能和使用便捷性上不是最好的，但是中检院背书，在国内资质申报等注册相关渠道来说，无疑是最容易被接受的。 安装软件是基于Python2开发的，借助python重点pysam(&gt;=0.9.4)模块进行对Bam文件进行处理，对比对到参考基因组上的数据进行编辑制造预期的变异。在完成对reads进行编辑的操作以后，同样会对reads进行重新的比对，所以环境中需要提前进行bwa和samtools的安装。1234567891011121314151617181920python2 -m pip install pysampython2 -m pip install numpy# samtools (http://samtools.sourceforge.net/)git clone https://github.com/samtools/htslib.gitmake -C htslibgit clone https://github.com/samtools/samtools.gitmake -C samtoolscp samtools/samtools $HOME/bingit clone https://github.com/samtools/bcftools.gitmake -C bcftoolscp bcftools/bcftools $HOME/bin# bwa (http://bio-bwa.sourceforge.net/)git clone https://github.com/lh3/bwa.gitmake -C bwacp bwa/bwa $HOME/bin 使用方法该软件支持多种不同类型的变异模拟。同时也支持多种不同的测序平台（Illumina、life 、MGI），根据要模拟的不同变异类型，需要准备不同的配置文件进行模拟。因为仓库有相对系统的说明，在这里不进行展开，主要介绍一些容易被忽视的细节和局限性。 只处理Uniq ReadsNGS数据处理过程中，Markdup已经成为流程处理的一个标准化步骤，不管使用什么方法，这步其实都是难以避免的。如果使用的是类似Picard的方法，进行Markdup处理（Bam数据中存在Duplication Reads）那么需要注意，数据模拟只会对Uniq Reads进行处理。实例如下图：所以如果你的下游处理流程如果会对Dup簇信息进行整体的评估（例如评估变异的可信度），那么需要仔细的评估确认你检测流程是否适合使用这种方法进行模拟。当然一个比较投机的方案是可以使用一个没有Markdup的Bam进行模拟（但是分析流程也无法进行模拟操作），比如在处理前，先把Bam中的Duplication Reads进行删除，使用rmDup而不是Markdup。所以对应的如果我们处理的是压缩的方法构建consensus Reads（数据中没有Duplication Reads）那就没什么影响， Reads名称变化在进行SV等类型模时，软件会对模拟Bam中的reads名称进行变更，可以在临时目录（tempDir/readname_convert.txt）查看进行reads名称变更（模拟过程中对reads进行编辑）的reads。示例如下：12345678910(base)[Ben@ECS1 tempDir]$head tempDir/readname_convert.txtdd10d900-02c6-48f5-8f71-10ea8844fea5: F350021481L2C003R05000360198_TGTCTGTGTCTG_TGTCTGTGTCTG, True, 43611912-43612005dd10d900-02c6-48f5-8f71-10ea8844fea5: F350021481L2C003R05000360198_TGTCTGTGTCTG_TGTCTGTGTCTG, False, 43612017-436121108e77a7cc-baa4-4e2e-9f78-bb70ae3b53ff: F350021481L1C001R06500913354_TGTCTGTGTCTG_TGTCTGTGTCTG, True, 43611800-436118938e77a7cc-baa4-4e2e-9f78-bb70ae3b53ff: F350021481L1C001R06500913354_TGTCTGTGTCTG_TGTCTGTGTCTG, False, 43611976-436120693e37c602-85db-4018-8a6a-daaf252839af: F350021481L1C003R03701364230_TGTCTGTGTCTG_TGTCTGTGTCTG, True, 43611904-436119973e37c602-85db-4018-8a6a-daaf252839af: F350021481L1C003R03701364230_TGTCTGTGTCTG_TGTCTGTGTCTG, False, 43611953-4361204609e73b73-c5c5-4c42-956a-61b42b0cab8c: F350021481L2C002R01000826519_TGTCTGTGTCTG_TGTCTGTGTCTG, False, 43611794-4361188709e73b73-c5c5-4c42-956a-61b42b0cab8c: F350021481L2C002R01000826519_TGTCTGTGTCTG_TGTCTGTGTCTG, True, 43611952-436120454020f12a-3e66-42c6-bbd2-13ce87783f7d: F350021481L2C001R02700414361_TGTCTGACAGAC_TGTCTGACAGAC, False, 43611921-43612014 其实我们可以看到，前面第一列是Read更新后的ID，第二列是reads更新前的原始ID，第三列疑似正负链信息，第四列是该条Reads在染色体上的位置。因为处理后支持融合的ReadsID名称及ID格式都发生了变化，所以在使用中需要确认这个ID变化对你下游检测流程的影响。比如示例的数据如果想从Bam生成对应的Fastq测试完整流程显然是不可行的，因为ReadsID中的UMI信息由于ID变化已经完全丢失了，这时候，就需要基于上述ReadsID对应文件，将ReadsID逐一进行数据的还原。相反，如果直接基于Bam进行检测，下游分析对ReadID格式不敏感，那就不需要考虑这部分影响。 CNVCNV检测，目前常规检测方法都是基于深度的，而模拟过程其实就是基于Bam提取想要模拟CNV区域的Reads进行double，所以针对CNV，模拟后的数据不适合进行再次的Markdup，因为重新Markdup，会直接导致你的模拟数据都被剔除了。]]></content>
      <categories>
        <category>NGS</category>
        <category>software</category>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>数据模拟</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[IGV-report工具]]></title>
    <url>%2F03.%E5%BA%94%E7%94%A8-01.software_in_NGS%2FSoftware-%E5%8F%AF%E8%A7%86%E5%8C%96-igv%2F</url>
    <content type="text"><![CDATA[简介IGV 是基因组可视化方向一个被广为熟知的软件，相比JBrowse和GBrowse等需要进行服务器部署的高门槛服务，IGV本身提供了Windows桌面版，也满足了很多非生信背景的数据查看诉求。最近在进行流程可视化时，偶然发现IGV有开发了很多的扩展，比如igv-reports 和 igv-notebook 来适配更多的应用场景。 安装整个igv report模块是基于python(&gt;3.6)开发的，可以通过pip安装，在这里为了避免出现一些和现有环境的版本冲突，我们直接使用conda创建一个新的环境并进行安装，示例如下：1conda create -n igv_report igv-reports 生成报告报告的生成也比较简单，执行如下命令，然后打开 example.html 即可。1234567create_report Demo.vcf \--genome hg19 \--tracks Demo.vcf Demo.bam \--info-columns DUPLEX BAYES \ # vcf info 字段的key--sample-columns AF DP \ # vcf FORMAT 字段的Key--fasta $path/hg19.fa \--output example.html 由于结果表格中可以对vcf的INFO字段和FORMAT字段的信息进行提取，用于在最终表格结果中进行展示，因此我们可以通过对vcf进行个性化定制，实现最终表格信息的定制化展示。 查看结果实际效果，我们可以查看官方给的一些demo示例。Demo]]></content>
      <categories>
        <category>software</category>
        <category>可视化</category>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>可视化</tag>
        <tag>IGV</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[天池项目：生命科学赛道——生物学年龄评价与年龄相关疾病风险预测]]></title>
    <url>%2F03.%E5%BA%94%E7%94%A8-99.other%2F%E5%A4%A9%E6%B1%A0%E9%A1%B9%E7%9B%AE%E8%AE%B0%E5%BD%95%2F</url>
    <content type="text"><![CDATA[一些比赛项目的时间记录，本文档记录 阿里云天池 项目，生命科学赛道——生物学年龄评价与年龄相关疾病风险预测 的相关记录。 环境安装项目使用机器学习的方法，基于甲基化数据进行年龄预测。所以需要先进行相关环境的安装。避免历史环境的干扰，本项目使用conda直接创建了一个全新的基础环境。12# 基于python=3.11 创建全新的conda环境，同时安装 h5py和scikit-learn包conda create -n tianchi2023 python=3.11 h5py scikit-learn 后期为了更好的对数据进行可视化操作，和交互式的进行模型的训练，安装了jupyter1pip3 install jupyter 阿里云的jupyter存在端口权限，需要进行端口映射。1ssh liubo4@120.24.188.250 -L127.0.0.1:8889:127.0.0.1:8889 使用官方baseline| 迭代版本 | 使用方法 | 成绩 || 第 1 次 |天池提供的baseline （ElasticNet()） | (8.579980758705524, 9.329089, 7.830872352677162) || 第 2 次 | ElasticNet(0.1) | (5.659034220859258, 6.588447, 4.729621347754892) || 第 3 次 | ElasticNet(0.05) | (5.513496300186773, 6.4005265, 4.626466076783459) || 第 4 次 | ElasticNet(0.01) | (6.3587627338640615, 7.2758136, 5.441711888168797) || 第 5 次 | Lasso(alpha=0.05) | (5.638377567734381, 6.5338798, 4.742875378541272) | 训练集和仅使用健康人群| 迭代版本 | 训练参数 | 成绩 || 第 2 次 | ElasticNet(0.1) | (6.068306326553264, 6.2521014, 5.884511231750328) || 第 3 次 | ElasticNet(0.05) | (6.002052213263324, 6.1584253, 5.845679095410925) || 第 5 次 | Lasso(alpha=0.05) | (6.085405076269716, 6.349815, 5.820995260724246) | 训练集和仅使用健康人群，位点数扩展至20000| 迭代版本 | 训练参数 | 成绩 | 用时 || 第 1 次 | ElasticNet(0.1) | (4.474091597156978, 4.4790444, 4.469138756905508) | 33s || 第 2 次 | ElasticNet(0.05) | (4.429926482228302, 4.287037, 4.57281606870465) | || 第 3 次 | Lasso(alpha=0.05) | (4.400938993393588, 4.433994, 4.367884170411444) | 34s | 训练集和仅使用健康人群，位点数扩展至100000（ 20W已经无法再16GB下运行）| 迭代版本 | 训练参数 | 成绩 | 用时 || 第 1 次 | ElasticNet(0.1) | (3.8419492549466012, 3.6467113, 4.037187160405898) | 165s || 第 2 次 | ElasticNet(0.05) | (3.726990224723529, 3.4416575, 4.012322906264685) | 162s || 第 3 次 | Lasso(alpha=0.05) | (3.886337189746082, 3.7988966, 3.9737778283599625) | 165s | 训练集混用健康和非健康人群，位点数扩展至100000（ 20W已经无法再16GB下运行）| 迭代版本 | 训练参数 | 成绩 | 用时 || 第 1 次 | ElasticNet(0.1) | (3.4642932672596456, 3.707379, 3.221207431812382) | 215s || 第 2 次 | ElasticNet(0.05) | (3.262297007905778, 3.4586637, 3.0659303138004477) | 213s || 第 3 次 | Lasso(alpha=0.05) | (3.4673917658963997, 3.76499, 3.1697934404689465) | 209s | 训练集混用健康和非健康人群，服务器运行位点扩展至全部数据(训练集 70%)| 迭代版本 | 训练参数 | 成绩 | 用时 || 第 1 次 | ElasticNet(0.05) | (3.1139186428174943, 3.1427836, 3.0850536438198026) | 839s || 第 1 次 | ElasticNet(0.1) | (3.2288436084070304, 3.2900968, 3.1675904570179183) | 817s || 第 1 次 | Lasso (0.1) | (3.153165288490824, 3.384533, 2.9217976485148514) | 816s | 训练集用健康人群，服务器运行位点扩展至全部数据(训练集 70%)| 迭代版本 | 训练参数 | 成绩 | 用时 || 第 1 次 | ElasticNet(0.05) | (3.9090402715439425, 3.5266752, 4.291405318783686) | 639s | 训练集用全部，服务器运行位点扩展至全部数据,Nan使用位点的甲基化均值填充 (训练集 70%)| 迭代版本 | 训练参数 | 成绩 | 用时 || 第 1 次 | ElasticNet(0.05) | (2.976909323353664, 3.2731206, 2.6806980049989533) | 815s | 训练集用全部，服务器运行位点扩展至全部数据,Nan使用位点的甲基化均值填充 (训练集 80%)| 迭代版本 | 训练参数 | 成绩 | 用时 || :——- | —————- | ————————————————– | —: || 第 1 次 | ElasticNet(0.01) | (2.9917520688953543, 3.058976, 2.9245282028084088) | 964s | 20230828]]></content>
      <categories>
        <category>机器学习</category>
        <category>天池项目</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[人类参考基因组序列知多少]]></title>
    <url>%2F01.%E7%9F%A5%E8%AF%86-04.Database%2FDatabase-reference%2F</url>
    <content type="text"><![CDATA[做人源相关的研究，不管是科研探索还是医学方向的临床应用。都离不开人类参考基因组的使用。人类参考基因组有很多版本，在2022年也新发布了T2T的全基因组完整图谱。之前我们可能了解到的只有Hg19和GRCh38比较多。知道Hg19是旧的，GRCh38是更新的版本。所以借此机会，捋一下人类基因组参考序列发布的时间节点，同时也梳理下各个不同版本参考基因组版本的特点。 第一阶段 - 国际人类基因组测序联盟提到人类基因组，离不开人类基因组计划，这项计划和曼哈顿原子弹计划 、阿波罗登月计划并称为20世纪人类三大科学计划。人类基因组计划 (HGP)于 1990年 启动，该计划的标志性目标是对人类基因组的 30 亿个碱基 进行测序。其他目标包括生成人类基因组的物理和遗传图谱，以及生物医学研究中使用的关键模型生物的绘图和测序。为了执行人类基因组测序的这部分工作，国际人类基因组测序联盟 (IHGSC) 成立，这是一个开放式合作组织，涉及六个国家的 20 个中心，包括美国、英国、法国、德国、日本和我国科学家。1998年，HGP正式实施了百慕大原则：自动发布&gt;1kb的序列组装体，最好在24小时内；立即出版完成的注释序列；并将整个序列在公共领域免费提供以供研究和开发。来尽可能规避专利和商业化问题限制基因组序列的应用。2000年3月14日，美国总统克林顿和英国首相托尼·布莱尔 联合宣布人类基因组序列“应该世界各地的科学家都可以免费获得”2001年，国际人类基因组测序联盟（the International Human Genome Sequencing Consortium）报告了人类基因组常染色质部分的草图序列。2001年2月，同时发表了两篇文章（由Venter 等人在 Science 和国际人类基因组测序联盟在 Nature 上发表），描述了人类基因组测序草案基因组序列。该序列包括 26,588 个有强有力确凿证据的蛋白质编码转录本，以及另外约 12,000 个具有小鼠同源物或其他薄弱证据的计算衍生基因。2004年，HGP的工作最终发表了高度准确（每 100,000 个碱基约 1 个错误）的人类基因组序列（Build 35/hg17）包含 28.5 亿个核苷酸，仅被 341 个缺口打断，覆盖了约 99% 的常染色质基因组。项目内容发表在Nature:Finishing the euchromatic sequence of the human genome，此前发布的版本还有NCBI33、NCBI34、NCBI35。2006年，国际人类基因组测序联盟提交了他们的最终版本的参考基因组序列（NCBI36/hg18）。 第二阶 - 段参考基因组联盟后续人类参考基因组由 GRC（Genome Reference Consortium）进行维护。GRC 也是有多个国家机构组成，成员包括Sanger研究所、 McDonnell基因研究所、EMBL-EBI:欧洲生物信息研究所、NCBI:国际生物信息中心等。 2009年,GRC 发布了GRCh37，也就是搭乘基因测序发展的快车，成为行业内使用最多的 Hg19。该参考基因组于2009年发布，后续共计进行了13个补丁版本的发布。其中最后一个修正版本 GRCh37.p13 发布于2013年6月。是人类基因组的第19版参考序列，完成了2.9GB长度的测序（总预计3.1GB）。包含了基因、非编码区域和其他功能元件的位置信息。 2013年底，GRC 发布了[GRCh38]，截止2022年2月，GRCh38累计发布了14个修正版本。 在人类基因组草图发布后的20多年里，随着技术的进步不断进行和升级，我们可以看到参考基因组序列也一直在进行着非常高频升级和迭代。早期的基因组版本，NCBI33、NCBI34、NCBI35、NCBI36已经退出了历史的舞台，甚至于很多人可能不曾听到。而Hg19、GRCh38为疾病研究、医学研究等研究的开展奠定了基础，极大的推进了人类科学研究进展。但放眼整个人类参考基因组仍然仅仅覆盖了基因组的常染色质部分，而重要的异染色质区域由于技术上的问题，一直存在缺失，整个基因组中仍有8%的区域未覆盖。 参考基因组 GRCh37 GRCh38.p14 Ref.Version GRCh37 GRCh38 Genome size 3.1 Gb 3.1 Gb Total ungapped length 2.9 Gb 2.9 Gb Gaps between scaffolds 271 349 Number of chromosomes 24 24 Number of scaffolds 249 473 Scaffold N50 46.4 Mb 67.8 Mb Scaffold L50 21 16 Number of contigs 350 999 Contig N50 38.5 Mb 57.9 Mb Contig L50 24 18 GC percent 40.5 40.5 Assembly level Chromosome Chromosome 第三阶段 - T2T联盟2022年， Telomere-to-Telomere (T2T)国际研究联盟基于细胞系（葡萄胎）构建了第一个完整的从头到尾无间隙人类参考基因组T2T-CHM13，填补了最后缺失的约2亿碱基对的测序，成果发布在Science上发表（“The complete sequence of a human genome”）。该研究针对CHM13进行了多种测序（包括30× PacBio HiFi 、120× ONT 、100× ILMN、70× Hi-C 、BioNano optical maps 、Strand-seq ）并结合新开发的组装算法，组装增加了五个完整的染色体臂，实现了除 Y 染色体之外的所有染色体的无间隙组装。T2T-CHM13 组装代表了比 GRCh38 更完整、更具代表性和更准确的参考。 2023年7月14日，浙江大学张国捷教授团队与深圳农业基因组研究所阮珏团队，以及华大生命科学研究院合作，通过开发算法，以个体的父本和母本数据作为参考系，完美地将不同染色体上的数据区分，将人的46条染色体的数据分别组装出来。然后对因为数据过于复杂而仍然存在的69个缺口进行了手工补洞。最终，获得了健康个体（汉族男性）的完整二倍体基因组。该完整基因组作为东亚人群遗传学研究的参考序列，可以提高东亚人群的序列比对并降低错误率，对单碱基多态性的检测准确率也会更高。该完整图谱的绘制，为我国开展精准医疗研究提供了更准确的参考基因组。 在人类全基因组图发布不久，上海交通大学毛亚飞课题组在Genome Biology发表题为Characterization of large-scale genomic differences in the first complete human genome的研究论文，比较分析了T2T-CHM13完整基因组与当前人类参考基因组模版（GRCh38）之间的大规模基因组差异。，系统地表征了两个人类基因组组装之间的大型结构变异（≥10 kbp），通过新开发的结构变异分析工具网站（SynPlotter）验证238个基因组差异区域并发现了67个新鉴定的结构差异区域。 参考资料相关文献 Yang, C., Zhou, Y., Song, Y. et al. The complete and fully-phased diploid genome of a male Han Chinese. Cell Res (2023). https://doi.org/10.1038/s41422-023-00849-5 Yang, X., Wang, X., Zou, Y. et al. Characterization of large-scale genomic differences in the first complete human genome. Genome Biol 24, 157 (2023). https://doi.org/10.1186/s13059-023-02995-w Sergey Nurk et al. ,The complete sequence of a human genome.Science376,44-53(2022).DOI:10.1126/science.abj6987 Bonfield, James K. and James E. Galagan. “Finishing the euchromatic sequence of the human genome.” Nature 431 (2004): 931-945. J. Craig Venter et al. ,The Sequence of the Human Genome.Science291,1304-1351(2001).DOI:10.1126/science.1058040 International Human Genome Sequencing Consortium. Initial sequencing and analysis of the human genome. Nature 409, 860–921 (2001). https://doi.org/10.1038/35057062 相关基因组获取 Genome assembly GRCh37 Genome assembly GRCh37.p13 Genome assembly GRCh38 Genome assembly GRCh38.p14 Genome assembly T2T-CHM13v2.0 T2T human genome GitHub仓库 Milestones in Genomic Sequencing]]></content>
      <categories>
        <category>知识沉淀</category>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>数据库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[人工智能、机器学习、深度学习和神经网络的关系解析]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-20.MachineLearn%2F0001.%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD.%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0.%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%92%8C%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%85%B3%E7%B3%BB%E8%A7%A3%E6%9E%90%2F</url>
    <content type="text"><![CDATA[随着2023年初，ChatGPT的出圈，人工智能（Artificial Intelligence，简称AI）成为科技领域最炙手可热的话题之一。它被赋予了广泛的定义，涵盖了模拟人类智能的各个方面。人工智能（Artificial Intelligence）、机器学习（Machine Learning，简称ML）、深度学习（Deep Learning，简称DL）和神经网络（Neural Networks）这些重要概念也席卷而来。 之前看机器学习的时候，关注的也主要是监督学习、无监督学习的相关算法和原理（这部分后续也会逐步整理下之前的笔记放到公众号上来）。对于深度学习、神经网络这些了解非常有限，正好上一篇文章“Artificial Intelligence in Molecular Medicine” 中提供了一个AI相关方法逻辑图，借此机会整理下相关概念，形成一个相对完整的框架。也希望对大家有帮助。 人工智能（英语：artificial intelligence，缩写为AI）亦称智械、机器智能，指由人制造出来的机器所表现出来的智能。通常人工智能是指通过普通计算机程序来呈现人类智能的技术。人工智能方向也有多个研究课题，包括但不限于：演绎推理、知识表示、专家系统、自然语言处理、机器学习、机器人学、计算机视觉和语音识别、规划与调度等。 机器学习（Machine Learning，简称ML）是人工智能的一个研究分支。人工智能的研究历史有着一条从以“推理”为重点，到以“知识”为重点，再到以“学习”为重点的自然、清晰的脉络。显然，机器学习是实现人工智能的一个途径之一，即以机器学习为手段，解决人工智能中的部分问题。机器学习，从学习方式上可以分成三类： 监督学习（任务驱动），是从外部监督者提供的带标注训练集中进行学习，学习得出一个函数，当新的数据到来时，可以根据这个函数预测结果。 无监督学习（数据驱动），是一个典型的寻找未标注数据中隐含结构的过程。与监督学习相比，训练集没有人为标注的结果。 强化学习（从错误中学习），机器为了达成目标，随着环境的变动，而逐步调整其行为，并评估每一个行动之后所到的回馈是正向的或负向的。半监督学习介于监督学习与无监督学习之间。 但是这些不同的学习方式其实对应着多种不同的算法，如监督学习常用的算法有决策树、支持向量机、随机森林、最近邻算法、逻辑回归、等；无监督学习常用的算法有K均值、主成分分析、奇异值分解、高斯混合模型等；半监督学习常用算法有自训练、生成式模型等；强化学习常用的方法有蒙特卡洛模型、Q学习、深度强化学习等；当然还有一些泛化的算法，对学习方式并没有非常明确界定，例如深度学习（英语：deep learning）是一种以神经网络为架构，对资料进行表征学习的算法。再比如集成学习（英语：Ensemble learning）方法使用多种学习算法来获得比单独使用任何单独的学习算法更好的预测性能。 深度学习（英语：deep learning）是机器学习的分支，是一种以人工神经网络为架构，对数据进行表征学习的算法。深度学习中的形容词“深度”是指在网络中使用多层。已有数种深度学习框架，如深度神经网络、卷积神经网络和深度置信网络和循环神经网络已被应用在计算机视觉、语音识别、自然语言处理、音频识别与生物信息学等领域并获取了极好的效果。人工神经网络（英语：Artificial Neural Network，ANN），简称神经网络（Neural Network，NN）或类神经网络，在机器学习和认知科学领域，是一种模仿生物神经网络（动物的中枢神经系统，特别是大脑）的结构和功能的数学模型或计算模型。神经网络由大量的人工神经元联结而成，常见的神经网络结构图如下。 参考资料 [1]. wikipedia[2]. The mostly complete chart of Neural Networks, explained]]></content>
      <categories>
        <category>machine_learning</category>
      </categories>
      <tags>
        <tag>machine_learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Recommendations for Next-Generation Sequencing Germline Variant Confirmation]]></title>
    <url>%2F01.%E7%9F%A5%E8%AF%86-03.standard%2FRecommendations_for_Next-Generation_Sequencing_Germline_Variant_Confirmation%2F</url>
    <content type="text"><![CDATA[背景新一代测序 (NGS) 的引入是临床实验室遗传学史上最重要的技术进步之一。自 2000 年代初问世以来，NGS 已经从一种创新的研究工具转变为一种通用且有效的临床分子诊断方法。 , 与传统的 DNA 基因分型和测序方法相比，NGS 已被证明更加灵活和可扩展，并且既省时又经济。然而，NGS 有局限性。在其他挑战中，NGS 变异检出的准确性可能因方法、变异类型和大小、基因组区域以及样本类型和质量而有很大差异。对提供假阳性 (FP) NGS 结果的适当担忧导致早期依赖验证性检测来减少这种可能性，并且验证性测试仍然被许多当前的生育相关的临床实践指南所推荐，但是细节描述确很少被明确的提及。在本指南中，为临床时间提供了更具体的建议，这些建议与现有指南相一致，提供了额外的细节，因此可能有助于促进实验室之间的标准化、透明度和质量改进。 迄今为止，一些专业组织已经通过立场文件或推荐的最佳实践解决了种系检测中的交叉确认问题。在一项早期工作中，美国疾病预防控制中心召集了 Nex-StoCT 工作组，该工作组在 2012 年发布了一套用于基于 NGS 的临床测试质量保证的原则和指南。他们建议“对 NGS 检测到的所有临床可操作变异进行确认测试。”美国医学遗传学会 (ACMG) 发布的 2013 年 NGS 标准中包含类似的建议，即“所有以疾病为重点的和/或诊断测试都包括使用配套技术确认最终结果”。该指南进一步指出，实验室“必须具有丰富的 NGS 技术经验……才能决定使用正交技术进行结果确认。”该指南没有就什么构成广泛经验或需要哪些数据来验证不包括确认所有报告变异的实验室工作流程提供进一步指导。 2015 年，美国病理学家学会发布了 NGS 标准，指出每个执行 NGS 的实验室“必须制定一项政策，明确记录确认测试的适应症和/或记录他们的化验验证如何确定不需要此类测试。”美国病理学家学会在其 2022 年版分子病理学清单中指出，“实验室按照其政策中的规定对 NGS 结果进行验证性测试，并记录 NGS 验证性结果的相关性”，并指出实验室必须在验证过程中确定是否和当指示进行确认测试时。EuroGentest 和欧洲人类遗传学学会在 2016 年提出了 NGS 指南，包括在补充材料中，“目前，建议确认所有报告的变异，以确保没有发生样本交换和/或验证信息学管道”这些出版物中的每一个都承认，他们关于 NGS 变异交叉确认的立场受到最近采用的 NGS 技术和实验室缺乏其性能的可靠经验的影响。这些出版物预计，随着 NGS 技术和生物信息学的发展，确认的必要性将会降低。ACMG 在 2021 年更新了其 NGS 标准，建议“实验室应根据从大型和多样化数据集中提取的实验室特定质量指标识别变异的工作流程，为每个变异类别建立并提供确认测试策略以及读取比对的目视检查。在没有经过验证的方法的情况下，实验室应继续进行交叉确认。” 同样，2020 年 ACMG CFTR 测试技术标准指出，“实验室应确定……报告某些变异是否需要交叉确认，以及将使用什么标准来做出该决定。建议基于 NGS 方法的 CFTR 报告明确说明报告的变异是否已使用替代方法得到确认；如果不采用交叉确认，建议在报告的方法部分简要说明用于做出该决定的标准。 技术背景NGS与 Sanger 测序相比，NGS 产生的 reads 相对较短，并且这些 reads 中的每一个都可能具有相对较高的原始数据错误率。 NGS 通过生成覆盖每个目标位置的大量读数来弥补这些挑战。由于产生的数据量大且数据复杂，NGS 依赖于精心设计的生物信息学软件、数据库和算法来识别每个 DNA 标本中的遗传变异。正如其他 AMP/美国病理学家学院指南中所述，这些生物信息学系统还计算各种质量指标，这些指标可以帮助表明每个变异调用的技术可靠性。使用这些指标，很可能是 FP 的调用通常会被过滤掉。但是，过滤后的剩余调用仍可能包含 TP 和 FP 结果的混合。 NGS 错误率的临床考虑FP 错误会对医疗保健和患者结果产生重大影响。在某些临床情况下，阳性（无论是 TP 还是 FP）基因检测报告直接影响有关预防性手术、终止妊娠、医疗干预或治疗选择的决定。 FP 还可能导致误诊和过早结束对患者疾病真正原因的搜索。而是随着NGS检测的快速发展，应用范围越来越光放，即时错误率已大大降低，但是具有临床意义的FP的可能性也会增加。例如，考虑每测试 1,000,000 个碱基对 (1 Mb) 仅产生一个 FP 的 NGS 测试。这种低错误率与最近的一些（但不是全部）NGS 性能研究一致。在蛋白质编码区，人类大约每 2 kb 就有一个种系 DNA 变异（这一比率在非编码区更高）。因此，每 Mb 产生 1 FP 的测试将具有 99.8% 的分析（或技术）阳性预测值 (PPV) 和基于每个 bp 的分析 FP 率为 0.0001%。这种性能水平可能看起来很高，但可能还不够。例如，BRCA1 和 BRCA2 总计大约 10 kb（10,000 bp）的蛋白质编码序列，因此以这个 FP 率测试这两个基因的每 100 名患者预计会有一个分析 FP。如果将测试扩展到包括跨越 200 kb 的基因组，那么预计每五名测试患者就有一个 FP。在跨越 30 Mb 的外显子组序列中，每位患者的结果中预计有 30 个 FP。 要了解 FP 的影响，应该考虑临床 PPV，而不仅仅是分析 PPV。临床 PPV 考虑了变异解释和接受测试的患者群体。继续上面使用的 BRCA1/2 示例，假设每 100 名患者发生一个分析性 FP 错误，并且这些 FP 中有一半可能看起来是致病性的，导致每 200 名测试的患者有一个潜在的重要 FP。一般而言，当将 BRCA1/2 检测应用于具有强烈提示遗传性乳腺癌或卵巢癌个人史或家族史的患者时，大多数（并非所有）种族群体中约有 10% 的患者被发现为真正阳性。 , 因此，此示例测试的临床 PPV 在这些患者中大约为 95%——21 个阳性结果中有 1 个是假的。然而，在筛查环境中，如果 0.5% 的患者真正呈阳性，则临床 PPV 将仅为 50%。换句话说，这个示例筛选测试的阳性结果可能是假的，也可能是真的。在筛选环境中使用的基因组将具有更低的临床 PPV（TP/(TP+FP))。 总之，即使分析性 FP 的比率很低，NGS 测试的基因和患者数量越来越多，也可能导致大量 FP 临床报告。 NGS 错误的技术考虑所有实验室方法都可能出错。已建立的技术，如 Sanger 测序，通常被称为金标准，只有在认识到这些方法的局限性时才适用。由于 NGS 更新且发展迅速，其局限性可能不太为人所知，尽管这种情况也在发生变化。对于某些类别的变异，某些 NGS 方法现在可能与 Sanger 测序一样准确， , , 这提出了一个有效的问题：使用 Sanger 测序确认 NGS 结果会降低整体准确性吗？如本文所述，如果使用得当，答案是否定的，因为 Sanger 和 NGS 是正交技术，这意味着它们产生的错误在大多数情况下是不相关的（参见建议 4）。然而，由于这两种方法都不是完美的，因此当两种方法不一致时盲目地假设一种方法（在本例中为 Sanger）是正确的可能会增加错误率，正如已经证明的那样。相反，如果差异得到适当调查和解决，那么组合过程（NGS + Sanger 确认）的特异性将优于单独使用任何一种方法。实验室或许能够建立严格的标准，将高置信度的 TP 变异调用与那些不太自信且可能是也可能不是 TP 的变异调用区分开来。重要的是要认识到这些严格的标准与质量过滤器是分开的，质量过滤器用于删除很可能是 FP 的变异调用。各种研究表明，简单的技术标准（例如 NGS 读取深度）并不能单独充分区分高置信度 TP 和候选 TP 类别。理想情况下，单个稳健的变异调用质量分数可以做到这一点，尽管最常用的变异调用者产生的分数（截至本出版物发布时）同样被证明是不够的。相反，标准的组合似乎更有效。 对于包括 NGS 在内的任何实验室技术，FP 错误都可以分为两种类型：系统错误或随机错误。随机错误可以在任何时间或任何地点发生，而系统错误优先发生在特定情况下（例如，在重复序列中）。在现代 NGS 中，系统错误占所有 NGS FP 的很大一部分，这既带来了机遇，也带来了复杂性。机会：由于系统误差背后的因素可以通过适当的研究进行量化，因此可以识别最有可能成为系统性 FP 的患者的变异调用，并确保这些变异得到确认。复杂的是，根据定义，系统错误会反复出现，这意味着使用相同的技术在多个样本中观察相同或相似的变异几乎无法确信变异调用实际上是真实的。要知道这样的变异调用是否是 TP，需要一种正交方法。 本指南的局限性建议旨在应用于使用 NGS 检测种系（宪法）DNA 变异的临床测试。我们简要描述了可能适用于通过种系检测检测到的某些其他变异类型的特殊注意事项（例如，种系镶嵌和线粒体变异，需要单倍型分析才能正确解释的变异）。其他临床 DNA 测试不在范围内（包括肿瘤测序和液体活检测试，两者都旨在检测体细胞突变，以及无创产前筛查，旨在检测母体血液中的胎儿变异）。建议旨在适用于撰写本文时临床实验室常用的短读长 NGS 平台和化学品 [例如，Illumina 和 Ion Torrent ]。我们没有详细考虑临床实验室中不太常见的 NGS 平台、不再销售的平台或任何较新的单分子长读长平台。 建议 Index Recommendation 1 Clinical laboratories offering germline testing using NGS should establish a written policy regarding orthogonal confirmation of NGS results./使用 NGS 提供种系检测的临床实验室应制定有关 NGS 结果交叉确认的书面政策。 2 Laboratories’ orthogonal confirmation policy should be overseen and approved by a qualified and appropriately certified medical professional with training and experience in NGS./实验室的交叉确认政策应由具有 NGS 培训和经验的合格且经过适当认证的医疗专业人员监督和批准。 3 Laboratories’ confirmatory methods, platforms, and associated bioinformatics should be validated and maintained under appropriate regulatory oversight, as for other aspects of the test./实验室的确认方法、平台和相关的生物信息学应在适当的监管监督下得到验证和维护，就像测试的其他方面一样。 4 Laboratories’ confirmatory methods should be orthogonal. Discrepant results between NGS and a confirmatory assay should be investigated and resolved, rather than accepting any one method to be always correct./实验室的确认方法应该是正交的。应调查和解决 NGS 与验证性测定之间的差异结果，而不是接受任何一种方法始终是正确的。 5 Laboratories should perform confirmatory testing for reported germline variants with significant clinical implications, except for variant calls meeting technical criteria rigorously demonstrated to ensure high positive predictive value from NGS alone./实验室应对已报告的具有重大临床意义的种系变异进行验证性测试，但符合严格证明的技术标准的变异检出除外，以确保单独从 NGS 获得高阳性预测值。 6 Laboratories should clearly articulate their specific policies, criteria, and methods regarding orthogonal confirmation in written materials readily available on request./实验室应在可应要求提供的书面材料中清楚地阐明其关于交叉确认的具体政策、标准和方法。 7 Laboratories’ clinical test reports should summarize orthogonal confirmation policy in every report, and when exceptions to the policy are made, these should be clearly indicated./实验室的临床试验报告应在每份报告中总结交叉确认政策，当政策有例外时，应明确指出。 8 Special considerations apply to certain NGS-based test types and findings./特殊注意事项适用于某些基于 NGS 的测试类型和结果。 因全文较长，在此仅针对生信和方法学相关部分进行展开，其他部分（更多的是关于整体执行流程的规范和要求等，请参考原文） Recommendation 1: Clinical Laboratories Offering Germline Testing Using NGS Should Establish a Written Policy Regarding Orthogonal Confirmation of NGS Results实验室应该形成规范，明确什么情况下需要对结果进行确认。制定策略时，考虑的变量包括测试内容（例如，单基因与外显子组测序）、NGS 平台、变异类型、变异分类、质量指标、临床影响和其他因素。 Recommendation 2: Laboratories’ Orthogonal Confirmation Policy Should be Overseen and Approved by a Qualified and Appropriately Certified Medical Professional with Training and Experience in Next-Generation Sequencing制定验证策略的人，应该是结合遗传学相关背景，由经验丰富的专业人员制定和实施。 Recommendation 3: Laboratories’ Confirmatory Methods, Platforms, and Associated Bioinformatics Should be Validated and Maintained Under Appropriate Regulatory Oversight as for Other Aspects of the Test由于实际限制，特别是需要确认新的变异，分析验证可以接受特定于平台或方法的。例如，可以接受基于方法的基于 Sanger 扩增子测序的验证，而不是验证单个引物对——这是一项不切实际且繁重的任务。这种基于方法的方法应包括对整个确认工作流程的端到端验证，包括 PCR 引物设计、湿实验室 PC​​R 扩增测定以及用于读取 Sanger 测序数据的手册或软件程序。如果使用多种 PCR 扩增方案（例如，富含 GC 的 PCR 和长程 PCR），则必须分别验证每种 PCR 方法。由于此方法未验证单个引物对，因此在 NGS 结果与确认结果不同的情况下，对任何差异进行额外调查至关重要。 Recommendation 4: Laboratories’ Confirmatory Methods Should be Orthogonal: Discrepant Results between NGS and a Confirmatory Assay Should be Investigated and Resolved, Rather Than Accepting any One Method to be Always Correct确认测试的目的是发现 FP 并防止它们被报告。重要的是要记住，许多 NGS FP 错误是系统性的 , （即，可能会重复）并且没有任何检测是 100% 准确的（请参阅技术背景）。因此，应选择验证性检测，以使验证性检测的错误概况与主要 NGS 检测的错误概况尽可能少相关。以不同方式操作且具有最小重叠误差分布的两种测定的概念通常称为正交性。选择验证性检测需要实验室主任仔细判断。尽管可能无法实现完全正交，但可以避免明显的问题。例如，简单地重复主要的 NGS 测定不会提供任何程度的正交性，并且可能会导致报告系统性 FP。我们建议确认分析既不使用相同的核心测序技术，也不使用相同的文库制备方法。强烈反对使用不同的生物信息学管道重新分析相同的原始数据作为确认方法。扩增子的 Sanger 测序在当前实践中经常用于对由 NGS 识别的序列变异进行交叉确认，NGS 通常使用基于杂交的靶向。同样，微阵列、定量 PCR 或多重连接依赖性探针扩增通常用于确认 NGS 检测到的 CNV。鉴于方法上的差异，人们期望这些策略具有很大程度的正交性。因为没有一种检测是 100% 准确的，所以当主要检测和验证检测之间的结果发生冲突时，不应假定这两种检测都是正确的。相反，在从报告中包含或排除变异之前，应尽可能合理地调查和解决差异的原因（图 3）。调查应包括检查主要和确认化验的基础数据，还可能包括重复测试或第三种（已验证的）测试方法。实验室应将验证性检测的敏感性限制视为导致差异的潜在原因，并根据需要对特定患者或变异体采用另一种方法。例如，在感兴趣区域缺乏足够探针覆盖的拷贝数微阵列可能会产生 NGS 检测到的 TP CNV 的假阴性确认结果。同样，由于等位基因丢失，Sanger 测序可能会提供假阴性确认结果。在初始 NGS 测试或确认过程（即样本交换）中测试不正确的样本是造成差异的另一个潜在原因，应在调查期间予以排除。当无法在主要和确认检测之间实现明确的解决方案时，报告或省略相关变异的决定应由经验丰富的临床分子专业人员做出。如果报告了具有不一致的交叉方法结果的变异，则应在报告中描述验证性分析的结果以及对变异是 TP 的可能性的解释。 Recommendation 5: Laboratories Should Perform Confirmatory Testing for Reported Germline Variants with Significant Clinical Implications, Except for Variant Calls Meeting Technical Criteria Rigorously Demonstrated to Ensure High Positive Predictive Value from NGS Alone实验室关于哪些变异需要交叉确认的最终决定将取决于两种类型的标准，这两种标准都应在实验室的确认政策中明确描述，并分别应用于可能包含在测试报告中的每个变异调用。 技术标准：根据可用数据，此变异调用成为分析 TP 与 FP 的可能性有多大？ 医学标准：这种变异对患者护理产生重大临床影响的可能性有多大？ 技术标准可以制定严格的标准来识别极不可能是 FP 的变异调用（图 1）。尽管读取深度、变异类型、变异等位基因分数、基因组背景和许多当前的质量评分计算已被证明不足以单独用于此目的，但这些标准的组合已被证明是有效的（见前述技术考虑）。这些组合可能包括以下元素。 质量指标： 变异检出工具通常会为每个变异检出产生许多质量指标。例如，常用的 GATK 变异调用程序会生成指南推荐的多个分数，并且每个分数都有助于识别最高置信度的变异调用。汇总分数可能无法像单独考虑多个基础质量指标那样有效。 变异类型： SNV 在人类 DNA 中比插入缺失更为普遍。然而，Indel 变异调用往往比 SNV 调用具有更高的错误率，并且更有可能被解释为许多基因的致病性，而功能丧失是一种致病机制。对于某些变异类型（例如，SNV），通常更容易积累具有正交数据的大型数据集，以确定哪些调用可能需要或不需要确认。对于其他变异类型（插入缺失和 CNV），实验室可能没有足够大的数据集来以统计合理的方式确定标准。由于这些原因，变异类型可能是确定哪些变异需要确认的一个重要标准。 SNV 和插入缺失的最佳质量阈值可能不同。 复杂区域： 基因组背景可能是识别容易出现 FP 且可能未被通用质量指标充分标记的区域的重要标准。这些区域通常包括低复杂性重复序列，例如均聚物和短串联重复序列、移动元件以及具有高度相似的旁系同源基因或假基因的基因。在验证过程中，实验室可能会识别基因组中由 NGS 识别的变异不可靠的区域，并且应始终进行交叉确认。 变异频率： 变异频率是具有复杂含义的重要质量指标。等位基因分数的变异调用（种系 DNA 中远离 50:50 变异等位基因分数的杂合子）通常是 FP，尽管它们可能是受技术人工制品（例如，参考偏差或错误映射）影响的 TP。 如果没有额外的充足验证，不建议使用以下方法 在不同样本中重复确认相同突变。一些指南建议，在验证研究或常规实践中，实验室可能会在特定变异被确认为 TP 一定次数后停止确认。不幸的是，系统错误在 NGS 中很普遍，不同样本中相同变异的调用可能具有截然不同的质量水平。研究表明，对于相同变异的后续观察结果的准确性，重复确认的预测能力有限。 人工审查 NGS 数据作为确认的替代方法。人工审查 NGS 数据可以识别变异检出软件出现 FP 错误的某些情况。实验室政策可能允许在不使用二次检测的情况下去除此类变异。然而，反之亦然：在人工审查中没有发现问题并不一定意味着变异是没问题的。可能导致 FP 的生化和绘图问题在人工审查中并不总是很明显。 不同实验室使用的 NGS 过程可能存在许多实质性的和细微的差异，并且不同测试检查的目标之间通常存在重要差异。每个实验室都应使用自己的数据集，并使用这些数据制定自己的标准，而不是试图使用其他实验室的数据或标准。优化算法（可能是启发式的、统计的或基于机器学习的）已被证明可用于确定哪些质量指标和阈值的组合最能提供信息。这些方法可能需要大量的输入数据集。具有良好特征的样本，例如瓶中基因组联盟目前发布的七个样本（https://www.nist.gov/programs-projects/genome-bottle。但是这些数据也会存在一个问题，1.没有覆盖全部的基因组区域。2.该项目数据主要是蛋白编码区的SNV变异。不建议将确定的标准应用于不同类型的数据集合。一旦建立了技术标准应该进行详细的验证，验证方案可以参考AMP提供的详细建议（https://www.amp.org/resources/validation-resources） 医学标准当技术评估表明可报告变异存在成为分析性 FP 的风险时，如果此类 FP 可能对当前或未来医疗状况的诊断、治疗或管理质量产生不利影响，则建议进行交叉确认病人。对于与患者检测指征相关的高或中度外显率基因的致病性和可能致病性发现，通常就是这种情况。相比之下，良性变异通常没有临床影响，通常不会被报道。因此通常不需要确认良性变异。 不确定的变异解释：尽管意义不明的变异 (VUS) 通常不会影响临床决策，但一些 VUS 可能在以后被重新分类为致病性或可能致病性。因此，如果变异由于其初始 VUS 分类而未得到确认，则分析 FP 可能会出现问题。如果实验室政策是部分或所有 VUS 因其分类而未得到确认，则应向临床医生明确说明该政策及其含义。如果 VUS 后来被重新分类为致病性或可能致病性，则实验室应将其当前的标准政策应用于变异，以确定在发布修订报告之前是否需要进行确认测试。 主要发现与次要发现：虽然主要发现（即与患者的检测指征直接相关的那些）通常具有最大的临床影响，但次要（有时称为偶然）发现也可能具有重大影响。例如，考虑在接受心血管疾病诊断的患者的外显子组序列中继发发现致病性 BRCA1。 ACMG , 推荐的基因变异或药物遗传学相关基因变异是常见的二次发现类型，可能会在临床种系检测中报告，并且值得交叉确认。 携带者发现：在隐性基因中发现的单等位基因致病变异可能对生殖医学产生重大临床影响。此外，在一些发现这种携带者的患者中，稍后可能会使用不同的测试方法发现同一基因中的第二个致病变异，从而使最初的发现与他们的医疗保健直接相关 外显率：尽管低外显率变异可能带来相对较低的疾病风险，但高外显率和低外显率的致病变异都可以表明在当前的医疗实践指南下临床护理发生了重大变化。 在决定哪些变异需要基于医学理由进行确认时，实验室应仔细权衡风险。实验室政策应该让临床医生清楚了解，以便他们可以自信地知道哪些变异在任何临床报告中需要确认，哪些不需要 Recommendation 6: Laboratories Should Clearly Articulate Their Specific Policies, Criteria, and Methods Regarding Orthogonal Confirmation in Written Materials Readily Available on Request实验室应提供有关交叉确认政策的书面材料和总结确认政策验证结果的书面文件，包括实验室是否已将某些变异排除在技术确认之外。书面文件应该是最新的，包括更新和修正，并包括实验室的标准标准，报告的变异是否接受或不接受确认测试。请注意，一般性陈述（例如，符合高质量标准的变异）并不能单独满足此建议。书面文件还应包括针对初始 NGS 数据和确认测试数据不一致的变异检出所采取的措施。此外，文件应包括为标准政策的例外情况采取的行动，包括由于技术原因无法进行确认的情况，例如样本可用性不足或确认化验未能产生可靠数据。 Recommendation 7: Laboratories’ Clinical Test Reports Should Summarize Orthogonal Confirmation Policy in Every Report, and when Exceptions to the Policy Are Made, These Should be Clearly Indicated实验室应在报告中包含其标准交叉确认政策的摘要。这可以是一个简明的总结，也可以是对实验室标准的参考，以确定哪些报告的变异需要进行确认测试。如果实验室没有确认政策或不定期进行交叉确认，则应说明这一点。可能存在由于技术原因无法进行确认的情况，例如样本可用性不足或确认化验未能产生可靠数据。临床报告中应明确记录实验室标准变异确认标准的所有例外情况。 示例报告语言：此变异未通过既定的 NGS 置信度阈值，因此需要根据实验室的标准确认政策通过正交方法进行确认。反复的 Sanger 测序确认尝试未能产生可解释的数据。我们建议在具有经过验证的基因特异性方法的临床实验室中确认此变异。 Recommendation 8: Special Considerations Apply to Certain NGS-Based Test Types and FindingsSample Identity Confirmation 样本身份确认正交检测可用于双重目的，即确认变体是分析性 TP 并确保它存在于正确患者的 DNA 样本中。因此，确认化验可以检测 NGS 期间的样本或数据混淆，特别是当确认化验是对患者初始标本的单独等分试样进行时。如果实验室不对某些或所有具有临床意义的变异进行确认，则强烈建议使用替代的阳性样本追踪方法。示例包括单核苷酸多态性检测，可以与 NGS 结果进行比较以确认样本身份，或使用掺入寡核苷酸。这些机制还有一个额外的优势，即可以跟踪具有阴性或阳性发现的样本。 Location Resolution 位置分辨率短读长 NGS 平台不能总是准确地确定在具有同源假基因或基因家族成员的基因中观察到的变异的具体位置。例如，映射到 PMS2 基因的变体实际上可能源自 PMS2CL 假基因，该假基因在该基因的特定区域与 PMS2 的序列有 99% 相同。使用例如长程 PCR 或长读长测序的确认分析既可以确定潜在致病变异位于哪个位点，也可以确定它是否实际上是一个分析的TP。 Mosaic Variants在 NGS 结果中以低变异等位基因分数观察到的变异提出了具体的考虑。由于多种原因，无论是生物学原因还是技术原因，变体都可能出现在低等位基因部分，包括以下原因: 嵌合体 不确定的克隆性造血，可在血液、口腔和唾液标本的淋巴细胞衍生 DNA 中观察到。 来自同源区域（例如，假基因）的读数错配。 参考偏差：NGS 读数包含与参考基因组中存在的不同等位基因的事实可能不容易映射到正确的基因组位置。 系统错误如果实验室在临床上报告了低等位基因分数的变异，那么尝试解决等位基因不平衡原因的适当程序就很重要。报告语言也很重要，特别是当对低等位基因分数的根本原因存在歧义时。确认技术，尤其是 Sanger 测序，可能降低了对实际镶嵌变异的敏感性，因此可以看到相对常见的情况，即通过 NGS 观察到的 TP 在确认分析中是假阴性。 Mitochondrial Variants 线粒体变异由于异质性，线粒体基因组中的变异可能存在于低等位基因部分。除了上述与镶嵌变异相关的挑战外，线粒体变异可以是多等位基因，在 DNA 混合物中存在超过两个等位基因，这种情况类似于肿瘤测序中因异质性而遇到的情况。实验室确认政策应详细说明如何处理和报告此类情况。 Haplotyping/Phasing 单体型/定相某些变异的临床解释可能需要单倍型分析（即确定同一基因或区域中多个变异的顺式或反式关系）。该步骤在隐性基因（确定何时观察到双等位基因、复合杂合变异）和需要多个顺式变异来识别特定等位基因（例如，HLA 或细胞色素基因中的星号等位基因）的基因中可能很重要。短读长 NGS 平台通常无法确定大于几百个碱基对的基因组区域的相位。可以通过以下确定更大距离的单倍型和相位。 Mendelian analysis (when family members are available for testing). Imputation or other statistical inference methods. Certain confirmatory assays, such as long-range PCR or long-read sequencing. Familial Sample Testing 家族样本检测通常在基因检测中，家族样本与先证者的样本一起进行检测，以帮助进行变异分类（即确定共分离和/或阶段）。如果家庭成员使用 NGS 进行测试并收到个性化报告，则应按照与先证者相同的标准对向这些人报告的变异进行交叉确认。因为许多 NGS FPs 是系统的，而不是随机的，所以强烈建议不要简单地在多个个体中观察相同的变异，无论是否相关，作为一种确认技术。 Tumor Testing仅肿瘤测序可以检测种系变异，尽管目前的肿瘤测试对是否以及如何报告此类变异存在差异。关于这个主题的指南不断发展。 , 在某些情况下，患者被转介到不同的实验室进行 NGS 种系后续检测，在这种情况下，种系实验室可能会将先前的肿瘤检测视为对未发现的任何种系变异的确认。这是否合适取决于种系实验室主任对这两个测试是否正交的判断（见建议 4）。例如，如果两个测试使用相同的测序平台和捕获方法，则可能会重复出现系统错误。肿瘤测试和种系测试使用不同标本这一事实并不能单独使测试正交。]]></content>
      <categories>
        <category>NGS</category>
        <category>肿瘤检测</category>
        <category>标准化</category>
      </categories>
      <tags>
        <tag>AMP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[git仓库清理的方式]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-09.%E7%89%88%E6%9C%AC%E6%8E%A7%E5%88%B6%2FGit-3.git%E4%BB%93%E5%BA%93%E6%B8%85%E7%90%86%E7%9A%84%E6%96%B9%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[相关材料官方文档官方文档-中文 git stash 暂存没完成的工作项目的一部分上已经工作一段时间后，所有东西都进入了混乱的状态， 而这时你想要切换到另一个分支做一点别的事情。 问题是，你不想仅仅因为过会儿回到这一点而为做了一半的工作创建一次提交。 针对这个问题的答案是 git stash 或 git stash push ，这两个命令等价，可以暂时保存当前工作目录的变化，包括修改的文件、删除的文件、新建的文件、修改的配置文件、HEAD 指针的位置以及索引的内容到一个缓存栈，并将工作区恢复到初始状态。 123456789101112131415161718192021# 暂存工作区$ git stashSaved working directory and index state \ "WIP on master: 049d078 added the index file"HEAD is now at 049d078 added the index file(To restore them type "git stash apply")# 交互式的确定要暂存的文件,而不是所有被更改的文件。$ git stash --patch# 查看被暂存的工作区清单$ git stash liststash@&#123;0&#125;: WIP on master: 049d078 added the index filestash@&#123;1&#125;: WIP on master: c264051 Revert "added file_size"stash@&#123;2&#125;: WIP on master: 21d80a5 added number to log# 恢复被暂存的工作区$ git stash apply# 移除暂存的工作区$ git stash drop stash@&#123;2&#125; 背景在具体的业务时间过程中，会不断的产生需求，而在开发测试过程中，也会出现一次开发过程中，提交了多次commit的情况。因此有时候为了保证整体git log的记录完整性和仓库]]></content>
      <categories>
        <category>编程拾慧</category>
        <category>Git</category>
      </categories>
      <tags>
        <tag>培训</tag>
        <tag>Git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[git commit 的一些执行规范]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-09.%E7%89%88%E6%9C%AC%E6%8E%A7%E5%88%B6%2FGit-4.%E6%89%A7%E8%A1%8C%E8%A7%84%E8%8C%83-commit%2F</url>
    <content type="text"><![CDATA[相关材料官方文档官方文档-中文 背景git commit message 是我们使用 git 进行代码提交时的日常操作。写好 log 不仅有助于他人 review, 还可以有效的输出 CHANGELOG, 对项目的管理实际至关重要, 但是实际工作中却常常被大家忽略。尤其是对于大型协作项目，commit的内容对于项目的维护非常重要。 commit目前，社区有多种 Commit message 的写法规范。这里介绍Angular 规范，这是目前使用最广的写法，比较合理和系统化，并且有配套的工具。每次提交，Commit message 都包括三个部分：Header，Body 和 Footer。 commit12345&lt;type&gt;(&lt;scope&gt;): &lt;subject&gt;// 空一行&lt;body&gt;// 空一行&lt;footer&gt; 其中，Header 是必需的，Body 和 Footer 可以省略。不管是哪一个部分，任何一行都不得超过72个字符（或100个字符）。这是为了避免自动换行影响美观。 HeaderHeader部分只有一行，包括三个字段：type（必需）、scope（可选）和subject（必需）。 typetype用于说明 commit 的类别，不同规范中可能会略有差异，但是整体上应该只允许有限且界定鲜明的几个标识。比如相对常用的有： feat：新功能（feature） fix：修补bug docs：仅仅修改了文档，比如README, CHANGELOG, CONTRIBUTE等等 style： 仅仅修改了空格、格式缩进等等，不改变代码运行逻辑 refactor：重构（即不是新增功能，也不是修改bug的代码变动） perf: 优化相关，比如提升性能、体验。 chore：构建过程的变更，改变构建工具、或者增加依赖库、工具等 test：测试用例的调整，包括单元测试、集成测试等 revert: 回滚到上一个版本 如果type为 feat 和 fix ，则该 commit 将肯定出现在 Change log 之中。其他情况（docs、chore、style、refactor、test）由你决定，要不要放入 Change log，建议是不要。 scopescope用于说明 commit 影响的范围，比如数据层、控制层、视图层等等，视项目不同而不同。 subjectsubject是 commit 目的的简短描述，不超过50个字符。123以动词开头，使用第一人称现在时，比如change，而不是changed或changes第一个字母小写结尾不加句号（.） BodyBody 部分是对本次 commit 的详细描述，可以分成多行。下面是一个范例。 12345More detailed explanatory text, if necessary. Wrap it to about 72 characters or so. Further paragraphs come after blank lines.- Bullet points are okay, too- Use a hanging indent 有两个注意点。 使用第一人称现在时，比如使用change而不是changed或changes。 应该说明代码变动的动机，以及与以前行为的对比。 FooterFooter 部分只用于两种情况。 不兼容变动 如果当前代码与上一个版本不兼容，则 Footer 部分以BREAKING CHANGE开头，后面是对变动的描述、以及变动理由和迁移方法。 123456789101112BREAKING CHANGE: isolate scope bindings definition has changed.To migrate the code follow the example below:Before:scope: &#123; myAttr: 'attribute',&#125;After:scope: &#123; myAttr: '@',&#125;The removed `inject` wasn't generaly useful for directives so there should be no code using it. 关闭 Issue 如果当前 commit 针对某个issue，那么可以在 Footer 部分关闭这个 issue 。 123Closes #234# 也可以一次关闭多个 issue 。Closes #123, #245, #992 生成Change log如果你的所有 Commit 都符合 Angular 格式，那么发布新版本时， Change log 就可以用脚本自动生成（例1，例2，例3）。 生成的文档包括以下三个部分。 123New featuresBug fixesBreaking changes. 每个部分都会罗列相关的 commit ，并且有指向这些 commit 的链接。当然，生成的文档允许手动修改，所以发布前，你还可以添加其他内容。 conventional-changelog 就是生成 Change log 的工具，运行下面的命令即可。 123$ npm install -g conventional-changelog$ cd my-project$ conventional-changelog -p angular -i CHANGELOG.md -w 上面命令不会覆盖以前的 Change log，只会在CHANGELOG.md的头部加上自从上次发布以来的变动。 如果你想生成所有发布的 Change log，要改为运行下面的命令。 1$ conventional-changelog -p angular -i CHANGELOG.md -w -r 0 一些工具commitizen终端工具commitizen是一个基于npm的版本控制工具集，仓库内包含多个提交工具，cz-cli是commit提交的命令行程序；cz-conventional-changelog 提供了日志的生成标准。12# 安装npm install -g commitizen # 基于npm进行安装 commitizen-tools是一个基于python的发布管理工具。提供了一套标准版本管理和提交的规范，同时用户可以进行一定的自定义设置。说明文档示例 123# 安装conda install -c conda-forge commitizen #基于conda进行安装pip install --user -U Commitizen #基于pip 进行安装 conventional-changelog/standard-version 一步解决提交信息和版本发布。 一些moji标记https://gitmoji.dev/ ##commit规范 参考资料知乎]]></content>
      <categories>
        <category>编程拾慧</category>
        <category>Git</category>
      </categories>
      <tags>
        <tag>培训</tag>
        <tag>Git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[浏览器-Firefox配置]]></title>
    <url>%2F03.%E5%BA%94%E7%94%A8-03.windows%2F%E6%B5%8F%E8%A7%88%E5%99%A8-Firefox%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[Firefox Multi-Account Containers针对Firefox浏览器的多账号管理插件，主要针对一些网站，可能会存在多个账号（因公注册/因私注册）等，或为了进行权限分割，有时候需要注册多个账号（一个运维管理/个人使用）等。通过该插件可以更好地实现多账号的分离和管理。]]></content>
      <categories>
        <category>环境配置</category>
      </categories>
      <tags>
        <tag>Windows</tag>
        <tag>Firefox</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Recommendations for the Use of in Silico Approaches for Next-Generation Sequencing Bioinformatic Pipeline Validation]]></title>
    <url>%2F01.%E7%9F%A5%E8%AF%86-03.standard%2FRecommendations-Use_of_in_Silico_Approaches_for_Next-Generation_Sequencing_Bioinformatic_Pipeline_Validation%2F</url>
    <content type="text"><![CDATA[背景随着NGS在分子诊断中变异（体系和胚系）检测的应用普及。临床NGS检测产品的检测范围越来越大，带来的一个优势是可以识别更多基因中的变异；但是同时也使性能验证面临巨大的挑战。临床方法学验证过程中很难获得真实样本，能同时具有大量关注且能够被检测到频率也合适的变异。所以大多数实验室都是通过对具有已知变异的标准品或细胞系进行测序分析，并依靠测序指标（即具有足够覆盖率的目标部分）来推断其他区域的性能。 在这个大背景下，很多实验室会开始考虑使用模拟数据进行性能评估，使用数据模拟的方法在临床实验室中具有非常高的实用性，可作为临床检测验证的辅助工具。数据模拟可以采用多种形式，包括纯模拟数据或将变异插入现有测序数据中。通过模拟数据可以便捷的构建出真是样本中难以获取的变异数据。这些数据使实验室能够更准确地测试生物信息学流程的性能，而无需对其他病例进行测序。例如，可以模拟低频变异，以测试流程的分析灵敏度，或模拟一系列不同长度的插入/缺失以确定软件的性能边界。目前也有一些已经被广泛应用的方案，例如样本层面通过合成基因组混入目标变异，或生成目标变异的计算机模拟数据混入测序数据（Fastq/Bam）文件中。从而进行更全面的方法学验证和生信流程验证 在本文中，主要介绍不同类型的数据模拟类型及其优势和局限性、模拟实现方法，以及如何在临床分子诊断实验室中更好的使用模拟数据。 数据模拟类型模拟数据可以广义地定义为人工操作或生成的任何数据。大体上可以主要划分为下述几个大类。 A. 纯模拟数据（Purely Simulated Data ）通过模拟参考序列的测序reads（纯模拟数据）从头生成Fastq数据。这类数据通常是用于生信流程的原始数据，可以较便捷的大量生成几乎任何类型的变异。这些数据对于样本稀缺且方法不成熟的复杂变异类型特别有用。但模拟reads生成过程通常不能完整的模拟测序技术的噪音或偏差，尤其是在变异难以检测的重复区域（例如，在均聚物处）。也很难对临床测试中最常用的靶向测序方法的偏差和覆盖分布进行模拟，同样也很难对福尔马林固定、石蜡包埋过程引入的噪音进行模拟。 B. 混合样本（Mixing Sample Data）来自物理样本的两个数据文件以不同的比例混合，模拟具有不同频率的变异，和真实数据相比，可以更好的帮助评估生信流程在不同变异频率的性能表现。该策略可以混合来自两个充分表征的正常样本的 BAM 或 FASTQ 文件，通过不同的组成可以混合出不同频率范围的变异，能够测试大量不同变异的检测限。这个方法不能模拟肿瘤中出现的许多体细胞变异的复杂性（例如，拷贝数变异和大的结构变异）。正常基因组通常不存在临床感兴趣的变异，因此实验室也可以将包含感兴趣变异的肿瘤基因组与相应的正常样本混合，以测试它们在不同等位基因部分检测这些变异的能力。正常细胞大部分是已知的胚系变异，而且两个样本可能会有一些变异位点的重叠也会对下游变异检出带来一些困扰。正常细胞系通常会在一部分细胞中出现变异，如果这些变异在参考样本中没有得到很好的表征，则这些变异可能看起来是假阳性体细胞变异。 C. Fastq抽样（Downsampling FASTQ Files ）对来自单个物理样本的数据进行采样，以模拟较低覆盖深度对变体检出的影响（下采样 FASTQ 文件）。为了测试流程在不同覆盖率水平下检测变异的能力，可以对高覆盖率的 FASTQ 文件进行下采样（即，可以从更高覆盖率的数据集中随机选择一小部分读取）。一般来说，重要的是从整个 FASTQ 文件中随机抽取所需的reads数据，而不仅仅是读取的前 X 百分比。对于双端测序，进行PE同步采样 [例如，使用像 seqtk 这样的工具（https://github.com/lh3/seqtk)]。该策略可以识别流程在较低覆盖水平检测变异的能力局限性，尽管它需要使用上述策略之一确保数据中存在感兴趣的变异信息。 D. 修改实验数据（Manipulated Assay Data ）将单核苷酸变异 (SNV)、插入/缺失 (indel) 甚至结构变异等变异插入到实验室数据文件（BAM 或 FASTQ）中，以评估生信流程正确识别和注释变异的能力。这种方法的优点是可以很好地处理目标或全基因组测序数据，并保留真实数据的许多错误概况和偏差。它还可以在不同等位基因分数的不同基因组背景下验证比实际样本允许的数量多得多的不同类型的变体。将这些 BAM 文件转换回 FASTQ 通常很重要，以便从初始对齐步骤开始测试流程。这种方法有一些重要的局限性：i) 因为它依赖于在修改reads之前正确比对的reads，它不会模拟在难以比对区域中可能发生的所有比对错误，例如带有假基因的基因； ii) 它不能模拟变异出现在异常区域的偏差（例如，导致更高测序错误率的均聚物或串联重复序列的扩展，或引入可能导致系统测序错误的 GGT 序列基序）； iii) 对较大变异进行模拟具有挑战性（例如，对由大删除或大插入中的排序错误引起的覆盖率下降和断点进行建模）； iv）目前的工具不能便捷的操作一些特殊测序技术产生的原始数据格式，例如 Ion Torrent（Thermo Fisher Scientific，Waltham，MA）流数据，来自 PacBio HiFi（Pacific Biosciences，Menlo Park，CA）的原始读数，以及来自纳米孔测序的 fast5 文件（ Oxford Nanopore Technologies, Oxford, UK)， E. 重分析（Data Reanalysis）对仅对生物信息学流程进行更改而不对上游的湿实验流程进行更改时，可以通过使用新流程对来自各种样本（包括参考材料和临床样本）的现有未修改数据进行分析来帮助验证优化的效果。这种方法具有使用现有真实数据的优势，这些数据具有所用方法的所有偏差和错误。实验室可以测试重现以前版本的流程检测到所有变异的能力，但不会测试以前未检测到的变异的检测性能。 Modifying Reference Genome针对单倍体基因组，还可以通过编辑参考基因组，当参考基因组发生变化时，被测序的个体应该在该位置有一个变异（假设个体与原始参考相匹配）。这种方法最适用于单倍体样本或单倍体染色体，如假常染色体区域外男性的 X 染色体和 Y 染色体。一个例外是，如果二倍体个体在某个位置具有杂合变异，并且更改参考以匹配该变异，则该变异将被逆转（例如，C&gt;T SNV 将变为 T&gt;C SNV , 或 2-bp 删除将更改为 2-bp 插入)。 可以用于进行数据模拟的软件清单 模拟数据的应用流程的不同开发阶段/不同程度的变更测试，对模拟数据的需求本身也会存在一些差异。这里主要需要明确，如果涉及下机数据获取前（建库、测序等）的变更，则一定需要使用真实数据进行补充验证。每种用途原文由相对详实的介绍，但因为整体不复杂也比较符合大家的主观认识，所以这里只列出每种用途的建议模拟方法。|用途|模拟数据方法||-|-||基准生物信息学工具|纯模拟数据；修改实验数据（如适用）||(a)新变异(b)检测限(c)最少的测序读数|(a) 修改实验数据 (b) 混合多个样本；(c)Fastq抽样||(a)实验室协议变更(b)变更不影响管道中工具的限制(c)变更影响管道中工具的限制|(a) 生物样本。计算机数据可以根据变化进行补充。(b) 现有的分析数据。计算机数据可以根据变化进行补充。||Proficiency testing 能力验证|修改实验数据||Variant annotations 变体注释|修改实验数据， VCF 文件操作| 模拟数据的未来发展Copy Number Variants拷贝数变异是临床上重要的一类遗传改变，在癌症和体质性疾病的管理中具有诊断、治疗和预后意义。拷贝数变异通常比小变异更难检测，特别是当拷贝数改变 (CNA) 存在于亚克隆（肿瘤）或嵌合体（种系）时等级。因此，对旨在识别 CNA 的临床 NGS 检测进行全面验证非常重要。但是对应的CNA样本却相对少见，很难采购样本进行全面验证。所以也表现出对CNA数据模拟的需求。 Bamgineer 是最近发布的一种算法，可以将任何所需级别的用户定义的等位基因特异性 CNA 引入到 BAM 文件中。当从 BAM 文件中采样读取时，该算法会考虑配对末端测序数据中的reads对。这种方法试图保留 BAM 文件中的原始偏差，并更好地模拟真实样本中的 CNA。该算法可应用于许多用例，例如在无细胞 DNA 样本和亚克隆 CNA 检测中以低等位基因负荷模拟 CNA。 VarBen 是一种新的综合性计算机变异模拟算法，可在 BAM 文件中引入各种遗传改变，包括 SNV、插入、删除、大型结构变异，包括拷贝数改变、重复以及平衡和不平衡易位。 Translocation (Gene Fusion) Assessment在临床 NGS 分析中，易位通常通过靶向 DNA/RNA 测序、RNA 测序或全基因组测序进行检测。在 DNA 水平上，大多数易位发生在内含子中，内含子可能包含难以分析的重复或低复杂性区域。类似地，易位必须导致足够数量的基因融合转录本才能被 RNA 测序检测到。这些问题使得易位检测性能的广泛验证对于 NGS 分析至关重要。然而，对于许多易位，例如 ROS1、RET 或 NTRK 中的易位，可能很难找到足够数量的易位病例来全面测试检测验证过程中的易位检测。因此通过数据模拟进行全面的验证是很有必要的。 RNA SequencingRNA 测序在临床实验室中变得越来越普遍，用于检测易位/融合事件、测量基因水平表达、解决不确定意义的变异以及测量等位基因特异性表达。 RNA 本身非常不稳定，因此难以为质量控制目的对物理样本进行重复测试。存在多种 RNA 测序模拟工具： Polyester，rlsim ,RNASeqReadSimulator和 simCT TMB and MSI TestingTMB 和 MSI 均已被证明是癌症治疗反应的重要标志物，这两个指标的报告现已包含在肿瘤芯片检测中。 TMB 和 MSI 分别表示根据观察到的每兆 DNA 碱基的体细胞变异数和基因组特定区域中二核苷酸重复序列的扩展计算的测量值。两种类型的潜在事件（体细胞变异或微卫星扩展）都可以通过当前的计算机基因组建模工具进行模拟，但工作组不知道专门设计用于模拟 TMB 或 MSI 的软件。 Minimal Residual Disease Testing 最小残留病害检测UMI/UID等技术的开发已经可以帮助克服NGS测序错误率瓶颈，从而可以检测频率远低于1%的变异。但是迄今为止，还没有开发出通过将独特的分子指标与掺入变体相结合来模拟计算机中最小残留疾病测试的软件。随着这种测试形式的激增，此类技术将对临床社区产生巨大的价值。 Long-Read Sequencing Methods直到最近，Pacific Biosciences 和 Oxford Nanopore Technologies 的长读长测序方法对于大多数临床应用来说都是昂贵的和/或具有不可接受的高错误率。然而，长读可以准确识别短读具有挑战性的变异，例如具有同源基因或假基因和结构变异的基因。随着长读对临床应用的成本效益和准确性越来越高，还需要开发在长读文件中模拟和编辑变体的技术。 临床实验室中使用模拟数据的建议 Recommendation 1: The Laboratory May Use in Silico Data Files to Supplement NGS Analytical Validation, Particularly to Assess Analytical Sensitivity or False-Negative Rates for Specific Variants; However, in Silico Data Files Cannot Supplant the Use of Physical Samples (eg, Patient Samples)计算机数据可以用用来作为补充，但是不能替代物理样本，同时模拟数据只能用来评估灵敏性，无法对特异性进行评估 Recommendation 2: The Laboratory Should Understand the Functional Limitations of the Type(s) of in Silico Data Being Utilized在使用模拟数据时，用该充分了解所使用模拟数据的局限性。前文讨论了每种类型的模拟数据局限性。了解在 NGS 生物信息学管道验证中使用的计算机数据类型的功能限制以及对建立和/或监测分析性能特征的潜在下游影响以避免严重缺陷至关重要。例如，与从头模拟的计算机数据相比，通过修改现有数据文件生成的模拟数据可能更好地反映系统测序错误、脱靶reads、配对末端距离和临床测序面板靶向基因的覆盖变异性.一般来说，应该使用自己实验室按照标准工作流程生成的多个数据集。 Recommendation 3: The Laboratory Should Understand the Limitations of Most in Silico Data for Assessing Performance in Particular Genome Contexts and Variant Types Susceptible to Systematic Sequencing Errors (eg, Homopolymers and Tandem Repeats) and Mapping Errors (eg, Genes with Pseudogenes)重要的是要了解大多数计算机数据的局限性，以评估特定基因组背景和变异类型的性能。特别是，即使修改真实数据文件也不会模仿一些系统错误，例如均聚物和串联重复。因为修改真实数据取决于读取的正确映射，它通常也无法评估难以映射区域或片段重复的错误，例如具有假基因或高度同源基因的基因，如 PRSS1、PMS2 和 SMN1/SMN2，或有错误的基因在 GRCh38 中，如 CBS、U2AF1 和 KCNE1。 Recommendation 4: The Laboratory May Use in Silico Samples for Testing Required for Minor Updates to Clinical Bioinformatics Software Pipelines用于测试软件/工具/数据库更新或版本更改的模拟数据应使用实验室的现有数据。模拟数据可以用于进行生信软件和流程的升级。 Recommendation 5: Commercial Vendors and Internal Pipeline Developers Should Include Options in Their Analysis Pipelines to Facilitate Easier in Silico Data File Import and Analysis by Clinical Laboratories和其他生物信息学软件一样，数据模拟软件包在所需输入和预期输出、许可条款、操作系统兼容性和软件依赖性、错误修复和维护的规律性以及安装和使用的简易性方面各不相同。除了功能和可用性之外，在临床测序工作流程中采用之前，还应考虑特定软件的质量和社区接受度，尽管不可否认，这些可能难以严格评估。]]></content>
      <categories>
        <category>NGS</category>
        <category>肿瘤检测</category>
        <category>标准化</category>
      </categories>
      <tags>
        <tag>AMP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ACP考试知识点]]></title>
    <url>%2F05.%E5%A4%87%E8%80%83-01.%E4%BA%91%E8%AE%A1%E7%AE%97ACP%2F%E4%BA%91%E8%AE%A1%E7%AE%97-ACP%E8%80%83%E8%AF%95%E7%9F%A5%E8%AF%86%E7%82%B9-%E5%A4%9A%E9%80%89%E9%A2%98%2F</url>
    <content type="text"><![CDATA[ECS 用户购买ECS后，无需购买单独的服务就可以抵御的安全攻击包括： DDos攻击、异地登录、暴力密码破解、Web漏洞发现。 ECS产品的实例系列（实例系列I采用IntelXeon CPU实例系列II采用Haswell CPU):实例系列I的用户可以获得更大的实例规格 ;实例系列II相对于实例系列I增加了一些新的指令集使整数和浮点运算的性能翻倍 ; 实例系列I1全部为I/0优化实例，配合SSD云盘使用可以获得更高的I/O性能 专有网络内的ECS使用安全组防火墙进行三层网络访问控制 ECS从经典网络迁移到专有网络， 公网IP、实例ID、实例用户名和密码都不会发生变化；私网IP会重新分配；。 ECS实例磁盘上的数据如果需要回滚，要求ECS实例的状态为已停止 想长期使用某个公网IP地址可以选择EIP（绑定到ECS上）； 实例带宽设置为 0Mbit/s,可以释放NatPublicIP； PublicIP会随实例一起释放。一个ECS仅能绑定一个公网IP 企业级异构计算规格簇群可以提供更好的浮点运算性能 ； 实例均为I/O优化实例，可以发挥更好的性能； Windows系统的ECS实例部署Web环境，至少选择2GB内存的实例规格。 按量付费的ECS转为包年包月实例，需要满足条件：不是停售的实例规格、不是抢占式实例、没有未支付的订单、未设置自动释放时间、处于运行中或已停止状态。 ECS实例释放保护不能阻止因合理原因自动执行的释放行为，包括但不限于:账号欠费超过15天实例被自动释放。实例设置了自动释放时间，到期后被自动释放。实例存在安全合规风险，被停止或释放。实例由弹性伸缩自动创建，在缩容时被移出伸缩组并释放。 保证ECS正常运行，不要卸载相关硬件的驱动程序、不要开启SElinux、不要修改Linux实例的/etc/issue文件内容、不要使用ECS做流量穿透服务 操作。 SMC服务器迁移服务 能够将单台或多台迁移源迁移至阿里云。 迁移源包括IDC服务器、虚拟机、其他云平台的云主机或其他类型的服务器。 数据传输安全有保证默认采用SSL2048位RSA密钥加密传输通道，支持通过VPN网关、高速通道物理专线等私网迁移。 支持不停机迁移,迁移过程无需停机，不影响源服务器系统业务。 一键迁移，自动在待迁移服务器执行导入迁移源脚本，迁移脚本会下载、解压并运行SMC客户端 部署集注意事项 部署集内不能创建抢占式实例 部署集不支持创建专有宿主机 部署集之间不支持相互合并 OSS 通过Web前端技术向OSS上传数据的方式有：使用表单上传、使用OSS Brdser.js SDK上传、 通过支付宝小程序上传。 OSS的静态托管支持html格式的文件。 阿里云数据存储平台中，有三类角色，分别是Client（客户端）、Master（主服务器）和Chunk Server（数据块服务器） OSS请求数据不存在时，会返回404错误，如果设置了正确的回源规则，可以通过回源规则从OSS获取正确的数据，回源类型有 镜像回源 和 重定向 两种。 SS标准存储适合于各种 社交、分享类的图片、音视频应用、大型网站、大数据分析 等业务场景。OSS简单上传 上传单个文件不超过5G； 上传单个文件过程中可以携带meta信息； 基于PUT方式的http请求上传。 bucket属性 自定义域名绑定、读写权限设置、防盗链设置可以通过Bucket属性进行设置 存储空间（Bucket）具备多种配置属性包括：地域、访问权限、存储类型等 OSS高防 每个地域可以创建一个高防OSS实例，每个实例最多可以绑定同一地域下的10个Bucket。 Bucket绑定高防实例后，无法在浏览器预览Bucket内的资源。 OSS高防只有在超出DDos原生防护阈值时才生效 OSS高防默认不对Bucket关联的自定义域名进行防护。 图片处理限制（处理收费） 只支持JPG、PNG、BMP、GIF、WebP、TIFF格式 原图大小不超过20M 图片旋转对原图高宽不超过4096px，其他操作高或宽不超过30，000px，总像素不超过2.5亿px 其中gif格式的图片支持指定宽高缩放，不支持等比缩放（所以对缩略后图片大小有限制）图片处理方式 用户可以通过多种方式处理图片： 通过文件URL、 API、SDK使用图片样式对OSS内图片进行处理。 CPFS CPFS支持两种存储规格： 100MB/s/TiB基线和200MB/s/TiB基线。 云数据库 云数据库默认提供 按备份集恢复 和 按指定时间点恢复。 云数据库Redis(键值对数据库) 云数据库Redis支持三种不同的部署架构，即标准架构、集群架构、读写分离架构。慢日志 数据节点慢日志数据节点慢日志中统计的命令执行时间仅包含命令在数据节点中的执行时间，不包含数据节点与代理或客户端的通信时间以及命令在单线程队列上的排队延迟等。 代理慢日志代理慢日志中统计的命令执行时间从代理向数据节点发出请求开始，到代理从数据节点收到相应的回复为止，包含了命令在数据节点中的执行时间、数据在网络中的传输时间以及命令的排队延迟等。 云数据库RDS(关系型云数据库) 云数据库RDS提供三种数据存储类型，分别是：ESSD云盘，本地SSD盘，SSD云盘。 RDS支持多种数据库引擎，包括SQL Server、PostgreSQL、MYSQL、MariaDB ，并提供了容灾、备份、回复、监控、迁移等方面的解决方案 进行变更配置操作的实例状态需要是运行中；变更配置后实例的链接地址不会发生变化备灾实例 RDS通过数据传输服务(DTS)实现主实例和异地灾备实例之间的实时同步 异地灾备实例切换为主实例后，需要在应用端修改数据库连接地址，恢复业务系统 DTS支持全量迁移也支持增量迁移，DTS 的增量迁移是实时获取在迁移过程中，源数据库产生的增量数据，然后在全量迁移完成后，开始同步到目标 RDS 实例中。当增量迁移第一次追平源库的写入时，增量迁移的状态为无延迟，此后增量迁移会一直同步源数据库的业务写入。 VPC VPC创建后，其主IPv4网段不能修改 每个VPC有且只有一个路由器； 删除VPC时，会自动删除对应的路由器，创建VPC时，会自动创建一个路由器。不支持直接创建或删除路由器。 路由表支持通过专有网络VPC的控制台 和 阿里云VPC的OpenAPI进行配置。 若VPC中创建了ECS实例规格族限制中的ECS实例，则该VPC不支持使用高级功能，包括：括网络ACL、流日志和自定义路由表 VPC的交换机通常指的是二层交换机，但也可能涉及到三层交换机。 VPC对等链接、云企业网和VPN可以实现VPC的跨地域安全互访。 网络ACL无任何规则时，会拒绝所有出入方向的访问。 VPC支持操作 创建自定义路由条目 创建新的系统路由表进行流量管理 创建自定义路由表，并将自定义路由表和交换机（一个交换机只能绑定一张路由表）绑定。附加网段 添加附加网段后，主网段和附加网段同时生效，但创建交换机时，交换机只能属于VPC的一个网段 同VPC下，如果主网段内的ECS实例与附加网段内的ECS实例加入同一安全组且被VPC的网络ACL规则允许通行，则可以实现互通。 EIP 为方便关系，阿里云支持申请多个连续的EIP，连续EIP掩码范围为/24~/28,对应的EIP数目分别为 /28对应16个EIP；/27默认分配32个；/26默认分配64个；/25默认分配128个；/24默认分配256个 交换机 交换机创建完成后，不能修改CIDRBlock(Classless Inter-Domain Routing，无类域间路由：表示一个连续的IP地址范围) 删除交换机之前，必须先删除目标交换机所链接的云产品实例。 VPC的交换机通常指的是二层交换机，但也可能涉及到三层交换机 VBR（Virtual border router）边界路由器功能 作为VPC和本地IDC的中间路由器，负责交换数据包。 决定物理专线端口模式为三层路由接口或基于VLAN的三层子接口。 支持BGP协议 每个BGP有且只有一张路由表 VBR和VPC之间需要使用静态路由实现网络互通。 HaVip（High-Availability Virtual IP Address）高可用虚拟IPHaVip是一种可以独立创建和释放的私网IP资源，具备与ECS实例主私网IP地址一样的网络接入能力，可以与高可用软件，例如Keepalived配合使用，搭建高可用主备服务，提高业务的可用性。 HaVip支持绑定一个弹性公网IP（EIP）、多个(最多10个）ECS实例或多个ECS实例的主网卡或辅助网卡，以实现同可用区、多服务器高可用架构下的IP漂移，确保对外提供服务的私网IP始终不变。 HaVip只支持单播。 AS/ESS 弹性伸缩 伸缩报警任务支持：CPU、内存、系统平均负载、内外网出和入流量、TCP总连接数和已建立连接数、系统盘读写BPS、系统盘读和写IOPS、内网网卡收包数和发包数。 弹性伸缩过程中，指定多种实例规格，伸缩组会按照单位vCPU的价格从低到高排序，优先选择单位vCPU价格更低的实力规格。 伸缩组的状态： Active、Inactive和Deleting。 伸缩配置支持的镜像类型有 自定义镜像、公共镜像、共享镜像， 不支持云市场镜像，主要是因为云市场镜像是收费的。 弹性伸缩的伸缩配置支持 实例RAM角色、SSH密钥对、标签、实例自定义数据 特性，用于帮助用户高效、灵活的自定义ECS实例配置。不支持设置密码。 期望实例数只能在新建伸缩组时开启，不能对已有伸缩组进行修改开启该功能。 在一个伸缩组中，您可以创建多个伸缩配置，且最多可以拥有70个伸缩配置。但一个伸缩组中只能有一个伸缩配置处于生效状态，选用一个新的伸缩配置后，原生效中的伸缩配置会进入未生效状态。 伸缩组类型为 ECI (Elastic Container Instance弹性容器实例) 的伸缩组可以选择启动模板作为组内实例配置信息来源； 伸缩组类型为ECS的伸缩组可以选择已有实例作为组内实例配置信息来源。 一个伸缩组可以支持多个SLB实例，一个SLB实例可以绑定多个伸缩组。 同一个伸缩组内，同一时刻只能有一个伸缩活动在执行。 弹性伸缩支持多种模式，如 定时模式、动态模式、固定数量模式、自定义模式以及健康模式。 SLB SLB绑定EIP必须满足如下条件：SLB的网络类型是专有网络；SLB和EIP地域相同； 一个SLB仅能绑定一个EIP。 性能保障型负载均衡实例的关键指标：最大连接数、每秒新建连接数、每秒查询数。 云上架构搭配使用了云安全中心(安骑士)和DDS高防IF产品。业务架构搭建完毕后，用户访问时出现以下情况:Web应用报502错误(Bad Gateway)，以下哪几项可能是导致该错误的原因： 在业务层面存在通过SLB实例IP地址访问SLB实例的情况 SLB防火墙(iptables)规则配置错误，不能正常接收请求 源站配置了高防，但是没有在高防中配置七层转发规 Web应用处理HTTP请求的时间超过了负载均衡的超时时间 后端实例权重设置为零时，负载均衡不会在将流量转发给服务器（四层监听异常、7层监听不会显示异常）； 相当于ECS移除实例。 七层协议转发中，实现基于Cookie的会话保持，阿里云的负载均衡提供 Cookie重写 和 Cookie植入的处理方式 负载均衡支持HTTPS监听，并提供证书托管：服务器证书（PEM）需要上传证书内容和私钥 ， CA证书只需要上传证书内容。 负载均衡组成部分：LoadBalancer、Listener、Backend Server。 SLB实例必须开启健康检查，否则无法和弹性伸缩配合使用。 私网负载均衡 经典网络类型的私网负载均衡实例，其服务地址由阿里云统一分配和管理 专有网络类型私网负载均衡实例，其服务地址从指定的专有网络的交换机网段内分配 私网负载均衡是免费的 LVS 主要组成部分： 负载均衡器 服务器池 共享存储网络型负载均衡 NLB NLB支持UDP、TCPSSL、TCP协议，提供四层负载均衡能力。 网络型负载均衡 ALB ALB健康检查支持 GRPC、TCP、HTTP协议 ； ALB支持灰度发布，可以选择基于Cookie、 HTTP标头、 不同服务器组 实现灰度发布。 传统型负载均衡CLB的限制 需要通过域名和URL进行请求转发时，可以选择使用虚拟服务器； 不同的监听可以关联不同的服务器组，这样一个负载均衡实例可以将请求根据不同监听转发给不同的服务器组内不同端口的后端服务器。 如果您在配置监听时，选择使用虚拟服务器组或主备服务器组，那么该监听会将请求转发到关联的服务器组中的后端服务器，而不会再将请求转发给默认服务器组中的后端服务器 一个主备服务器组只包括两台后端服务器，一台作为主服务器，一台作为备服务器。由于备服务器不会做健康检查，所以只要主服务器健康检查失败，系统会直接将流量切到备机。当主服务器健康检查成功恢复服务后，流量会自动切到主服务器。轮询算法 SLB四层支持轮询、加权轮询(WRR)和-致性哈希(CH)调度算法。 七层支持轮询、加权轮询(WRR)调度算法 CDN CDN节点的系统关键组件有哪些： LVS(四层负载均衡)、Tengine（七层负载均衡）、Swift（做参考链接）、http(缓存)。 CND由 缓存系统、调度系统、链路质量系统和支撑系统 这四大系统组成。 CDN加速指定源站，在添加域名时，可以选择 函数计算域名、 源站域名、OSS域名。 可以通过OpenAPI和SDK进行CDN的开通和关闭； Web管理控制台可以自定义防盗链、缓存策略、HTTP响应头 ； CDN和加速域名在同一个阿里云账号下可以实现自动修改解析； CDN访问日志可以通过openAPI下载也可以通过CDN控制台下载。 CDN回源仅支持公网，会产生公网流量费或带宽费用。 CDN用户想要获取访客真实IP的方法有： “访客真实IP”保存在HTTP协议的”X-Forwarded-For” Header中，可以在Apache和Nginx的自定义Log中直接获取； Windows下，如果使用IIS，需要安装一个”FXForwardedFor”的扩展模块,才可以在日志里面看到”访客真是IP” 云监控 创建EIP后，云监控服务自动开启，无需手动开启即可对EIP进行监控。 云监控是意向针对阿里云资源和互联网应用进行监控的服务，可以监控主机的CPU、内存、磁盘、网络等监控项，不支持GPU的性能监控；在主机监控中，可以为单个主机设置报警规则 ；可以通过在主机上安装插件采集丰富的操作系统层面的监控项； 除阿里云的ECS还可以监控其他厂商的虚拟机或物理机。 报警服务 云监控的报警服务支持 电话、短信、邮件、钉钉机器人 等多种方式。 云监控可以在报警规则中选择日志服务，将报警信息写入日志服务的日志库 可以支持绑定弹性伸缩 日志服务日志服务SLS是云原生观测与分析平台，为Log（日志）、Metric（时序数据）、Trace（链路数据）等数据提供大规模、低成本、实时的平台化服务。 安全阿里云提供的安全解决方案有： 等保合规解决方案、企业上云安全建设解决方案。 DDos 购买DDos高防后，可以通过 域名接入和端口接入的方式接入DDos高防。 DDos防护说法正确的是 DDoS原生防护提供基础版默认为阿里云资源公网IP免费开启，无需购买 DDoS原生防护企业版实例的地域必须与需要防护的阿里云公网IP资产地域一致 内容安全 OSS违规检测功能，检测到违规时可以进行冻结，冻结方式支持：将违规文件进行删除、 将违规文件访问权限设置为私有。 内容安全是一款多媒体内容智能识别服务，可以通过API调用，提供 人工审核、人脸识别、内容审核、图片OCR识别 功能。 云安全中心Agent Agent卸载后有一段保护期，在保护期内重新安装的Agent会被自动卸载。 一键自动安装仅在阿里云服务器支持； 云安全中心Agent支持Linux系统和Windows系统 如果服务端在10个小时内没有收到Agent客户端登录、采集到的数据等信息，会将客户端状态更改为离线。 WAF负载均衡的WAF可以针对 跨站脚本、SQL注入、CSRF、本地文件包含、WebShell 进行安全防护。 Web应用防火墙WAF Web应用防火墙可以为不同网站防护模块设置白名单，使满足条件的请求忽略指定模块的检测，支持设置的模块白名单包括：Bot管理白名单、Web入侵防护白名单、数据安全白名单。 WAF支持使用透明接入和CNAME接入两种方式。 地域镜像、快照 VPC安全组 可用区实例挂载磁盘 阿里云系统 飞天：是阿里巴巴自研的分布式操作系统，能够支持大规模分布式计算和存储任务。由盘古和伏羲构成 夸父：是阿里巴巴自研的集群部署服务。 盘古：是阿里巴巴自研的存储管理服务。 神龙：是阿里巴巴自研的弹性裸金属服务。 伏羲：是阿里巴巴自研的资源调度服务。 ACK容器服务 容器服务ACK支持企业级容器化应用的全生命周期管理，包含三种产品形态：专有版kubernetes、Serverless kubernetes、托管版 kubernetes。ACK专有版 Master节点的系统盘支持开启云盘备份以备份云盘数据 ACK集群仅支持专有网络VPC ACK专有版需要自行创建Master节点及Worker节点。容器网络 每个Pod都拥有自己独立的网络空间和IF地址。不同Pod的应用可以监听同样的端口而不冲突。Pod可以通过各自的IP地址互相访问。集群中Pod可以通过它独立的IP地址与其他应用互通:同一个集群中，Pod之间相互访问。Pod直接访问同一个VPC下的ECS。同一个VPC下的ECS直接访问Pod。备份中心 备份中心提供了 定时增量备份、 按时间点的全量急速快照备份 功能 全球加速实例 基础型：基础型全球加速可用于三层（IP协议）网络加速场景，您只需要配置加速区域和终端节点组即可实现业务加速。 标准型：标准型全球加速主要用于四层（TCP和UDP协议）和七层（HTTP和HTTPS协议）网络加速。 其他 服务器迁移中心支持的操作系统有： Linux 和 Windows 数据传输服务DTS(Data Transmission Service）支持 RDBMS(Relational Database Management System)、NoSQL(Not Only SQL)、OLAP(Online Analytical Processing)、DB2 等数据源间的数据交互，集数据迁移/订阅/同步于一体。 开启注册局安全锁后，域名将被设置为三种状态：禁止转移、禁止更新、禁止删除。 域名注册完成后支持多种操作，包括：域名转移和域名交易。 阿里云SSL证书服务支持 ECC、RSA、 SM2 三种加密算法。 在使用DSC(数据安全中心)检测云产品中存在的敏感数据或审计数据库活动前，需要完成对应产品的授权，支持一键授权（对目标资产添加只读账户）和输入数据库账号密码授权。 云解析DNS是由管控层和解析数据层两部分组成管控层:云解析DNS为客户提供可视化的域名解析管理平台，可以帮助客户实现快速关系域名解析的增删改查。解析数据层: 云解析DNS会将客户在管控层的解析配置，实时同步到云解析DNS在全球部署的解析服务器节点上。 数字证书管理服务，支持购买 DV域名型、 OV企业型、 EV企业增强型三种类型的SSL证书。 加密服务支持的密码机类型包括金融数据密码机和通用数据密码机。 Web应用防火墙WAF和内容分发网络CDN结合使用，为开启内容加速的域名提供Web攻击防御。该管理员将CDN作为入口层用来实现内容加速，流量经过作为中间层的WAF进行防护再到源站。下列关于这个架构的配置，正确的是： 在CDN中配置源站信息时，源站域名不能与加速域名相同，否则会造成循环解析，无法回源 需要在WAF中生成要防护的源站域名的专用CNAME地址 一个分布式系统不可能同时满足 一致性、可用性和分区容错性 这三个基本需求。]]></content>
      <categories>
        <category>云计算</category>
        <category>考试资料</category>
      </categories>
      <tags>
        <tag>阿里云</tag>
        <tag>ACP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ACP考试知识点]]></title>
    <url>%2F05.%E5%A4%87%E8%80%83-01.%E4%BA%91%E8%AE%A1%E7%AE%97ACP%2F%E4%BA%91%E8%AE%A1%E7%AE%97-ACP%E8%80%83%E8%AF%95%E7%9F%A5%E8%AF%86%E7%82%B9%2F</url>
    <content type="text"><![CDATA[地域、可用区问题负载均衡： 不支持跨地域，可跨可用区。OSS: 不支持跨地域，可跨可用区。SLB（Server Load Balance）服务通过设置虚拟服务地址（IP），将位于同一地域（Region）的多台云服务器（Elastic Compute Service，简称ECS）资源虚拟成一个高性能、高可用的应用服务池 存储OSS OSS 提供多种鉴权和授权机制及IP黑白名单、防盗链（用户只有通过URL签名或匿名访问Object时，才会进行防盗链验证）、主子账号功能。 OSS 存储Object上传文件的大小，通过简单上传、表单上传、追加上传的方式上传单个文件，文件的大小不能超过5 GB ；通过分片上传的方式上传单个文件，文件的大小不能超过48.8 TB。 OSS 提供多种文件上传的方法，以方便用户在不同场景中使用，OSS支持 从OSS管理控制台直接上传、通过OpenAPI上传、通过SDK上传、通过云市场例的FTP工具上传 。 OSS图片处理API 一个用户最多可以创建10个channel、每个channel中存放的数量没有限制，但是单个object的最大大小为20M，每个channel存放的总和没有限制。 图片名称具有全局唯一性且不能修改。 OSS开启版本控制后，则不支持设置合规保留策略、镜像回源或静态网站托管； 如果bucket已经开启版本控制，则无法返回到非版本化状态。 OSS收费包含三个部分，存储空间费用、流量费用和OSS API的费用， OSS API请求付费，目前云服务器和OSS间的请求次数不分内外网都会计费，API请求次数，仅支持按量付费。 存储费用支持包年包月 和 按量付费。 仅公网下行收费，公网上传/内网上下行都不收费。 OSS免费支持 DDos攻击、自动流量清洗和黑洞功能。 修改已上传Object的元数据方法：将Object下载到本地，删除原来的Object,重新上传更改元数据后的Object； 通过CopyObject或者UploadPartCopy接口，对Object进行拷贝修改目标文件的元数据，再将原文件删除。 23年6月，新增加了set-meta 可以直接对数据的meta数据进行更新或删除 OSS支持使用对象标签对存储的Bucket 进行分类，用户可以针对桶标签的Object设置生命周期规则、访问权限 OSS 在断电、断网等导致某个机房不可用时，仍然能够提供强一致性的服务能力，切换过程无感知满足关键业务 RTO=0 和RPO=0 的强需求。 RPO（Recovery Point Objective）即数据恢复点目标，主要指的是业务系统所能容忍的数据丢失量。 RTO（Recovery Time Objective）即恢复时间目标，主要指的是所能容忍的业务停止服务的最长时间，也就是从灾难发生到业务系统恢复服务功能所需要的最短时间周期。 OSS提供保存访问日志的功能，对BucketA开启访问日志记录后，自动将请求日志，以小时为单位，按着固定命名规则，生成一个Object写入用户指定的BucketB， BucketA 和Bucket B可以是不同的Bucket，但是必须属于同一个用户。 OSS域名绑定描述正确的是：OSS cname绑定的域名必须经过工信部（不是阿里云）的备案 ； OSS域名绑定，仅支持OSS以三级域名访问方式进行绑定，访问方式为（Bucket name.${region}.aliyuncs.com 。 阿里云OSS和自建存储相比，有以下优点： 海量存储 、 低成本、 安全、 多线BGP接入。 针对不同的应用场景， OSS有 1.使用KMS托管秘钥进行加解密（密钥需要自管理、可指定），2.使用OSS完全托管加密 两种服务器端加密方式。 OSS 原生图片处理限制有：1.调用resize默认不允许放大； 2. 支持bmp、webp图片格式。 常见状态码| 状态码 | 含义 || —— | ———————— || 403 | 用户账号禁用、AK不匹配等 |本地盘 云盘和本地盘异同点：云盘和本地盘均支持SSD和快照，性能均和磁盘类型有关和容量无关（相同类型块存储产品的单位容量的I/O性能均一致，但云盘性能随容量增长而线性增长，直至达到该类型块存储的单盘性能上限） ； 本地盘不支持多副本的机制 ； 磁盘快照不支持压缩。 ECS ECS数据恢复在使用云服务器ECS的过程中，有时难免会出现数据被误删除的情况，在阿里云上恢复被误删除的数据有多种方式，比如： 通过ECS管理控制台回滚已创建的快照 使用开源工具Extundelete恢复误删的数据 使用开源工具ext3grep恢复误删的数据 ECS自动快照是保存在独立于用户自己的OSS Bucket里面，不能保存在用户的Bucket里面 阿里云服务器ECS出于安全考虑默认自带安全组（仅开放了22号和3389号端口，22：远程连接Linux服务器；3389：远程连接Windows云服务器） 云服务器ECS实例镜像选择Aliyun Linux镜像后，可以兼容Red Hat系列的操作系统（包含 Red Hat Enterprise Linux, Fedora, CentOS, Scientific Linux, Oracle Linux) 如果ECS实例发生了非预期宕机或运维，阿里云默认自动重启恢复实例，如果挂在了本地盘，可以选择的恢复方式如下：1.自动重启恢复、2.禁止重启恢复，3.自动重新部署。 阿里云的ECS支持用户名和密码登陆、SSH密钥对的鉴权方式。 阿里云可以使用SSH密钥对登陆云服务器ECS，SSH秘钥在应用过程中的优势有:1.密钥对安全强度远高于常规用户口令，可以杜绝暴力破解威胁； 2. SSH密钥对便于远程登录大量Linux实例，方便管理。 云服务器ECS主要包含以下功能组件： 实例：等同于一台虚拟服务器，内含CPU、内存、操作系统、网络配置、磁盘等基础的计算组件。 ECS 系统盘挂载盘：/dev/xvda ； ECS数据盘：/dev/xcdb,/dev/xcdc……/dev/xcdz 只有在稳定（运行中、已停止、已过期）状态才能挂载磁盘，其他状态都不行。 ECS使用过程中，可以使用扩容功能（创建一块新云盘，作为数据盘挂载到实例上），挂载扩容后只是扩大存储容量（不会扩容文件系统），需要通过LVM手动扩容分区和刷新文件系统。 一块全新的Windows数据盘挂载到ECS实例后，还不能直接存储数据，通常您需要完成磁盘联机、新建分区、格式化等初始化操作后，才能供系统读写数据。 更换ECS实例/操作系统后，重新分配新的系统盘（系统盘ID会更新），旧系统盘会被释放。系统盘的云盘类型、实例IP地址以及弹性网卡和MAC地址保持不变；旧系统盘的自动快照策略自动失效，需要重新设置自动快照策略；旧系统盘如果开启了自动快照随云盘释放，则自动快照会被自动删除，未开启则不会自动删除，等到到期后释放。 ECS相比于传统服务器，拥有每份数据多分副本、 单份数据损坏可在短时间内快速恢复、无需额外的开发，支持自动的磁盘数据备份。 Windows示例系统盘由40G扩容到100G，分区C显示大小99.9GB，但是可用空间（百分比）反而比之前更小，这个错误出现的原因是： 虚拟内存被开启且设置的是系统自动管理。 ECS的相关操作注意事项： 禁止使用ECS实例做流量穿透服务 不要随意修改网卡MAC地址 对于4GiB以上内存的云服务器，需选择64位操作系统 不要开启SeLinux 不要编译Linux系统的内核，或对内核进行任何其他操作 在Liux实例里，您重启系统后，可能会出现数据盘分区丢失或者数据丢失的问题。这可能是因为您未在etc/fstabi文件里设置自动挂载。此时，您可以先手动挂载数据盘分区。如果手动挂载时报分区表丢失，您可以尝试如下三种办法进行处理：1.通过fdisk恢复分区；2.通过testdisk恢复分区；3.通过testdisk直接恢复数据。 阿里云api的访问地址是： http://ecs.aliyuncs.com/ 如果仅在特定的时段有较高的CPU性能需求，为了尽可能节约成本，应该选用突发性能型实例。不同类型实例的适用场景： 突发性能型实例（Burstable Performance Instances）： 示例：AWS T3、Google Cloud N1、Azure B系列 适用场景：适合工作负载的 CPU 使用率具有突发性需求的场景，例如开发环境、小型应用服务器、低流量网站、测试和开发环境。 密集计算型实例（Compute-Optimized Instances）： 示例：AWS C5、Google Cloud C2、Azure F系列 适用场景：适合需要高性能计算的工作负载，例如科学计算、模拟、高性能集群、批处理处理、视频编码和游戏服务器等。 通用计算型实例（General Purpose Instances）： 示例：AWS M5、Google Cloud N2、Azure Dv3系列 适用场景：适用于广泛的工作负载，包括 Web 服务器、应用服务器、小型数据库、微服务、中间件和开发/测试环境等。 共享型实例（Shared Burstable Instances）： 示例：AWS T2、Google Cloud E2、Azure B1S 适用场景：1.中小型网站和Web应用程序；2.开发环境、构建服务器、代码存储库、微服务、测试和暂存环境等：3.轻量级数据库、缓存；4.轻量级企业应用、综合应用服务；适合个人项目、学习、小型网站、轻负载应用或低流量网站等对计算资源需求较低的场景。 ECS的生命周期管理状态包括：中间状态（待启动、启动中、停止中）和稳定状态（已删除、已停止、即将过期、已过期、欠费回收中、过期回收中、已锁定、等待释放） ECS通过使用AccessKey ID和AccessKey Secret进行对称加密，以验证请求的发送者身份。 其中AccessKey ID用于标识访问者的身份，AccessKey Secret是用于加密签名字符串和服务器端验证签名字符串的密钥，必须严格保密。 在重置ECS密码时，ECS实例必须在稳定状态下：已停止或运行中。 ECS的磁盘控制台，可以进行磁盘的扩容、回滚和备份，不能进行磁盘的共享。 磁盘操作时间更换系统盘需要ECS实例处于已停止状态； 更换系统盘，原系统的所在实例的ID不会发生变化； 更换系统盘，原系统的自动快照会被释放，手动创建的快照不会被释放，这些快照仍然可以创建自定义镜像。 更换迁移 ECS从经典网络迁移至VPC网络，实例本身系统硬件配置无变化（实例ID不变、系统盘ID不变）和公网IP不变；网络切换VPC网络，实例的私网IP会变化。 监控指标 ECS监控指标分基础监控和操作系统级别指标监控，操作系统级别监控指标包含内存使用率、平均负载、磁盘IO读/写、磁盘使用率、TCP连接数、进程总数。 ECS监控指标分基础监控和操作系统级别指标监控，操作系统级别监控指标包含内存使用率、平均负载、磁盘IO读/写、磁盘使用率、TCP连接数、进程总数。 云盘 ESSD AutoPL云盘：支持根据业务需求自定义云盘的预配置性能以及性能突发。该类云盘在保持ESSD云盘原有功能与性能的同时，可以实现云盘容量与云盘性能解耦。 建议在以下业务场景中使用： 应用于ESSD云盘所适用的场景（大型OLTP数据库、NoSQL数据库和ELK分布式日志等场景）。 业务所需的云盘容量固定，但需要更高的云盘性能支撑业务的运行。 业务波动较大，波峰高频出现。需要云盘具备应对突发业务的能力。 ESSD PL-X云盘：具备超高IOPS（Input/Output Operations Per Second）、超高吞吐量和超低时延等多维度的超高性能。您可以在配置ESSD PL-X云盘容量的同时，根据业务需求灵活自定义云盘的IOPS。更多信息，请参见ESSD PL-X云盘。建议在对云盘性能有更高要求的OLTP数据库和KV数据库场景中使用。 ESSD云盘：基于新一代分布式块存储架构的超高性能云盘产品，结合25GE网络和RDMA技术，单盘可提供高达100万的随机读写能力和更低的单路时延能力。建议在大型OLTP数据库、NoSQL数据库和ELK分布式日志等场景中使用。 SSD云盘：具备稳定的高随机读写性能、高可靠性的高性能云盘产品。建议在I/O密集型应用、中小型关系数据库和NoSQL数据库等场景中使用。 高效云盘：具备高性价比、中等随机读写性能、高可靠性的云盘产品。建议在开发与测试业务和系统盘等场景中使用。 弹性伸缩 弹性伸缩只支持ECS的伸缩，不支持其他服务的伸缩。只能使用阿里云的ECS实例。 用户手动添加到伸缩组中的ECS实例不会停止和释放，弹性伸缩只会停止和释放自动创建的ECS。 弹性伸缩的伸缩模式有多种，如定时模式、动态模式、固定数量模式、自定义模式以及健康模式。 开启实例释放保护后，用户不能手动释放，但是不能阻止合理的自动释放行为：1.账号欠费15天； 2.设置了自动释放时间到期； 3. 实例存在安全合规风险；4.弹性伸缩自动创建，缩容时被移除释放 。 报警任务支持：CPU、内存、系统平均负载、内外网出和入流量，TCP总连接数和已建立连接数。 在Forcedelete为False时，删除伸缩组需要满足：1. 伸缩组没有任何伸缩活动正在执行。2.伸缩组当前的ECS实例数量（Total Capacity）为0 阿里云的同一个ECS只能加入一个伸缩组。经典网络的ECS不能加入专有网络VPC。 伸缩活动特点：1.伸缩活动不可以中断，2. 伸缩活动保持ECS实例级的完整性，而非伸缩活动级事物的完整性。 伸缩组支特关联负载均衡实例和RDS实例，但是暂时不能关联Redis实例。如果您有业务数据存诸在Redis实例上，手动配置ECS实例加入或移出Rdis实例白名单，操作效率较低。您可以通过生命周期挂钩和OOS模板将ECS实例自动加入和移出Redis实例白名单。 弹性伸缩使用伸缩配置创建ECS实例，这种方式不够灵活，需要进行更多的自定义设置时，可以使用生命周期挂钩 或 实例自定义数据。 伸缩配置 弹性伸缩的伸缩配置支持多种特性，例如： 标签、SSH密钥对、实例RAM角色、实例自定义数据。 弹性伸缩创建出来的机器配置和规格相同，但是不会出现IP相同的情况。 伸缩组 创建伸缩组后，负载均衡实例、RDS数据库实例 不可以修改。 ECS同一时刻只能加入一个伸缩组。 伸缩组自动扩展必备的活动是: 1. ECS实例分配IP； 2. 启动ECS实例。 冷却时间 如果在伸缩活动中，没有ECS实例成功加入或者移出伸缩组，则弹性伸缩服务不会开始计算冷却时间 如果您停用再启用伸缩组，伸缩组启用后的首次伸缩活动可以立即执行，不会受冷却时间影响。当伸缩组启用后首次成功执行伸缩活动，弹性伸缩服务才开始计算冷却时间。 冷却时间不能为空，如果不配置使用默认冷却时间。 由于伸缩组正在发生伸缩活动或者伸缩组停用等原因，导致定时任务触发执行伸缩规则失败后，在LaunchExpirationTime内，定时任务会自动重试触发，否则放弃本次定时触发。 自动触发的伸缩活动有冷却时间，冷却时间指伸缩组成功执行伸缩活动后的一段锁定时间。在冷却时间内，伸缩组会拒绝由报警任务触发的伸缩活动请求。但非报警任务（手动执行任务、定时任务、健康检查等）触发的伸缩活动可以立即执行，绕过冷却时间。ECS实例状态 保护状态：处于保护状态的ECS实例负载均衡权重不受影响。弹性伸缩不会检查处于保护状态的ECS实例健康状态，也不会释放ECS实例，不希望被移出伸缩组的ECS实例转为保护状态。 备用状态：设置备用状态会把ECS的负载均衡权重置零，弹性伸缩不会检查处于备用状态的ECS实例健康状态，也不会释放实例。 移除策略 移除策略选择最早伸缩配置对应的实例：筛选添加时间最早的伸缩配置和启动模板对应的实例。手动添加的实例没有关联伸缩配置或启动模板，因此不会首先选出手动添加的实例。如果已移出全部关联的实例，仍需要继续移出实例，则随机移出手动添加的实例。 负载均衡 性能保障型实例的三个关键指标： 最大连接数-Max Connection最大连接数定义了一个负载均衡实例能够承载的最大连接数量。当实例上的连接超过规格定义的最大连接数时，新建连接请求将被丢弃。 每秒新建连接数-Connection Per Second（CPS）每秒新建连接数定义了新建连接的速率。当新建连接的速率超过规格定义的每秒新建连接数时，新建连接请求将被丢弃。 每秒查询数-Query Per Second（QPS）每秒请求数是七层监听特有的概念，指的是每秒可以完成的HTTP或HTTPS的查询（请求）的数量。当请求速率超过规格所定义的每秒查询数时，新建连接请求将被丢弃。 SLB判断流量转发的顺序：当用户流量经过负载均衡端口时，首先判断其是否能够匹配上某条“转发规则”， 如果匹配，则将流量转发到该规则的后端服务器组上；若不匹配并且在该监听上设置了虚拟服务器，那么将流量转发到该虚拟服务器组上； 若用户没有在该监听上设置虚拟服务器组，即将流量转发到实例级别添加的各后端服务器中。 公网类型负载均衡实例，系统会为其分配一个公网服务地址。 负载均衡的必要配置： 负载均衡SLB实例的属性配置 / LoadBalancer：负载均衡实例。 负载均衡SLB实例的监听配置 / Listener：用户定制的监听器，定义了负载均衡策略和转发规则。 负载均衡SLB实例的后端ECS实例配置 / BackendServer：后端的一组ECS（云服务器）。 四层和七层负载均衡差异点 四层负载均衡，采用开源软件LVS（linux virtual server），并根据云计算需求对其进行了定制化。适合无状态的 ; 七层负载均衡，采用开源软件Tengine 四层网络是基于 源IP实现， 7层网络是基于Cookie实现的 健康检查过程中，4层服务健康检查基于端口，七层服务检查是基于返回的健康码。 四层无法关闭健康检查 ，七层可以关闭健康检查。 共享实例带宽, 如果给某个监听设置带宽峰值，则会再总带宽中剥离出对应的带宽作为该监控的独享带宽（其他监听任务无法使用） 主备服务器组仅可适用于TCP和UDP监听。 负载均衡SLB不提供CNAME地址只提供IP地址；后端服务器的权重之和没有限制； 后端服务池中的ECS可能由于异常机制导致不能正常运行，会出现非运行中的情况。 一般情况下默认路由在有外网情况下会先走外网网卡，如无外网则走内网网卡。 用户在使用SLB负载均衡时，发现在HTTP请求的头部增加了Transfer-Encoding:chunked字段，但是从本地主机直接访问后端服务器时是没有这个字段的是因为：由于七层负载均基于Tengine反向代理实现。 健康检查： 4层负载均衡健康检查通过检查端口，7层负载均衡健康检查通过服务器端返回码。 四层负载均衡可以直接获取用户真实IP； 七层服务可以通过X-Forwarded-For获取用户真实IP。 CLB可以包年包月（计费项：实例费用、规格费用、公网网络费）和按量付费（实例费用、规格费用/LCU费、公网网络费） 监听四层负载均衡（传输层） 四层负载均衡，采用开源软件LVS（linux virtual server），并根据云计算需求对其进行了定制化。 会话保持基于IP 在4层SLB服务中，不支持添加进服务器池的云服务器既作为RS,又作为客户端向所在的SLB发送请求。返回的数据包只在云服务器内部转发，不经过SLB，所以通过配置在SLB内的云服务器去telnet SLB的IP是不通的。 TCP（Transmission Control Protocol，传输控制协议) 建议的应用场景 注重可靠性，对数据准确性要求高，速度可以相对较慢的场景； 示例：文件传输、发送或接收邮件、远程登录、无特殊要求的 Web 应用 健康检查： 健康检查通过定制的TCP探测来获取状态信息。 特性 面向连接的协议； 在正式收发数据前，必须和对方建立可靠的连接； 基于源地址会话保持，在网络层可直接看到来源地址； 监听支持 TCP 和 HTTP 两种方式进行健康检查； 转发路径短，所以性能比 7 层更好，数据传输速度更快 UDP （User Datagram Protocol，用户数据包协议） 建议的应用场景 关注实时性而相对不注重可靠性的场景； 示例：视频聊天、金融实时行情推送 健康检查： 健康检查通过UDP报文探测来获取状态信。 特性 面向非连接的协议； 在数据发送前不与对方进行三次握手，直接进行数据包发送，不提供差错恢复和数据重传； 可靠性相对低；数据传输快 通过UDP保温探测来获取状态信息。 七层负载均衡（应用层） 七层负载均衡，采用开源软件Tengine。 会话保持基于cookie 针对七层（HTTP或HTTPS协议）监听，健康检查通过HTTP HEAD探测来获取状态信息， HTTP/HTTPS监听可使用植入cookie和重写cookie来进行会话保持。HTTP 建议的应用场景 需要对数据内容进行识别的应用，如 Web 应用、小的手机游戏等 特性 应用层协议，主要解决如何包装数据； 基于 Cookie 会话保持；使用 X-Forward-For 获取源地址； 监听只支持 HTTP 方式健康检查 HTTPs 建议的应用场景 有更高的安全性需求，需要加密传输的应用 特性 加密传输数据，可以阻止未经授权的访问； 统一的证书管理服务，用户可以将证书上传到负载均衡，解密操作直接在负载均衡上完成 网络经典网络经典网络支持私网互通。 专有网络VPC 专有网络VPC下面的ECS，无论是否绑定EIP，在设置安全组的时候，规则同时控制公网和内网流量。 专有网络VPC的高级功能包括：网络ACL、流日志和自定义路由表。 创建专有网络后，系统会在路由表中自动添加一条以 100.64.00.0/10 为目标网段的路有条目，用于VPC内的云产品通信。 每个交换机的第一个和最后三个IP地址为系统保留地址。以192.168.1.0/24为例，192.168.1.0、 192.168.1.253、192.168.1.254和192.168.1.255这些地址是系统保留地址。 一个交换机只能绑定一张路由表，交换机的路由策略由其关联的路由表管理。 VPC网络下，可以通过弹性公网IP、NAT网关、公网负载均衡和ECS固定公网IP等方式链接公网。 实现VPC中ECS服务器切换/迁移到同VPC下的其他交换机，包括以下几步， 打开云服务器管理控制台； 找到对应的需要切换/迁移的云服务器； 修改云服务器的私网地址; 选择您所需的交换机，同时指定新交换机下的IP; 使用隧道封装技术对云服务器的IP报文进行封装，所以云服务器的数据链路层（二层MAC地址）信息不会进入物理网络，实现了不同云服务器间二层网络隔离，因此也实现了不同VPC间二层网络隔离。；VPC内的云服务器使用安全组防火墙进行三层网络访问控制。 对VPC下的云服务器实例的私网IP进行修改，可以通过 需要修改的地址超出交换机地址范围时，变更ECS实例所在的交换机并变更ECS的私网IP ； 需要修改的地址没有超出ECS所在交换机地址范围时，直接变更ECS实例的私网IP 方式变更。 安全组和交换机绑定； 子网和交换机绑定； 路由表和交换机绑定。 vpc目前只有两种状态： pending（配置中）， available(可用) 。 创建VPC后，可以通过创建交换器为专有网络划分一个或多个子网，设置交换机的IPv4网段时，用户需要了解交换机的网段限制： 交换机的网段必须是VPC网段的子集； 交换机的第一个和最后三个IP为系统保留地址； 如果交换机有和其他专有网络的交换机或本地数据中心通信的需求，确保交换机的网段和要通信的网段不冲突； 交换机的网段不能与所属VPC路由表中路由的目标网段范围相同或大于该范围； 交换机网段不能与所属VPC下其他交换机的网段重叠。 创建VPC后，需要完成创建交换机的操作之后才能够在专有网络内创建其他的云产品实例。 默认交换机和非默认交换机的异同点： 默认交换机只能创建在默认专有网络中 默认交换机与非默认交换机支持相同操作方式，且规格限制一致 默认交换机由阿里云为您创建，您自行创建的均为非默认交换机 路由条目包括系统路由、自定义路由、动态路由。NAT网关 DNAT:Destination Network Address Translation 目的网络地址转换。内部服务对外发布。 SNAT:Source Network Address Translation 源网络地址转换，其作用是将ip数据包的源地址转换成另外一个地址。主要用于内部共享IP访问外部 一个公网NAT 网关支持绑定20个EIP。EIP 经典网络公网IP转换为EIP后，1. 采用按使用流量计费方式，2.公网带宽值和原ECS实例保持一致，3.不能挂在到经典网络类型ECS实例上， 4.不同于专有网络VPC类型ECS实例，经典网络ECS实例具有公网网卡，如果经典网络公网IP转换为EIP，则无法保留公网网卡和实例的MAC地址。 通过对绑定EIP的ECS实例分配一块弹性网卡，并将EIP绑定到弹性网卡，可以实现公网出口IP保持一致。 云数据库不能绑定EIP 1个弹性公网IP只能绑定到1个ECS上 ； 弹性公网IP只能绑定到相同地域（可以是不同可用区）的VPC类型的云服务器ECS实例上；不能绑定到经典网络上。 网络ACL 特性 网络ACL规则仅过滤绑定的交换机中ECS实例的流量（包括SLB实例转发给ECS实例的流量）。 网络ACL的规则是无状态的，即设置入方向规则的允许请求后，需要同时设置相应的出方向规则，否则可能会导致请求无法响应。无状态指的是每个网络ACL规则都是独立的，不考虑其他规则或之前的通信历史记录。换句话说，ACL规则仅根据每个流量数据包的源地址、目的地址、协议类型和端口号等信息来决定是否允许该数据包通过，而不关心之前是否有类似的数据包通过了。因此，即使两个数据包从同一个源地址发送，但如果它们的目的地址、协议类型或端口号不同，则它们可能会被ACL处理为完全不同的情况。-- by chatGPT 网络ACL无任何规则时，会拒绝所有出入方向的访问。 网络ACL与交换机绑定，不过滤同一交换机内的ECS实例间的流量 CDN CDN加速，在添加域名页面，源站信息可以选择：IP、源站域名、OSS域名、函数计算域名。 CND节点在全国多地均有分布，所以与其他云产品间流量通过公网传输，不管是否同一地域都会产生回源流量费。 开启CND加速时，源站域名不能与加速域名相同，否则会造成循环解析，无法回源。 CND可以开启状态码过期时间，避免无效文件的频繁回源导致源站压力过大。 安全阿里云的云盾补丁管理服务里发布的补丁来自阿里云自己研发。 安全管家 大型互联网用户，现有网上资产比较多；但是不清楚自己那些资产联网了，无法及时对这些资产进行清理和关停，存在较大的安全隐患，可以使用阿里云的 安全管家。 操作审计 操作审计（ActionTrail）帮助您监控并记录阿里云账号的活动，包括通过阿里云控制台、OpenAPI、开发者工具对云上产品和服务的访问和使用行为。您可以将这些行为事件下载或保存到日志服务SLS或对象存储OSS，然后进行行为分析、安全分析、资源变更行为追踪和行为合规性审计等操作。 云盾 数据风控 目前只支持在Web中插入指定的JS代码的方式来采集信息。 数据风控 产品具体可以防范 垃圾注册、营销作弊、盗卡支付 云盾面可以免费开通的功能包括：DDoS基础防护、阿里绿网、安骑士 云盾加密服务采用 符合 安骑士 安骑士是云盾提供的保护服务器的产品，功能包括：1.木马文件检查，异地登陆报警，操作系统暴力破解，高危漏洞修复，网页防篡改 事态感知（可以预测即将发生的安全事件） 态势感知具备异常登录检测、网站后门查杀、进程异常行为、敏感文件算改、异常网络连接、Linux软件漏洞、Windows系统漏洞、Web-CMS漏洞、应急漏洞、Web漏洞扫描、主机基线、云产品基线、资产指纹、AK和账号密码泄露、数据大屏、日志检索、全量日志分析等功能。 事态感知支持通过手机短信和电子邮件的方式向用户发送报警信息。 事态感知是云上资产的诊断服务，不是针对ECS的托管服务。 安全组 安全组授权主要限制IP,端口，和安全组。无法指定MAC地址。 安全组是一种虚拟防火墙，具备状态监测和包过滤功能。而不是ECS实例之间的隔离。 实例加入安全组的规则如下： 实例至少加入一个安全组，可以同时加入多个安全组。每个ECS最多可以加入5个安全组。 实例上挂载的弹性网卡中，辅助网卡可以加入和实例不同的安全组。 实例不支持同时加入普通安全组和企业安全组。 安全组只能设置内网访问规则（无论是否绑定EIP） 创建安全组的顺序是：创建安全组-添加安全组规则-ECS加入安全组-管理安全组-管理安全组规则 安全组优先级的取值范围是1~100，数字越小优先级越高。 云安全 云安全中心提供 添加白名单 的功能，开启后，云安全中心不在对对应风险项进行告警。 防火墙（收费） Web应用防火墙的功能包括： 防Web应用系统的密码破解、敏感信息泄露。 云防火墙是用户上云后的首个安全组件，支持全网流量识别、统一策略管控、入侵检测、日志等核心功能。 应对CC攻击 实人认证 实人认证服务是指依托活体检测、人脸比对等生物识别技术、证件OCR识别技术等，进行的自然人真实身份的校验服务，目前仅支持拥有中华人民共和国第二代居民身份证的人士进行认证。 云监控 自定义监控是指：用户可以对自己关心的业务进行监控，将采集到的监控数据上报至云监控，由云监控进行数据的处理，并根据结果进行报警； 自定义的监控项的数量是没有数量限制的，且阿里云提供接口可以给开发者上报数据。 云监控可以通过手机短信、钉钉机器人、电子邮件、电话进行报警通知。 云监控提供站点监控、 云服务监控、自定义监控、报警服务 功能， 负载均衡开启云监控后，可以使用的报警方式有短信、邮件、旺旺。不支持电话报警。 攻击 跨站脚本攻击(XSS: Cross-site scripting) 可被用于进行窃取隐私，钓鱼欺骗，偷取密码，传播恶意代码等攻击行为，主要发生在用户浏览器。 CC(Challenge Collapsar)是指攻击者借助代理服务器生成指向受害主机的合法请求，实现DDOS和伪装。CC主要是用来攻击页面的。预防CC注册是Web应用防火墙的功能。 内容安全内容安全提供： 内容审核（机审）、人工审核、图片OCR识别、人脸识别功能 日志服务如果想利用日志服务提供的分析功能，则必须将日志存储在Standard Logstore中，且在配置索引时打开对应字段的开启统计开关。 告警管理 告警策略是告警管理系统的配置实体，当告警管理系统接收到告警事务（含恢复通知）时，自动按照对应的告警策略，进行告警降噪等操作。 告警合并告警合并是将具有相同特征的告警进行分组，便于进行统一通知或后续处理，在一定程度上避免告警风暴。 告警静默告警静默用于禁止一段时间内的告警通知。例如在特定时间内维护测试环境，会产生大量的相关告警，此时可通过静默功能避免接收到大量的告警通知。 告警策略继承告警策略之间可以有继承关系，最终的作用效果相当于父策略和子策略合并后的作用效果。更多信息，请参见告警策略继承机制。 告警抑制告警抑制用于阻止有某告警引发的其他告警通知。 数据隔离不同告警策略之间的数据是完全隔离的。 域名注册局安全锁注册局安全锁是目前最高等级的域名安全保护措施，由注册局在根服务器层面操作，禁止域名被恶意转移、篡改及删除。开启注册局安全锁后，域名将被置为以下三种锁定状态： 注册局设置禁止删除（serverDeleteProhibited） 注册局设置禁止转移（serverTransferProhibited） 注册局设置禁止更新（serverUp dateProhibited） 其他 针对常见的HTTP字段（如IP、URL、Referer、UA、参数等）进行条件组合，配置支持业务场景定制化的防护策略，可用于盗链防护、网站管理后台保护等 IaaS、PaaS、SaaS IaaS（Infrastructure as a Service，基础架构即服务）： 基于云的服务，按需付费，用于存储，网络和虚拟化等服务。 PaaS（Platform as a Service，平台即服务）： Internet上可用的硬件和软件工具。 SaaS（Software as a Service，软件即服务）： 可通过互联网通过第三方获得的软件。 DaaS（Data as a Service，数据即服务）：最新产生的，稳定的数据提供 虚拟化技术包括：全虚拟化、半虚拟化、硬件辅助虚拟化。 SDN（Software Defined Network) :软件定义网络 阿里云作为云计算服务公提供商，提供的安全保障服务 APP客户端经常因本地DNS篡改导致连不上服务器，可以使用HTTPDNS， HTTPDNS是一款面向移动开发者推出的一款域名解析产品，具有域名防劫持、精准调度等特性。 SMC 服务器迁移中心SMC（Server Migration Center）是阿里云自主研发的迁移平台。使用SMC，可将您的单台或多台迁移源迁移至阿里云。迁移源（或源服务器）概指您的待迁移IDC服务器、虚拟机、其他云平台的云主机或其他类型的服务器。迁移过程无需停机，不影响原服务器业务。 一个完整的云计算环境由云端、计算机网络和终端三部门组成（也就是常说的云、管、端）。云端就是指计算设备，负责完成软件的计算；终端是指我们用来完成输入/输入的设备；计算机网络负责将云端和终端连接起来，完成信息传输（将终端的输入指令传输到云端，将云端的执行结果反馈给终端）。 CAP原则又称为CAP理论，主要思想是在任何一个分布式系统中都无法同时满足CAP。 C（Consistency）：表示一致性，所有的节点同一时间看到的是相同的数据。 A（Avaliablity）：表示可用性，不管是否成功，确保一个请求都能接收到响应。 P（Partion Tolerance）：分区容错性，系统任意分区后，在网络故障时，仍能操作。 SQL注入可能造成的危害包括： 网页被篡改，数据被篡改，核心数据被窃取，数据库所在服务器被攻击编程傀儡主机 可以公网接入的云服务是 EIP、 NAT网关、 固定公网IP 堡垒机服务功能特性：1.满足《萨班斯法案》、金融监管、《等级保护》的审计合规要求；2. 支持SSH、Windows远程桌面、SFTP等常见运维协议。 某用户在创建ECS实例(分配了公网P地址)后的六小时内，想要更换该ECS实例的公网IP地址，但在控制台操作界面找不到更换公网P选项，这种问题的原因可能是: ECS实例为VPC网络，停止实例时选择停机不收费。 停机不收费的ECS则为按量付费的ECS, 按量付费的ECS在购买创建后的6小时内，并不能更换公网IP，这是由于更换公网IP需要停止实例，而按量付费的ECS实例在停止后会自动释放公网IP地址，所以按量付费ECS实例停止后，并没有“更换公网IP”的选项。 PPPoE是拨号上网相关协议，已经过时了。不可能在API协议中使用。 在创建ECS实例后，如果需要更换ECS实例的操作系统，用户可以通过更换系统盘来更换操作系统。用户进行更换系统盘操作后，下列选项中不一定发生的是：原系统盘自动快照被自动删除。（如果未开启自动快照随云盘释放，则到期后自动释放。） ECS服务器使用注意事项说法正确的是：1.云服务器的内核和操作系统版本不要随意升级；2. Liux的云服务器数据盘未做分区和格式化，使用前请挂载数据盘；3. 网卡的MAC地址不要修改。 Google在2003年到2006年公布了关于GFS、MapReduce和BigTable三篇大数据处理系统技术论文。 云数据库RDS不能绑定EIP。 网络通信五元组： 源IP地址，源端口，目的IP地址，目的端口，传输层协议。 BGP机房特点： BGP机房的服务器并不是都具备双机或多机冗错功能。参考BGP机房的优点：(1)服务器只需要设置一个P地址，最佳访问路由是由网络上的骨干路由器根据路由跳数与其它技术指标来确定的，不会占用服务器的任何系统资源。服务器的上行路由与下行路由都能选择最优的路径，所以能真正实现高速的单P高速访问。(2)由于BGP协议本身具有冗余备份、消除环路的特点，所以当DC服务商有多条BGP互联线路时可以实现略由的相互备份，在一条线路出现故障时路由会自动切换到其它线路。(3)使用BGP协议还可以使网络具有很强的扩展性可以将DC网络与其他运营商互联，轻松实现单P多线路，故到所有互联运营商的用户访问都很快。这个是双IP双线无法比拟的。 PolarDB 是阿里巴巴自研的新一代云原生数据库，在计算存储分离架构下，利用了软硬件结合的优势，为用户提供具备极致弹性、高性能、海量存储、安全可靠的数据库服务。100%兼容MySQL和PostgreSQL生态，高度兼容Oracle语法。 主机边界防火墙可以对ECS实例间的入流量和出流量进行访问控制，限制ECS实例间的未授权访问。 TTL：TTL是Time-To-Live的缩写，指生存时间。而域名解析中提到的TTL值是指全国各地的localdns服务器中缓存解析结果的时间周期。 ACK集群需要自行创建Master节点和Worker节点，仅支持专有网络VPC。 ACK支持5种公网访问方式：节点访问，负载均衡，NGinx Ingress ,域名，DNAT网关 加密服务支持的加密机类型包括：通用服务器密码机GVSM、金融数据密码机EVSM和签名验证服务器SVSM。 使用弹性伸缩管理业务所用的ECS实例时，伸缩组是基本的管理单元。伸缩组用于管理具有相同应用场景的ECS实例，并支持关联多个负载均衡实例和RDS实例。 OCSP Stapling功能，可实现由CDN预先缓存在线证书验证结果并下发给客户端，无需浏览器直接向CA站点查询证书状态，从而减少用户验证时间。本文主要介绍OCSP Stapling功能的概念、使用前提和配置方法。 MapReduce:是一种编程模型，用于大规模数据集的并行运算，可以非常好地和云计算相结合以处理海量数据计算。MapReduce的设计目标包括：1.易于编程； 2.易于扩展；3.高容错性 如果您的应用访问量很高，您可以通过配置监听规则将流量分发到不同的云服务器ECS（Elastic Compute Service）实例上。此外，您可以使用 功能将同一客户端的请求转发到同一台后端ECS，提高访问效率。 SSL证书类型：DV（域名型）、OV（企业型）、EV（企业增强型）。 云数据库RDS提供了以下数据存储类型：ESSD云盘、本地SSD盘、SSD云盘、通用云盘。 云数据库Redis支持标准架构、集群架构和读写分离架构，可满足不同的业务场景对业务读写能力、数据量和性能的要求。 HaVip,]]></content>
      <categories>
        <category>云计算</category>
        <category>考试资料</category>
      </categories>
      <tags>
        <tag>阿里云</tag>
        <tag>ACP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ACP考试知识点]]></title>
    <url>%2F05.%E5%A4%87%E8%80%83-01.%E4%BA%91%E8%AE%A1%E7%AE%97ACP%2F%E4%BA%91%E8%AE%A1%E7%AE%97-ACP%E8%80%83%E8%AF%95%E7%9F%A5%E8%AF%86%E7%82%B9-P3%2F</url>
    <content type="text"><![CDATA[单选 网络ACL无任何规则时，会拒绝所有出入方向的访问。网络ACL中数字越小优先级越高。 每台LVS通过组播报文将会话同步到集群内的其他LVS机器上，实现回话同步。 DDOS高防如果配置了多个源站IP地址，则会通过IP Hash的方式转发访问流量至源站。 A地址记录可以将域名映射到IPv4地址进行解析。 限制ECS实例间的未授权访问，可以使用云防火墙的主机边界防火墙功能。 云防火墙的失陷感知功能可以在检测到可疑事件时发出告警或采取主动反应措施。 全球加速标准型可以实现应用的四层和七层网络加速。 全球加速基础型可以实现应用的三层网络加速。 ACK支持多种访问方式，包括节点访问、负载均衡、Nginx Ingress，但是不支持EIP访问。 云数据库RDS不能绑定EIP。 本地应用服务制作的镜像不支持上传到阿里云作为镜像提供服务。 DNAT:Destination Network Address Translation 目的网络地址转换。内部服务对外发布。 SNAT:Source Network Address Translation 源网络地址转换，其作用是将ip数据包的源地址转换成另外一个地址。主要用于内部共享IP访问外部 大型互联网用户，现有网上资产比较多；但是不清楚自己那些资产联网了，无法及时对这些资产进行清理和关停，存在较大的安全隐患，可以使用阿里云的 安全管家。 PostObject不能上传大文件（超过5G）。 一个公网NAT 网关支持绑定20个EIP。 本地快照与云盘存储同一个存储集群中；可用区容灾；仅支持ESSD云盘；快速，秒级。 普通快照存储在同地域的OSS中；地域容灾；支持所有云盘；时间较长，分钟级别； 通过配置扩展域名，可以将多个域名绑定到同一个负载均衡服务地址上。（扩展域名的主要作用是在一个VIP下提供对多个域名的支持，而监听转发的主要作用是将请求转发到不同的后端服务器。两者之间的区别在于它们的作用对象不同：扩展域名是作用于域名，而监听转发是作用于请求。） OSS支持 对象标签 对object进行分类。 多选 用户购买ECS后，无需购买单独的服务就可以抵御的安全攻击包括：DDos攻击、异地登录、暴力密码破解、Web漏洞发现。 vpc目前只有两种状态： pending（配置中）， available(可用)。 专有网络内的ECS使用安全组防火墙进行三层网络访问控制 负载均衡的服务监听包含了： 监听端口、负载均衡策略、健康检查配置。 一个ECS仅能绑定一个公网IP（EIP或PublicIP） TCP监听支持 TCP 和 HTTP 两种方式进行健康检查； OSS不支持对终端类型（PC端或手机端）进行监控 ECS自带监控服务，除了提供vCPU使用率外，还提供网络流量和磁盘I/O的监控。 SSD云盘独有优势：SSD云盘的随机和顺序读写的IOPS都比普通云盘高； 访问时延可以达到0.5~2ms。 MapReduce的设计目标包括：易于编程、易于扩展、高容错性。 OSS支持 使用KMS托管密钥进行加密（可以指定密钥） 和 使用OSS完全托管加密 两种服务器端加密方式。 阿里云SSL证书服务支持 ： ECC、RSA、SM2 三种加密算法。 云数据库Redis实例类型提供 企业版和社区版 两种版本可按需选择。 云数据库Redis支持三种不同的部署架构，即标准架构、集群架构、读写分离架构。 云数据库RDS默认提供 按备份集恢复 和 按指定时间点恢复 两种恢复功能。&lt;&lt;&lt;&lt;&lt;&lt;&lt; Updated upstream 域名注册完，支持 域名转移和域名交易。 开启注册局安全锁后，域名将被设置为三种状态：禁止更新、禁止转移、禁止删除。 ALB支持灰度发布，可以选择基于Cookie、 HTTP标头、 不同服务器组 实现灰度发布。 日志服务SLS是云原生观测与分析平台，为Log（日志）、Metric（时序数据）、Trace（链路数据）等数据提供大规模、低成本、实时的平台化服务。 购买DDos高防后，可以通过 域名接入和端口接入的方式接入DDos高防。 WAF支持使用透明接入和CNAME接入两种方式。 对VPC下的云服务器实例的私网IP进行修改，可以通过 需要修改的地址超出交换机地址范围时，变更ECS实例所在的交换机并变更ECS的私网IP ； 需要修改的地址没有超出ECS所在交换机地址范围时，直接变更ECS实例的私网IP 方式变更。 RDS支持多种数据库引擎，包括SQL Server、PostgreSQL、MYSQL、MariaDB ，并提供了容灾、备份、回复、监控、迁移等方面的解决方案。 按量付费的ECS转为包年包月实例，需要满足条件：不是停售的实例规格、不是抢占式实例、没有未支付的订单、未设置自动释放时间、处于运行中或已停止状态。 内容安全提供： 内容审核（机审）、人工审核、图片OCR识别、人脸识别、增量检查、全量检查功能 报警任务支持：CPU、内存、系统平均负载、内外网出和入流量，TCP总连接数和已建立连接数。 伸缩组支特关联负载均衡实例和RDS实例，但是暂时不能关联Redis实例。如果您有业务数据存诸在Redis实例上，手动配置ECS实例加入或移出Rdis实例白名单，操作效率较低。您可以通过生命周期挂钩和OOS模板将ECS实例自动加入和移出Redis实例白名单。 弹性伸缩使用伸缩配置创建ECS实例，这种方式不够灵活，需要进行更多的自定义设置时，可以使用生命周期挂钩 或 实例自定义数据。 创建伸缩组后，负载均衡实例、RDS数据库实例 不可以修改。 堡垒机服务功能特性：1.满足《萨班斯法案》、金融监管、《等级保护》的审计合规要求；2. 支持SSH、Windows远程桌面、SFTP等常见运维协议。 加密服务支持的加密机类型包括：通用服务器密码机GVSM、金融数据密码机EVSM和签名验证服务器SVSM、通用密码机FIPS。 SSL证书类型：DV（域名型）、OV（企业型）、EV（企业增强型）。 CND由 缓存系统、调度系统、链路质量系统和支撑系统 这四大系统组成。 CDN加速指定源站，在添加域名时，可以选择 函数计算域名、 源站域名、OSS域名。 部署集是控制ECS实例分布的策略，使您能在创建ECS实例的时候就设计容灾能力和可用性，部署集注意事项：部署集内不能创建抢占式实例、部署集不支持创建专有宿主机、部署集之间不支持相互合并。 通过Web前端技术向OSS上传数据的方式有：使用表单上传、使用OSS Brdser.js SDK上传、 通过支付宝小程序上传。 阿里云数据存储平台中，有三类角色，分别是Client（客户端）、Master（主服务器）和Chunk Server（数据块服务器）。 容器服务ACK支持企业级容器化应用的全生命周期管理，包含三种产品形态：专有版kubernetes、Serverless kubernetes、托管版 kubernetes。 HaVip支持绑定一个弹性公网IP（EIP）、多个(最多10个）ECS实例或多个ECS实例的主网卡或辅助网卡，以实现同可用区、多服务器高可用架构下的IP漂移，确保对外提供服务的私网IP始终不变。HaVip只支持单播。 伸缩配置重点 负载均衡的WAF主要用于保护网站的安全性，可以针对 跨站脚本、SQL注入、CSRF、本地文件包含、WebShell 进行安全防护。 CDN节点的系统关键组件有哪些： LVS(四层负载均衡)、Tengine（七层负载均衡）、Swift（做参考链接）、http(缓存)。 Web应用防火墙可以为不同网站防护模块设置白名单，使满足条件的请求忽略指定模块的检测，支持设置的模块白名单包括：Bot管理白名单、Web入侵防护白名单、数据安全白名单。 ECS监控指标分基础监控和操作系统级别指标监控，操作系统级别监控指标包含内存使用率、平均负载、磁盘IO读/写、磁盘使用率、TCP连接数、进程总数。 NLB支持UDP、TCPSSL、TCP协议，提供四层负载均衡能力。 ALB健康检查支持 GRPC、TCP、HTTP协议 ； 在使用云服务器ECS的过程中，有时难免会出现数据被误删除的情况，在阿里云上恢复被误删除的数据有多种方式，比如： 通过ECS管理控制台回滚已创建的快照 通过ECS管理控制台恢复该磁盘对应的数据块副本 使用开源工具Extundelete恢复误删的数据 Web应用防火墙WAF和内容分发网络CDN结合使用，为开启内容加速的域名提供Web攻击防御。该管理员将CDN作为入口层用来实现内容加速，流量经过作为中间层的WAF进行防护再到源站。下列关于这个架构的配置，正确的是： 在CDN中配置源站信息时，源站域名不能与加速域名相同，否则会造成循环解析，无法回源 需要在WAF中生成要防护的源站域名的专用CNAME地址 一个分布式系统不可能同时满足 一致性、可用性和分区容错性 这三个基本需求。]]></content>
      <categories>
        <category>云计算</category>
        <category>考试资料</category>
      </categories>
      <tags>
        <tag>阿里云</tag>
        <tag>ACP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Highly aneuploid non-small cell lung cancer shows enhanced responsiveness to concurrent radiation and immune checkpoint blockade]]></title>
    <url>%2F01.%E7%9F%A5%E8%AF%86-05.Paper%2FPMID36443406%2F</url>
    <content type="text"></content>
      <categories>
        <category>NGS</category>
        <category>文献</category>
      </categories>
      <tags>
        <tag>变异检测</tag>
        <tag>氧化损伤</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[单细胞检测概述]]></title>
    <url>%2F01.%E7%9F%A5%E8%AF%86-01.NGS%2F2022-11-22.%E5%8D%95%E7%BB%86%E8%83%9E%E6%A3%80%E6%B5%8B%2F</url>
    <content type="text"><![CDATA[参考资料单细胞测序的技术概述单细胞转录组测序技术全流程解析]]></content>
      <categories>
        <category>NGS</category>
        <category>单细胞</category>
      </categories>
      <tags>
        <tag>NGS</tag>
        <tag>单细胞</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[免疫组库]]></title>
    <url>%2F01.%E7%9F%A5%E8%AF%86-01.NGS%2F2022-11-03.%E5%85%8D%E7%96%AB%E7%BB%84%E5%BA%93%2F</url>
    <content type="text"><![CDATA[概念什么是免疫组库从出生开始，人体每天都会受到大量外来物质的侵扰，但并不会每天得病。这是因为人类在演化的过程中，拥有了一套强大的免疫系统来对抗外来物质的入侵，这些物质称为抗原(Antigen)。免疫系统包括物理屏障、非特异性免疫、特异性免疫系统。T/B 细胞是特异性免疫系统的两大细胞群。免疫组库(immune repertoire, IR)是指在某一时间, 某个个体的循环系统内所有功能多样性T细胞和B细胞的总和。 B细胞/T细胞简介而B细胞和T细胞则会记忆人类出生后所接触过的抗原，并同时产生记忆B细胞和记忆T细胞，记忆B细胞和记忆T细胞使人体在受到相同抗原的第二次刺激后可以更迅速的发出反应，这便形成了免疫记忆。 T细胞在外观上与B细胞非常相似，在一个普通的显微镜下可能无法将它们区分开来。但B细胞与T细胞也有区别： B细胞在骨髓中成熟，而T细胞在胸腺中成熟； B细胞制造的抗体可以识别所有有机分子，但T细胞专门识别蛋白抗原； B细胞可以以抗体的形式分泌受体，但是T细胞受体保持在细胞表面上； 最重要的区别是，B细胞能够自己识别抗原，而T细胞不能直接识别抗原表位，只有在被抗原呈递细胞（APC）“正确呈递”的情况下才会识别出抗原，即只能特异性识别APC或靶细胞表面提成的抗原肽-MHC分子复合物（pMHC）。 T细胞与B细胞表面上都存在受体分子，分别称为T细胞受体（TCRs:T-cell receptors）和B细胞受体（BCRs:B-cell receptors）。机体的每一种抗体都与一种特殊的抗原结合在一起，因此，为了使抗体能够与许多不同的抗原结合，需要分泌许多不同的抗体分子。 根据免疫学家粗略估计，大概需要1亿种抗体才可以满足人体需要。拿BCRs来说，由于抗体的每个抗原都是由一个重链和一个轻链组成的，所以我们可以将大约10000个不同的重链和10000个不同的轻链组合在一起，从而得到我们需要的1亿个不同的抗体。 BCR/TCR简介▲图2 BCR的结构以其在B细胞上的位置 ▲图3 TCR的结构以其在T细胞上的位置 BCR是表达于B细胞表面的免疫球蛋白（Immunoglobulin, Ig），B细胞通过BCR识别抗原，接受抗原刺激，启动体液免疫应答。BCR的重链（Heavy Chain, IgH）由编码可变区（Virable Domain, V区）的V基因片段（variable gene segment）、D基因片段（diversity gene segment）和J基因片段（joining gene segment）以及编码恒定区（Constant Domain, C区）的C基因片段组成。BCR的轻链（Light Chain，IgL）V区只有V基因片段和J基因片段。 TCR每条肽链的细胞膜外区各含1个V区和1个C区，V区中含有3个互补决定区（CDR1、CDR2、CDR3），是TCR识别pMHC的功能区。TCR跨膜区带有正电，可与CD3形成盐桥，形成TCR-CD3复合物，TCR识别抗原所产生的活化信号由CD3转导至T细胞内。CD3具有5种肽链，即γ、δ、ε、ζ 和 η 链，均为跨膜蛋白。 免疫组库的克隆型通过在免疫系统的免疫球蛋白和T细胞受体产生的早期阶段可变的（V）、多样性（D）和连接（J）基因片段经由体细胞重组的重排来确定。受体的重排V（D）J部分（即V区）非常重要，因为它负责表位识别，当V（D）J被翻译成氨基酸序列时， V-区域可以被细分成由前导系列、框架（FR）1、互补性决定区1（CDR1）、FR2、CDR2、FR3、CDR3、FR4和C-结构域组成的几个部分。其中CDR3特别重要，因为既往研究表明该区域与抗原特异性有关，故研究CDR3序列的特异性与多样性对于识别TCR/BCR具有重要意义。 因此有些“免疫组库”意指实际在一个或多个个体的淋巴细胞中检测到的一组不同的CDR3序列。 抗体多样性1977年的诺贝尔奖得主利根川解开了机体抗体多样性这个谜团：机体通过模型化设计（modular design）达到抗体的多样性。 （重链胚系基因经过重排先形成D-J连接，然后发生V-DJ连接，编码功能性V区基因） 即在每一个B细胞中，含有重链基因序列的染色体上有四种不同的DNA基因片段分别称为V、D、J、C。在人类中，大约有40个不同的V段，大约25个不同的D段，6个不同的J段等，为了合成一个成熟的重链基因，每一个B细胞都选择（随机的）一种基因片段并将它们像这样组合在一起，含有抗体轻链遗传信息的染色体也通过选择基因片段并将它们粘在一起来组装，多种不同的基因片段进行混合和匹配增加了产生抗体的多样性。 但这还不够，为了使抗体的多样化继续增强，当基因片段连接在一起时，额外的DNA碱基会被增添或删除，这种重组多样性和连接多样性使得只需要少量的遗传信息就能创造出超出想象的抗体多样性。 与BCRs一样，TCRs也是由一种混合与搭配模块化设计策略来制造的。因此，TCRs与BCRs一样具有多样性。可简单理解为：T细胞和B细胞基因座上大量的V、D、J基因片段在T细胞受体（TCR）和B细胞受体（BCR）的形成过程中会产生各种多样性重组，V-D-J基因的重组赋予了每一种T、B细胞独特的TCR、BCR，从而使得每一个TCR和BCR序列能有效的成为一个T、B细胞克隆的唯一生物标志物。 免疫组库的应用预后评估Keane（2017）等通过高通量免疫组库测序技术（HST-IR）对92例弥漫性大B细胞淋巴瘤患者肿瘤组织中TCR库进行测序分析，结果显示 ：与正常淋巴结组织相比，病变淋巴结组织中TCR多样性明显受限，TCR的多样性程度与肿瘤组织中免疫检查点相关蛋白（如PD-L1和PD-L2等）的表达呈正相关，这可能为选择免疫治疗方案提供证据支持 ；且在患者肿瘤组织微环境中，TCR库表现为高克隆低多样性特征，这些高频克隆型的扩增与患者不良预后相关。 血液MRD免疫组库的测定一般TCRs/BCRs的免疫组库测序有2种策略： （1）基于细胞DNA，利用TCR/BCR基因的重排规律，在TCR（α/β/γ/δ 链）和BCR（重链/轻链）的可变区基因设置上游引物，在连接区J基因和恒定区C基因设置下游引物，通过PCR扩增出目的链的PCR产物，但该方法存在扩增不均一的不足，影响结果的准确性； （2）基于细胞RNA，采用5’RACE技术，基于引入的接头序列进行第二次无偏好PCR扩增，克服不均一性的问题，但RNA样品处理较DNA样品复杂，重复性也不如DNA样品好。 得益于液体活检技术和高通量技术的快速发展和广泛应用，以血浆cfDNA/ctDNA为起始样本，针对特异性免疫亚克隆扩增的免疫组库测序技术使得“液体活检”更好的应用于疾病的风险评估与预测。 其主要流程一般是先提取血浆的cfDNA，对BCR H链（重链）进行全长多重PCR扩增和TCR β链的CDR3区域进行多重PCR扩增，然后将2种扩增产物混合后再进行一次PCR扩增，获得文库后进行上机测序，再对测序数据进行免疫组库精准信息分析。 参考资料海普洛斯 (1) Tang J. et al. Trends in the global immuno-oncology landscape [J] .Nat Rev Drug Discov. 2018 Oct 26. (2) Tang J. et al. Comprehensive analysis of the clinical immuno-oncology landscape [J] .Ann Oncol. 2018 Jan 1;29(1):84-91. (3) Ye B. et al. High-throughput sequencing of the immune repertoire in oncology: Applications for clinical diagnosis, monitoring, and immunotherapies [J] . Cancer Lett. 2018 Mar 1;416:42-56. (4) Keane C. et al. The T-cell Receptor Repertoire Influences the Tumor Microenvironment and Is Associated with Survival in Aggressive B-cell Lymphoma [J] . Clin Cancer Res, 2017, 23(7):1820-1828. (5) 曹雪涛.医学免疫学. [M] .第9版.北京：人民卫生出版社.2018-07 (6) Lauren Sompayrac. How the Immune System Works [M] .FIFth EdItIon.2016. (7) the production mechanism and show utility in noninvasive prenatal testing. Proc Natl Acad Sci USA. May 29;115(22):E5106-E5114.（2018） (8) Horlbeck MA, et al. Nucleosomes impede Cas9 access to DNA in vivo and in vitro. eLife 5:e12677. (2016)]]></content>
      <categories>
        <category>NGS</category>
        <category>知识沉淀</category>
      </categories>
      <tags>
        <tag>免疫组库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[cromwell 配置文件-设置任务投递模式]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-04.pipeline%2Fpipeline-frameworks-cromwell-02.%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6-%2F</url>
    <content type="text"><![CDATA[上一篇文章，我们介绍了 run 模式，并且针对常见的分析场景 SGE &amp; docker 介绍了基本配置文件的撰写方式，及指定配置文件进行任务投递的方法。1java -Dconfig.file=/path/to/yourOverrides.conf cromwell.jar run pipeline.wdl -i input.json 但是其实除了后端任务调度平台的相关配置，我们也提到还有其他的一些内容比如：资源限制，调用缓存，文件系统等，我们都可以通过配置文件进行管理，今天我们来介绍一下其他的常用配置项。 文件格式我们在之前的示例中提供了json格式的文件，但是实际上，配置文件还有其他方式提供给Cromwell， 比如下列几种格式其实是等价的： JSON-like stanza: 类似 JSON 的节：12345include required(classpath("application"))webservice &#123; port = 8000 interface = 0.0.0.0&#125; Dot-separated values: 点分隔值：123include required(classpath(&quot;application&quot;))webservice.port = 8000webservice.interface = 0.0.0.0 通过命令行进行配置1java -Dwebservice.port=8080 Dwebservice.interface=0.0.0.0 cromwell.jar ... systemI/O 管理我们的Pipelines的中会有一些任务会访问后端请求，为了保证后端服务的稳定不至于过载爆掉，我们有时候需要对API每秒可以进行的查询数施加配额。12345system.io &#123; number-of-requests = 100000 # 设置访问请求数目 per = 100 seconds # 设置访问等待时间 number-of-attempts = 5 # 设置失败重试次数&#125; 工作流除了 API 的请求，我们也许要对我们的执行任务数进行管理。 Cromwell对一次运行的工作流数量有一个可配置的上限。您可以通过设置以下设置来调整默认 5000 限制 Cromwell也会以固定的时间（单位s)建个进行轮询，查看是否有新的工作流出现，默认时间时 20s： Cromwell默认只会同时接受1个工作流的提交，如果需要同时启动多个工作流也可以进行设置。 123system.max-concurrent-workflows = 5000 #并行任务数system.new-workflow-poll-rate = 20system.max-workflow-launch-count = 1 资源管理我们进行任务投递的集群，往往是共享资源，有些时候任务紧急，也许我们可以一个人占用所有的分析资源，但是大多数情况下这可能会被公用资源的其他同学暴打一顿。所以大多时间，我们需要限制任务对资源的使用，从而让其他同学的任务也可以运行，所以在v35版本， Cromwell 提供了配置参数，让我们可以对流程调用的资源进行管理和限制。我们可以通过 3个参数对资源调度情况进行管理。 Hog Group对提交的不同工作流（独立的WDL）进行分组，从而帮助我们对多个不同的工作流进行分组，从而实现同一组内所有工作流程的资源管理，对整组的资源进行限制。 调用的所有子任务（task）都会继承主流程的hog-group 不同的主流程，可以划分到同一个hog-group。每个pipeline都对应一个唯一的工作组，每个工作组可包含多个pipeline。配置文件中指定的是关键字（wdl中赋值的信息），而不是具体分组。 Hog FactorHog Factor 是大于或等于 1 的整数。表示所有资源会被多少个 group 平均分配。它代表了以下两者之间的权衡： Cromwell可以使用所有的资源完成工作，设置 Hog Factor = 1 时。 为其他分组预留资源，只使用其中一部分（1/Hog Factor）资源进行分析。 Hog Factor=2，则每个分组可以使用一半的资源进行分析。 Hog LimitHog Limit 是 Cromwell内部计算的，每个分组所能运行的最大任务数，这部分不需要我们进行手动的配置。 示例123456system &#123; hog-safety &#123; hog-factor = 1 workflow-option = &quot;hogGroup&quot; &#125;&#125; 在对应的WDL中，应该提供对应的group 关键字，如果未提供对应关键字，会使用pipeline的名称作为默认值。123&#123; &quot;hogGroup&quot;: &quot;hogGroupA&quot;&#125; 为了整体保持资源在所有工作组之间的均匀分配，Cromwell在执行任务时，不同工作组间轮转的任务运行，组内按先先后顺序进行任务运行。 123456789101112system &#123; abort &#123; scan-frequency: 30 seconds cache &#123; enabled: true concurrency: 1 ttl: 20 minutes size: 100000 &#125; &#125; dns-cache-ttl: 3 minutes&#125; 中止任务对于支持中止作业的后端，可以将 Cromwell 配置为在收到 Control-C（也称为 SIGINT）时自动尝试中止所有调用。所有当前正在运行的调用也会将其状态设置为 Aborted。123system &#123; abort-jobs-on-terminate=true&#125; databaseCromwell 跟踪工作流的执行，并将任务调用的输出存储在 SQL 数据库中。默认情况下，Cromwell 使用内存数据库，该数据库仅在 JVM 期间存在。这提供了一种无需设置 MySQL 即可在本地运行工作流的快速方法，尽管它也使工作流执行有些短暂。Cromwell 也支持外部 MySQL 数据库。若要将 Cromwell 配置为指向 MySQL 数据库，请首先创建空数据库。使用Singularity 创建Mysql数据库可以参考文档Mysql服务启动-基于Singularity 在下面的示例中，数据库名称为 cromwell 。然后，编辑配置文件 database 节，如下所示：123456789101112database &#123; profile = &quot;slick.jdbc.MySQLProfile$&quot; db &#123; driver = &quot;com.mysql.cj.jdbc.Driver&quot; url = &quot;jdbc:mysql://host/cromwell?rewriteBatchedStatements=true&quot; user = &quot;user&quot; password = &quot;pass&quot; connectionTimeout = 5000 &#125; # For batch inserts the number of inserts to send to the DB at a time insert-batch-size = 2000&#125; 这部分数据库除了mysql还会存在其他各类数据库的支持，因为我个人在使用过程中并没有涉及到各种不同类型的数据库需求，如果有复杂数据库配置需求，可以参考官方文档 调用缓存调整个参数，重新分析一遍，在研究性项目和流程开发阶段不可避免的工作，这时候我们会对数据进行多次任务分析，每次都从头进行分析会带来很大的开销，所以Cromwell提供了缓存功能。Cromwell可以在以前运行的作业缓存中搜索具有完全相同的命令和完全相同的输入的作业。如果在缓存中找到以前运行的作业，Cromwell 将使用上一个作业的结果，而不是重新运行它。利用缓存默认时关闭的，为了更好的调用缓存利用以前运行的作业，最好将 Cromwell 配置为指向 MySQL 数据库，而不是默认的内存中数据库。 1234call-caching &#123; enabled = true # 引用或复制以前运行的作业的结果 invalidate-bad-cache-results = true # 使任何缓存结果无效&#125; 本地文件系统选项在 Config（共享文件系统）后端运行作业时，Cromwell 在后端的 config 部分提供了一些附加选项，可以进行文件系统相关的配置。比如使用缓存或者容器时，文件的需要调用到新的目录进行使用，这时候，我们时采用硬链接、软连接还是拷贝整个文件。以及我们在类似call-cache阶段校验文件一致性的方案。1234567891011121314151617config &#123; filesystems &#123; local &#123; localization: [ &quot;hard-link&quot;, &quot;soft-link&quot;, &quot;copy&quot; ] caching &#123; duplication-strategy: [ &quot;hard-link&quot;, &quot;soft-link&quot;, &quot;copy&quot; ] hashing-strategy: &quot;md5&quot; fingerprint-size: 10485760 check-sibling-md5: false &#125; &#125; &#125;&#125; 工作流日志目录WDL 执行过程中会产生大量的工作日志，为了更好的访问和管理这些日志文件，我们可以通过 workflow-options.workflow-log-dir=&quot;target_log_dir&quot; 设置日志保存目录, 默认情况下，工作流完成时，Cromwell 会擦除每个工作流日志，以减少磁盘使用量。我们可以通过设置 workflow-options.workflow-log-temporary = false 来保存对应的日志。 shell 配置默认在执行工作流程时，使用的是 /bin/bash ，但是有些时候，可能我们需要使用其他的 shell 调用,那我们可以通过设置系统层面的配置键或后端对应的配置键实现，12345# system-wide setting, all backends get this-Dsystem.job-shell=/bin/sh# override for just the Local backend-Dbackend.providers.Local.config.job-shell=/bin/sh 流程的心跳检测Cromwell ID每个 Cromwell 实例都被分配了一个 cromwell_id，默认生成的格式是 cromid-&lt;7_digit_random_hex&gt;。如果需要，我们可以自定义标识符可以替换字符串的“cromid”部分。123system &#123; cromwell_id = &quot;main&quot; # 自定义cromwell_id前缀&#125; 生成id的格式会变更为 main-&lt;7_digit_random_hex&gt;，当然有些情况下，我们不需要生成随机后缀，也可以通过对应的配置实现。123system &#123; cromwell_id_random_suffix = false # 禁用cromwell_id_random_suffix&#125; Heartbeat TTL为了确保工作流正常执行，Cromwell 会以固定的时间间隔检测任务的状态，并将任务状态更新到对应的存储数据库中。以便我们确认任务状态，同时也方便任务以外中断后，通过其他的方式进行恢复。这部分一半采用默认配置即可，所以在此进行简单说明就不展开介绍了。 123456system.workflow-heartbeats &#123; ttl = 10 minutes # 过期时间，超时后会启动另一个Cromwel示例恢复任务。 heartbeat-interval = 2 minutes # 检测的间隔，必须比TTL时间短 write-failure-shutdown-duration = 5 minutes # Cromwell在超过一定时间无法写入心跳时自动关机&#125; 通过这些配置，已经可以帮助我们应对大多数场景了， referenceCromwell ReadtheDoc SGECromwell Example Backends[https://blog.csdn.net/tanzuozhev/article/details/120629170]]]></content>
      <categories>
        <category>pipeline</category>
        <category>framework</category>
        <category>编程拾慧</category>
        <category>任务调度</category>
        <category>python</category>
      </categories>
      <tags>
        <tag>流程开发</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[cromwell介绍 - 后端配置]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-04.pipeline%2Fpipeline-frameworks-cromwell-01.%E9%85%8D%E7%BD%AE-backend%2F</url>
    <content type="text"><![CDATA[Cromwell 是 Broad Institute 开发的工作流管理系统。通过 Cromwell 可以将 WDL 描述的 workflow 转化为批量计算的作业（Job）运行。是目前非常流行的一个用于生物信息学的开源工作流程管理系统。软件本身有专门的文档管理站点 Cromwell Readthedocs , 如果有问题也可以在社区仓库查看相关项目或提交代码 Cromwell Github。 截至目前 Cromwell Download 已经发布了多个版本，在我们要使用先下载对应的执行软件，不同版本可能存在细节差异，本教程撰写的时候使用的是 v84 版本，所以如果最新版本万一存在不兼容的情况，可能需要切换到对应的版本，当然也欢迎大家反馈留言，基于最新版本提供相关修正说明。 12wget https://github.com/broadinstitute/cromwell/releases/download/84/cromwell-84.jarwget https://github.com/broadinstitute/cromwell/releases/download/84/womtool-84.jar 其实简单的 cromwell 应用，在我们下载对应执行软件后，就可以直接使用命令投递基于wdl 语法撰写的工作流来进行任务投递。wdl的撰写可以参考我们之前的的文章，在此不再进行赘述。1java -jar cromwell.jar run pipeline.wdl -i input.json 这是最简单的一种最简单的任务投递模式，我们不需要进行任何额外的配置。这里的简单指的是： 任务运行在当前计算节点（不涉及跨节点的任务调度，也不涉及集群调度） 任务运行在当前环境，不涉及镜像，没用使用容器化、虚拟化。 任务数据在当前路径，不涉及云存储。 如果我们的任务本身不涉及这些需求的化，那么直接使用 run 模式进行任务投递已经足够满足我们的分析需求了。 进阶配置之前我们介绍了最简单的 run 模式，但是 run 默认是在当前环境、当前节点进行任务投递的，这最多只能用于我们前期的流程开发阶段，肯定满足不了后期业务落地应用的需求。因此我们需要对我们的投递过程进行升级，让我们的任务可以跨节点、支持容器化环境的运行。这部分升级也不复杂，我们只需要在投递阶段添加指定对应的配置文件 -Dconfig.file=/path/to/yourOverrides.conf 就可以实现我们的需求。 1java -Dconfig.file=/path/to/yourOverrides.conf cromwell.jar run pipeline.wdl -i input.json 通过 -Dconfig.file=bcs.conf 指定cromwell执行过程的配置文件。 可以让cromwell在 SGE、AWS、Docker 等多种平台和容器环境下执行 wdl 开发的流程 。官方针对多种场景都提供了配置文件模板，可以通过简单的配置实现基于各平台的调度。 在这里，我们以最常见的 SGE &amp; docker 作为示例，介绍一下配置文件。 后台配置 backend进行任务投递时， backend 是我们使用最多也是最基础的个性化配置，通过backend配置，我们就可以实现在不同平台进行WDL任务的投递，比如常用的 Local、SGE 和 LSF。这里主要进行任务调度系统的配置。 官方针对不同的集群/云作业管理系统提供了相关的配置文件，但是本质都是将对应调度系统的执行命令嵌入其中。作业调度系统的配置文件并非完整的配置文件，必须添加到 cromwell.examples.conf 的 backend 部分才可以正常工作。本章我们主要介绍常用的 SGE+docker 情况下的backend进行介绍，除了 backend 外还涉及其他配置信息，比如并行任务限制，调用缓存，容器分析目录等配置，并不会直接影响我们进行任务的投递和获取结果，将在后续再进行介绍。 SGE投递基于SGE进行任务提交的命令在配置键 backend.providers.SGE.config.submit 下指定。它使用与 WDL 中的命令相同的语法，并提供一些预定义的变量： 变量名称 变量说明 script 要运行的作业的 shell 脚本。它包含来自 WDL 代码的 command 部分的用户命令。 cwd 脚本应运行的路径。 out 标准输出的路径。 err stderr 的路径。 job_name 作业的唯一名称。 12345678910111213141516171819backend &#123; providers &#123; SGE &#123; config &#123; submit = """ qsub \ -terse \ -V \ -b n \ -N $&#123;job_name&#125; \ -wd $&#123;cwd&#125; \ -o $&#123;out&#125;.qsub \ -e $&#123;err&#125;.qsub \ $&#123;script&#125; """ &#125; &#125; &#125;&#125; 稍微啰嗦一嘴，结合这个示例，我们再回头看 backend.providers.SGE.config.submit 可能大家会更好地​理解配置文件中的配置键，理解这个含义，有助于大家理解后续配置键的含义。 使用Singularity容器123456789101112131415161718192021222324252627282930313233343536373839404142backend &#123;default = SGE_singularity providers &#123; SGE_singularity &#123; actor-factory = &quot;cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory&quot; config &#123; # 指定cromwell后端驱动器 # 任务投递的具体参数配置 concurrent-job-limit = 500 # 从wdl的runtime中获取的参数类型和值，若赋值相当于wdl未定义时的默认值 runtime-attributes = &quot;&quot;&quot; #指定投递时，支持从wdl中获取的参数 Int cpu = 1 Int memory String image String sge_project = &quot;P23Z15000N0116&quot; String sge_query = &quot;b2c_rd1.q&quot; &quot;&quot;&quot; # runtime配置中有容器时，投递任务的模式 submit = &quot;&quot;&quot; IMAGE=/jdfstj6/B2C_RD/liubo4/product/cWES/images/$&#123;image&#125;.sif qsub \ -terse \ -V \ -b y \ -N $&#123;job_name&#125; \ -wd $&#123;cwd&#125; \ -o $&#123;cwd&#125;/stdout \ -e $&#123;cwd&#125;/stderr \ -l vf=$&#123;memory&#125;,num_proc=$&#123;cpu&#125; \ -q $&#123;sge_query&#125; \ -P $&#123;sge_project&#125; \ /share/app/singularity/3.8.1/bin/singularity exec --writable-tmpfs --nohttps --containall --bind /ifstj2,/jdfstj4,/jdfstj6 $IMAGE bash $&#123;script&#125; &quot;&quot;&quot; kill = &quot;qdel $&#123;job_id&#125;&quot; check-alive = &quot;qstat -j $&#123;job_id&#125;&quot; job-id-regex = &quot;(\\d+)&quot; &#125; &#125; &#125;&#125; 使用docker容器如果后端支持docker，则可以指定可选配置键 backend.providers.&lt;backend&gt;.config.submit-docker 和 backend.providers.&lt;backend&gt;.config.kill-docker 。当 WDL 包含 docker 运行时属性时，该命令将提供几个额外的附加变量： 变量名称 变量说明 docker docker 镜像名称。 docker_cid 容器 ID 文件应写入的主机路径。 docker_cwd cwd 对应在 docker 容器中安装的路径。 docker_script docker 容器内 script 的路径。 docker_out docker 容器内 out 的路径。 docker_err docker 容器内 err 的路径。 12345678910111213141516171819202122backend &#123; providers &#123; SGE &#123; config &#123; # ... other configuration submit-docker = &quot;&quot;&quot; qsub \ -terse \ -V \ -b n \ -N $&#123;job_name&#125; \ -wd $&#123;cwd&#125; \ -o $&#123;out&#125;.qsub \ -e $&#123;err&#125;.qsub \ -l docker,docker_images=&quot;$&#123;docker&#125;&quot; -xdv $&#123;cwd&#125;:$&#123;docker_cwd&#125; $&#123;script&#125; &quot;&quot;&quot; &#125; &#125; &#125;&#125; 从wdl的传参当然有时候，可能并不是所有的参数都是固定的，而是需要通过 wdl 的输入文件或者配置参数获取，这个时候我们可以配置键 backend.providers.&lt;backend&gt;.config.runtime-attributes 中指定它们。它使用与在 WDL 中的任务中指定运行时属性相同的语法。除两个特殊的运行时属性配置：cpu 和 memory_\&lt;unit> cpu: 我们在runtime attribute 中配置为Int，但是在cromwell使用时，会校验必须为正整数（而不能是0或负整数） memory_\: 指定运行时属性配置Int memory_\ 或 Float memory_\ 时，会在wdl中取 memory（而不是memory_\) 对应的值，并完成单位的转换。除了 cpu 和 memory 两个常用的属性参数，其他参数也可以在 runtime-attributes 中进行指定定义。比如我们可以指定获取任务所需的镜像 docker（需要和wdl中定义的属性名称和类型对应）。同时属性参数可以在配置中指定默认值（String sge_queue = &quot;b2c_rd1.q&quot;)。也可以通过?指定非必须参数（（String? sge_queue = &quot;b2c_rd1.q&quot;) 123456789101112131415backend &#123; providers &#123; SGE &#123; config &#123; # ... other configuration runtime-attributes = """ Float memory_mb String docker String? sge_queue = "b2c_rd1.q" #指定默认值 """ &#125; &#125; &#125;&#125; 一个完整的示例12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061include required(classpath(&quot;application&quot;))call-caching &#123; enabled = true&#125;backend &#123;default = SGE providers &#123; SGE &#123; # 指定cromwell后端驱动器 actor-factory = &quot;cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory&quot; # 任务投递的具体参数配置 config &#123; concurrent-job-limit = 5 # 从wdl的runtime中获取的参数类型和值，若赋值相当于wdl未定义时的默认值 runtime-attributes = &quot;&quot;&quot; Int cpu = 1 Float? memory String? docker String? sge_project = &quot;P23Z15000N0116&quot; &quot;&quot;&quot; # # runtime配置中没有容器时，投递任务的模式 submit = &quot;&quot;&quot; qsub \ -terse \ -V \ -b y \ -N $&#123;job_name&#125; \ -wd $&#123;cwd&#125; \ -o $&#123;out&#125;.qsub \ -e $&#123;err&#125;.qsub \ $&#123;&quot;-l mem_free=&quot; + memory &#125; \ $&#123;&quot;-l p=&quot; + cpu &#125; \ $&#123;&quot;-q &quot; + sge_queue&#125; \ $&#123;&quot;-P &quot; + sge_project&#125; \ /usr/bin/env bash $&#123;script&#125; &quot;&quot;&quot; # runtime配置中有容器时，投递任务的模式 submit-docker = &quot;&quot;&quot; qsub \ -terse \ -V \ -b n \ -N $&#123;job_name&#125; \ -wd $&#123;cwd&#125; \ -o $&#123;out&#125;.qsub \ -e $&#123;err&#125;.qsub \ -l docker,docker_images=&quot;$&#123;docker&#125;&quot; -xdv $&#123;cwd&#125;:$&#123;docker_cwd&#125; $&#123;script&#125; &quot;&quot;&quot; kill = &quot;qdel $&#123;job_id&#125;&quot; check-alive = &quot;qstat -j $&#123;job_id&#125;&quot; job-id-regex = &quot;(\\d+)&quot; &#125; &#125; &#125;&#125; 细心的同学可能会发现，我们在配置文件中，多了一个call-caching，这是一个cromwell的特性配置，因为这部分和任务调度无关，我们后续再单独介绍一下，其他的相关配置。 referenceCromwell ReadtheDoc SGECromwell Example Backends[https://blog.csdn.net/tanzuozhev/article/details/120629170] 配置文件模板 Cloud Providers AWS: Amazon Web Services (documentation) TES: is a backend that submits jobs to a server with protocol defined by GA4GH (documentation) PAPIv2: Google Pipelines API backend (version 2!) (documentation) Containers Docker: an example backend that only runs workflows with docker in every command Singularity: run Singularity containers locally (documentation) Singularity+Slurm: An example using Singularity with SLURM (documentation) TESK is the same, but intended for Kubernetes. See the TES docs at the bottom. udocker: to interact with udocker locally documentation udocker+Slurm: to interact with udocker on SLURM (documentation) Workflow Managers HtCondor: a workload manager at UW-Madison (documentation) LSF: the Platform Load Sharing Facility backend (documentation) SGE: a backend for Sungrid Engine (documentation) slurm: SLURM workload manager (documentation)]]></content>
      <categories>
        <category>pipeline</category>
        <category>framework</category>
        <category>编程拾慧</category>
        <category>任务调度</category>
        <category>python</category>
      </categories>
      <tags>
        <tag>流程开发</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[docker - 常用命令 - 查看日志]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-04.pipeline%2Fpipeline-container-docker-%E6%9F%A5%E7%9C%8B%E6%97%A5%E5%BF%97%2F</url>
    <content type="text"><![CDATA[官方资料docker logs 使用1$ docker logs [OPTIONS] CONTAINER Options Name, shorthand Default Description –details Show extra details provided to logs –follow , -f Follow log output –since Show logs since timestamp (e.g. 2013-01-02T13:23:37Z) or relative (e.g. 42m for 42 minutes) –tail , -n all Number of lines to show from the end of the logs –timestamps , -t Show timestamps –until Show logs before a timestamp (e.g. 2013-01-02T13:23:37Z) or relative (e.g. 42m for 42 minutes) 示例 docker logs 命令，可以跟踪容器的日志并且输出日志的时间 1docker logs -f -t busybox 写入指定容器在某时间段的日志 1docker logs --since=&quot;2022-09-19T01:00:00&quot; --until &quot;2022-09-21T09:40:00&quot; busybox &amp;&gt;file.txt 将最近1分钟的日志写到file.txt文件 1docker logs --since 50m busybox &amp;&gt;file.txt 查看实时日志，仅仅显示最新的100条日志数据 1docker logs -f -t --tail 100 busybox &amp;&gt;file.txt]]></content>
      <categories>
        <category>pipeline</category>
        <category>镜像</category>
        <category>container</category>
      </categories>
      <tags>
        <tag>流程开发</tag>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[概念解析 线程进程核心cpu]]></title>
    <url>%2F01.%E7%9F%A5%E8%AF%86-08.IT%E5%90%91%E7%9F%A5%E8%AF%86%E6%9D%82%E8%AE%B0%2F%E6%A6%82%E5%BF%B5%E8%A7%A3%E6%9E%90%20-%20cpu%E7%BA%BF%E7%A8%8B%E8%BF%9B%E7%A8%8B%E8%A7%A3%E6%9E%90%2F</url>
    <content type="text"><![CDATA[概念解析物理 cpu 数（physical cpu）指主板上实际插入的 cpu 硬件个数（socket）。（但是这一概念经常被泛泛的说成是 cpu 数，这很容易导致与 core 数，processor 数等概念混淆，所以此处强调是物理 cpu 数）。 由于在主板上引入多个 cpu 插槽需要更复杂的硬件支持（连接不同插槽的 cpu 到内存和其他资源），通常只会在服务器上才这样做。在家用电脑中，一般主板上只会有一个 cpu 插槽。 核心（core）一开始，每个物理 cpu 上只有一个核心（a single core），对操作系统而言，也就是同一时刻只能运行一个进程/线程。 为了提高性能，cpu 厂商开始在单个物理 cpu 上增加核心（实实在在的硬件存在），也就出现了双核心 cpu（dual-core cpu）以及多核心 cpu（multiple cores），这样一个双核心 cpu 就是同一时刻能够运行两个进程/线程的。 多线程技术（simultaneous multithreading）和 超线程技术（hyper–threading/HT）本质一样，是为了提高单个 core 同一时刻能够执行的多线程数的技术（充分利用单个 core 的计算能力，尽量让其“一刻也不得闲”）。simultaneous multithreading 缩写是 SMT，AMD 和其他 cpu 厂商的称呼。 hyper–threading 是 Intel 的称呼，可以认为 hyper–threading 是 SMT 的一种具体技术实现。 在类似技术下，产生了如下等价术语： 虚拟 core： virtual core 逻辑 processer： logical processor 线程：thread 所以可以这样说：某款采用 SMT 技术的 4 核心 AMD cpu 提供了 8 线程同时执行的能力；某款采用 HT 技术的 2 核心 Intel cpu 提供了 4 线程同时执行的能力。 系统信息查询Linux1234567891011121314151617181920212223# 查看物理 cpu 数：cat /proc/cpuinfo| grep "physical id"| sort| uniq| wc -l# 查看每个物理 cpu 中 核心数(core 数)：cat /proc/cpuinfo | grep "cpu cores" | uniq#查看总的逻辑 cpu 数（processor 数）：cat /proc/cpuinfo| grep "processor"| wc -l#查看 cpu 型号：cat /proc/cpuinfo | grep name | cut -f2 -d: | uniq -c# 判断 cpu 是否 64 位：# 检查 cpuinfo 中的 flags 区段，看是否有 lm （long mode） 标识# lscpu 命令可以同时看到上述信息。比如：lscpu---$ CPU(s): 24$ On-line CPU(s) list: 0-23$ Thread(s) per core: 2$ Core(s) per socket: 6$ Socket(s): 2]]></content>
      <categories>
        <category>知识沉淀</category>
        <category>IT</category>
      </categories>
      <tags>
        <tag>线程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[微信公众号接入chatgpt]]></title>
    <url>%2F01.%E7%9F%A5%E8%AF%86-08.IT%E5%90%91%E7%9F%A5%E8%AF%86%E6%9D%82%E8%AE%B0%2F%E5%BE%AE%E4%BF%A1%E5%85%AC%E4%BC%97%E5%8F%B7%E6%8E%A5%E5%85%A5chatgpt%2F</url>
    <content type="text"></content>
      <categories>
        <category>知识沉淀</category>
        <category>IT</category>
      </categories>
      <tags>
        <tag>线程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[箱线图的详细解析]]></title>
    <url>%2F01.%E7%9F%A5%E8%AF%86-07.%E5%B8%B8%E8%A7%81%E7%BB%9F%E8%AE%A1%E5%90%91%E5%9B%BE%E8%A1%A8%E8%AF%A6%E8%A7%A3%2Fboxplot%2F</url>
    <content type="text"><![CDATA[箱线图介绍箱形图是一张图表，能很好地指示数据中的值如何分布，尽管与直方图或密度图相比，箱线图似乎是原始的，但它们具有占用较少空间的优势，这在比较许多组或数据集之间的分布时非常有用。上图箱线图，箱线图是一个能够通过5个数字来描述数据的分布的标准方式，这5个数字包括：最小值，第一分位，中位数，第三分位数，最大值，箱线图能够明确的展示离群点的信息，同时能够让我们了解数据是否对称，数据如何分组、数据的峰度； 箱线图是一种基于五位数摘要（“最小”，第一四分位数（Q1），中位数，第三四分位数（Q3）和“最大”）显示数据分布的标准化方法。 中位数（Q2 / 50th百分位数）：数据集的中间值； 第一个四分位数（Q1 / 25百分位数）：最小数（不是“最小值”）和数据集的中位数之间的中间数； 第三四分位数（Q3 / 75th Percentile）：数据集的中位数和最大值之间的中间值（不是“最大值”）； 四分位间距（IQR）：第25至第75个百分点的距离； 晶须（蓝色显示） 离群值（显示为绿色圆圈） 最大值：Q3 + 1.5 * IQR 范围内的最大值（剔除异常值） 最小值：Q1 -1.5 * IQR 范围内的最小值（剔除异常值） 特殊注意点如果会发现，箱线图中IQR相同，但是我们做箱线图时，图中的两条虚线(上下边缘到上下四分位的线)长度经常不同。上下边缘的定义应该是上下四分位数加1.5IQR范围内数据的最大最小值，也就是说IQR并不是个定值，而是个取值范围。 1234567891011121314举例[1，20，20，20，30，30，35]，n=7，可以容易看出其各项参数：中位数：20下四分位数：位置【（n+1）/4=2】，值=20上四分位数：位置【（n+1）*3/4=6】，值30IQR：30-20=10其中上下四分位数的求值，如果数组为奇数，可以通过举例的方式直接求得，但是如果数组为偶数，假设下四分位求得的位置为2.25，并且第二位为2，第三位为3，那么下四分位的值为：2*（2.25-2）+3*（3-2.25）=2*（0.25）+3*（0.75）=2.75再来看上下边缘，根据公式（Q1-1.5IQR或Q3+1.5IQR），我们可以得到相应值。下边缘=10上边缘=40但是我们观察数据可以发现，最大值为35，所以上边缘只能取到35。最小值为1，下边缘可以取到10。所以作图上虚线长度为35，尔下虚线长度为10。另外我们可以确定1是异常值。 箱线图和概率密度图关系对于一个近似正态数据，整天分布图和箱线图的概率分布关系如下图 ， 参考信息wikipedia统计学知识门户 - 箱线图如何深刻理解箱线图（boxplot）]]></content>
      <categories>
        <category>知识沉淀</category>
        <category>可视化</category>
      </categories>
      <tags>
        <tag>箱线图</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux-环境配置-底层库]]></title>
    <url>%2F03.%E5%BA%94%E7%94%A8-02.Linux%E5%92%8Cshell%2FLinux-%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE-%E5%BA%95%E5%B1%82%E5%BA%93%2F</url>
    <content type="text"><![CDATA[GLIBC安装软件时，提示 /lib64/libm.so.6: versionGLIBC_2.23’ not found `,查看并进行高版本GLIBC安装。 12345678910111213141516171819202122232425# 查看确实缺少对应的版本(base)[liubo4@tj-login-24-4 Special_Site]$strings /lib64/libc.so.6 |grep GLIBC_GLIBC_2.8GLIBC_2.9GLIBC_2.10GLIBC_2.11GLIBC_2.12GLIBC_2.13GLIBC_2.14GLIBC_2.15GLIBC_2.16GLIBC_2.17GLIBC_PRIVATE# 下载wget https://ftp.gnu.org/gnu/glibc/glibc-2.23.tar.gz# 解压准备tar xf glibc-2.23.tar.gzcd glibc-2.23/mkdir glibc-buildcd glibc-build (一定要在新建的目录中操作)../configure --prefix=/usr #如果没有root权限需要更改安装目录makemake install]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Shell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[查看系统信息命令]]></title>
    <url>%2F03.%E5%BA%94%E7%94%A8-02.Linux%E5%92%8Cshell%2FLinux-%E5%B8%B8%E7%94%A8shell%E5%91%BD%E4%BB%A4-%E6%9F%A5%E7%9C%8B%E7%B3%BB%E7%BB%9F%E4%BF%A1%E6%81%AF%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[命令简介 命令 简介 uname -a 查看内核/操作系统/CPU信息 head -n 1 /etc/issue 查看操作系统版本 cat /proc/cpuinfo 查看CPU信息 hostname 查看计算机名 lspci -tv 列出所有PCI设备 lsusb -tv 列出所有USB设备 lsmod 列出加载的内核模块 env 查看环境变量资源 free -m 查看内存使用量和交换区使用量 df -h 查看各分区使用情况 du -sh &lt;目录名&gt; 查看指定目录的大小 grep MemTotal /proc/meminfo 查看内存总量 grep MemFree /proc/meminfo 查看空闲内存量 uptime 查看系统运行时间、用户数、负载 cat /proc/loadavg 查看系统负载磁盘和分区 mount \ column -t 查看挂接的分区状态 fdisk -l 查看所有分区 swapon -s 查看所有交换分区 hdparm -i /dev/hda 查看磁盘参数(仅适用于IDE设备) dmesg \ grep IDE 查看启动时IDE设备检测状况网络 ifconfig 查看所有网络接口的属性 iptables -L 查看防火墙设置 route -n 查看路由表 netstat -lntp 查看所有监听端口 netstat -antp 查看所有已经建立的连接 netstat -s 查看网络统计信息进程 ps -ef 查看所有进程 top 实时显示进程状态用户 w 查看活动用户 id &lt;用户名&gt; 查看指定用户信息 last 查看用户登录日志 cut -d: -f1 /etc/passwd 查看系统所有用户 cut -d: -f1 /etc/group 查看系统所有组 crontab -l 查看当前用户的计划任务服务 chkconfig –list 列出所有系统服务 chkconfig –list \ grep on 列出所有启动的系统服务程序 rpm -qa 查看所有安装的软件包 du -sh 查看指定目录的大小 grep MemTotal /proc/meminfo 查看内存总量 grep MemFree /proc/meminfo 查看空闲内存量 uptime 查看系统运行时间、用户数、负载 cat /proc/loadavg 查看系统负载磁盘和分区 mount \ column -t 查看挂接的分区状态 fdisk -l 查看所有分区 swapon -s 查看所有交换分区 hdparm -i /dev/hda 查看磁盘参数(仅适用于IDE设备) dmesg \ grep IDE 查看启动时IDE设备检测状况网络 ifconfig 查看所有网络接口的属性 iptables -L 查看防火墙设置 route -n 查看路由表 netstat -lntp 查看所有监听端口 netstat -antp 查看所有已经建立的连接 netstat -s 查看网络统计信息进程 ps -ef 查看所有进程 top 实时显示进程状态用户 w 查看活动用户 id 查看指定用户信息 last 查看用户登录日志 cut -d\ -f1 /etc/passwd 查看系统所有用户 cut -d\ -f1 /etc/group 查看系统所有组 crontab -l 查看当前用户的计划任务服务 chkconfig –list 列出所有系统服务 chkconfig –list \ grep on 列出所有启动的系统服务程序 rpm -qa 查看所有安装的软件包 cat /proc/cpuinfo 查看CPU相关参数的linux系统命令 cat /proc/partitions 查看linux硬盘和分区信息的系统信息命令 cat /proc/meminfo 查看linux系统内存信息的linux系统命令 cat /proc/version 查看版本，类似uname -r cat /proc/ioports 查看设备io端口 cat /proc/interrupts 查看中断 cat /proc/pci 查看pci设备的信息 cat /proc/swaps 查看所有swap分区的信息 查看linux系统版本信息（Oracle Linux、Centos Linux、Redhat Linux、Debian、Ubuntu） 查看Linux系统版本的命令（3种方法） cat /etc/issue，此命令也适用于所有的Linux发行版。 123 [root@S-CentOS home]# cat /etc/issue CentOS release 6.5 (Final) Kernel \r on an \m lsb_release -a，即可列出所有版本信息： 123 [root@S-CentOS ~]# lsb_release -a LSB Version: :base-4.0-amd64:base-4.0-noarch:core-4.0-amd64:core-4.0-noarch:graphics-4.0-amd64:graphics-4.0-noarch:printing-4.0-amd64:printing-4.0-noarch Distributor ID: CentOS cat /etc/redhat-release，这种方法只适合Redhat系的Linux： 12 [root@S-CentOS home]# cat /etc/redhat-release CentOS release 6.5 (Final) | 系统 | 发行版本 | – | 内核版本、位数 || —— | ————– | ———————– | —————– || RedHat | cat /etc/issue | cat /etc/redhat-release | lsb_release -a || CentOS | cat /etc/issue | cat /etc/centos-release | cat /proc/version || Debian | cat /etc/issue | cat /etc/debian_version | cat /proc/version || Ubuntu | cat /etc/issue | cat /etc/lsb_release | cat /proc/version || Oracle | cat /etc/issue | cat /etc/oracle-release | lsb_release -a | 查看Linux内核版本命令（两种方法） cat /proc/version uname -a 查看CPU信息 12cat /proc/cpuinfo # 查看CPU信息cat /proc/cpuinfo |grep "model name" &amp;&amp; cat /proc/cpuinfo |grep "physical id" # CPU大小 ps 说明：Linux下可以在/proc/cpuinfo中看到每个cpu的详细信息。但是对于双核的cpu，在cpuinfo中会看到两个cpu。常常会让人误以为是两个单核的cpu。其实应该通过Physical Processor ID来区分单核和双核。而Physical Processor ID可以从cpuinfo或者dmesg中找到. flags 如果有 ht 说明支持超线程技术 判断物理CPU的个数可以查看physical id 的值，相同则为 内存大小 1cat /proc/meminfo |grep MemTotal 查看内存使用量和交换区使用量 1free -m 硬盘大小 1fdisk -l |grep Disk 查看各分区使用情况 1df -h 列出所有PCI设备 1lspci -tv 列出所有USB设备 1lsusb -tv 查看环境变量资源 1env]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Shell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CNV检测-CNVKit]]></title>
    <url>%2F03.%E5%BA%94%E7%94%A8-01.software_in_NGS%2FSoftware-CNV%E6%A3%80%E6%B5%8B-CNVKit%2F</url>
    <content type="text"><![CDATA[Publish原文链接: CNVkit Genome-Wide Copy Number Detection and Visualization from Targeted DNA Sequencing-annotatedcnvkit.readthedocs原理介绍PPTCNVKit若干算法问题详解 检测原理在上述文献、PPT中已经有相对详细的介绍，此处不再进行展开。针对目前一些可视化的需求，补充记录一些关于CNVKit结果展示的相关功能和使用方法。 可视化CNVKit提供了比较详细的可视化模块三种可视化的主体绘图命令如下:123cnvkit.py scatter -hcnvkit.py diagram -hcnvkit.py heatmap -h scatter说明1cnvkit.py scatter Sample.cnr -s Sample.cns 扩展参数1234567python cnvkit.py scatter Sample.cnr -s Sample.cns-c chr7 # 指定参考基因组，可以进一步细化指定具体范围chr5:100-50000000-g BRAF,MET # 指定展示的基因，多个基因使用逗号（","）分隔-v Sample.vcf #``` ### 示例 /jdfstj1/B2C_COM_P1/PipeAdmin/02.software/Conda/bin/python /jdfstj1/B2C_COM_P1/PipeAdmin/04.Pipeline/01.AIO.v2.01/bin/cnvkit.py scatter ../Analyze/cnv/pancancer689__DX1790_huangrenhua_20S8320498R_20B8320498__Cancer.markdup.cnr -s ../Analyze/cnv/pancancer689__DX1790_huangrenhua_20S8320498R_20B8320498__Cancer.markdup.cns -c chr3:150000000-220000000 -g EIF4A2,TIPARP,TP631234567![image](Software-CNV检测-CNVKit/demo_test1.jpg)## diagram ```shellpython cnvkit.py diagram Sample.cnr -s Sample.cns-t 2 # 对判定为CNV的阈值进行调整（仅达到阈值的基因会进行单独展示） 结果格式说明*.cnv1chromosome, start, end, gene, log2, depth and weight 异常处理记录Qt载入失败 具体报错示例如下： 12345678Reinstalling the application may fix this problem.Showing 918 probes and 1 selected genes in region chr15This application failed to start because it could not find or load the Qt platform plugin &quot;xcb&quot;in &quot;&quot;.Available platform plugins are: eglfs, minimal, minimalegl, offscreen, xcb.Reinstalling the application may fix this problem. 处理解决方案]]></content>
      <categories>
        <category>NGS</category>
        <category>software</category>
        <category>编程拾慧</category>
        <category>python</category>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>CNV</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Cancer statistics, 2022]]></title>
    <url>%2F01.%E7%9F%A5%E8%AF%86-05.Paper%2FCancer_statistics-2022%2F</url>
    <content type="text"><![CDATA[参考文献Cancer incidence and mortality in China, 2016Latest global cancer data: Cancer burden rises to 19.3 million new cases and 10.0 million cancer deaths in 2020 Cancer statistics, 2022]]></content>
      <categories>
        <category>NGS</category>
        <category>文献</category>
      </categories>
      <tags>
        <tag>cancer population</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[NGS数据去重/压缩]]></title>
    <url>%2F03.%E5%BA%94%E7%94%A8-01.software_in_NGS%2Fsoftware-%E8%BD%AF%E4%BB%B6%E7%BB%BC%E8%BF%B0-%E5%8E%8B%E7%BC%A9%E5%8E%BB%E9%87%8D%2F</url>
    <content type="text"><![CDATA[参考文献[1]. UMIErrorCorrect and UMIAnalyzer: Software for Consensus Read Generation, Error Correction,and Visualization Using Unique Molecular Identifiers # 全文下载 [2]. UMIc: A Preprocessing Method for UMI Deduplication and Reads Correction # 全文下载 [3]. UMI-linked consensus sequencing enables phylogenetic analysis of directed evolution # 全文下载 [4]. Gencore: an efficient tool to generate consensus reads for error suppressing and duplicate removing of NGS data # 全文下载 [5]. UMI-Gen: A UMI-based read simulator for variant calling evaluation in paired-end sequencing NGS libraries # 全文下载 [6]. UMI-tools: modeling sequencing errors in Unique Molecular Identifiers to improve quantification accuracy # 全文下载 [7]. Alignment-free clustering of UMI tagged DNA molecules # 全文下载 [8]. UMI-VarCal: a new UMI-based variant caller that efficiently improves low-frequency variant detection in paired-end sequencing NGS libraries # 全文下载 [9]. High efficiency error suppression for accurate detection of low-frequency variants # 全文下载]]></content>
      <categories>
        <category>NGS</category>
        <category>software</category>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>去重</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[seqkit]]></title>
    <url>%2F03.%E5%BA%94%E7%94%A8-01.software_in_NGS%2Fsoftware-%E5%BA%8F%E5%88%97%E5%A4%84%E7%90%86-seqkit%2F</url>
    <content type="text"></content>
      <categories>
        <category>NGS</category>
        <category>software</category>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>序列处理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[建库试剂盒供应商]]></title>
    <url>%2F04.%E6%9D%82%E8%B0%88%2F%E6%89%93%E5%B7%A5%E4%BA%BA%E7%9A%84%E8%87%AA%E6%88%91%E4%BF%AE%E5%85%BB%2F</url>
    <content type="text"><![CDATA[依据：N和2NN：是指劳动者不能胜任工作岗位，经过岗位培训和转岗仍然不能胜任，才可以进行进行辞退，经过考核（前提是有相关的绩效考核规定，公司没有的话是不能证明绩效不合格的）-培训-适岗后依然证明我不胜任，赔偿才会是N或N+1。否则就是违法辞退。2N:在员工本身没有问题和犯错，也能胜任现有岗位（没有不能胜任的证明）但是用人单位解除劳动合同。属于违法辞退，违法辞退是可以主张2N的赔偿的。参考《中华人民共和国劳动合同法》47、48、87条 剩余假期，可以主张按300%发放。 –参考《职工带薪年休假条例》第五条。 可以谈剩余的假期（年假、调休假） 300% 的赔偿， –参考《职工带薪年休假条例》第五条。日薪： 851/天 ； 2天调休+25年假1.5851 3.5 2 = 5957.0 ~ 6000851 3.5 3 = 8935.5 ~ 9000 2017.06 —&gt; 2026.01 8年7个月。 n=9N : 1.87229= 16.8498N+1 ：1.872210= 18.722 股票，如果达到发放标准，需要同步发放，写进合同。年终奖折算。 基于时间，可以谈让公司续交一个月的社保。]]></content>
      <categories>
        <category>其他</category>
      </categories>
      <tags>
        <tag>其他</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[建库试剂盒供应商]]></title>
    <url>%2F04.%E8%A1%8C%E4%B8%9A-%E4%B8%8A%E4%B8%8B%E6%B8%B8%E8%A1%8C%E4%B8%9A%E5%85%AC%E5%8F%B8%2F0004-01.%E5%BB%BA%E5%BA%93%E8%AF%95%E5%89%82%E7%9B%92%E4%BE%9B%E5%BA%94%E5%95%86%2F</url>
    <content type="text"><![CDATA[诺唯赞VAHTS Universal Pro DNA Library Prep Kit for MGI产品优势 高效修复DNA损伤: 有效修复碱基损伤、切刻、缺口和3’端封闭等问题 建库效率高: 文库转化率和扩增产出双提升 模板兼容性广: 兼容 gDNA，FFPE DNA，cfDNA，Amplicons 等样本 兼容不同质量样本: DNA修复酶对不同DIN值的FFPE DNA均有优异的表现 建库耗时短: 单个文库构建仅需90min IDTIntegrated DNA Technologies (IDT; Coralville, IA, USA),KAPA Biosystems (Wilmington, MA, USA)New England Biolabs (NEB; Ipswich, MA, USA).]]></content>
      <categories>
        <category>行业公司</category>
        <category>供应商</category>
      </categories>
      <tags>
        <tag>酶切建库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[捕获探针]]></title>
    <url>%2F04.%E8%A1%8C%E4%B8%9A-%E4%B8%8A%E4%B8%8B%E6%B8%B8%E8%A1%8C%E4%B8%9A%E5%85%AC%E5%8F%B8%2F0004-01.%E6%8D%95%E8%8E%B7%E6%8E%A2%E9%92%88%E4%BE%9B%E5%BA%94%E5%95%86%2F</url>
    <content type="text"><![CDATA[IDT官网：简介：相关业务备注： Agilent官网：简介：相关业务备注： TWIST官网：简介：相关业务备注： ROCHE官网：简介：相关业务备注： 艾吉泰康官网：简介：相关业务备注： 迈基诺官网：简介：相关业务备注： 影子基因官网：www.yingzigene.com简介：相关业务备注： 何因官网：简介：相关业务备注： 纳昂达官网：简介：相关业务备注： 博瑞迪官网：简介：相关业务备注： 伯科生物官网：简介：相关业务备注：]]></content>
      <categories>
        <category>行业公司</category>
        <category>供应商</category>
      </categories>
      <tags>
        <tag>探针合成</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[NGS-重测序常见的可视化方案]]></title>
    <url>%2F01.%E7%9F%A5%E8%AF%86-01.NGS%2FNGS-%E9%87%8D%E6%B5%8B%E5%BA%8F%E5%B8%B8%E8%A7%81%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%96%B9%E6%A1%88%2F</url>
    <content type="text"><![CDATA[SVSV可视化的原理如下 基于重构染色体，将设计融合的reads进行比对后，从bam中看到的结果如下 CNV使用CNVkit的话，本身提供了比较多的可视化组件1cnvkit.py scatter -s TR_95_T.cn&#123;s,r&#125; -c chr12:50000000-80000000 -g CDK4,MDM2 也可以展示整体染色体层面的CNV变异结果1cnvkit.py diagram -s Sample.cns Sample.cnr]]></content>
      <categories>
        <category>NGS</category>
        <category>变异检测</category>
      </categories>
      <tags>
        <tag>变异检测</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LoD LoB LoQ]]></title>
    <url>%2F01.%E7%9F%A5%E8%AF%86-01.NGS%2FNGS-Dev-LoDLoBLoQ%2F</url>
    <content type="text"><![CDATA[目前LOB、LoD概念整体上已经基本达成共识，但是在具体概念细节上，其实还是存在一些会引发歧义的内容。因此在展示LOB指标的时候，还是尽量在结果展示阶段，说明具体的计算方法或参考资料。以避免引起一些不必要的问题。 Limit of Blank (LoB)A blank sample was defined as a sample containing a single genotype (i.e pre-transplantation samples). The average measured background of a second genotype in blank samples detected in the informative markers for all theoretical combinations of pairs was defined as LOB. [1] LoB is the highest apparent analyte concentration expected to be found when replicates of a blank sample containing no analyte are tested. LoB = mean-blank + 1.645(SD-blank) . The raw analytical signal is preferable for establishing LoB as analysers may report all signal values below a certain fixed limit as “zero concentration”).[2] Limit of Detection (LoD)LOD was defined as the lowest chimerism value that could be reliably distinguished from the LOB.[1] LoD is the lowest analyte concentration likely to be reliably distinguished from the LoB and at which detection is feasible. LoD is determined by utilising both the measured LoB and test replicates of a sample known to contain a low concentration of analyte. LoD = LoB + 1.645(SD low concentration sample) [2] Limit of Quantitation (LoQ)The LOQ represents the lowest measured MC according to a predefined accuracy goal. We defined the LOQ as the lowest level of MC measured at or above the LOD with CV&lt;20%. [1] LoQ is the lowest concentration at which the analyte can not only be reliably detected but at which some predefined goals for bias and imprecision are met. The LoQ may be equivalent to the LoD or it could be at a much higher concentration.[2] 参考文献 Development and performance of a next generation sequencing (NGS) assay for monitoring of mixed chimerism Limit of Blank, Limit of Detection and Limit of Quantitation [高通量测序技术，主编：李金明] Method Validation Essentials, Limit of Blank, Limit of Detection and Limit of Quantitation FDA-SUMMARY OF SAFETY AND EFFECTIVENESS DATA Protocols of determining limits of detection and limits of quantitation for quantitative analytical methods FDA:Test Developers of Serology Tests that Detect or Correlate to Neutralizing Antibodies]]></content>
      <categories>
        <category>NGS</category>
        <category>肿瘤检测</category>
        <category>标准化</category>
      </categories>
      <tags>
        <tag>基础概念</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度和检测限测算]]></title>
    <url>%2F01.%E7%9F%A5%E8%AF%86-01.NGS%2FNGS-Dev-%E6%B7%B1%E5%BA%A6%E5%92%8C%E6%A3%80%E6%B5%8B%E9%99%90%E6%B5%8B%E7%AE%97%2F</url>
    <content type="text"><![CDATA[测序深度或覆盖深度被定义为覆盖给定核苷酸位置的reads的数量，生物信息学工具极其依赖于足够的覆盖深度，以便灵敏和特异地检测变异。覆盖深度与稳定检测样本的变异之间的关系很简单，因为更高数量的高质量测序数据为特定位置的碱基检测供了信心，无论来自测序样本的碱基调用是否是与参考碱基相同（未识别出变异）或者是非参考碱基（识别出变异）。然而，许多因素会影响所需的深度，包括测序平台，目标区域的序列复杂性（与基因组的多个区域具有同源性的区域、重复序列元件或假基因的存在以及GC富集区域）。此外，用于目标富集的文库制备和需要评估的变异类型也是重要的考虑因素。因此，必须在检测开发和验证过程中系统地评估每个 NGS 测试的覆盖模型。 深度下限(仅考虑检测灵敏性达标)NGS检测过程中，为了报证临床检测的灵敏性，需要位点深度达到一定水平，才能报证一些低频的变异位点可以被有效的检出。ps：由于该深度仅关注灵敏性，因此针对部分错误富集区域特异性可能并不理想 3条/正负支持 0.01 0.015 0.02 0.025 0.03 0.035 0.04 0.045 0.05 300 49.22% 74.59% 88.25% 94.72% 97.63% 98.93% 99.52% 99.78% 99.90% 400 67.56% 88.20% 95.93% 98.59% 99.51% 99.83% 99.94% 99.98% 99.99% 500 80.16% 94.66% 98.58% 99.62% 99.89% 99.97% 99.99% 100.00% 100.00% 600 88.15% 97.59% 99.50% 99.89% 99.98% 99.99% 100.00% 100.00% 100.00% 700 93.00% 98.90% 99.82% 99.97% 99.99% 100.00% 100.00% 100.00% 100.00% 800 95.88% 99.50% 99.93% 99.99% 100.00% 100.00% 100.00% 100.00% 100.00% 900 97.57% 99.77% 99.98% 100.00% 100.00% 100.00% 100.00% 100.00% 100.00% 1000 98.56% 99.89% 99.99% 100.00% 100.00% 100.00% 100.00% 100.00% 100.00% 4条/正负支持 0.01 0.015 0.02 0.025 0.03 0.035 0.04 0.045 0.05 300 32.34% 61.92% 81.62% 91.88% 96.57% 98.57% 99.40% 99.74% 99.89% 400 52.87% 81.56% 93.84% 98.06% 99.39% 99.80% 99.93% 99.98% 99.99% 500 69.64% 91.79% 98.04% 99.53% 99.88% 99.97% 99.99% 100.00% 100.00% 600 81.49% 96.49% 99.37% 99.88% 99.98% 99.99% 100.00% 100.00% 100.00% 700 89.13% 98.52% 99.79% 99.97% 99.99% 100.00% 100.00% 100.00% 100.00% 800 93.76% 99.37% 99.93% 99.99% 100.00% 100.00% 100.00% 100.00% 100.00% 900 96.47% 99.73% 99.98% 100.00% 100.00% 100.00% 100.00% 100.00% 100.00% 1000 98.01% 99.88% 99.99% 100.00% 100.00% 100.00% 100.00% 100.00% 100.00% 5条/正负支持 0.01 0.015 0.02 0.025 0.03 0.035 0.04 0.045 0.05 300 17.56% 45.21% 69.91% 85.59% 93.72% 97.43% 98.98% 99.60% 99.84% 400 35.69% 69.85% 88.91% 96.47% 98.96% 99.70% 99.91% 99.97% 99.99% 500 54.24% 85.47% 96.44% 99.22% 99.83% 99.96% 99.99% 100.00% 100.00% 600 69.78% 93.59% 98.93% 99.83% 99.97% 99.99% 100.00% 100.00% 100.00% 700 81.18% 97.33% 99.69% 99.96% 99.99% 100.00% 100.00% 100.00% 100.00% 800 88.79% 98.92% 99.91% 99.99% 100.00% 100.00% 100.00% 100.00% 100.00% 900 93.55% 99.57% 99.97% 100.00% 100.00% 100.00% 100.00% 100.00% 100.00% 1000 96.38% 99.83% 99.99% 100.00% 100.00% 100.00% 100.00% 100.00% 100.00% 测算数据下载表1234567891011121314151617supportRead=4 # 支持reads条数下限depthList=c(300,400,500,600,700,800,900,1000)freqList=c(0.01,0.015,0.02,0.025,0.03,0.035,0.04,0.045,0.05)result=""for (depth in depthList)&#123; for (freq in freqList)&#123; p=0 for (k in supportRead:depth) &#123; if(!is.nan(choose(depth,k)*((1-freq)**(depth-k))*(freq**k)*(1-0.5**(k-1))))&#123; p=p+(choose(depth,k)*((1-freq)**(depth-k))*(freq**k)*(1-0.5**(k-1))) &#125; &#125; tmp=paste(depth,freq,p) print(tmp) result=paste(result,tmp,sep="\n") &#125;&#125; Establishing Criteria for Depth of Sequencing一个理想的测序深度，应该是可以有效的对阳性位点和阴性位点进行区分的。而不是单纯的保证灵敏性，忽略假阳性带来的风险。因此在上述基础上，可以延伸出来一个更为立项的深度预测模型，预先对检测体系进行评估,获得整个检测体系的错误率。然后根据错误率、预期的检测下限和假阳性或假阴性结果的容忍度来估计所需的覆盖深度。（这些性能参数可以并且应该在开发阶段进行评估，以帮助定义验证的接受标准。例如，对于给定比例的突变等位基因，可以使用二项分布方程来确定检测到最小数量等位基因的概率：整体方法，和前面的一样，只是在这个部分中，我们需要把错误率（引发假阳性）纳入考量，我们把阳性位点和阴性位点的检出频率分布放到一起，可以得到如下图关系, 我们要做的，就是寻找一个深度，可以让对应的理想性能的灵敏性和特异性都能满足我们预期的性能需求，为了更好的理解这部分，可以查看动态视图。 123456789101112131415161718192021222324252627282930313233343536373839404142library("ggplot2")Totaldepth=5000 # 预期深度，控制评估的深度上限ErrorRate=0.004 # 基于产品首批高深度产品，评估获得LOBLoDRate = 0.01 # 产品预期的检测性能，后期LOD需要单独进行评估补充。CI = 0.9 # 对性能结果要求的置信区间。depthlist = c(1:Totaldepth)ErrorReadNumlist = depthlistSupportReadNumlist = depthlistfor (depth in 1:Totaldepth) &#123; ErrorReadNum = round(depth * ErrorRate) # 考虑错误率统计过程中本身已经是错误率的最高值，所以不再进行二项分布扩展。 #ErrorReadNum = qbinom(CI,depth,ErrorRate) # 错误率考虑上95置信区间。 SupportReadNum = qbinom(1-CI,depth,LoDRate) ErrorReadNumlist[depth] = ErrorReadNum SupportReadNumlist[depth] = SupportReadNum&#125;Difference= SupportReadNumlist - ErrorReadNumlisttype=c(rep(paste('Error:',ErrorRate),times=Totaldepth),rep(paste('Detect:',LoDRate),times=Totaldepth),rep('Difference',times=Totaldepth))depth=c(depthlist,depthlist,depthlist)read_num=c(ErrorReadNumlist,SupportReadNumlist, Difference)data=data.frame(type, depth,read_num)# ggplot(data,aes(x=depth,y=read_num,cluster=type,color=type))+geom_line()+geom_hline(yintercept = 2)## 确定目标深度Target_depth="Not Find "for(i in Totaldepth:1)&#123; if(Difference[i]&lt;2)&#123; Target_depth = i+1 break &#125;&#125;## 绘图ggplot(data,aes(x=depth,y=read_num,cluster=type,color=type)) + geom_line() + geom_hline(yintercept = 2) + annotate(geom = 'text', x= 2000, y = 30, label = paste("Totaldepth=5000\nLOBRate=0.0013\nLODRate=0.01\nCI=0.99\nDepth=",Target_depth )) 示例结果如下图 202303补充《Standardization of Sequencing Coverage Depth in NGS: Recommendation for Detection of Clonal and Subclonal Mutations in Cancer Diagnostics》 发布计算方法和上述介绍原理一致，同时开源了计算代码和在线工具。 参考文献[1]. Guidelines for Validation of Next-Generation Sequencing-Based Oncology Panels [2]. Standardization of Sequencing Coverage Depth in NGS: Recommendation for Detection of Clonal and Subclonal Mutations in Cancer Diagnostics]]></content>
      <categories>
        <category>NGS</category>
        <category>肿瘤检测</category>
        <category>标准化</category>
      </categories>
      <tags>
        <tag>基础概念</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[概念-方法学相关指标]]></title>
    <url>%2F01.%E7%9F%A5%E8%AF%86-01.NGS%2FNGS-Dev-%E6%96%B9%E6%B3%95%E5%AD%A6%E7%9B%B8%E5%85%B3%E6%8C%87%E6%A0%87%2F</url>
    <content type="text"><![CDATA[准确性相关指标测序得到的变异与参考变异结果的一致性程度 ，主要是测试方法得到的检测结果与已知样本变异信息的一致性匹配程度。 对准确性的评价可通过两部分进行：①通过检测已知序列的人基因组DNA（如标准细胞株）来评价测序本身的准确性；②通过检测临床样本进行验证，包括含有疾病相关突变的样本和含有与待检突变相同突变类型的样本。可将高通量测序与另一方法同时检测临床样本来评价，比较高通量测序与另一方法之间结果的差异，不一致的结果再用第三种方法确认，通过PPA和NPA来评价定性测定的准确度。不同变异类型的比较方法如下，且将这些方法作为比较方法之前，均应先经过性能验证或性能确认： 变异类型 不同变异类型验证对比方法 SNV、InDel Sanger测序、等位基因特异性PCR、SNP arrays 等 CNV 实时荧光定量PCR、荧光原位杂交FISH、微阵列比较基因组杂交CGH SV 实时荧光定量PCR、荧光原位杂交FISH 通过检测阳性符合率（PPA，positive percent agreement），阴性符合率（NPA，negative percent agreement），技术阳性预测值（TPPV，technical positive predictive value） 来评价检测准确性，并预先对这3个指标进行检测阈值设定，以评估测试是否可以达到预期目标。需要对这3个指标阈值设定可接受的具体值以及95%置信区间的下限值。并且需要针对受检的每种变异类型均需要设定。 准确性计算可以先对数据进行统计整理，构建如下表格：然后基于该表格，按下述方式进行计算。 阳性符合率（PPA）PPA定义为检测到的已知变异数量（TP，true posities）除以总的已知变异数量（TP加FN，false negatives），需要对每种变异类型计算PPA。PPA会影响假阴性检出频率。 PPA=A / (A+C)；根据PPA计算假阴性率，FN=1-PPA阴性符合率（NPA）NPA定义为检测到的真阴性（TN，true negative）结果除以变异检测到的总野生变异（wt，wild-type=TN+FP）数量，需要对每种变异类型计算NPA。NPA会影响假阳性检出频率。 NPA=D / (D+B)；根据NPA计算假阳性率，FP=1-NPA技术阳性预测值（TPPV，technical positive predictive value） TPPV=A / (A+B) 精密度（Precision）精密度指同一样本在多次检测中结果的一致程度，即在相同产品及实验条件下，相同样本相同批次以及不同批次的测序实验得到的结果之间的一致性程度，主要利用重复性（repeatability）和重现性（reproducibility）进行分析。以上检测不需要金标准测序结果，只需要测试者计算每个样本相同检测结果的重复比例即可。无检出或者无效检出重复比例也需要进行统计。数据精确性可通过生物学重复（至少三个样本）和技术重复（相同样本不同barcode）来评估；另有标准性材料建议使用3个参考样本，并且每个检测3-5次，在相同以及不同的测试批次中，这个建议作为评估平台精确度的最低要求。评价重复性或重现性可以计算突变定性结果（阳性/阴性）的一致率，也可计算突变频率的变异系数，检测质量指标是否存在较大波动也可作为判断精密度的参考。 重复性（repeatability）重复性指在同一条件下（相同环境、相同操作人员、相同检测流程、相同仪器）对相同或者短期内相似样本进行多次检测，评估变异检测结果的一致性程度。 重现性（reproducibility）重现性指由不同操作人员、不同仪器（相同型号）进行相同或者短期内相似样本进行检测。评估变异检测结果的一致性程度。建议同一样本应包括不同操作人员、不同试剂批号、不同测序仪的检测。 检测限（limit of detection）通常情况下，最低检测限定义为至少可以检出95%阳性变异的最低检测浓度以及变异频率。可以通过进行样本混合构建具有不同频率百分比范围的梯度稀释样本来评估对于不同变异类型的最低检测限。每种变异类型均需对LoD进行性能确认。 考虑到不同方法对特殊样本（肿瘤样本，ctDNA，嵌合体）检测局限性，有时Sanger测序并不是所有变异检测的金标准。所以在这种情况下，可通过稀释样本混合实验（dilution of samples with known allele frequencies）来测试NGS技术检验变异能力的动态范围。 计算LoD可采用两种方式：①直接用符合率为100%的最低MAF作为LoD，这种方式相对比较简单；②采用统计学分析来计算95%的LoD水平。例如，Probit分析是最常见的分析方法，具体做法是：首先列出每个MAF水平的阳性结果数量和总测定样本数量，然后将每个MAF水平检测的阳性百分比转换至Probit，最后构建Probit值与MAF的回归线图，查Probit表确定C95。 建立LoD时，需同时建立空白限（Limit of blank，LoB），LoB的定义是一定概率下测量空白样本时可能得到的最高检测结果，在高通量测序检测中指对阴性结果（野生型位点）检测可能得到的最高检测结果。ps 在FDA审评中，检测下限也作为分析敏感性进行评价。 目前LOB、LoD概念整体上已经基本达成共识，但是在具体概念细节上，还存在歧义。因此在展示LOB指标的时候，还是尽量在结果展示阶段，说明具体的计算方法或参考资料。以避免引起一些不必要的歧义。]]></content>
      <categories>
        <category>NGS</category>
        <category>肿瘤检测</category>
        <category>标准化</category>
      </categories>
      <tags>
        <tag>基础概念</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[细胞遗传学位置]]></title>
    <url>%2F01.%E7%9F%A5%E8%AF%86-01.NGS%2F%E7%BB%86%E8%83%9E%E9%81%97%E4%BC%A0%E5%AD%A6%E4%BD%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[细胞遗传学位置（Cytogenetic Location）：细胞遗传学定位是遗传学家描述基因的细胞遗传位置的一种标准方法。在大多数情况下，基因的位置是通过描述被染色的染色体上某一条“带”来定位。例如17q12。如果不知道精确位置，也可以描述为一组“带”，例如17q12-q21。 这一组数字和字母的组合提供可一个基因在染色体上的“地址”。这个地址由以下几部分组成： 基因所在的染色体编号。“地址”的第一位数字或者字母用来指示基因所在的染色体。第1-22号常染色体使用染色体的编号来指示染色体。性染色体则使用X或者Y。 染色体的臂。每条染色体上都有且仅有一个狭窄部位，称为着丝粒。着丝粒将染色体分为两部分又称为两条臂。习惯约定较短的臂（称为短臂）代码为p，较长的臂（称为长臂）代码为q。基因“地址”中的第二位是指示基因所在染色体的臂。例如5q指代的是5号染色体长臂，Xp则指代X染色体短臂。 基因在短臂p或者长臂q上的位置。当染色体被某些方式染色时，会呈现出明暗相间的条带。基因定位，正是基于这些条带。基因在染色体臂上的定位一般由两位数字组成，分别指代基因所在的“区”和“带”。有时在两位数字之后还会有小数点和其后的数字，指示的是在“带”区域中的“亚带”。数字随着基因与着丝粒距离的增加而增大。例如：14q21指示为14号染色体2区1带，比14q22指示的14号染色体2区2带距离着丝粒更近。 有些时候，细胞遗传学定位中还会出现“cen”和“ter”标识。“cen”代表基因非常接近着丝粒。例如，16pcen代表16号染色体近着丝粒位置。“ter”则代表基因非常接近臂的末端。例如14qter指示的是14号染色体长臂末端。（ 有时定位标识中还会出现“tel”。“tel”指代的是位于每条染色体臂末端的端粒。“ter”和“tel”指代的是相同的意思，均为染色体臂的末端）。 利用常规显带技术人类中期染色体显示的带纹数较少，一套单倍体染色体带纹总数仅有320条带。70年代后期，由于相关技术的改进，可以从早中期、前中期、晚前期细胞得到更长、带纹更多的染色体。一套单倍体染色体即可显示550～850条或更多的带纹，即在原有的带纹上分出更多的带，这种染色体称为高分辨显带染色体（high resolution banding chromosome，HRBC）。由于染色体高分辨显带能为染色体及其所发生的畸变的提供更多细节，所以有助于我们发现更多、更细微的染色体结构的异常，使染色体发生畸变的断裂点定位更准确，因此这一技术在临床细胞遗传学、分子细胞遗传学检查上，或在肿瘤染色体的研究和基因定位上都具有非常广泛的应用价值。 “人类细胞遗传学高分辨命名的国际体制（ISCN1981）”的模式图 ，标示了更多条带的高分辨带型。 高分辨显带的命名方法是在原带之后加“.”，并在“.”之后写新的带号，称为亚带 。 例如：原来的1p31带被分为三个亚带，命名为1p31.1，1p31.2，1p31.3，即表示1号染色体短臂3区1带第1亚带、第2亚带、第3亚带。1p31.3再分时，称为次亚带，则直接在 后面加序号，写为1p31.31，1p31.32，1p31.33。 染色体芯片分析技术是提高染色体核型分析精确度的一种新方法 ，可以结合 G 显带 、FISH 技术诊断亚显微结构的染色体重复或缺失。 ISCN2009介绍的常用方法有：aCGH（array based comparative genomic hybridization）和MLPA（Multiplex ligation dependent Probe amplification），这两种方法都是基于将目的基因作为芯片对全基因进行扫描，分析结果中提示的缺失或重复 DNA 片段进一步选择相关染色体区域探针FISH 确认分析，这样将染色体缺失或重复片段的分辨率提高到了 Mb 水平。 相应的描述方式发生了一些变化 。 如果阵列检测整个染色体结果正常 ，则可以如下方式描述。arr(1-22,X)×2表示正常女性 ；arr(1-22)×2,(XY)×1表示正常男性 ；如果阵列检测结果异常则只列出异常染色体。 性染色体异常要先列出，然后是其他异常的常染色体按照染色体序号排列，同时描述出现异常的核苷酸。需要注意的是在arr 和第一个异常染色体之间加一个空格。 如：arr 20q13.2q13.33(51,001,876.62,375,085)x1，阵列分析显示20号染色体长臂缺失部分节段，从1区3带2亚带到1区3带 3亚带3次亚带，括号内是缺失的核苷酸链 获取细胞学位置对应的染色体坐标：UCSC downloadNCBI human Hg19:GCF_000001305]]></content>
      <categories>
        <category>NGS</category>
        <category>知识沉淀</category>
        <category>肿瘤检测</category>
        <category>缩略语</category>
      </categories>
      <tags>
        <tag>基础概念</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[肿瘤突变负荷检测及临床应用中国专家共识（2022年版）]]></title>
    <url>%2F01.%E7%9F%A5%E8%AF%86-03.standard%2F%E5%85%B1%E8%AF%86-%20ctDNA%E9%AB%98%E9%80%9A%E9%87%8F%E6%B5%8B%E5%BA%8F%E4%B8%B4%E5%BA%8A%E5%AE%9E%E8%B7%B5%E4%B8%93%E5%AE%B6%E5%85%B1%E8%AF%86-2022%E5%B9%B4%E7%89%88%2F</url>
    <content type="text"><![CDATA[原文链接ctDNA高通量测序临床实践专家共识（2022年版）官网ctDNA高通量测序临床实践专家共识（2022年版）文档 随着液态活检的快速发展，采用体液对患者的分子特征进行分析成为可能，特别是基于循环肿瘤DNA（circulating tumor DNA，ctDNA）的高通量测序（nextgeneration sequencing，NGS）技术，因其无创或微创、检测时间短、能够反映瘤内和转移灶异质性、可动态监测治疗疗效等优势而在临床得到越来越广泛的应用。与肿瘤组织样本相比，采用ctDNA进行基因检测在样本收集和处理、检测技术要求、结果解读和临床应用等方面存在诸多不同，且目前国内尚缺少ctDNA基因检测标准，因此限制了其在临床上的规范应用。为此，中国抗癌协会肿瘤标志专业委员会组织国内肿瘤临床、病理、检验、生物信息分析和高通量检测领域专家，参考国内外 ctDNA 临床应用共识、指南和最新文献，结合国内外现有的高通量测序技术要求和临床实践，从ctDNA的生物学特征、临床应用价值和范围、高通量检测的标准要求及其未来发展趋势等方面提出本共识，以促进ctDNA NGS的健康规范发展。 专家共识 ctDNA是由肿瘤细胞主动分泌或肿瘤细胞在凋亡或坏死过程中释放入循环系统的 DNA片段；其丰度受多种因素影响，波动较大，在内外因素特别是治疗压力下，其携带的生物信息可能会发生演变，且受正常细胞胚系变异或克隆性造血细胞体系突变干扰，在临床检测和报告解读过程中应特别注意。 目前已有临床证据支持ctDNA NGS检测可应用于肺癌、乳腺癌、前列腺癌、卵巢癌等晚期实体肿瘤的伴随诊断，但涉及的驱动基因及其变异类型与相应分析体系均有严格限定，若超适应症应用时，建议与患者就检测必要性、检测费用以及局限性等内容进行充分知情。ctDNA NGS 检测已被国内外专家共识或指南建议作为多种晚期恶性肿瘤组织基因检测的替代方式，但依据其分析结果实时制定临床治疗策略时仍需高级别循证证据支持。 晚期实体肿瘤分子靶向或免疫检查点抑制剂治疗开始后，基于NGS检测的ctDNA水平定量和动态变化分析，有望成为新兴的疗效评估途径。ctDNA MRD 检测是全新的个性化技术应用领域，尚难以建立通用性技术标准，亟待通过大样本、多中心、前瞻性的临床试验验证其临床效用。ctDNA NGS 检测在临床上用于靶向或免疫治疗评估和分层时，建议就检测价值、局限性和费用等进行充分知情。 在临床环境中，ctDNA NGS检测可用于识别分子靶向治疗的耐药机制，尤其对于疑难复杂的肿瘤患者，该结果有助于后续的治疗选择决策。免疫检查点抑制剂治疗获得性耐药机制复杂，治疗选择压力下的肿瘤亚克隆演进仅为其部分原因，ctDNA靶向测序、全外显子和（或）全基因组检测仅作为其转化研究工具之一。 ctDNA NGS检测实验室质量管理需贯穿全程，ctDNA 收集、样本处理和自动化过程应按照标准化和临床验证程序进行，最大程度防范因操作差异而引发的假阴性可能。样本采集建议采用含细胞稳定剂的抗凝管，尽快完成血浆分离，提取的 cfDNA 建议在24 h内进行后续检测，否则，置于-30 ℃至-15 ℃下储存并避免反复冻融。 ctDNA NGS检测应根据项目需求选择技术路线，可依据检测基因数量及覆盖范围大小选择不同测序策略。在进行基于 ctDNA 的超高灵敏度突变检测时，建议使用分子标签技术和优化对应的生物信息分析设置，以降低由于测序平台随机误差导致的假阳性结果；建议通过建立测序噪音和克隆性造血背景库的方法降低克隆性造血及背景噪音带来的影响。 专家共识：ctDNA NGS临床检测报告应包含受检者基本信息、样本信息、实验室信息、检测项目、检测结果及变异解读、检测方法的实验室内部验证结果、检测局限性及不确定性以及进一步检测的建议等内容。实验室应建立报告 SOP，建议根据国内外文献、共识指南、临床试验证据和实践对检出的肿瘤基因突变进行分类或分级报告。 ctDNA检测优劣势ctDNA是由肿瘤细胞主动分泌或肿瘤细胞在凋亡或坏死过程中释放入循环系统的 DNA片段；其丰度受多种因素影响，波动较大，在内外因素特别是治疗压力下，其携带的生物信息可能会发生演变，且受正常细胞胚系变异或克隆性造血细胞体系突变干扰，在临床检测和报告解读过程中应特别注意。，长度 132~145 bp，半衰期较短（一般&lt;2 h）。 ctDNA 水平一般呈动态变化，且受多种因素影响： 肿瘤病理组织类型、部位、分期、肿瘤负荷等因素可影响ctDNA的释放。在肿瘤负荷较轻、特定部位（如颅内肿瘤）和特定组织学（如胶质瘤），以及增殖、凋亡和/或血管化水平较低的肿瘤患者中，ctDNA水平通常较低［1］。 大量其他来源的 DNA 干扰。如其他正常细胞或白细胞来源的DNA，与ctDNA一起被称为游离 DNA（cell free DNA，cfDNA）。多种生理和病理因素，如怀孕、剧烈运动、外伤、炎症、心肌梗死、自身免疫性疾病和急性中风等也会影响cfDNA的释放［2⁃3］。 克隆性造血细胞产生的cfDNA携带的基因突变信息可能会干扰ctDNA检测结果［4］。此外，其突变基因还受不同内外因素影响（包括年龄、吸烟、种族和肿瘤治疗等），如ASXL1突变在吸烟者中富集，DNA损伤应答（DNA⁃damaged response，DDR）基因（TP53、PPM1D、CHEK2）突变在接受放射、铂类药物和拓扑异构酶Ⅱ抑制剂治疗的肿瘤患者中更为常见［5］。 ctDNA 半衰期较短（一般&lt;2 h），不同采样时间可能影响 ctDNA 含量［6］。 药物治疗影响 ctDNA 含量。有研究显示，非小细胞肺癌（non⁃small cell lung cancer，NSCLC）患者接受酪氨酸激酶抑制剂（tyrosine kinase inhibitor，TKI）治疗后，ctDNA 含量在 24 h 达到顶峰，随后快速降低，提示药物也会影响ctDNA含量 与组织学检测相比，优势： ctDNA检测具有无创或微创，可反复取材，收集、处理和分析报告周转时间（turn⁃around time，TAT）短等优势； 能克服肿瘤空间异质性，可相对全面地实时反映患者的肿瘤分子特征； 与单独组织活检相比，血浆ctDNA还可增加驱动基因突变检出率。 劣势： 外周血 ctDNA含量较低，容易造成临床检测假阴性结果； 由于胚系变异或克隆性造血突变的存在，若未用白细胞作为对照，也可能导致假阳性结果； 不同肿瘤患者或同一患者不同时段释放入血的ctDNA量也存在差异，也给 ctDNA 的临床检测和结果解读带来困难。 ctDNA NGS检测的标准质控标准干实验”涵盖生物信息学分析流程各步骤，该阶段质控应对 最低测序深度（血浆cfDNA标本的NGS有效测序深度应达到1000×以上，并应在80%以上的目标区域达到这个深度 平均测序深度、 覆盖均一性、 鸟嘌呤和胞嘧啶（guanine and cytosine，GC）含量、 碱基识别质量值、 比对质量值、 在靶率等作出相应要求并遵照执行。 生信方法的要求基于分子标签技术的数据质控和数据分析ctDNA 检测常出现 PCR 重复序列比例高（50%~90%）、PCR 扩增和测序错误等引入的背景噪音高等问题。同时，血液中白细胞的克隆性造血突变也影响ctDNA变异的鉴定。基于分子标签技术的分析流程可帮助有效去除背景噪音，配对白细胞样本或背景库的建立可有效去除来自白细胞中克隆性造血突变引入的假阳性，提升ctDNA检测的准确性。UMI通过给每一条原始DNA片段加上一段特有的标签序列，经文库构建和PCR扩增后一同测序，根据不同的标签序列可以区分不同来源的 DNA 模板，分辨源于 PCR 扩增及测序过程中随机错误产生的假阳性突变，识别cfDNA中真正来源于肿瘤的突变，从而提高检测灵敏度和特异性，可达0.1%的检出限［71］。一般推荐1条UMI对应并至少3条reads。 测序噪音控制建立背景库过滤噪声的大致思路是通过使用相同测序流程的样本估计基因组坐标对应区域或碱基的测序错误率，然后使用假设检验等统计方法比较目标样本对应坐标的突变频率是否显著高于估计得出的测序错误率。通过对患者肿瘤样本建立背景库过滤噪声的常用方法有 SiNVICT 和OutLyzer。SiNVICT使用肿瘤样本计算出一段基因组区域的噪音水平，通过计算信噪比的方法过滤潜在的假阳性结果［72］。OutLyzer 通过计算目标突变附近200 bp 内的每个坐标的测序错误率，并通过多次Thompson’s Tau Test 过滤离群噪声［73］。在可用较为充足的情况下，使用多个患者的正常样本建立背景库能较准确地估计噪声水平。其中代表方法有Mutect1/Mutect2（GATK）［74］，该法使用不少于 40 个没有肿瘤细胞污染的正常样本构建背景库。在此基础上发展的基于不同算法的构建背景库算法，如基于二项分布的 TNER、基于高斯分布模拟的 IDES［75］、基于泊松分布的AmpliSolve［76］等，均可有效提升突变检出准确率和召回率，去除背景噪音。 克隆性造血变异过滤RAZAVI 等［77］发现ctDNA 存在大量未知来源的基因变异（variants ofunknown source，VUSo），大部分 VUSo 突变丰度&lt;1%，与样本测序深度、DNA 起始量、位点的测序深度、样本染色体拷贝数无相关性，且有很好的重复性结果。在白细胞样本中也检测到大量的 VUS，且与年龄呈正相关，证实了 ctDNA 中的大部分 VUSo 来自白细胞，并非来源于测序背景噪音，这部分 VUSo 突变被定义为克隆性造血突变。因此，在不同 ctDNA 高通量测序临床应用场景，需要考虑 ctDNA 中包含有源于白细胞的克隆性造血突变。ctDNA 检测时通过配对的白细胞样本，或者汇集整合检测到的克隆性造血突变，构建克隆性造血突变背景库，可有效帮助去除克隆性造血造成的假阳性。 数据的储存与管理随着测序深度的增加，ctDNA 下机数据有效安全的储存需要配备相应的基础设施。高通量测序会生成大量文件，下机的fastq或bam 原始文件、vcf结果文件等需长期保存，同时应保存好测序过程中完整的日志文件，以便区分高通量测序分析软件版本信息、追溯异常结果。诊断实验室应保存相关数据至少 3年，并在相关技术人员的支持下制定数据备份计划和恢复计划。 变异结果的命名规则对基因变异的描述应遵循一定的原则和规范，推荐参考人类基因组变异协会命名指南（www.hgvs.org，登录日期2022年6月22日），转录本的选择建议采用基因座参考基因组序列数据库（Locus reference genomic；www.lrg⁃sequence.org，登录日期2022年6月22日）界定的转录本或多个国际数据库公认的主要转录本。对遗传性肿瘤相关基因变异的说明，建议以美国医学遗传学学院（American college of medical genetics and genomics）指南为标准，在检测结果中列出具体的变异位点信息，包括基因名称、所参考的人类基因组版本号、转录本参考序列版本号、核苷酸变异、氨基酸变异、外显子/内含子序号、等位基因杂合性、染色体编号及坐标等。 ctDNA NGS检测临床应用展望临床研究表明基于 ctDNA NGS检测的bMSI （blood MSI）和bTMB（blood TMB）具有免疫检查点抑制剂疗效预测价值［35］。FDA于2020年8 月 26 日 批 准 基 于 ctDNA NGS 数 据 拟 合 bMSI 和bTMB 的试剂盒用于泛实体瘤多个靶向药物及免疫检查点抑制剂的伴随诊断。但 bMSI 和 bTMB 在临床应用中受多种因素影响，如样本采集时间、检测 panel基因覆盖范围、panel 大小、测序深度、生信算法及阈值设定等，因此还需要更多的前瞻性临床研究证据支持 ctDNA NGS 数据拟合的 bMSI 和 bTMB 的临床应用前景。 HRD 状态可通过同源重组相关基因突变（homologous recombinationrepair，HRR）检测和基因瘢痕（genomic scar，GS）检测两种方式判断。两者均采用 HRR 联合 GS 检测策略评估 HRD 状态。由于基于 ctDNA NGS 对 GS 进行评估技术上具有巨大挑战，所以目前基于 ctDNA NGS 检测进行HRD 评估主要集中在 HRR 信号通路基因 SNP 检测，但国内外对 HRR 基因的定义尚缺乏统一标准；另外，随着检测技术的进步，基于 ctDNA NGS 评估 GS逐渐成为可能，但目前基于 ctDNA NGS 检测评估HRD尚有巨大的探索空间。 ctDNA中包含核酸序列、位点突变、拷贝数、甲基化、MSI、序列长度、序列丰度、出现的时空模式、在基因组上的位置特征、起始位点和终止位点特征等丰富的生物信息，如肿瘤 cfDNA 片段的长度分布、核小体位置与正常细胞显著不同，这些信息可应用于肿瘤筛查和诊断中。CancerSEEK 的癌症早筛项目通过监测 ctDNA 中 16 个基因的突变及血清 8 个蛋白质生物标志物水平，并构建 Logistic回归模型，结果该模型在 8种肿瘤共包含 1 005例患者中的敏感性为69%~98%，特异性为 99%［84］。最新的应用机器学习分类器辅助的深度甲基化测序能检测到 52%~81%临床分期为ⅠA~Ⅲ期的患者，且检测限低至万分之一，显示了更高的敏感性和特异性（特异性为 96%，95%CI：93%~98%）［85］。 总之，基于 ctDNA NGS 检测的 bTMB 具有免疫检查点抑制剂疗效预测价值，但仍有诸多因素影响bTMB 检测在临床中的应用，未来还需开展更多的前瞻性临床研究。机器学习等人工智能算法不仅能降低假阳性、提高灵敏度，还能有效整合多维度的异质信息进行全面分析，大幅提高 ctDNA 数据效力，是ctDNA数据分析的主要方向]]></content>
      <categories>
        <category>NGS</category>
        <category>肿瘤检测</category>
        <category>共识</category>
      </categories>
      <tags>
        <tag>中国癌症防治杂志</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C语言程序异常处理]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-07.c%2Fc%E8%AF%AD%E8%A8%80%E7%A8%8B%E5%BA%8F%E7%9B%B8%E5%85%B3%E5%BC%82%E5%B8%B8%E5%A4%84%E7%90%86%E5%8A%9E%E6%B3%95%2F</url>
    <content type="text"></content>
      <categories>
        <category>编程拾慧</category>
        <category>C++</category>
      </categories>
      <tags>
        <tag>编程语言</tag>
        <tag>C</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[c语言异常核查-gdb处理core文件]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-07.c%2Fc%E8%AF%AD%E8%A8%80%E5%BC%82%E5%B8%B8%E6%A0%B8%E6%9F%A5-gdb%E5%A4%84%E7%90%86core%E6%96%87%E4%BB%B6%2F</url>
    <content type="text"></content>
      <categories>
        <category>编程拾慧</category>
        <category>C++</category>
      </categories>
      <tags>
        <tag>编程语言</tag>
        <tag>C</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Markdown - 进阶小技巧]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-06.markdown%2Fmarkdown-%E8%BF%9B%E9%98%B6%E5%B0%8F%E6%8A%80%E5%B7%A7%2F</url>
    <content type="text"><![CDATA[文本优化文本设置设置字体、字号和颜色的代码如下： 字体&lt;font face=&quot;黑体&quot;&gt; 我是黑体字 &lt;/font&gt; 我是黑体字 &lt;font face=&quot;微软雅黑&quot;&gt; 我是微软雅黑 &lt;/font&gt; 我是微软雅黑 颜色&lt;font color=gray size=7&gt; color=gray &lt;/font&gt; color=gray &lt;font color=#00ffff size=7&gt; color=#00ffff &lt;/font&gt; color=#00ffff $\color{green}{绿色} $$\color{green}{绿色} $ 角标上标： n&lt;sup&gt;2&lt;/sup&gt;=n ：n2=n下标： n&lt;sub&gt;2&lt;/sub&gt;=n ：n2=n 综合设置&lt;font color=#0099ff size=7 face=&quot;黑体&quot;&gt; color=#0099ff size=7 face=&quot;黑体&quot; &lt;/font&gt; color=#0099ff size=7 face=”黑体” PS: Size规定文本的尺寸大小。可能的值：从 1 到 7 的数字。浏览器默认值是 3。 文本位置12345&lt;center&gt;文字居中&lt;/center&gt;## 添加脚注markdown可以添加脚注`添加[^footnote]实现`，比如引用的参考文献等信息，从而在需要的时候，帮助进行跳转和查询，同时不影响文本阅读的连续性。实例如下，添加了三个脚注，同时通过不同的footnote标签进行识别和跳转。 使用 Markdown[^1] 可以效率的书写文档，直接转换成 HTML[^2], 你可以使用 Typora[^T] 软件。 [^1]:Markdown 是一种纯文本标记语言。[^2]:HyperText Markup Language 超文本标记语言。[^T]:Typora 官网 https://typora.io/123456789101112131415使用 Markdown[^1] 可以效率的书写文档，直接转换成 HTML[^2], 你可以使用 Typora[^T] 软件。[^1]:Markdown 是一种纯文本标记语言。[^2]:HyperText Markup Language 超文本标记语言。[^T]:Typora 官网 &lt;https://typora.io/&gt;# 排版## markdown中实现缩进$\qquad$Markdown不太注重格式排版的文章，类似首行缩进是无法直接通过**空格/tab**实现的，因此在存在排版需求时，需要借助一些其他方案实现首行缩进### 利用LaTeX空格符缩进 示例代码如下： ```小标题 $\qquad$① 条件1 $\qquad\qquad$ 1° 子条件1 $\qquad\qquad$ 2° 子条件2 缺点：\qquad ① 在CSDN博客显示时，选中复制会把空格符代码给复制出来\qquad ② 公式结束符$后紧跟数字将导致公式无法渲染（Typora编辑器不会，CSDN-Markdown可以将公式结束符后$紧跟的数字套上） 优点：\qquad 选中时，不会明显看到空格符号被选中 适用场景：\qquad 面向观赏的博文，可能会被多次选中，但不会被复制内容（更适合我这样希望选中无空格的强迫症） 利用html空格符缩进123小标题&amp;emsp;&amp;emsp;① 条件1&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;1° 子条件1 缺点： 选中时，可以明显看到空格符号被选中 优点： 在CSDN博客显示时，选中复制不会像LaTeX一样把空格符代码给复制出来，而只会复制出简单的空格符号，粘贴后清除方便 适用场景： 面向复制的博文，可能会被多次复制，不过粘贴后改起来不麻烦，选中起来明显发现空格 利用列表控制缩进12345678无序列表标题- ① 条件1 - 1° 子条件1 - 2° 子条件2- ② 条件2 - 1° 子条件1 - 2° 子条件2 - &gt; 可以缩进引用块 缺点： ① 无序列表和有序列表的表项缩进时会产生表项的标志 ② 无序列表和有序列表的表项的标志无法赋予颜色 ③ 无序列表嵌套有序列表时，两种列表的表项的标志缩进长度从视觉效果上看不同（Typora编辑器两种长度差不多） ④ 有序列表嵌套无序列表时，会显示为有序列表嵌套有序列表（Typora编辑器不会） ⑤ 自定义列表的标题也会被缩进，并且在使用CSDN-Markdown编辑器编辑时预览的显示中标题会被默认加粗，但在发布后文章的显示中标题并没有被加粗 ⑥ 不能控制一次缩进的长度，而且下一级缩进必须依靠嵌套列表来实现 优点： ① 用有序列表和无序列表体现，表项的标志便于区分不同的表项 ② 真实的缩进，不会被选中，选中复制粘贴后也不会出现不相干的字符 ③ 可以缩进标签生成的引用块 适用场景： 觉得列表缩进样式还行、不需要任意缩进长度的博文]]></content>
      <categories>
        <category>小技巧</category>
        <category>hexo</category>
      </categories>
      <tags>
        <tag>markdown</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python-包-python-openpyxl处理excel]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-02.python%2FPython-%E5%8C%85-python-openpyxl%E5%A4%84%E7%90%86excel%2F</url>
    <content type="text"><![CDATA[workbook = openpyxl.load_workbook(‘test.xlsx’) # 返回一个workbook数据类型的值print(workbook.sheetnames) # 打印Excel表中的所有表 sheet = workbook[‘Sheet1’] # 获取指定sheet表sheet = workbook.active # 获取活动表print(sheet) cell1 = sheet[‘A1’] # 获取A1单元格的数据cell2 = sheet[‘B7’] # 获取B7单元格的数据 cell2 = sheet[‘B7’].value # 另一种写法正确示范cell1.value获取单元格A1中的值cell2.value获取单元格B7中的值print(cell1.value,cell2.value) # 姓名 18]]></content>
      <categories>
        <category>编程拾慧</category>
        <category>python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Office处理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基本功能-页面跳转]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-10.miniprogram%2FWechat-miniprogram-%E5%9F%BA%E6%9C%AC%E5%8A%9F%E8%83%BD-%E9%A1%B5%E9%9D%A2%E8%B7%B3%E8%BD%AC%2F</url>
    <content type="text"><![CDATA[参考资料开发者文档云函数 小程序的开发开发小程序的基础介绍小程序的代码构成12345678910111213141516171819202122232425262728293031323334// 保留当前页面，跳转到应用内的某个页面，使用wx.navigateBack可以返回到原页面。// 注意：调用 navigateTo 跳转时，调用该方法的页面会被加入堆栈，但是 redirectTo wx.navigateTo(&#123; url: &apos;page/home/home?user_id=111&apos;&#125;)// 关闭当前页面，返回上一页面或多级页面。可通过 getCurrentPages() 获取当前的页面栈，决定需要返回几层。wx.navigateTo(&#123; url: &apos;page/home/home?user_id=111&apos; // 页面 A&#125;)wx.navigateTo(&#123; url: &apos;page/detail/detail?product_id=222&apos; // 页面 B&#125;)// 跳转到页面 Awx.navigateBack(&#123; delta: 2&#125;)// 关闭当前页面，跳转到应用内的某个页面。wx.redirectTo(&#123; url: &apos;page/home/home?user_id=111&apos;&#125;)// 跳转到tabBar页面（在app.json中注册过的tabBar页面），同时关闭其他非tabBar页面。wx.switchTab(&#123; url: &apos;page/index/index&apos;&#125;)// 关闭所有页面，打开到应用内的某个页面。wx.reLanch(&#123; url: &apos;page/home/home?user_id=111&apos;&#125;) 2. wxml 页面组件跳转（可以通过设置open-type属性指明页面跳转方式）：1234567891011121314// navigator 组件默认的 open-type 为 navigate &lt;navigator url=&quot;/page/navigate/navigate?title=navigate&quot; hover-class=&quot;navigator-hover&quot;&gt;跳转到新页面&lt;/navigator&gt;// redirect 对应 API 中的 wx.redirect 方法&lt;navigator url=&quot;../../redirect/redirect/redirect?title=redirect&quot; open-type=&quot;redirect&quot; hover-class=&quot;other-navigator-hover&quot;&gt;在当前页打开&lt;/navigator&gt;// switchTab 对应 API 中的 wx.switchTab 方法&lt;navigator url=&quot;/page/index/index&quot; open-type=&quot;switchTab&quot; hover-class=&quot;other-navigator-hover&quot;&gt;切换 Tab&lt;/navigator&gt;// reLanch 对应 API 中的 wx.reLanch 方法&lt;navigator url=&quot;../../redirect/redirect/redirect?title=redirect&quot; open-type=&quot;redirect&quot; hover-class=&quot;other-navigator-hover&quot;&gt;关闭所有页面，打开到应用内的某个页面&lt;/navigator&gt;// navigateBack 对应 API 中的 wx.navigateBack 方法&lt;navigator url=&quot;/page/index/index&quot; open-type=&quot;navigateBack&quot; hover-class=&quot;other-navigator-hover&quot;&gt;关闭当前页面，返回上一级页面或多级页面&lt;/navigator&gt;]]></content>
      <categories>
        <category>编程拾慧</category>
        <category>miniprogram</category>
      </categories>
      <tags>
        <tag>微信小程序</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[微信小程序开发测试]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-10.miniprogram%2FWechat-miniprogram-%E5%85%A5%E9%97%A8%E6%B5%8B%E8%AF%95%2F</url>
    <content type="text"><![CDATA[参考资料开发者文档云函数 小程序的开发开发小程序的基础介绍小程序的代码构成1234.json 后缀的 JSON 配置文件.wxml 后缀的 WXML 模板文件.wxss 后缀的 WXSS 样式文件.js 后缀的 JS 脚本逻辑文件 JSON 配置JSON 是一种数据格式，并不是编程语言，在小程序中，JSON扮演的静态配置的角色。 我们可以看到在项目的根目录有一个 app.json 和 project.config.json，此外在 pages/logs 目录下还有一个 logs.json，我们依次来说明一下它们的用途。 小程序配置 app.jsonapp.json 是当前小程序的全局配置，包括了小程序的所有页面路径、界面表现、网络超时时间、底部 tab 等。QuickStart 项目里边的 app.json 配置内容如下：123456789101112&#123; &quot;pages&quot;:[ &quot;pages/index/index&quot;, &quot;pages/logs/logs&quot; ], &quot;window&quot;:&#123; &quot;backgroundTextStyle&quot;:&quot;light&quot;, &quot;navigationBarBackgroundColor&quot;: &quot;#fff&quot;, &quot;navigationBarTitleText&quot;: &quot;Weixin&quot;, &quot;navigationBarTextStyle&quot;:&quot;black&quot; &#125;&#125; 我们简单说一下这个配置各个项的含义: pages字段 —— 用于描述当前小程序所有页面路径，这是为了让微信客户端知道当前你的小程序页面定义在哪个目录。window字段 —— 定义小程序所有页面的顶部背景颜色，文字颜色定义等。其他配置项细节可以参考文档 小程序的配置 app.json 。 工具配置 project.config.json通常大家在使用一个工具的时候，都会针对各自喜好做一些个性化配置，例如界面颜色、编译配置等等，当你换了另外一台电脑重新安装工具的时候，你还要重新配置。 考虑到这点，小程序开发者工具在每个项目的根目录都会生成一个 project.config.json，你在工具上做的任何配置都会写入到这个文件，当你重新安装工具或者换电脑工作时，你只要载入同一个项目的代码包，开发者工具就自动会帮你恢复到当时你开发项目时的个性化配置，其中会包括编辑器的颜色、代码上传时自动压缩等等一系列选项。 其他配置项细节可以参考文档 开发者工具的配置 。 页面配置 page.json这里的 page.json 其实用来表示 pages/logs 目录下的 logs.json 这类和小程序页面相关的配置。 如果你整个小程序的风格是蓝色调，那么你可以在 app.json 里边声明顶部颜色是蓝色即可。实际情况可能不是这样，可能你小程序里边的每个页面都有不一样的色调来区分不同功能模块，因此我们提供了 page.json，让开发者可以独立定义每个页面的一些属性，例如刚刚说的顶部颜色、是否允许下拉刷新等等。 其他配置项细节可以参考文档 页面配置 。 JSON 语法这里说一下小程序里 JSON 配置的一些注意事项。JSON文件都是被包裹在一个大括号中 {}，通过 key-value 的方式来表达数据。JSON的 Key 必须包裹在一个双引号中，在实践中，编写 JSON 的时候，忘了给 Key 值加双引号或者是把双引号写成单引号是常见错误。JSON的值只能是以下几种数据格式，其他任何格式都会触发报错，例如 JavaScript 中的 undefined。123456数字，包含浮点数和整数字符串，需要包裹在双引号中Bool值，true 或者 false数组，需要包裹在方括号中 []对象，需要包裹在大括号中 &#123;&#125;Null 还需要注意的是 JSON 文件中无法使用注释，试图添加注释将会引发报错。 WXML 模板从事过网页编程的人知道，网页编程采用的是 HTML + CSS + JS 这样的组合，其中 HTML 是用来描述当前这个页面的结构，CSS 用来描述页面的样子，JS 通常是用来处理这个页面和用户的交互。 同样道理，在小程序中也有同样的角色，其中 WXML 充当的就是类似 HTML 的角色。打开 pages/index/index.wxml，你会看到以下的内容 和 HTML 非常相似，WXML 由标签、属性等等构成。但是也有很多不一样的地方，我们来一一阐述一下： 标签名字有点不一样 往往写 HTML 的时候，经常会用到的标签是 div, p, span，开发者在写一个页面的时候可以根据这些基础的标签组合出不一样的组件，例如日历、弹窗等等。换个思路，既然大家都需要这些组件，为什么我们不能把这些常用的组件包装起来，大大提高我们的开发效率。 从上边的例子可以看到，小程序的 WXML 用的标签是 view, button, text 等等，这些标签就是小程序给开发者包装好的基本能力，我们还提供了地图、视频、音频等等组件能力。 更多详细的组件讲述参考下个章节 小程序的能力 多了一些 wx:if 这样的属性以及 { { } } 这样的表达式 在网页的一般开发流程中，我们通常会通过 JS 操作 DOM (对应 HTML 的描述产生的树)，以引起界面的一些变化响应用户的行为。例如，用户点击某个按钮的时候，JS 会记录一些状态到 JS 变量里边，同时通过 DOM API 操控 DOM 的属性或者行为，进而引起界面一些变化。当项目越来越大的时候，你的代码会充斥着非常多的界面交互逻辑和程序的各种状态变量，显然这不是一个很好的开发模式，因此就有了 MVVM 的开发模式（例如 React, Vue），提倡把渲染和逻辑分离。简单来说就是不要再让 JS 直接操控 DOM，JS 只需要管理状态即可，然后再通过一种模板语法来描述状态和界面结构的关系即可。 小程序的框架也是用到了这个思路，如果你需要把一个 Hello World 的字符串显示在界面上。 WXML 是这么写 : 1&lt;text&gt;&#123; &#123;msg&#125; &#125;&lt;/text&gt; JS 只需要管理状态即可: 1this.setData(&#123; msg: &quot;Hello World&quot; &#125;) 通过 { { } } 的语法把一个变量绑定到界面上，我们称为数据绑定。仅仅通过数据绑定还不够完整的描述状态和界面的关系，还需要 if/else, for等控制能力，在小程序里边，这些控制能力都用 wx: 开头的属性来表达。 更详细的文档可以参考 WXML WXSS 样式WXSS 具有 CSS 大部分的特性，小程序在 WXSS 也做了一些扩充和修改。 新增了尺寸单位。在写 CSS 样式时，开发者需要考虑到手机设备的屏幕会有不同的宽度和设备像素比，采用一些技巧来换算一些像素单位。WXSS 在底层支持新的尺寸单位 rpx ，开发者可以免去换算的烦恼，只要交给小程序底层来换算即可，由于换算采用的浮点数运算，所以运算结果会和预期结果有一点点偏差。 提供了全局的样式和局部样式。和前边 app.json, page.json 的概念相同，你可以写一个 app.wxss 作为全局样式，会作用于当前小程序的所有页面，局部页面样式 page.wxss 仅对当前页面生效。 此外 WXSS 仅支持部分 CSS 选择器 更详细的文档可以参考 WXSS 。 JS 逻辑交互一个服务仅仅只有界面展示是不够的，还需要和用户做交互：响应用户的点击、获取用户的位置等等。在小程序里边，我们就通过编写 JS 脚本文件来处理用户的操作。12&lt;view&gt;&#123; &#123; msg &#125; &#125;&lt;/view&gt;&lt;button bindtap=&quot;clickMe&quot;&gt;点击我&lt;/button&gt; 点击 button 按钮的时候，我们希望把界面上 msg 显示成 “Hello World”，于是我们在 button 上声明一个属性: bindtap ，在 JS 文件里边声明了 clickMe 方法来响应这次点击操作：12345Page(&#123; clickMe: function() &#123; this.setData(&#123; msg: &quot;Hello World&quot; &#125;) &#125;&#125;) 响应用户的操作就是这么简单，更详细的事件可以参考文档 WXML - 事件 。 此外你还可以在 JS 中调用小程序提供的丰富的 API，利用这些 API 可以很方便的调起微信提供的能力，例如获取用户信息、本地存储、微信支付等。在前边的 QuickStart 例子中，在 pages/index/index.js 就调用了 wx.getUserInfo 获取微信用户的头像和昵称，最后通过 setData 把获取到的信息显示到界面上。更多 API 可以参考文档 小程序的API 。 小程序组件小程序提供了丰富的基础组件给开发者，开发者可以像搭积木一样，组合各种组件拼合成自己的小程序。 就像 HTML 的 div, p 等标签一样，在小程序里边，你只需要在 WXML 写上对应的组件标签名字就可以把该组件显示在界面上，例如，你需要在界面上显示地图，你只需要这样写即可：1&lt;map&gt;&lt;/map&gt; 使用组件的时候，还可以通过属性传递值给组件，让组件可以以不同的状态去展现，例如，我们希望地图一开始的中心的经纬度是广州，那么你需要声明地图的 longitude（中心经度） 和 latitude（中心纬度）两个属性:1&lt;map longitude=&quot;广州经度&quot; latitude=&quot;广州纬度&quot;&gt;&lt;/map&gt; 组件的内部行为也会通过事件的形式让开发者可以感知，例如用户点击了地图上的某个标记，你可以在 js 编写 markertap 函数来处理：1&lt;map bindmarkertap=&quot;markertap&quot; longitude=&quot;广州经度&quot; latitude=&quot;广州纬度&quot;&gt;&lt;/map&gt; 当然你也可以通过 style 或者 class 来控制组件的外层样式，以便适应你的界面宽度高度等等。 更多的组件可以参考 小程序的组件。 小程序api为了让开发者可以很方便的调起微信提供的能力，例如获取用户信息、微信支付等等，小程序提供了很多 API 给开发者去使用。 要获取用户的地理位置时，只需要：1234567wx.getLocation(&#123; type: &apos;wgs84&apos;, success: (res) =&gt; &#123; var latitude = res.latitude // 纬度 var longitude = res.longitude // 经度 &#125;&#125;) 调用微信扫一扫能力，只需要：12345wx.scanCode(&#123; success: (res) =&gt; &#123; console.log(res) &#125;&#125;) 需要注意的是：多数 API 的回调都是异步，你需要处理好代码逻辑的异步问题。 更多的 API 能力见 小程序的API。 不重复造轮子一些微信小程序项目的开源代码。可以进行代码参考和开发逻辑的学习。 一些开源Demohttps://github.com/caochangkui/miniprogram-project1：仿豆瓣电影微信小程序2：微信小程序移动端商城3：Gank微信小程序4：微信小程序高仿QQ应用5：微信中的知乎6：实现一个移动端小商城7：微信小程序demo8： 跑步微信小程序Demo9：简单的v2ex微信小程序10：腾讯云微信小程序11：微信小程序-微票12：微信小程序demo 仿手机淘宝13：一个为微信小程序开发准备的基础骨架14：巴爷微信商城的简单版本15：微信小程序 - 电影推荐16：微信小程序-知乎日报17：微信小程序： 音乐播放器18：使用微信小程序实现分答这款APP的基础功能19：微信小程序开发demo-地图定位20：微信小程序 - 豆瓣电影21：wepy仿微信聊天界面22：仿 「ONE · 一个」 的微信小程序23：微信小程序集成Redux实现的Todo list24： 基于Zhihu Live数据的微信小程序25：微信小程序之小熊の日记26：仿网易云音乐APP的微信小程序27：微信小程序的Flex布局demo28：番茄时钟微信小程序版29：Wafer 服务端 Demo30：微信小程序版聊天室31：微信小程序版简易计算器，适合入门练手32：微信小程序示例一笔到底33：基于面包旅行 API 制作的微信小程序示例34：新闻阅读器35：一个简单的微信小程序购物车DEMO36：微信小程序-公众号热门文章信息流37：通过Node.js实现的妹子照片爬虫微信小程序38：从FlexLayout布局开始学习微信小程序39：HiApp 微信小程序版40：微信小程序的简单尝试41：集美大学图书馆的便捷工具42：微信小程序版妹纸图43：V2ex 微信小程序版44：微信小程序仿百思不得姐45：微信小程序音乐播放器应用46：医药网原生APP的微信小程序DEMO47：微信小程序跟读48：微信小程序瀑布流布局模式49：微信小程序HotApp云笔记50：小程序模仿——网易云音乐51：微信小程序商城demo52：微信小程序版的扫雷53：专注管理时间的微信小程序54：微信小程序版干货集中营55：英雄联盟(LOL)战绩查询56：微信小程序首字母排序选择表57：微信小程序版豆瓣电影58：简单的实现了1024的游戏规则59：微信小程序试玩60：微信小程序逗乐61：一步步开发微信小程序62：一个 meteor 的 React todo list 例子63：微信小程序健康菜谱64： jspapa微信小程序版本65：微信小程序版的CNodeJs中文社区66：LeanCloud 的微信小程序用户登陆Demo67： 微笑话微信小程序68：微信小程序开发的App69：体育新闻微信小程序70：基于Labrador和mobx构建的小程序开发demo]]></content>
      <categories>
        <category>编程拾慧</category>
        <category>miniprogram</category>
      </categories>
      <tags>
        <tag>微信小程序</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python包-time]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-02.python%2FPython-%E5%8C%85-time%2F</url>
    <content type="text"><![CDATA[Python中的时间间隔是以秒为单位的浮点小数。每个时间戳都以自从1970年1月1日午夜（历元）经过了多长时间来表示。时间戳单位最适于做日期运算。但是1970年之前的日期就无法以此表示了。太遥远的日期也不行，UNIX和Windows只支持到2038年。 常见用法 获取当前时间 123$ localtime = time.localtime(time.time())$ print "本地时间为 :", localtime本地时间为 : time.struct_time(tm_year=2016, tm_mon=4, tm_mday=7, tm_hour=10, tm_min=3, tm_sec=27, tm_wday=3, tm_yday=98, tm_isdst=0) 格式化时间 12345$ import time$ localtime = time.asctime( time.localtime(time.time()) )$ print "本地时间为 :", localtime本地时间为 : Thu Apr 7 10:05:21 2016]]></content>
      <categories>
        <category>编程拾慧</category>
        <category>python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python包-sqlite3]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-02.python%2FPython-%E5%8C%85-sqlite3%2F</url>
    <content type="text"><![CDATA[官方文档sqlite3 Documentation 123456789101112131415161718import sqlite3con = sqlite3.connect(&apos;example.db&apos;)cur = con.cursor()# Create tablecur.execute(&apos;&apos;&apos;CREATE TABLE stocks (date text, trans text, symbol text, qty real, price real)&apos;&apos;&apos;)# Insert a row of datacur.execute(&quot;INSERT INTO stocks VALUES (&apos;2006-01-05&apos;,&apos;BUY&apos;,&apos;RHAT&apos;,100,35.14)&quot;)# Save (commit) the changescon.commit()# We can also close the connection if we are done with it.# Just be sure any changes have been committed or they will be lost.con.close()]]></content>
      <categories>
        <category>编程拾慧</category>
        <category>python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python包-PyMongo]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-02.python%2FPython-%E5%8C%85-PyMongo%2F</url>
    <content type="text"><![CDATA[官方文档PyMongo 4.1.1 Documentation 仍然需要启动MongoDB的服务。不适用跨集群的迁移]]></content>
      <categories>
        <category>编程拾慧</category>
        <category>python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Database-TCGA]]></title>
    <url>%2F01.%E7%9F%A5%E8%AF%86-04.Database%2FDatabase-TCGA%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[pipeline-Serverless_Cloud_Function-入门简介及测试]]></title>
    <url>%2F01.%E7%9F%A5%E8%AF%86-06.cloud_computing%2Fpipeline-SCF-Hello_world%2F</url>
    <content type="text"><![CDATA[测试整体测试过程参考 腾讯云-官方文档 ；腾讯云-控制台 ，进行函数的创建、和函数的配置及触发器配置。测试demo的代码编辑(https://console.cloud.tencent.com/scf/list-detail?rid=1&amp;ns=default&amp;id=helloworld-1650789454&amp;tab=codeTab) 触发服务访问控制台中配置的触发api，启动函数服务。 直接在浏览器地址输入链接，调用服务helloworld-1650789454 在Linxu集群通过访问链接可以调用服务]]></content>
      <categories>
        <category>pipeline</category>
        <category>SCF</category>
      </categories>
      <tags>
        <tag>流程开发</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pipeline-Serverless_Cloud_Function-入门简介及测试]]></title>
    <url>%2F01.%E7%9F%A5%E8%AF%86-06.cloud_computing%2Fpipeline-SCF-%E5%85%A5%E9%97%A8%E7%AE%80%E4%BB%8B%E5%8F%8A%E6%B5%8B%E8%AF%95%2F</url>
    <content type="text"><![CDATA[基本概念无服务器无服务器架构说法的来源可以根据 Mike Roberts 在 Martin Fowler 的博客网站上发表的 无服务器架构 一文中得到解释。 无服务器并不是没有服务器就能够进行计算，而是对于开发者来说，无需了解底层的服务器情况，也能使用到相关资源，因此称为无服务器。 无服务器也可以从更广的角度来识别，针对无需配置和了解底层的服务器就可以直接使用的云服务，在一定程度上也可以称为无服务器。 在云函数 SCF 产品中，我们针对的是无服务器场景中的计算场景。云函数产品提供的是无服务器模式下的 FaaS 能力。 函数即服务函数即服务提供了一种直接在云上运行无状态的、短暂的、由事件触发的代码的能力。 函数即服务和传统应用架构不同，函数服务提供的是事件触发式的运行方式，云函数不是始终运行的状态，而是在事件发生时由事件触发运行，并且在一次运行的过程中处理这一次事件。因此在云函数的代码中，仅需考虑针对一个事件的处理流程，而针对大量事件的高并发处理，由平台实现云函数的多实例并发来支持。 为了实现对高并发的支持，云函数平台提供了自动的弹性伸缩能力，会在有大量请求到来时启动更多实例来处理事件请求，也会在没有事件到来时缩减函数实例甚至到零实例。因此为了匹配自动扩缩能力，需要函数代码使用的是无状态开发方式，即不在云函数的运行内存中保留相关的状态数据并在多次运行时依赖这些状态数据。云函数的状态数据，可以依赖外部的持久存储能力例如云缓存、云数据库、云存储来进行。 触发器和触发源任何可以产生事件，触发云函数执行的均可以被称为触发器或触发源。触发器在本身产生事件后，通过将事件传递给云函数来触发函数运行。 触发器在触发函数时，可以根据自身特点，使用同步或异步方式触发函数。同步方式触发函数时，触发器将等待函数执行完成并获取到函数执行结果；异步方式触发函数时，触发器将仅触发函数而忽略函数执行结果。 腾讯云云函数在和腾讯云的某些产品或服务对接时，也有自身实现的一些特殊方式，例如推（PUSH）模式和拉（PULL）模式。 推模式：触发器主动将事件推送至云函数平台并触发函数运行。 拉模式：云函数平台通过拉取模块，从触发器中拉取到事件并触发云函数运行。 触发事件触发器在触发函数时会将事件传递给云函数。事件在传递时以一个特定的数据结构体现，数据结构格式在传递时均为 JSON 格式，并以函数 event 入参的方式传递给云函数。 触发事件的 JSON 数据内容，在不同的语言环境下将会转换为各自语言的数据结构或对象，无需在代码中自行进行从 JSON 结构到数据结构的转换。例如，在 Python 环境中，JSON 数据内容会转变为一个复杂 dict 对象，即函数的入参 event 就是一个 Python 的复杂 dict 对象。而在 Golang 或 Java 中，入参是一个需要和 event 数据结构可以匹配的对象。更具体的实现方式可以见 开发语言说明。 工作原理函数运行时的实例模型云函数 SCF 将在函数接收到触发请求时为您执行函数。SCF 执行请求的资源为实例，根据函数的配置信息（如内存大小等）进行资源分配，并启动一个或多个实例处理函数请求。SCF 平台负责所有函数运行实例的创建、管理和删除清理操作，用户没有权限对其进行管理。 Serverless 的价值首先，从开发者使用的来说，不用更多的去考虑服务器的相关内容，无需再去考虑服务器的规格大小、存储类型、网络带宽、自动扩缩容问题；同时，也无需再对服务器进行运维了，无需不断的打系统补丁、应用补丁、无需进行数据备份、软件配置等工作了。 其次，Serverless 产品是完全自动化的弹性扩缩容的；在业务高峰时，产品的计算能力、容量自动扩容，承载更多的用户请求，而在业务下降时，所使用的资源也会同时收缩，避免资源浪费。 再次，跟随着完全自动化的弹性所带来的，是全新的计量计费模式；开发者仅需根据使用量来付费，而在深夜无业务量的情况下，不会有空闲资源占用，因此也不会有费用产生。 1、降低运维需求： Serverless 使得应用与服务器解耦，业务上线前无需预估资源，无需进行服务器购买、配置； Serverless 也使得底层运维工作量进一步降低，业务上线后，也无需担忧服务器运维，而是全部交给了云平台或云厂商； 2、降低运营成本： Serverless 的应用是按需执行的。应用只在有请求需要处理或者事件触发时才会被加载运行，在空闲状态下 Serverless 架构的应用本身并不占用计算资源； 而在使用 Serverless 产品时，用户只需要为处理请求的计算资源付费，而无须为应用空闲时段的资源占用付费； 3、缩短迭代周期、上线时间： Serverless 架构带来的是进一步的业务解耦，应用功能被解构成若干个细颗粒度的无状态函数，开发可以聚焦在单功能的快速开发和上线上； 同时拆解后的云函数，也都可以进行独立的迭代升级，更快速的实现业务迭代，缩减功能的上市时间； 4、快速试错 利用 Serverless 架构的简单运维、低成本及快速上线能力，可以来快速尝试业务的新形态、新功能； 利用 Serverless 产品的强弹性扩容能力，在业务获得成功时，也无需为资源扩容而担心； Serverless 的技术特点这里提到的技术特点的对象，特指 Serverless 产品中的计算产品，也就是云函数。云函数包含了如下的技术特性： 事件驱动 云函数的运行，是由事件驱动起来的，在有事件到来时，云函数会启动运行 Serverless 应用不会类似于原有的监听-处理类型的应用一直在线，而是按需启动 事件的定义可以很丰富，一次 http 请求，一个文件上传，一次数据库条目修改，一条消息发送，都可以定义为事件 单事件处理 云函数由事件触发，而触发启动的一个云函数实例，一次仅处理一个事件 无需在代码内考虑高并发高可靠性，代码可以专注于业务，开发更简单 通过云函数实例的高并发能力，实现业务高并发 自动弹性伸缩 由于云函数事件驱动及单事件处理的特性，云函数通过自动的伸缩来支持业务的高并发 针对业务的实际事件或请求数，云函数自动弹性合适的处理实例来承载实际业务量 在没有事件或请求时，无实例运行，不占用资源 无状态开发 云函数运行时根据业务弹性，可能伸缩到 0，无法在运行环境中保存状态数据 分布式应用开发中，均需要保持应用的无状态，以便于水平伸缩 可以利用外部服务、产品，例如数据库或缓存，实现状态数据的保存 Serverless 的应用场景Serverless 架构或者技术，可以用在什么样的场景下，来充分发挥它的优势呢？如下的场景，都适合使用 Serverless 架构或产品，来实现所需的业务逻辑。 WEB 及移动后端 通过结合使用云函数和 API 网关或 HTTP 触发器，可以对外提供 URL 访问地址，成为 Web、小程序、或移动应用等的后端服务。Serverless 架构既可以直接用于构建后台来服务应用，也可以通过类似 BFF 模式，构建中台和应用间的桥梁。 Serverless 架构提供的强弹性能力，使得可以支撑业务或应用的暴涨；而提供的低运维需求，使得开发者可以专注于业务实现和优化；同时，按实际使用量的付费方式，使得开发者无需预配置资源，无需担心预配置资源的浪费。 消息处理 Serverles 架构的应用本身是由事件触发的，因此极其适合于进行消息处理。无论是消息队列中传递的业务消息，还是 Kafka 中采集应用日志，均可以对接到云函数上，进行实时的消息处理、分析。 对象存储文件处理 在 Serverless 应用场景中，由对象存储中的文件上传事件，来触发云函数的运行，也是一种常见场景。针对图片文件的上传，可以借助云函数完成图片的缩略图生成、二维码或水印标记、图片优化处理；而针对数据文件的上传，可以启动数据的自动化分析， 物联网 物联网意味着成千上万的设备会连入网络，时刻在不断的产生数据，这对数据的分析、处理的及时性提出了很高的挑战。通过使用 Serverless 架构，物联网设备所采集的数据将可以作为云函数的触发事件，而实现数据的实时处理、分析和应用。 随着物联网设备计算能力的进一步提升，云函数作为最小粒度的计算单元，有机会被调度到设备端运行，实现边缘计算，达到端-云联合的 Serverless 架构。 运维及集成 通过对接云函数以及云上的各个产品、日志服务、监控告警系统，云时代的运维也都可以用云函数来构建。定时触发的云函数，将可以方便的替代需要在主机上来运行的定时任务；而日志或告警触发的云函数，将可以对云中的事件作出立刻回应及处理。 Serverless - 云原生时代的应用架构云计算已经进入了新的时期，目前上云已经不是应不应该，而是如何上云的问题。在这种情况下，云原生的概念也随之而生。云原生的架构或应用，是基于云而设计的，充分的利用现代云计算平台所具备的弹性和分布式特性来实现应用架构。 而 Serverless 架构、产品、以及应用，均是完全依托于云而构建的，是典型且完全的云原生的架构、产品或应用。 Serverless 产品所具备的产品特性优势、技术优势、费用优势，将成为新一代云产品的发展方向；而基于 Serverless 架构推进完成开发的应用或架构，将充分享受到云时代带来的强大助力，使得云计算能真正成为业务的助推器。 Serverless 的计算产品-云函数，作为云虚拟机、容器技术之后的下一代计算形态，将引来云计算中新的热潮。围绕着云函数而建设的产品能力、工具、生态、以及应用开发，也将引来新的一轮发展。随着无服务器的产品和生态走向成熟，将逐步承载起企业核心业务。 在这个持续向前高速发展的过程中，腾讯云的云函数，将作为腾讯云云原生的重点发力领域，跟随客户需求、行业发展、技术发展，为用户提供完整全套的 Serverless 解决方案。]]></content>
      <categories>
        <category>pipeline</category>
        <category>SCF</category>
      </categories>
      <tags>
        <tag>流程开发</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux-配置文件-vim配置]]></title>
    <url>%2F03.%E5%BA%94%E7%94%A8-02.Linux%E5%92%8Cshell%2FLinux-%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6-vimrc%E8%AE%BE%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[Vimdiff背景颜色太重无法看清代码方案11234567891011121314set t_Co=256if ! has("gui_running") set t_Co=256 endifif &amp;diff colors blueendifset nuset tabstop=4set history=100set icset syntax=cset autoindentsyntax on 示例如下：ps: .vimrc的注释标记符为半角双引号，在本代码中可以看到&quot; colors delek 是用半角双引号注释掉的。 方案2123456if &amp;diff hi DiffAdd cterm=bold ctermfg=12 guibg=LightBlue hi DiffDelete cterm=bold ctermfg=13 ctermbg=14 gui=bold guifg=blue guibg=LightCyan hi DiffChange cterm=bold ctermbg=green ctermfg=15 guibg=Magenta hi DiffText term=reverse cterm=bold ctermfg=9 gui=bold guibg=Redendif 示例如下：]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>vim</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[文件格式说明 - yaml]]></title>
    <url>%2F01.%E7%9F%A5%E8%AF%86-03.standard%2Ffileformat-yaml%2F</url>
    <content type="text"><![CDATA[官方资料https://www.jianshu.com/p/413576dc837ehttps://yaml.org/spec/1.2.2/https://www.redhat.com/en/topics/automation/what-is-yamlhttps://www.runoob.com/w3cnote/yaml-intro.html yaml格式规范YAML 与 XML、JSON YAML 与 XML 具有 XML 同样的优点，但比 XML 更加简单、敏捷等 YAML 与 JSON JSON 可以看作是 YAML 的子集，也就是说 JSON 能够做的事情，YAML 也能够做 YAML 能表示得比 JSON 更加简单和阅读，例如“字符串不需要引号”。所以 YAML 容易可以写成 JSON 的格式，但并不建议这种做 YAML 能够描述比 JSON 更加复杂的结构，例如“关系锚点”可以表示数据引用（如重复数据的引用）。 YAML 编写规范它的基本语法规则如下： 12341）大小写敏感2）使用缩进表示层级关系3）缩进时不允许使用Tab键，只允许使用空格。4）缩进的空格数目不重要，只要相同层级的元素左侧对齐即可 规范一：文档使用 Unicode 编码作为字符标准编码，例如 UTF-8规范二：使用“#”来表示注释内容1234567# 客户订单date: 2015-02-01customer: - name: Jaiitems: - no: 1234 # 订单号 - descript: cpu 规范三：使用空格作为嵌套缩进工具。通常建议使用两个空格缩进，不建议使用 tab （甚至不支持） 规范四：序列表示 使用“-”（横线） + 单个空格表示单个列表项 123--- # 文档开始- 第一章 简介- 第二章 设计目录 使用”[]”表示一组数据 12--- # 文档开始[blue, red, green] 组合表示。每个结构都可以嵌套组成复杂的表示结构。 1234--- # 文档开始- [blue, red, green] # 列表项本身也是一个列表- [Age, Bag]- site: &#123;osc:www.oschina.net, baidu: www.baidu.com&#125; # 这里是同 键值表 组合表示 规范五：键值表 使用 “:”（冒号） + 空格表示单个键值对 12345678# 客户订单date: 2015-02-01customer: - name: Jaiitems: - no: 1234 # 订单号 - descript: cpu - price: ￥800.00 使用”{}”表示一个键值表 12345# 客户订单date: 2015-02-01customer: - name: Jaiitems: &#123;no: 1234, descript: cpu, price: ￥800.00&#125; “? “ 问号+空格表示复杂的键。当键是一个列表或键值表时，就需要使用本符号来标记。 1234567# 使用一个列表作为键? [blue, reg, green]: Color# 等价于? - blue - reg - gree: Color 组合表示。每个结构都可以嵌套组成复杂的表示结构。 123456789101112131415161718192021222324 Color: - blue - red - green# 相当于 (也是 JSON 的表示)&#123;Color: [blue, red, green]&#125;div: - border: &#123;color: red, width: 2px&#125; - background: &#123;color: green&#125; - padding: [0, 10px, 0, 10px]# 使用缩进表示的键值表与列表项items: - item: cpu model: i3 price: ￥800.00 - item: HD model: WD price: ￥450.00# 上面使用 “-” 前导与缩进来表示多个列表项，相当于下面的JSON表示items: [&#123;item:cpu, model:i3, price:￥800.00&#125;, &#123;item:HD, model:WD, price: ￥450.00&#125;] 规范六：文本块 使用 “|” 和文本内容缩进表示的块：保留块中已有的回车换行。相当于段落块 12yaml: | # 注意 &quot;:&quot; 与 &quot;|&quot; 之间的空格 JSON的语法其实是YAML的子集，大部分的JSON文件都可以被YAML的解释器解释。 使用 “&gt;” 和文本内容缩进表示的块：将块中回车替换为空格，最终连接成一行。 123yaml: &gt; # 注意 &quot;:&quot; 与 &quot;&gt;&quot; 之间的空格，另外可以使用空行来分段落 JSON的语法其实是YAML的子集， 大部分的JSON文件都可以被YAML的解释器解释。 使用定界符“”（双引号）、‘’（单引号）或回车表示的块：最终表示成一行。 1234567yaml: # 使用回车的多行，最终连接成一行。 JSON的语法其实是YAML的子集， 大部分的JSON文件都可以被YAML的解释器解释。yaml: # 使用了双引号，双引号的好处是可以转义，即在里面可以使用特殊符号 &quot;JSON的语法其实是YAML的子集， 大部分的JSON文件都可以被YAML的解释器解释。&quot; 使用 “—“ 来分割一个文本文件中的多个yaml配置 12345678910111213141516​spring:profiles:#激活开发环境active: dev---#开发环境配置spring:profiles: devserver:port: 8080---#生产环境配置spring:profiles: prodserver:port: 8082 使用 “…” 来表示一个yaml文件的结束 虽然在大多数yaml解析的情况下，结束符可以省略，但是通过添加结束符可以在数据读取过程中明确文件被完整读取。123456789101112#开发环境配置spring:profiles: devserver:port: 8080---#生产环境配置spring:profiles: prodserver:port: 8082... # yaml文件结束符 规范七：数据类型的约定 对一些常用数据类型的表示格式进行了约定，包括: 1234567891011121314151617integer: 12345 # 整数标准形式octal: 0o34 # 八进制表示，第二个是字母 ohex: 0xFF # 十六进制表示float: 1.23e+3 # 浮点数fixed: 13.67 # 固定小数minmin: -.inf # 表示负无穷notNumber: .NaN # 无效数字null: # 空值boolean: [true, false] # 布尔值string: &apos;12345&apos; # 字符串date: 2015-08-23 # 日期datetime: 2015-08-23T02:02:00.1z # 日期时间iso8601: 2015-08-23t21:59:43.10-05:00 # iso8601 日期格式spaced: 2015-08-23 21:59:43.10 -5 # ? “!”（叹号）显式指示类型，或自定义类型标识。单叹号通常是自定义类型，双叹号是内置类型 123456789101112131415161718192021222324252627282930313233343536373839404142434445isString: !!str 2015-08-23 # 强调是字符串不是日期数据 picture: !!binary | # Base64 图片 R0lGODlhDAAMAIQAAP//9/X 17unp5WZmZgAAAOfn515eXv Pz7Y6OjuDg4J+fn5OTk6enp 56enmleECcgggoBADs= #下面是内置类型 !!int # 整数类型 !!float # 浮点类型 !!bool # 布尔类型 !!str # 字符串类型 !!binary # 也是字符串类型 !!timestamp # 日期时间类型 !!null # 空值 !!set # 集合 !!omap, !!pairs # 键值列表或对象列表 !!seq # 序列，也是列表 !!map # 键值表 #下面是一些例子： --- !!omap - Mark: 65 - Sammy: 63 - Key: 58 --- !!set # 注意，“?”表示键为列表，在这里列表为 null ? Mark ? Sammy ? Key # 下面是自定义的类型或标识 %TAG ! tag:clarkevans.com,2002: # % 是指令符号 --- !shape # Use the ! handle for presenting # tag:clarkevans.com,2002:circle - !circle center: &amp;ORIGIN &#123;x: 73, y: 129&#125; radius: 7 - !line start: *ORIGIN finish: &#123; x: 89, y: 102 &#125; - !label start: *ORIGIN color: 0xFFEEBB text: Pretty vector drawing. 规范八：锚点与引用，定义数据的复用。 第一步：使用 “&amp;” 定义数据锚点（即要复制的数据） 第二步：使用 “*” 引用上述锚点数据（即数据的复制目的地） 12345678---hr: - Mark McGwire # Following node labeled SS - &amp;SS Sammy Sosa # 定义要复制的数据rbi: - *SS # Subsequent occurrence 这里是数据复制目标 - Ken Griffey yaml 格式相关api/包python yaml123import yamlConfig = yaml.load(open(self.driver_yaml), Loader=yaml.FullLoader) # 读取配置文件Config[&apos;vep2BGI&apos;] # 获取配置文件相关信息]]></content>
      <categories>
        <category>知识沉淀</category>
        <category>文件格式</category>
      </categories>
      <tags>
        <tag>文件格式</tag>
        <tag>yaml</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python-包-scikit-learn]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-02.python%2FPython-%E5%8C%85-sklearn%2F</url>
    <content type="text"><![CDATA[官方资料scikit-learnscikit-learn中文社区scikit-learn Git skleran安装使用pip安装，terminal直接执行即可1pip install -U scikit-learn 使用Anaconda安装，推荐Anaconda，因为里面已经内置了NumPy，SciPy等常用工具1conda install scikit-learn 安装完成后可以在python中检查一下版本，import sklearn不报错，则表示安装成功，我这里用的是老版本了，基本功能差不多。123&gt;&gt;import sklearn&gt;&gt;sklearn.__version__&apos;0.19.1&apos;]]></content>
      <categories>
        <category>编程拾慧</category>
        <category>python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Use of synthetic DNA spike-in controls (sequins) for human genome sequencing]]></title>
    <url>%2F01.%E7%9F%A5%E8%AF%86-05.Paper%2FPMID31217595-Use_of_synthetic_DNA_spike-in_controls_for_human_genome_sequencing%2F</url>
    <content type="text"><![CDATA[文献全文%20for%20human%20genome%20sequencing.pdf) 二代测序(NGS)已被广泛应用于识别基因变异及其相关性研究与疾病。然而，由于人类的复杂性，测序数据的分析仍然具有挑战性文库制备、测序和分析过程中引入的遗传变异和混淆错误。本文比较开创性的开发了一种方法，synthetic DNA spike-ins—termed ‘sequins’ (sequencing spike-ins)，直接添加到DNA中样品库准备前。sequins可以用来测量技术偏差，并作为内部定量以及整个流程的定性控制。sequins可以用于全基因组测序和靶向测序。在实验开始阶段将sequins添加到人类DNA样本中，然后按照生物信息学步骤分离sequins和样本衍生的测序reads，并评估检测的诊断性能。 由于sequins是参考人类 DNA 序列合成的镜像序列，因此可以轻松的和人类基因组序列进行区分，同时由于序列的镜像，可以最大可能的保证分子动力学的相似性，使得sequins可以模拟在整个实验中，各个环节引入的错误。]]></content>
      <categories>
        <category>NGS</category>
        <category>文献</category>
      </categories>
      <tags>
        <tag>变异检测</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Enhanced detection of minimal residual disease by targeted sequencing of phased variants in circulating tumor DNA]]></title>
    <url>%2F01.%E7%9F%A5%E8%AF%86-05.Paper%2FPMC8678141-Enhanced_detection_of_minimal_residual_disease_by_targeted_sequencing_of_phased_variants_in_circulating_tumor_DNA%2F</url>
    <content type="text"><![CDATA[文献链接文章线上链接文章原文 文献主题内容最近的方法已经通过追踪多个具有错误抑制测序的体细胞突变来改善ctDNA MRD性能。这种方法允许使用现成的面板或个性化的检测方法对有限的cfDNA进行检测，检测限度低至2-10 / 100000。部分参考方法如下： Integrated digital error suppression for improved detection of circulating tumor DNA Phylogenetic ctDNA analysis depicts early stage lung cancer evolution An ultrasensitive method for quantitating circulating tumor DNA with broad patient coverage 先前旨在降低LOD的方法集中于在亲本DNA双链的互补链上检测到的体细胞变异。“Duplex sequencing”由于需要两个一致的测序结果，所以有效的降低了单个核苷酸变异(SNV)检测的错误率。在之前的研究中，利用双序列对ctDNA进行分析，分析LOD低至400000个分子。然而，这种方法能通过DNA双链恢复的效率非常低，只有少数人(通常为20-25%)能恢复原模板链。这种低效率使得双工测序在实际ctDNA检测中不是最理想的，因为实际血液容量的输入DNA有限(每标准10ml采血管约4000 - 8000个基因组)，最大限度的基因组恢复是至关重要的。为了改进MRD检测，仍然需要同时实现低分析检测限和高分子回收率的方法来检测多个突变 Integrated digital error suppression for improved detection of circulating tumor DNA Detection of ultra-rare mutations by next-generation sequencing Detecting ultralow-frequency mutations by Duplex Sequencing 作为一种降低背景错误率的双相测序的替代方法，涉及到检测 顺式的变异体，即在顺式中发生两个或多个突变(如图1a所示，扩展数据图1所示)。与双相测序类似，由于对单个分子中两个独立的非参考事件的一致检测，该方法提供了更低的误差剖面。 芯片设计We identified and summarized the frequency of these ‘putative phased variants,’ (Methods) controlling for the total number of SNVs, from 2538 tumors across 24 cancer histologies including solid tumors and hematological malignancies (Fig 1b, Extended Data Fig 2, Table S1). Interestingly, PVs were most significantly enriched in two B-cell lymphomas (DLBCL and follicular lymphoma, FL, P&lt;0.05 vs all other histologies).各癌种下PVs变异比例情况如下图b： 可以看到在DLBCL中，存在PVs的比例整体较高，同事研究也发现，这部分变异在基因组上存在一定的富集，所以使用了79例样本进行PVs的富集，进行定制化的探针设计针对PVs富集区域进行设计； This final Phased variant Enrichment and Detection Sequencing (PhasED-Seq) panel targeted ~115kb of genomic space focused on PVs, along with an additional ~200kb targeting genes known to be recurrently mutated in B-NHLs; this single panel was used for both identification of PVs from tumor and/or plasma samples and tracking residual disease (Fig 2b). While the 115kb of space dedicated to PV-capture targets only 0.0035% of the human genome, it captures 26% of phased variants observed by WGS (Extended Data Fig 4a), yielding a ~7500-fold enrichment over WGS. 性能验证片段含有多个突变对杂交捕获的影响When subjecting an equimolar mixture of these oligonucleotides to capture and sequencing, molecules with as high as 5% mutation rate were captured with nearly the same efficiency as wildtype counterparts (85% vs 100%), while molecules with 10% mutations were captured with only 27% relative efficiency (Fig 4a). Notably, only 7% of cases had any region exceeding 10% mutation frequency across the panel (Methods, Extended Data Fig 5b–c), and in all cases the 90th percentile mutation rate was &lt;5%, suggesting the majority of phased mutations are recoverable by hybrid capture. 性能效率上When considering unique molecular depth, duplex sequencing recovered only 19% of all unique cfDNA fragments (Fig 4c). In contrast, the unique depth of reads covering PVs within a genomic distance of &lt;20bp was nearly identical to the overall sample depth. Similarly, PVs up to 80bps in size had depth greater than 50% of the median unique depth for a sample. Importantly, almost half (48%) of all PVs were less than 80bp in length (Fig 4d). 准确性所有方法在0.01%(1 part in 10,000)的水平上检测性能类似，但是在更低的水平 (e.g., 0.001%, 0.0002%, 0.0001%, and 0.00005%), PhasED-Seq and duplex sequencing 的变现明显优于 single-strand UMI based SNV (P&lt;0.0001 for duplex, ‘2x’ PhasED-Seq, and ‘3x’ PhasED-Seq; Fig 5a) 在几个方法对比过程中，PhasED-Seq也表现除最低的错误率水平 (Fig 5b). PVs中表现的更低的错误率，也可以提高ctDNA的检测限。]]></content>
      <categories>
        <category>NGS</category>
        <category>文献</category>
      </categories>
      <tags>
        <tag>MRD</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[肿瘤临床检测中的点突变（SNV&InDel）变异检测方案调研]]></title>
    <url>%2F01.%E7%9F%A5%E8%AF%86-01.NGS%2FNGS-%E7%82%B9%E7%AA%81%E5%8F%98%E5%8F%98%E5%BC%82%E6%A3%80%E6%B5%8B%E6%96%B9%E6%A1%88%E8%B0%83%E7%A0%94%2F</url>
    <content type="text"><![CDATA[变异检测变异过滤一些可以获取的临床变异检测资料信息中检院TMB标准化项目采用多个变异检测软件进行交叉验证，而后进行人工审核确认。 全国实体肿瘤体细胞突变 高通量测序（大 Panel）检测借助质评自身的特殊性，通过多家检测机构的结果进行较差验证，按着特定标准筛选获得标准答案集合。 基于IGV的人工审核针对如果对变异结果进行人工审核，2019年有一篇发表在GENETICS IN MEDICINE (IF 9.108)的文章，可以为我们提供一些参考。Standard operating procedure for somatic variant refinement of sequencing data with paired tumor and normal samples其中针对突变审核包含如下过程： 步骤1：把突变可视化 使用IGV观察突变；确保IGV与插件中突变选取保持一致；通过插件的“S”使突变发生reads置顶；确认IGV、插件的reads展示一致； 步骤2：确认支持突变的数量 查看突变的①链方向、总覆盖度、②位点的突变频率、③非突变频率；考虑突变受到的其他因素影响，比如原发性肿瘤 DNA、复发性DNA、肿瘤RNA等； 步骤3：确认支持突变的质量 寻找多个不匹配或者与ref高度差异性的区域；寻找 半透明或透明的reads或碱基(比对质量低的)；①对有疑问reads进一步确认比对质量、碱基质量；②看突变对应的normal检出情况； 步骤4：检查测序误差 ①切换“成对查看”确认短插入片段情况；②IGV缩小确认是否在高度差异性区域，临近区域是否有indel存在；③看参考序列是否在低复杂性区域，是否存在串列重复序列； 步骤5：给突变选择符合哪个Call标签 通过突变质量、突变数量的信息，选择合适的Call标签； 步骤6：给突变选择符合哪个Tag标签 可以对每个突变进行tag标签标记，对Call标记 不确定、失败的突变，尤其重要； 步骤7：给突变写附加的备注信息 其中提供了19中tag，作为变异过滤过程中参考判断的指标 燃石检测流程实际比较难获得泛生子内部资料，只能借助一些发布文章的method，这也往往是一个非常有效的办法。在燃石的官网查到燃石的学术发表找到其中一篇文章从题目看是和NGS检测为主的Evaluating the analytical validity of circulating tumor DNA sequencing assays for precision oncology从文章的method中，我们可以看到泛生子流程的整体检测逻辑如下： 制作模拟数据 通过wgsim (v1.9) 进行数据模拟 变异的分析流程 首先对下机数据进行Trim TrimGalore 使用anaquin 进行sequin（spike-in controls）分析, 将sequin的Reads进行分离，然后矫正到样本相同的深度，并通过Anaquin somatic 进行变异检测。详细见参考文献Use of synthetic DNA spike-in controls (sequins) for human genome sequencing bwa mem (v0.7.16) 进行数据比对，比对到Hg38 剔除捕获区间外的Reads，并用gatk MarkDuplicates (v4.0). 标记重复 VarScan (v2.4.3) 用来进行 SNVs and indels 的检测（最少支持数是3条read-fragments) 泛生子检测流程介绍在泛生子官网查找泛生子的成果,2016:The genome-wide mutational landscape of pituitary adenomas的附件中提到了分析流程 2019:Detection of early-stage hepatocellular carcinoma in asymptomatic HBsAg-seropositive individuals by liquid biopsy 详细分析方法参考文档中提及的方法材料： Sequencing reads were primarily processed with our own program to extract tags and remove sequence adapters. Residual adapters and low-quality regions were subsequently removed using Trimmomatic (v0.36). The cleaned reads were mapped to the hg19 and HBV genomes using ‘bwa(v0.7.10) mem’ with the default parameters. Candidate somatic mutations, consisting of SNP and INDEL, were identified using samtools mpileup (9) across the targeted regions of interest. To ensure accuracy, reads with the same tags, and start and end coordinates were grouped into Unique Identifier families (UID families). UID families containing at least two reads and in which at least 80% of reads were the same type were defined as Effective Unique Identifier families (EUID families). Each mutation frequency was calculated by dividing the number of alternative EUID families by the sum of alternative and reference ones. The mutations were further manually reviewed in IGV. The candidate variations were annotated with Ensembl Variant Effect Predictor (VEP) (10). HBV integrations were identified using Crest (11) , and at least 4 soft-clip reads supports were needed. 世和参考文章Tumor-derived DNA from pleural effusion supernatant as a promising alternative to tumor tissue in genomic profiling of advanced lung cancer Sequencing and data processing Target enriched libraries were sequenced on the HiSeq4000 platform (Illumina) with 2×150bp pair-end reads. Sequencing data were demultiplexed by bcl2fastq (v2.19), analyzed by Trimmomatic 24 to remove low-quality (quality&lt;15) or N bases, mapped to the reference hg19 genome (Human Genome version 19) using the Burrows-Wheeler Aligner. PCR duplicates were removed by Picard. The Genome Analysis Toolkit (GATK) was used to perform local realignments around indels and base quality reassurance. SNPs and indels were called by VarScan2 and HaplotypeCaller/UnifiedGenotyper in GATK, with the mutant allele frequency (MAF) cutoff as 0.5% for tissue samples, 0.1% for liquid biopsy samples, and a minimum of three unique mutant reads. Common variants were removed using dbSNP and the 1000 Genome project. Germline mutations were filtered out by comparing to patient’s whole blood controls. The resulting somatic variants were further filtered through an in-house list of recurrent sequencing errors that was generated from over 10,000 normal control samples on the same sequencing platform. Gene fusions were identified by FACTERA. copy number variations (CNVs) were analyzed with ADTEx . The log2 ratio cut-off for copy number gain was defined as 2.0 for tissue samples and 1.6 for liquid biopsy samples. A log2 ratio cut-off of 0.67 was used for copy number loss detection in all sample types. The thresholds were determined from previous assay validation using the absolute CNVs detected by droplet digital PCR (ddPCR). Allele-specific CNVs were analyzed by FACETS 30 with a 0.2 drift cut-off for unstable joint segments. The proportion of chromosomal instability (CIN) was calculated by dividing the size of drifted segments by the total segment size. Tumor mutational burden (TMB) in this study was defined as the number of somatic synonymous mutations per megabase in each sample, with hotspot/fusion mutations excluded. 贝瑞参考文章Genetic profiling of primary and secondary tumors from patients with lung adenocarcinoma and bone metastases reveals targeted therapy options In brief, DNA extracted from FFPE tissue biopsies was fragmented to an average size of 300 bp, molecules were then end repaired and A-tailed and finally T tailed linkers were ligated on. The added linkers were a mix of 96 different molecular barcodes giving a high probability that each molecule was marked differently at both ends and thus uniquely barcoded. Libraries were amplified by PCR and resulting amplicons captured using biotinylated probes (120 nucleotides) for the 457 genes. Following elution, molecules were re-amplified using complementary sequencing primers and then paired end (PE) sequenced (2 × 150 bp) on the NovaSeq platform (Illumina). Fastq sequencing reads were aligned to the hg19 reference genome using the Burrows Wheeler algorithm (Li and Durbin 2009). The resulting SAM files were converted to BAM file format and then sorted on genome coordinates using Samtools. To remove PCR bias (reads with the same molecular barcodes and same start and same stop positions), only the unique coded molecules were used for copy number analysis. After filtering out low mapping quality reads (MAQ &lt; 20), the average depth of coverage (DoC) for each target was calculated using the GATK Depth Of Coverage algorithm (McKenna et al. 2010). After GC correction using LOESS regression method (Alkan et al. 2009), reads were normalized using the RPKM method (Chiang et al. 2019). For these steps, the tumor and matched normal sample was processed separately. Somatic SNVs and indels were finally identified by MutLoc (Berry Genomics in-house tools, unpublished), which maps the alternative base fraction compared to the hg19 reference genome.]]></content>
      <categories>
        <category>NGS</category>
        <category>变异检测</category>
      </categories>
      <tags>
        <tag>变异检测</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[经典文献清单]]></title>
    <url>%2F01.%E7%9F%A5%E8%AF%86-05.Paper%2FPaper_list%2F</url>
    <content type="text"><![CDATA[NGS-CDS检测参考Standard operating procedure for somatic variant refinement of sequencing data with paired tumor and normal samples Use of synthetic DNA spike-in controls (sequins) for human genome sequencing%20for%20human%20genome%20sequencing.pdf) MRD方向Enhanced detection of minimal residual disease by targeted sequencing of phased variants in circulating tumor DNA 早筛]]></content>
      <categories>
        <category>文献</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Circulating cell-free DNA for cancer early detection]]></title>
    <url>%2F01.%E7%9F%A5%E8%AF%86-05.Paper%2FPMC9133648-Circulating_cell-free_DNA_for_cancer_early_detection%2F</url>
    <content type="text"><![CDATA[摘要简介图形总结 cfDNA BIOLOGY cfDNA主要通过细胞凋亡和坏死释放，也可能通过活性分泌释放。 cfDNA的半衰期为16分钟至2.5小时。 在体内，cfDNA要么被核酸酶清除，然后被肾脏排泄到尿液中，要么被肝脏和脾脏吸收，随后被巨噬细胞降解。 体外分离cfDNA的保存温度不超过 -20 ℃，且不超过1次冻融循环，以防止cfDNA降解。 cfDNA片段的稳定性可能通过与细胞膜、细胞外囊泡或蛋白质的结合而提高。 cfDNA在正常情况下浓度通常较低(在100 ng/mL以内)，但在某些生理和病理条件下，如运动、炎症、糖尿病、、癌症等，cfDNA浓度会显著升高。健康个体中，cfDNA主要来自淋巴细胞，而在癌症患者中，来自癌症组织的cfDNA含量显著增加。 采用基于测序的方法测定cfDNA时，166bp的峰值显著，而肿瘤来源的cfDNA长度(约为144 bp)较正常cfDNA较短。 目前基于cfDNA进行肿瘤早筛的测序方法癌症早期检测的策略通常是基于检测癌细胞释放的cfDNA中与癌症相关的改变，即 ctDNA(circulating tumor DNA)。血浆中ctDNA的浓度较低，占cfDNA总浓度的不到0.01%，尤其是在早期癌症中，对癌症的早期检测提出了很大的挑战。目前，突变、甲基化和片段模式是cfDNA早期检测癌症的主要测序生物标志物。 Mutation-based sequencing approach and strategy基于突变的测序方法在早期癌症检测中的理想应用需要较低的检测下限(LOD)来区分在DNA提取、文库制备、靶标富集、杂交过程和测序本身过程中可能引入的背景噪声(如假阳性)，这使得检测真正的突变更加困难。一些背景噪声可以减轻，如在提取过程中氧化应激损伤引起的序列改变，碎片化和杂交过程。为了提高目标测序的分析灵敏度，独特分子标识符(uniquemolecular identifiers, UMIs)，即直接连接在文库DNA片段两端的独特分子条形码，被用于促进同一DNA片段序列的生物信息学比对。这种UMI策略可以帮助减少后续分析中的人为错误。深度测序(CAPP-Seq)方法结合double-umi进行癌症个性化分析时，cfDNA的LOD可达0.02%。 基于cfDNA突变的分析有几个局限性： 靶向测序的芯片设计主要基于现有的关于癌症基因改变的知识。 假阴性结果不仅在癌变突变超出定制检测范围时不可避免，而且血浆ctDNA浓度不足也可能导致假阴性结果。 ctDNA在晚期和转移癌患者中的浓度高于早期癌症患者，且随癌症类型的不同而不同。它在肝癌、胆道癌、食道癌和卵巢癌中的含量也较高，但在前列腺癌、乳腺癌和结肠癌中的含量相对较低此外，一些因素可能影响cfDNA浓度的变化，如年龄、体重指数、和生理参数基于突变的分析也受限于肿瘤肿块中最初出现的突变数量，这因癌症而异。黑色素瘤和肺癌平均有超过8.9个突变/Mbp，而低级别胶质瘤、乳腺癌、胰腺癌和前列腺癌平均有少于2.2个突变/Mbp此外，早期癌症患者的肿瘤突变负担比晚期癌症患者低，这使得利用基于突变的技术检测癌症更加困难。 假阳性结果可能发生在大量未患癌症的个体中，由于克隆造血(染色质免疫沉淀[CHIP])相关突变。在来自NSCLC和健康对照的cfDNA中检测到的变异中，58.0%和90.0%也在匹配的白细胞(WBC)中检测到，强调了对匹配的WBC DNA应用等效测序深度以排除CHIP干扰的重要性。 Methylation-based sequencing approach基于表观遗传改变的cfDNA测序方法被认为是有希望的癌症早期检测的替代方法。CpG位点甲基化是基因表达、组织分化、器官发育、衰老和肿瘤发生的重要表观遗传学调控机制。肿瘤特异性DNA甲基化的变化发生在肿瘤发生的早期，有时甚至在基因突变发生之前。先前的一项研究表明，在临床癌症诊断前四年，血浆中就检测到了甲基化的变化。此外，与只发生在少数基因组位置的典型癌症突变不同，近3000万个甲基化位点分布在人类基因组中，使它们无处不在，成为癌症检测的丰富信号。值得注意的是，甲基化模式通常是组织特异性的，这使得TOO(tissue of origin)分析成为可能。 全基因组亚硫酸氢盐测序(WGBS：Whole genomic bisulfite sequencing)是获取全基因组DNA甲基化图谱的金标准。然而，然而高昂的成本，输入DNA回收率低，测序深度有限，使其无法在临床应用。基于二代测序(NGS)的甲基化测序方法由于成本较低、测序深度较高，受到越来越多的关注。基于ngs的方法包括亚硫酸氢盐预处理(还原亚硫酸氢盐测序[RRBS])、酶切(甲基化敏感限制性内切酶测序)和亲和富集(甲基化DNA免疫沉淀和高通量测序)。亚硫酸氢盐转化后的DNA回收率低也是基于ngs方法的一个问题。cfMeDIP-Seq (cfMeDIP-Seq)是一种基于免疫沉淀的协议，采用传统的MeDIP-Seq，允许在无亚硫酸氢盐的过程中，低输入DNA的全基因组甲基化分析。 与需要100 - 2000 ng DNA输入的MeDIP-Seq、RRBS和WGBS相比，cfMeDIP-Seq在观察到的和预期的差异甲基化区域数量之间产生了近乎完美的线性关联(r2 = 0.99, p &lt;0.0001)，只有1 - 10ng的DNA输入。 近年来的技术进步，如methylBEAMing、单细胞RRBS和增强的线性分裂扩增测序(ELSA-Seq)，通过减少所需的DNA输入量和提高分析灵敏度来促进cfDNA甲基化测序的应用。例如，ELSA-Seq构建了一个单链文库，其甲基化覆盖深度高，扩增偏置小，可重复性极高，输入量可达500 pg。此外，相邻的CpG位点通常代表共甲基化状态，将多个基因组距离近、相关性高的CpG位点整合为甲基化单倍型区块，可进一步提高检测精度。 cfdna甲基化分析也有局限性： 癌症中存在的表观遗传改变也可能存在于其他非癌组织中，这可能导致假阳性。例如，食道癌和巴雷特食管有几个相似的甲基化改变。此外， 甲基化改变随着年龄的增长而积累，大约5%的CpG位点表现出与衰老和肿瘤发生共同的显著变化。同时， 当检测信号低于LOD时，可能会出现假阴性结果。同样，cfDNA的检出率与cfDNA的浓度有完整的相关性，而cfDNA的浓度也受癌症病理类型的影响。此外，这些 基于甲基化的测序方法可能无法检测主要由基因突变、体细胞拷贝数畸变或融合事件驱动的癌症，例如主要由EGFRL858R突变、EML4-ALK融合或ERBB2扩增驱动的几种肺癌亚型。 cfDNA fragmentation-based sequencing approachcfDNA的全基因组片段化为癌症的早期检测开辟了新的领域。全基因组测序(WGS)显示癌源cfDNA片段的长度比非癌源cfDNA片段的长度变化更大。cfDNA片段模式的差异反映了癌症中染色质结构的变化，以及其他基因组和表观基因组异常。最近，一种结合全基因组片段化的机器学习模型可以将多个癌症患者与健康对照组区分开来，敏感性从57%到99%以上不等，特异性为98%，并在75%的病例中识别出癌症的TOO。除了片段长度之外，cfDNA片段在基因组中的断裂位点还揭示了核小体占据的全基因组地图，提供了丰富的信号。局限性： 尽管WGS能够从极少量的cfDNA中同时分析数十到数百种肿瘤特异性异常，但其覆盖深度较低，成本较高。 基于cfDNA片段的方法与cfDNA甲基化分析有相同的局限性，如生理等病理条件导致的假阳性和技术限制导致的假阴性。 三种技术方法的整体比较 文献原文]]></content>
      <categories>
        <category>NGS</category>
        <category>文献</category>
      </categories>
      <tags>
        <tag>cancer early detection</tag>
        <tag>circulating cell-free DNA</tag>
        <tag>liquid biopsy</tag>
        <tag>methylation</tag>
        <tag>multi-cancer early detection</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Software-PennCNV-基于SNP位点进行CNV检测]]></title>
    <url>%2F03.%E5%BA%94%E7%94%A8-01.software_in_NGS%2FSoftware-%E5%9F%BA%E4%BA%8ESNP%E4%BD%8D%E7%82%B9%E8%BF%9B%E8%A1%8CCNV%E6%A3%80%E6%B5%8B-PennCNV%2F</url>
    <content type="text"><![CDATA[软件相关资源信息 软件文献pennCNV Home]]></content>
      <categories>
        <category>NGS</category>
        <category>software</category>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>变异检测</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Software-varscan-变异检测]]></title>
    <url>%2F03.%E5%BA%94%E7%94%A8-01.software_in_NGS%2FSoftware-%E5%8F%98%E5%BC%82%E6%A3%80%E6%B5%8B-varscan%2F</url>
    <content type="text"><![CDATA[软件相关资源信息 软件文献 VarScan 2: Somatic mutation and copy number alteration discovery in cancer by exome sequencingDaniel Software implementation 软件获取 varscan.sourceforgevarscan.githubThe VarScan 2 core software was developed in Java; the false-positive filter was implemented in Perl. Binary executables, scripts, and source code are free for noncommercial use and available at http://varscan.sourceforge.net.The false-positive filter requires the bam-readcount utility (D. Larson et al., https://github.com/genome/bam-readcount), which is written and compiled in C. Varscan的使用安装下载Varscan的Jar包 使用samtools mpileup -f ref.fasta sample.bam |java -jar VarScan.v2.3.3.jar mpileup2indel–output-vcf 1–vcf-sample-list sample_names.list sample.varscan.vcf]]></content>
      <categories>
        <category>NGS</category>
        <category>software</category>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>变异检测</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Software-终端工具-MobaxTerm]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-00.%E7%BC%96%E7%A8%8B%E8%BE%85%E5%8A%A9%E5%B7%A5%E5%85%B7%2FSoftware-%E7%BB%88%E7%AB%AF%E5%B7%A5%E5%85%B7-MobaxTerm%2F</url>
    <content type="text"><![CDATA[SSH 为 Secure Shell 的缩写，是目前较可靠，专为远程登录会话和其他网络服务提供安全性的协议。利用 SSH 协议可以有效防止远程管理过程中的信息泄露问题。 每天都需要与linux服务器打交道，有很多人直接使用linux系统，比如ubuntu和centos等。但是也有很多人不喜欢linux系统，虽然它对于做计算很方便，但是对于日常办公软件的支持极差，比如QQ、微信、office等。这个时候SSH客户端应运而生，它的作用就是帮助我们在windows下去连接并操作linux服务器。在学校使用更多的是xshell，但是到了公司因为版权等问题，xshell并不能成为一个好的选择。便改用了MobaxTerm，在这里记录下MobaxTerm的一些常见的配置内容。 安装可以在官网下载安装可以通过下载可执行文件(v21.5)直接运行。 解决 MobaxTerm 自动断开链接使用MobaXterm工具通过SSH连接Linux服务器，如果一段时间没有操作，MobaXterm会把连接自动断开，这个设定很是不方便。通过更改下面的设置可以使SSH保持长连接，不会自动断开。分别依次选择setting 》》》 configuration 》》》 ssh SETTINGS然后再配置界面选择 keepalive， 即可保持链接不中断。]]></content>
      <categories>
        <category>software</category>
        <category>Windows</category>
      </categories>
      <tags>
        <tag>软件工具</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Software-transvar_变异坐标转换]]></title>
    <url>%2F03.%E5%BA%94%E7%94%A8-01.software_in_NGS%2FSoftware-%E5%8F%98%E5%BC%82%E5%9D%90%E6%A0%87%E8%BD%AC%E6%8D%A2-transvar%2F</url>
    <content type="text"><![CDATA[坐标转换基因组学研究中经常会进行的操作是将测序检测得到的染色体侧面的变异检测结果（SNV、InDel等），注释到基因上，因为大多数功能研究和蛋白研究都是针对基因进行的，将变异注释到基因上，可以更好的帮助我们预测变异对基因表达，和蛋白合成过程中带来的影响，而这个层级的影响可以帮助我们更好的进行功能组学的相关研究。因此一些发表的论文或数据库中经常提到的变异，一般有三种格式：1）基因组坐标：2）cDNA 坐标；3）蛋白氨基酸坐标。举个例子TP53上的某个变异的基因组坐标是g.chr17:74026C&gt;A，cDNA坐标是c.1001G&gt;T，蛋白氨基酸坐标是p.G334V。当然这几种注释的写法都是有标准规范可以参考的，可以参考文章 在数据分析的过程中经常会遇到这三种坐标相关转换的情况，例如你从文献或者某个数据库中收集到了几百个肿瘤靶向药的用药位点，而你在你样本中检测到了很多变异，想知道你的样本中包含多少收集到的已知的用药位点。但通常文献或者数据库会以第二种或者第三种形式表示变异，而我们自己检测的变异通常会以vcf格式存储，这样就无法直接匹配。当然可以对vcf格式的变异进行ANNOVAR注释，然后对cDNA或者蛋白氨基酸坐标形式的变异进行比较，但尝试过的人都表示特别痛苦：需要考虑的规则太多！尝试两次，还是放弃了：一是匹配规则不通用；二是总担心有没有考虑到过的情况。所以急需一个能完成这种坐标转换的工具。15年发表在NATURE METHODS上的题为：TransVar: a multilevel variant annotator for precision genomics的文章中推出了一款名为TransVar的软件成了解决不同层面变异坐标转换的神器。 文献下载 TransVar软件简介Transvar 是一款多种方向的突变/坐标转换工具，它支持基因组坐标、cDNA 坐标以及蛋白氨基酸坐标之间的转换。 如上图所示，该软件的功能可细分为下面3种：1）正向注释：对于基因组坐标的变异进行mRNA（cDNA）和蛋白注释，这款工具会提供所有的可能结果；2）反向注释：将mRNA（cDNA）坐标和蛋白坐标的变异转换成所有可能基因组坐标形式的变异；3）等价注释：对于某一给定的蛋白坐标的变异，搜索所有可能的与其为相同基因组坐标，但在不同转录本上的蛋白坐标变异。 软件的官方文档 ReadtheDoc 软件的使用Linux版本安装软件下载可以从github仓库获取 通过python 安装12345sudo pip install transvar ## 全局安装，需要root权限或者：pip install --user transvar ##用户安装，没有root权限的用此方法软件更新：pip install -U transvar 数据库的配置链接数据库，可通过命令行添加。最开始，不存在transvar.cfg这个文件，在第一次链接后，会创建transvar.cfg文件，并将你创建的对应关系写入文件中，transvar.cfg 存放的路径：os.path.dirname({PYTHON_PATH})/lib/python3.7/site-packages/transvar/transvar.cfg1234567891011121314151617181920212223242526272829303132# set up databasestransvar config --download_anno --refversion hg19 #默认的hg19的 dbSNP 数据库是2016年的，部分数据库如dbSNP新版数据库收录内容有很大变化（主要是数量的提升），所以建议自行重新下载# in case you don&apos;t have a referencetransvar config --download_ref --refversion hg19# in case you do have a reference to linktransvar config -k reference -v [path_to_hg19.fa] --refversion hg19transvar config -k aceview -v $PATH/hg19.aceview.gff.gz.transvardb --refversion hg19transvar config -k ccds -v $PATH/hg19.ccds.txt.transvardb --refversion hg19transvar config -k ensembl -v $PATH/hg19.ensembl.gtf.gz.transvardb --refversion hg19transvar config -k gencode -v $PATH/hg19.gencode.gtf.gz.transvardb --refversion hg19transvar config -k kg -v $PATH/transvar.download/hg19.knowngene.gz.transvardb --refversion hg19transvar config -k refseq -v $PATH/hg19.refseq.gff.gz.transvardb --refversion hg19transvar config -k ucsc -v $PATH//hg19.ucsc.txt.gz.transvardb --refversion hg19cat lib/python3.7/site-packages/transvar/transvar.cfg[DEFAULT]refversion = hg19[hg19]reference = $PATH/ucsc.hg19.fastarefseq = $PATH/hg19.refseq.gff.gz.transvardbccds = $PATH/hg19.ccds.txt.transvardbucsc = $PATH/hg19.ucsc.txt.gz.transvardbgencode = $PATH/hg19.gencode.gtf.gz.transvardbaceview = $PATH/hg19.aceview.gff.gz.transvardbensembl = $PATH/hg19.ensembl.gtf.gz.transvardbkg = $PATH/hg19.knowngene.gz.transvardb 使用这款软件即可以单点注释，也可以批量处理，下面分别介绍一下： 单点注释用 -i传入待注释位点，包括3种： 1234567891011121314# 基因组正向注释transvar ganno --ccds -i &apos;chr3:g.178936091G&gt;A&apos; # cDNA反向注释transvar canno --ccds -i &apos;PIK3CA:c.1633G&gt;A&apos;# 氨基酸反向注释transvar panno -i &apos;PIK3CA:p.E545K&apos; --ensembl # 已知 p. 进行注释，可以一次只注释一个数据库，也可以同时注释多个数据库transvar panno -i &apos;ERBB2:p.Leu755_Thr759del&apos; --aceview --ccds --ensembl --gencode --kg --refseq --ucsc# 其中--ccds、--ensembl为使用不同的数据库，如网页版，可以同时多选，\# 如 --ccds --ensembl --refseq --ucsc 来进行多选 批量注释 -l传入待注释位点 12345678910/*/software/anaconda3/bin/transvar canno -l mutiation.canno.list -m 1 -o 2 --refseq --longestcoding --gseq ###canno：指cDNA反向注释，备选包括panno（ 蛋白氨基酸反向注释）和ganno（基因组正向注释）-l：输入文件，变异与canno、panno、ganno对应。格式示例如下：![image.png](https://upload-images.jianshu.io/upload_images/22041438-ba466242c2050f60.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)-m：-l指定的输入文件可以有多列，通过-m指定哪列是待注释列，不加-m参数默认是第一列-o：同时可以通过-o来指定-l中的那一列作为输出文件的首列，不加-o，默认是第一列--refseq：使用哪个数据库的转录本进行注释，还有其他数据库可选如 ensembl/gencode/ucsc/ccds/aceview等。--longestcoding： 有多个转录本时，仅选择最长的转录本。如果不加这个参数会把涉及到的所有转录本都输出出来，这时候你就要自己制定标准进行筛选了--gseq ：在输出文件中增加类似VCF格式的变异信息，包括染色体，起始位置，终止位置，参考基因组序列，突变后的序列。 网页版Transvar Web版使用相对比较简单，界面也非常清晰]]></content>
      <categories>
        <category>NGS</category>
        <category>software</category>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>格式转换</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[VS_Code-提升效率的配置]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-00.%E7%BC%96%E7%A8%8B%E8%BE%85%E5%8A%A9%E5%B7%A5%E5%85%B7%2FSoftware-%E7%BC%96%E7%A8%8B%E7%BB%88%E7%AB%AF-VSCode-3.%E5%B8%B8%E7%94%A8%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[Python解析器选择Ctrl+Shift+p ==》 python: Select Interpreter ==》选择所需环境]]></content>
      <categories>
        <category>software</category>
        <category>Windows</category>
        <category>VSCode</category>
      </categories>
      <tags>
        <tag>环境配置</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[VS_Code-提升效率的配置]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-00.%E7%BC%96%E7%A8%8B%E8%BE%85%E5%8A%A9%E5%B7%A5%E5%85%B7%2FSoftware-%E7%BC%96%E7%A8%8B%E7%BB%88%E7%AB%AF-VSCode-2.%E5%B8%B8%E7%94%A8%E6%89%A9%E5%B1%95%2F</url>
    <content type="text"><![CDATA[所有常用插件可以通过账号进行多机器的自动同步！ 体验优化vscode-icons让 vscode 资源树目录加上图标，提高不同类型文件/目录的辨识度。描述性的图标可以帮你区分不同的文件和文件夹。图标也让开发过程更有趣。 下面是两个VSCode标签页的比较。一个有图标，另一个没有。有许多图标扩展可供选择。流行的图标扩展有： vscode-icons Material Icon ThemeMaterial Theme Icons Simple icons 项目管理插件Project-Manager一个帮助你管理大量本地项目的标签，可以快速打开项目，同时还可以快速搜索项目。也不需要把所有项目同时添加到一个工作空间带来干扰。 编程相关插件AI 插件 通义灵码 CursorCode Cline windsurf通用扩展Code Runner在编辑器里运行js代码，同时可在terminal里显示打印结果的工具，方便调试代码,支持多种语言比如 C++, Python, Java等等guides显示代码对齐辅助线，很好用 Git History可以快速可视化的查看仓库的branch、commit、提交人员的相关信息并进行筛选，还可以快速查看一个文件的 git 历史，并识别文件每个版本的改动内容，进行分支、commit、tag相关操作。 SnippetsSnippets 是节约时间提高生产力的最好办法。这并不是单单某一个语言的扩展，而是多种语言的各种扩展。下面是一些流行的 Snippets 扩展： Angular Snippts (version 11) Python JavaScript (ES6) code snippets HTML Snippets ES7 React/Redux/GraphQL/React-Native snippets Vue 3 Snippets Trailing Spaces高亮你代码中冗余的空格。同时可以对代码中的冗余空格进行批量删除 F1 =选择=&gt; Trailing Spaces: Delete 语言专属扩展MarkdownMarkdown All in OneAll you need for Markdown (keyboard shortcuts, table of contents, auto preview and more).MarkdownAll in One可以处理所有的markdown需求，例如自动预览、快捷键、自动完成等。从2004年发布以来，Markdown已成为最流行的标记语言之一。技术作者广泛使用Markdown转写文章、博客、文档等，因为它十分轻便、简单，而且可以在多个平台上使用。它的流行带动了许多Markdown变体的出现，如GitHub Flavored markdown、MDX等。例如，要在Markdown中加粗字体，只需要选中文字按快捷键Ctrl+B即可，这样可以提高生产力。 Markdown Mind Map PreviewPreviewing a Mind Map Snakemake Language支持snakemake语言的语法高亮 ；Basic syntax, language, and snippet support for Snakefiles (Snakemake workflow definition files) WDL开发相关扩展WDL Syntax HighlighterWDL DevToolspythonpython可以一键进行python代码的格式化。A Visual Studio Code extension with rich support for the Python language (for all actively supported versions of the language: &gt;=3.7), including features such as IntelliSense (Pylance), linting, debugging, code navigation, code formatting, refactoring, variable explorer, test explorer, and more! Python Environment ManagerA Visual Studio Code extension that provides the ability to via and manage all of your Python environments &amp; packages from a single place. C__相关Better C++ Syntax Docker相关Dev Containers ： 在一个容器里面打开一个目录或仓库，同时可以利用VScode的所有特性。]]></content>
      <categories>
        <category>software</category>
        <category>Windows</category>
        <category>VSCode</category>
      </categories>
      <tags>
        <tag>环境配置</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Software-编程终端-VSCode-1.进阶设置]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-00.%E7%BC%96%E7%A8%8B%E8%BE%85%E5%8A%A9%E5%B7%A5%E5%85%B7%2FSoftware-%E7%BC%96%E7%A8%8B%E7%BB%88%E7%AB%AF-VSCode-1.%E8%BF%9B%E9%98%B6%E8%AE%BE%E7%BD%AE-%E4%BB%A3%E7%A0%81%E7%89%87%E6%AE%B5%2F</url>
    <content type="text"><![CDATA[代码片段创建特定代码片段用到vscode的同学，大部分都是要进行代码开发工作的。而代码开发为了保证代码的可追溯、可查询、可维护等相关诉求，势必会对代码编写提出诸多的规范性要求和标准，以有效的提高项目协作和复杂代码的生命周期。这些规范/标准会要求我们在进行代码开发时，记录项目/代码名称，功能，开发时间，作者等一系列信息，而这部分格式化内容的填写，为保证格式满足规范，其实非常浪费时间，所以我们可以通过使用模板，只填写内容而不需要去手动完成标准化说明步骤的撰写，更多的时间用在编程本身。使用vs code建立自定义的模板，可以极大的提高我们的效率，模板可以设置多个，在这里我们以一段python开发代码为例 File-&gt;Preferences-&gt;User Snippets（用户-&gt;首选项-&gt;用户片段）输入对应的模板名称，这里我们以 snippets_python_demo ，选择后会打开一个python.json的文件进行编辑。将内容改为下述内容：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950&#123; &quot;HEADER&quot;:&#123; &quot;prefix&quot;: &quot;snippets_python_demo&quot;, &quot;body&quot;: [ &quot;#!/usr/bin/python&quot;, &quot;# -*- encoding: utf-8 -*-&quot;, &quot;&apos;&apos;&apos;&quot;, &quot;@File : $TM_FILENAME&quot;, &quot;@Time : $CURRENT_YEAR/$CURRENT_MONTH/$CURRENT_DATE $CURRENT_HOUR:$CURRENT_MINUTE:$CURRENT_SECOND&quot;, &quot;@Author : Liu.Bo &quot;, &quot;@Version : 1.0.0.0&quot;, &quot;@Contact : liubo4@genomics.cn/614347533@qq.com&quot;, &quot;@WebSite : http://www.ben-air.cn/&quot;, &quot;&apos;&apos;&apos;&quot;, &quot;import logging&quot;, &quot;from logging.handlers import RotatingFileHandler&quot;, &quot;from argparse import ArgumentParser&quot;, &quot;&quot;, &quot;program = &apos;$TM_FILENAME&apos;&quot;, &quot;version = &apos;1.0.0.0&apos;&quot;, &quot;&quot;, &quot;parser = ArgumentParser(prog=program)&quot;, &quot;parser.add_argument(&apos;-in&apos; , dest=&apos;input&apos; , required=True, action=&apos;store&apos;, type=str, help=&apos;input File&apos;)&quot;, &quot;parser.add_argument(&apos;-out&apos;, dest=&apos;output&apos;, required=True, action=&apos;store&apos;, type=str, help=&apos;output File&apos;)&quot;, &quot;args = parser.parse_args()&quot;, &quot;&quot;, &quot;def log_config():&quot;, &quot; LOG_FORMAT = &apos;[%(asctime)s][%(levelname)s]: %(message)s&apos;&quot;, &quot; level = logging.INFO&quot;, &quot; logging.basicConfig(level=level, format=LOG_FORMAT)&quot;, &quot; #创建RotatingFileHandler对象,满2MB为一个文件，共备份3个文件&quot;, &quot; log_file_handler = RotatingFileHandler(filename=&apos;test.log&apos;, maxBytes=2*1024*1024, backupCount=3)&quot;, &quot; # 设置日志打印格式&quot;, &quot; formatter = logging.Formatter(LOG_FORMAT)&quot;, &quot; log_file_handler.setFormatter(formatter)&quot;, &quot; logging.getLogger(&apos;&apos;).addHandler(log_file_handler)&quot;, &quot;&quot;, &quot;def main():&quot;, &quot; Input = args.input&quot;, &quot; Output = args.output&quot;, &quot; log_config()&quot;, &quot;&quot;, &quot;if __name__ == &apos;__main__&apos;:&quot;, &quot; main()&quot;, &quot;&quot;, &quot;$0&quot; ], &#125;&#125; 然后再次创建python脚本后，我们在编码区输入snippets_python_demo（prefix 对应的内容） 就可以直接基于我们在模板中填写的内容生成一个初始化的python代码框架。 12345678910111213141516171819202122232425262728293031323334353637383940#!/usr/bin/python# -*- encoding: utf-8 -*-'''@File : tes.py@Time : 2023/10/24 14:06:31@Author : Liu.Bo @Version : 1.0.0.0@Contact : liubo4@genomics.cn/614347533@qq.com@WebSite : http://www.ben-air.cn/'''import loggingfrom logging.handlers import RotatingFileHandlerfrom argparse import ArgumentParserprogram = 'tes.py'version = '1.0.0.0'parser = ArgumentParser(prog=program)parser.add_argument('-in' , dest='input' , required=True, action='store', type=str, help='input File')parser.add_argument('-out', dest='output', required=True, action='store', type=str, help='output File')args = parser.parse_args()def log_config(): LOG_FORMAT = '[%(asctime)s][%(levelname)s]: %(message)s' level = logging.INFO logging.basicConfig(level=level, format=LOG_FORMAT) #创建RotatingFileHandler对象,满2MB为一个文件，共备份3个文件 log_file_handler = RotatingFileHandler(filename='test.log', maxBytes=2*1024*1024, backupCount=3) # 设置日志打印格式 formatter = logging.Formatter(LOG_FORMAT) log_file_handler.setFormatter(formatter) logging.getLogger('').addHandler(log_file_handler)def main(): Input = args.input Output = args.output log_config()if __name__ == '__main__': main() 特定语言开启代码片段功能一般的编程语言，例如Python，Perl等，可能会默认启用了模板功能的快速补齐，但是有些语言或者文件格式，也许会发现snippets无法启用。这时候就需要我们手动进行开启。这是因为在Visual Studio默认配置中，没有开启Markdown文件中的补全与匹配，需要我们进行配置，打开settings.json文件（快捷键Ctrl + Shift + P后输入settings ,然后选择open settings (json)打开）12345&#123; "[python]": &#123; "editor.formatOnType": true &#125;,&#125; 添加Markdown 的自动补齐功能123456789101112131415&#123; "[python]": &#123; "editor.formatOnType": true &#125;, "[markdown]": &#123; "editor.formatOnSave": true, "editor.renderWhitespace": "all", "editor.quickSuggestions": &#123; "other": true, "comments": true, "strings": true &#125;, "editor.acceptSuggestionOnEnter": "on" &#125;,&#125; 完成setting配置后，我们就可以在其他非默认配置中使用代码片段功能了。 代码块命名规范为了更好，更系统的对代码块进行管理，相关代码块均基于以下规则进行命名3段氏命名snippets_{编程语言}_{代码功能} git 配置]]></content>
      <categories>
        <category>software</category>
        <category>Windows</category>
        <category>VSCode</category>
      </categories>
      <tags>
        <tag>环境配置</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Software-编程终端-VSCode-1.基本配置]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-00.%E7%BC%96%E7%A8%8B%E8%BE%85%E5%8A%A9%E5%B7%A5%E5%85%B7%2FSoftware-%E7%BC%96%E7%A8%8B%E7%BB%88%E7%AB%AF-VSCode-1.%E5%B8%B8%E7%94%A8%E8%AE%BE%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[配置文件说明vscode本身可以支持多个配置文件，可以针对不同的项目(python项目、go项目、博客项目)，不同分辨率的显示器（主/副屏），构建不同的配置文件，来快速适配不同的项目需求，同时也避免每次都启动过多的冗余插件。vscode的所有配置信息，都会以 json 的形式保存在settings.json文件中，可以通过快捷键ctrl+shift+p，输入open settings(json)，打开默认配置文件，然后就可以看到所有的配置信息，通过修改配置文件，来达到需要的效果。也可以通过 profile ==&gt; 配置文件 ==&gt; 新建配置文件 来创建多个配置文件，然后通过快捷键ctrl+shift+p，输入workbench.action.openWorkspaceConfigFile，选择对应的配置文件，就可以使用对应的配置了。首先所有的配置文件都保存在%USERPROFILE%\.vscode\目录下，的所有配置文件，均可以通过json文件进行配置 更改扩展的缓存位置vscode的默认扩展均存储在用户家目录的隐藏目录（./.vscode/extensions）中，因为一般该目录在C盘，且扩展文件较大，会导致C盘存储激增。12#在vscode的Terminal界面执行如下命令，可以更改扩展的默认缓存位置code.cmd --extensions-dir D:\vscode.extensions 字体大小调整不同分辨率机器使用，可能会需要进行字体的设置，目录是通过比例调整字体，基于font的均为像素。vscode 目录字体： Setting =&gt; zoom levelvscode 编辑字体： Setting =&gt; Editor font sizevscode 扩展字体： Setting =&gt; font 在相关扩展查看字体大小设置 资源管理器排序方式资源管理器默认是先按文件夹，后按文件名称进行排序的，但是在有些情况下，其他排序会更符合我们的使用习惯，比如在使用Hexo管理博客文档时，文件夹是对应文件的相关附件内容，所以仅根据文件名排序会更加符合使用习惯。设置资源管理器种的显示顺序: Setting =&gt; 搜索 “ explorer sort order “ 自定义快捷键File -&gt; Preferences -&gt; Keyboard Shortcuts 或者 使用快捷键 Ctrl + K Ctrl + S]]></content>
      <categories>
        <category>software</category>
        <category>Windows</category>
        <category>VSCode</category>
      </categories>
      <tags>
        <tag>环境配置</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Standard operating procedure for somatic variant refinement of sequencing data with paired tumor and normal samples]]></title>
    <url>%2F01.%E7%9F%A5%E8%AF%86-05.Paper%2FPMC6450397-Standard%20operating%20procedure%20for%20somatic%20variant%20refinement%20of%20sequencing%20data%20with%20paired%20tumor%20and%20normal%20samples%2F</url>
    <content type="text"><![CDATA[该文献结合IGV可视化视图，对突变的人工审核方案提供了一些指导性的说明。 参考材料参考文献参考附件参考PPT]]></content>
      <categories>
        <category>NGS</category>
        <category>肿瘤检测</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[常用数据库-Uniprot]]></title>
    <url>%2F01.%E7%9F%A5%E8%AF%86-04.Database%2FDatabase-Uniport%2F</url>
    <content type="text"><![CDATA[最近由于临床报告中需要展示一些Uniprot的相关信息字段，因此需要对Uniprot中的部分重要信息进行获取，从而后续实现数据的本地化。因此需要对Uniprot数据库的使用方法和数据查询的机制进行了一些了解，记录如下 数据库简介维护机构通用蛋白质资源(UniProt)是蛋白质序列和注释数据的综合资源。 UniProt数据库包括UniProtKB (UniProt知识库)、UniRef (UniProt Reference Clusters)和UniProt Archive (UniParc)。 UniProt是欧洲生物信息学研究所(EMBL-EBI)、SIB瑞士生物信息学研究所(SIB Swiss Institute of Bioinformatics)和蛋白质信息资源研究所(PIR)的合作项目。UniProt联盟和主办机构EMBL-EBI、SIB和PIR都致力于长期保存UniProt数据库。 EMBL-EBI和SIB共同用于生成Swiss-Prot**和TrEMBL**, PIR生成Protein Sequence Database (PIR- psd)。 这两个数据集同时存在，但蛋白质序列覆盖范围和注释优先级不同。 TrEMBL(翻译后的EMBL核苷酸序列数据库)最初创建是因为序列数据的生成速度超过了Swiss-Prot的能力。 与此同时，PIR维护了PIR- psd及相关数据库，包括蛋白质序列数据库iProClass和整理的家族库。 2002年，这三家机构决定集中他们的资源和专业知识，成立了UniProt联盟。 EBI（ European Bioinformatics Institute）：欧洲生物信息学研究所（EMBL-EBI）是欧洲生命科学旗舰实验室EMBL的一部分。位于英国剑桥欣克斯顿的惠康基因组校园内，是世界上基因组学领域最强地带之一。 SIB（the Swiss Institute of Bioinformatics）：瑞士日内瓦的SIB维护着ExPASy（专家蛋白质分析系统）服务器，这里包含有蛋白质组学工具和数据库的主要资源。 PIR（Protein Information Resource）：PIR由美国国家生物医学研究基金会（NBRF）于1984年成立，旨在协助研究人员识别和解释蛋白质序列信息。 数据库组成截至目前数据库共包含4个subDatabase The UniProt Knowledgebase (UniProtKB)UniProt知识库， 特别是UniProtKB/Swiss-Prot，被用来访问蛋白质的功能信息。 每个UniProtKB条目都包含了氨基酸序列、蛋白质名称或描述、分类数据和引文信息，除此之外，我们还添加了尽可能多的注释。 这包括广泛接受的生物本体、分类和交叉引用，以及以实验和计算数据的证据归因的形式明确标注标注质量。 UniProt Reference Clusters (UniRef)UniRef数据库提供来自UniProtKB和UniParc记录的聚类序列集，以提供多个分辨率的序列空间的完整覆盖。 UniRef90和UniRef50的数据库大小分别减少了约40%和65%，提供了显著更快的序列搜索。 UniProt Archive (UniParc)UniParc是最全面的公开可访问的非冗余蛋白质序列数据库，提供这些序列的所有潜在来源和版本的链接。 你可以立即发现一个感兴趣的序列是否已经在公共领域，如果不是，就找出它最近的亲属。 UniProt Metagenomic and Environmental Sequences (UniMES)UniMES是一个专门存储宏基因组和环境数据的数据库。 更多详细信息可以参考官方说明文档 数据库使用UniProtKB 的使用由于本次数据的获取信息，均来自于Uniport的知识库，目前主要针对知识库进行介绍。进入知识库的主页,看到的信息如下图： 最上面是 搜索框， 左侧可以进行数据的过滤，例如肿瘤数据关注点主要是人的基因信息，可以直接选择Human剔除掉一些非人源的数据信息。 中间就是整个数据库数据的展示了。 重点介绍数据内容主题框的上面两个功能 Download 和 Columns Download字面意思，进行数据的下载，可以选择多种数据格式进行数据下载，tsv、gff、xml、fasta等等，我们可以根据需要选择相关格式进行下载 Columns这个功能可以说是一个非常人性化的功能，尤其是结合Download，可以完全不适用爬虫获取该数据库的所有需要的信息，点击进入Columns后，可以筛选在汇总表格中需要展示的字段信息（具体那些字段需要，可以在详细表中进行获取，毕竟下数据了，我们首先要知道获取什么数据）。示例如下图勾选我们需要的信息后，点击下方的 save, 就可以在内容中现实特定的信息，结合Download，可以实现快速的数据获取。 获取信息后，在进行简单的格式整理，就可以直接使用了，相比那些验收、IP检测、流量限制等方案层出不确定网站，可以说是非常友好了。 BGI的下载与处理需要的信息列： Entry (Names &amp; Taxonomy) Entry name (Names &amp; Taxonomy) Gene names (Names &amp; Taxonomy) Organism (Names &amp; Taxonomy) Length (Sequences) Repeat (Family &amp; Domains) Region (Family &amp; Domains) Zinc finger (Family &amp; Domains) Domain [FT] (Family &amp; Domains) Nucleotide binding (Function) Cross-reference (GeneID) DNA binding (Function) 基于肿瘤2022.3.1的解读需求，可以参考进行下载%20[9606]%22&amp;format=tab&amp;force=true&amp;columns=id,entry%20name,genes,organism,length,feature(REPEAT),feature(REGION),feature(ZINC%20FINGER),feature(DOMAIN%20EXTENT),feature(NP%20BIND),database(GeneID)&amp;sort=score&amp;compress=yes) 下载后，流程处理脚本使用 toolkits/07.DealWithDatabase/UniprotKB_DataClean.py（GitHub仓库） 对数据进行处理。处理后的文件结果示例如下： Gene GeneID GeneLength feature_key Position_region Uniport_description BLK 640; 505 Region 1..37; “Disordered” BLK 640; 505 Domain [FT] 58..118; “SH3” BLK 640; 505 Domain [FT] 124..220; “SH2” BLK 640; 505 Domain [FT] 241..494; “Protein kinase” BLK 640; 505 Nucleotide binding 247..255; “ATP” BDP1 KIAA1241 KIAA1689 TFNR 55814; 2624 Repeat 823..877; “1; approximate” BDP1 KIAA1241 KIAA1689 TFNR 55814; 2624 Repeat 878..932; “2” BDP1 KIAA1241 KIAA1689 TFNR 55814; 2624 Repeat 933..987; “3” BDP1 KIAA1241 KIAA1689 TFNR 55814; 2624 Repeat 988..1040; “4” BDP1 KIAA1241 KIAA1689 TFNR 55814; 2624 Repeat 1041..1094; “5” BDP1 KIAA1241 KIAA1689 TFNR 55814; 2624 Repeat 1095..1148; “6” BDP1 KIAA1241 KIAA1689 TFNR 55814; 2624 Repeat 1149..1203; “7” BDP1 KIAA1241 KIAA1689 TFNR 55814; 2624 Repeat 1204..1257; “8; approximate” BDP1 KIAA1241 KIAA1689 TFNR 55814; 2624 Repeat 1258..1327; “9; approximate” BDP1 KIAA1241 KIAA1689 TFNR 55814; 2624 Region 1..299; “Interaction with ZBTB43” BDP1 KIAA1241 KIAA1689 TFNR 55814; 2624 Region 1..142; “Disordered” BDP1 KIAA1241 KIAA1689 TFNR 55814; 2624 Region 193..241; “Disordered” BDP1 KIAA1241 KIAA1689 TFNR 55814; 2624 Region 355..470; “Required for phosphorylation by CSNK2A1” BDP1 KIAA1241 KIAA1689 TFNR 55814; 2624 Region 379..449; “Disordered”]]></content>
      <categories>
        <category>知识沉淀</category>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>数据库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[常用数据库 - HGNC]]></title>
    <url>%2F01.%E7%9F%A5%E8%AF%86-04.Database%2FDatabase-HGNC%2F</url>
    <content type="text"><![CDATA[常见的 Gene ID 官网这个标准比较多，有Ensembl ID，HGNC ID，Entrez ID（NCBI），Refseq ID 数据库 链接地址 Ensembl https://asia.ensembl.org/index.html HGNC https://www.genenames.org/ Entrez https://www.ncbi.nlm.nih.gov/gene/672 【案例】 Refseq https://www.ncbi.nlm.nih.gov/nuccore/NM_031991.4【所有物种，很少用】 https://www.genenames.org/about/guidelines/ 基因命名映射关系建立由于面向临床生产过程中的基因名称，参考相关指南和质评规范，统一以HGNC命名为统一参考。HGNC官网是随时都在进行更新的，没有固定的更新周期。但是本身每个月会保留一次版本镜像快照。所以日常更新建议使用快照版本，以便后续数据的相关回溯。 快照记录索引链接 快照记录的包含所有基因的名称映射（包含假基因，非编码RNA等）可以根据需要进行筛选以提高后续效率。 关注基因的基因ID获取针对关注的转录本及基因信息，获取现有注释体系下，基因ID和注释基因名称之间的对应关系 123456# 基于目前版本提报的基因和转录本信息，通过注释配置文件，获取基因ID和基因名称的对应关系总列表文件grep -w -f /jdfstj1/B2C_COM_P1/PipeAdmin/04.Pipeline/aio.v2.Dev.liubo/chip_info/PanCancer_IDT_v1/PanCancer.v1.Trans.list /jdfstj1/B2C_COM_P1/PipeAdmin/04.Pipeline/aio.v2.Dev.liubo/database/Ref104/ncbi_anno_rel104.dbref &gt; ncbi_ref104.geneSymble2ID#基于NCBI生成的ncbi_ref104.geneSymble2ID 文件中，其中第9、10两列为后续用于映射的列第9列： ncbi注释的基因symble第10列：entrez_id HGNC数据整理准备12345678910111213141516171819# 首先下载HGNC官方对应名称信息，根据需要调整相应的文件日期wget http://ftp.ebi.ac.uk/pub/databases/genenames/hgnc/archive/monthly/tsv/hgnc_complete_set_2021-06-01.txt其中重要的列为第1,2,9,19,24列。# 提取其中的映射相关的重要信息示例如下：awk -F &apos;\t&apos; &apos;&#123;print $1&quot;\t&quot;$19&quot;\t&quot;$2&quot;\t&quot;$9&quot;\t&quot;$24&#125;&apos; /jdfstj1/B2C_COM_P1/Research_and_Development/Database/HGNC/hgnc_complete_set_2021-06-01.txt |head head hgnc_complete_set_2021-06-01.fit.tsvhgnc_id entrez_id symbol alias_symbol refseq_accessionHGNC:5 1 A1BG NM_130786HGNC:37133 503538 A1BG-AS1 FLJ23569 NR_015380HGNC:24086 29974 A1CF &quot;ACF|ASP|ACF64|ACF65|APOBEC1CF&quot; NM_014576#其中比较主要的信息分别如下#第1洌hgnc_id：HGNC的基因ID#entrez_id：NCBI的基因ID#symbol：基因统一命名#alias_symbol：基因历史别名# 所在列数 字段头 官方含义 字段信息 1 hgnc_id ID used to designate a gene family or group the gene has been assigned to. HGNC对应的基因ID 2 symbol Status of the symbol report, which can be either “Approved” or “Entry Withdrawn”. 基因命名 3 name miRBase ID 4 locus_group Same as “location” but single digit chromosomes are prefixed with a 0 enabling them to be sorted in correct numerical order (e.g. 02q34). 5 locus_type A group name for a set of related locus types as defined by the HGNC (e.g. non-coding RNA). 6 status snoRNABase ID 7 location lncRNA Database ID 8 location_sortable Cytogenetic location of the gene (e.g. 2q34). 9 alias_symbol Other names used to refer to this gene as seen in the “SYNONYMS” field in the gene symbol report. 基因曾用名 10 alias_name The HGNC ID that the Alliance of Genome Resources (AGR) have linked to their record of the gene. Use the HGNC ID to link to the AGR. 11 prev_symbol Gene names previously approved by the HGNC for this gene. Equates to the “PREVIOUS SYMBOLS &amp; NAMES” field within the gene symbol report. 12 prev_name Orphanet ID 13 gene_group The HGNC ID used within the GenCC database as the unique identifier of their gene reports within the GenCC database. 14 gene_group_id Name given to a gene family or group the gene has been assigned to. Equates to the “GENE FAMILY” field within the gene symbol report. 15 date_approved_reserved Symbol used within the Catalogue of somatic mutations in cancer for the gene. 16 date_symbol_changed The date the gene name was last changed. 17 date_name_changed Date the entry was last modified. 18 date_modified The date the entry was first approved. 19 entrez_id Ensembl gene ID. Found within the “GENE RESOURCES” section of the gene symbol report. 20 ensembl_gene_id International Nucleotide Sequence Database Collaboration (GenBank, ENA and DDBJ) accession number(s). Found within the “NUCLEOTIDE SEQUENCES” section of the gene symbol report. 21 vega_id UniProt protein accession. Found within the “PROTEIN RESOURCES” section of the gene symbol report. 22 ucsc_id The HGNC approved gene symbol. Equates to the “APPROVED SYMBOL” field within the gene symbol report. 23 ena The date the gene symbol was last changed. 24 refseq_accession Pubmed and Europe Pubmed Central PMID(s). HGNC提供的转录本ID 25 ccds_id Symbol used to link to the SLC tables database at bioparadigms.org for the gene 26 uniprot_ids UCSC gene ID. Found within the “GENE RESOURCES” section of the gene symbol report. 27 pubmed_id Pseudogene.org 28 mgd_id ID used to link to the MEROPS peptidase database 29 rgd_id RefSeq nucleotide accession(s). Found within the “NUCLEOTIDE SEQUENCES” section of the gene symbol report. 30 lsdb The locus type as defined by the HGNC (e.g. RNA, transfer). 31 cosmic Symbol used within the Human Cell Differentiation Molecule database for the gene 32 omim_id HGNC approved name for the gene. Equates to the “APPROVED NAME” field within the gene symbol report. 33 mirbase Mouse genome informatics database ID. Found within the “HOMOLOGS” section of the gene symbol report. 34 homeodb HGNC ID. A unique ID created by the HGNC for every approved symbol. 35 snornabase Rat genome database gene ID. Found within the “HOMOLOGS” section of the gene symbol report. 36 bioparadigms_slc Other symbols used to refer to this gene as seen in the “SYNONYMS” field in the symbol report. 37 orphanet Online Mendelian Inheritance in Man (OMIM) ID 38 pseudogene.org Symbols previously approved by the HGNC for this gene. Equates to the “PREVIOUS SYMBOLS &amp; NAMES” field within the gene symbol report. 39 horde_id Homeobox Database ID 40 merops NCBI and Ensembl transcript IDs/acessions including the version number for one high-quality representative transcript per protein-coding gene that is well-supported by experimental data and represents the biology of the gene. The IDs are delimited by . 41 imgt Symbol used within HORDE for the gene 42 iuphar ID used to link to the Human Intermediate Filament Database 43 kznf_gene_catalog The objectId used to link to the IUPHAR/BPS Guide to PHARMACOLOGY database. To link to IUPHAR/BPS Guide to PHARMACOLOGY database only use the number (only use 1 from the result objectId:1) 44 mamit-trnadb The name of the Locus Specific Mutation Database and URL for the gene separated by a character 45 cd Consensus CDS ID. Found within the “NUCLEOTIDE SEQUENCES” section of the gene symbol report. 46 lncrnadb The gene symbol used to link to LNCipedia - a comprehensive compendium of human long non-coding RNAs. 47 enzyme_id Entrez gene ID. Found within the “GENE RESOURCES” section of the gene symbol report. ncbi的基因symbol 48 intermediate_filament_db Symbol used within international ImMunoGeneTics information system 49 rna_central_ids Rat genome database gene ID. Found within the “HOMOLOGS” section of the gene symbol report. 50 lncipedia ID used to link to the Human KZNF Gene Catalog 51 gtrnadb ID used to designate a gene family or group the gene has been assigned to. 52 agr The HGNC ID that the Alliance of Genome Resources (AGR) have linked to their record of the gene. Use the HGNC ID to link to the AGR. 53 mane_select ID to link to the Mamit-tRNA database 54 gencc ENZYME EC accession number 信息来源 进行HGNC映射文件更新12345# 生成HGNC基因名称转换文件 perl /jdfstj1/B2C_COM_P1/Research_and_Development/Database/HGNC/creat.change_genelist.pl -hgnc hgnc_complete_set_2021-06-01.txt -ncbi ncbi_ref104.geneSymble2ID -o change_gene.list.tmp 结果文件示例如下（change_gene.list.tmp） ： entrez_id HGNC_symble NCBI_symble Tag 1 A1BG A1BG Match 29974 A1CF A1CF Match 2 A2M A2M Match 144568 A2ML1 A2ML1 Match 53947 A4GALT A4GALT Match 51146 A4GNT A4GNT Match 8086 AAAS AAAS Match 65985 AACS AACS Match 13 AADAC AADAC Match 前三列为基因对应的信息，第四列tag为示例信息，提示是否能匹配上： 如果标记为match，则表明HGNC和NCBI的对应基因ID可以匹配。 如果标记为MisMatch，则表明两个数据对应的基因ID无法进行匹配， 更新后，在目录下readme中记录数据库的更新日期和操作人员。12eg：# liubo4 @ 20210616]]></content>
      <categories>
        <category>知识沉淀</category>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>数据库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[常用数据库-dbNSFP]]></title>
    <url>%2F01.%E7%9F%A5%E8%AF%86-04.Database%2FDatabase-dbNSFP%2F</url>
    <content type="text"><![CDATA[数据库简介Some dbNSFP contents (may not be up-to-date though) can also be accessed through variant tools, ANNOVAR, KGGSeq, VarSome, UCSC Genome Browser’s Variant Annotation Integrator, Ensembl Variant Effect Predictor, SnpSift and HGMD.dbNSFP 是一个为人类基因组中所有潜在非同义单核苷酸变异 (nsSNV) 进行功能预测和注释而开发的数据库。涵盖了 84,013,490 个nsSNVs 和 ssSNVs (splicing-site SNVs)。数据库内汇编了大量第三方数据库或预测软件的评估结果，包括： 43种预测软件的预测得分： SIFT SIFT4G Polyphen2-HDIV Polyphen2-HVAR LRT MutationTaster2 MutationAssessor FATHMM MetaSVM MetaLR MetaRNN CADD CADD_hg19 VEST4 PROVEAN FATHMM-MKL coding FATHMM-XF coding fitCons x 4 LINSIGHT DANN GenoCanyon Eigen Eigen-PC M-CAP REVEL MutPred MVP gMVP MPC PrimateAI GEOGEN2 BayesDel_addAF BayesDel_noAF ClinPred LIST-S2 VARITY ESM1b EVE AlphaMissense ALoFT 9个保守性评分：\ PhyloP x 3 phastCons x 3 GERP++ SiPhy bStatistic 其他相关信息：包括： 千人数据库（phase3）的人群频率信息， UK10K的队列信息， Exac、gnomAD和NHLBI 的ESP6500项目数据， 以及来自不同数据库的gene ID信息、基因功能描述、基因表达和基因互作的信息等。 一些 dbNSFP 内容可以通过多个软件进行注释（但是软件自带并不一定是最新版）： OpenCRAVAT variant tools ANNOVAR KGGSeq VarSome UCSC Genome Browser’s Variant Annotation Integrator Ensembl Variant Effect Predictor SnpSift HGMD访问。 请注意，dbNSFP 的某些组件分数/内容对于非学术用途有特定要求或许可，因此请为此目的联系原始数据库/内容提供商，确定相关授权。 参考官网 发表文献 Liu X, Jian X, and Boerwinkle E. 2011. dbNSFP: a lightweight database of human non-synonymous SNPs and their functional predictions. Human Mutation. 32:894-899. Liu X, Jian X, and Boerwinkle E. 2013. dbNSFP v2.0: A Database of Human Non-synonymous SNVs and Their Functional Predictions and Annotations. Human Mutation. 34:E2393-E2402. Liu X, Wu C, Li C and Boerwinkle E. 2016. dbNSFP v3.0: A One-Stop Database of Functional Predictions and Annotations for Human Non-synonymous and Splice Site SNVs. Human Mutation. 37:235-241. Liu X, Li C, Mou C, Dong Y, and Tu Y. 2020. dbNSFP v4: a comprehensive database of transcript-specific functional predictions and annotations for human nonsynonymous and splice-site SNVs. Genome Medicine. 12:103.]]></content>
      <categories>
        <category>知识沉淀</category>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>数据库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[math-百分位数的计算说明]]></title>
    <url>%2F01.%E7%9F%A5%E8%AF%86-02.Math%2Fmath-%E7%99%BE%E5%88%86%E4%BD%8D%E6%95%B0%E7%9A%84%E8%AE%A1%E7%AE%97%E8%AF%B4%E6%98%8E%2F</url>
    <content type="text"><![CDATA[定义统计学术语，如果将一组数据从小到大排序，并计算相应的累计百分位，则某一百分位所对应数据的值就称为这一百分位的百分位数。可表示为：一组n个观测值按数值大小排列。如，处于p%位置的值称第p百分位数。 说明一：用99个数值或99个点，将按大小顺序排列的观测值划分为100个等分，则这99个数值或99个点就称为百分位数，分别以Pl，P2，…，P99代表第1个，第2个，…，第99个百分位数。第j个百分位数j=1,2…100。式中Lj，fj和CFj分别是第j个百分位数所在组的下限值、频数和该组以前的累积频数，Σf是观测值的数目。百分位通常用第几百分位来表示，如第五百分位，它表示在所有测量数据中，测量值的累计频次达5%。以身高为例，身高分布的第五百分位表示有5%的人的身高小于此测量值，95%的身高大于此测量值。百分位数则是对应于百分位的实际数值。= 说明二：第25百分位数又称第一个四分位数（First Quartile），用Q1表示；第50百分位数又称第二个四分位数（Second Quartile），用Q2表示，该值对应的也是中位数；第75百分位数又称第三个四分位数（Third Quartile）,用Q3表示。若求得第p百分位数为小数，可完整为整数。分位数是用于衡量数据的位置的量度，但它所衡量的，不一定是中心位置。百分位数提供了有关各数据项如何在最小值与最大值之间分布的信息。对于无大量重复的数据，第p百分位数将它分为两个部分。大约有p%的数据项的值比第p百分位数小；而大约有(100-p)%的数据项的值比第p百分位数大。对第p百分位数，严格的定义如下。第p百分位数是这样一个值，它使得至少有p%的数据项小于或等于这个值，且至少有(100-p)%的数据项大于或等于这个值。高等院校的入学考试成绩经常以百分位数的形式报告。比如，假设某个考生在入学考试中的语文部分的原始分数为54分。相对于参加同一考试的其他学生来说，他的成绩如何并不容易知道。但是如果原始分数54分恰好对应的是第70百分位数，我们就能知道大约70%的学生的考分比他低，而约30%的学生考分比他高。 计算原理下面的步骤来说明如何计算第p百分位数。 方法一第1步：以递增顺序排列原始数据（即从小到大排列）。第2步：计算指数i=n * p%第3步：l）若 i 不是整数，将 i 向上取整。大于i的毗邻整数即为第p百分位数的位置。2) 若i是整数，则第p百分位数是第i项与第(i+l)项数据的平均值。 方法二除了以上方法，再介绍另外一种方法，这种方法是SPSS所用方法，也是SAS所用方法之一。第一步：将n个变量值从小到大排列，X(j)表示此数列中第j个数。第二步：计算指数，设(n+1)P%=j+g，j为整数部分，g为小数部分。第三步：1)当g=0时：P百分位数=X(j);2)当g≠0时：P百分位数=gX(j+1)+(1-g)X(j)=X(j)+g*[X(j+1)-X(j)]。 相关代码函数pyhton123456789101112131415import numpy as np a = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])# 中位数print(np.median(a))# 25%分位数print(np.percentile(a, 25))# 75%分位数print(np.percentile(a, 75))# 输出结果：5.53.257.75]]></content>
      <categories>
        <category>知识沉淀</category>
      </categories>
      <tags>
        <tag>统计学</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Git.常见问题]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-09.%E7%89%88%E6%9C%AC%E6%8E%A7%E5%88%B6%2FGit-%E5%BC%82%E5%B8%B8%E9%97%AE%E9%A2%98%E5%A4%84%E7%90%86%E8%AE%B0%E5%BD%95%2F</url>
    <content type="text"><![CDATA[终端使用git 命令中文显示异常执行如下配置git config配置，可以正常显示中文1git config --global core.quotepath false CAfile &amp;Capath none报错内容123server certificate verification failed. CAfile: /etc/ssl/certs/ca-certificates.crt CRLfile: none” 这种情况一般使用ssh：链接可以同步，但是不能正常使用https ； 可以通过取消验证解决该问题12git config --global http.sslverify falsegit config --global https.sslverify false 远程仓库指向错误1234liubo4@BCNTJOA14002B MINGW64 /d/Git_Repo$ git clone http://git.bgi.com/liubo4/test.gitCloning into 'test'...fatal: unable to access 'http://git.bgi.com/liubo4/test.git/': Failed to connect to 127.0.0.1 port 7890 after 2071 ms: Couldn't connect to server http://git.bgi.com 的实际ip是 10.17.1.2， 但是在链接时，访问的ip却是 127.0.0.1需要确认两个地方： 确认host是否设置了代理host文件 C:\Windows\System32\drivers\etc\hosts 是配置网站代理ip的文件。如果对网站进行了ip的代理，在访问相关域名时，会跳转到指定的ip上。示例如下，如果有gitlab的重定向，需要删除相关重定向，或将ip改为正确的ip地址。1234172.25.13.98 host.docker.internal172.25.13.98 gateway.docker.internal10.227.5.224 developer.phoenix-engine.bgi.com127.0.0.1 git.bgi.com 确认gitconfig~/.gitconfig 文件会有git仓库的相关配置信息。]]></content>
      <categories>
        <category>编程拾慧</category>
        <category>Git</category>
      </categories>
      <tags>
        <tag>问题处理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Survival Analysis]]></title>
    <url>%2F01.%E7%9F%A5%E8%AF%86-01.NGS%2FSurvival_Analysis%2F</url>
    <content type="text"><![CDATA[缩略语 删失数据： 在实验过程中丢失的、失去跟踪的数据。部分纳入实验的样本，在试验过程中会无法观测到死亡事件的发生，比如无法联系到、或主动退出、或其他需要紧急处理退出临床试验的情况、以及试验结束时还未发生死亡等，这些数据就称作删失数据 右删失： 一些患者记录了从一开始到删失前的进展，而丢失了后续的结局，我们将这类删失称作右删失。 左删失： 我们要统计从初次患病到最终死亡的生存时间的分析，有些病人已知患有疾病且知道其死亡时间，但无法确定初次患病的时间，这样的删失则称为左删失。 生存概率(Survival probability)：指的是研究对象从试验开始直到某个特定时间点仍然存活的概率，可见它是一个对时间t的函数，我们定义之为 S(t)。 风险概率(Hazard probability)：指的是研究对象从试验开始到某个特定时间 t 之前存活，但在 t 时间点发生观测事件如死亡的概率，它也是对时间 t 的函数，定义为 H(t)。]]></content>
      <categories>
        <category>NGS</category>
        <category>知识沉淀</category>
        <category>肿瘤检测</category>
        <category>缩略语</category>
      </categories>
      <tags>
        <tag>肿瘤</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[肿瘤体系检测过程中的胚系过滤方案]]></title>
    <url>%2F01.%E7%9F%A5%E8%AF%86-01.NGS%2FNGS-%E8%82%BF%E7%98%A4%E4%BD%93%E7%B3%BB%E6%A3%80%E6%B5%8B%E8%BF%87%E7%A8%8B%E4%B8%AD%E7%9A%84%E8%83%9A%E7%B3%BB%E8%BF%87%E6%BB%A4%E6%96%B9%E6%A1%88%2F</url>
    <content type="text"><![CDATA[他们是怎么做的？目前已经发表的一些体系检测软件，引用了一些检测方法，为我们提供了参考。主要有3个相对比较主流的方案： 从体系检测结果中减去胚系检出结果 体细胞变异调用者使用贝叶斯方法 Fisher精确统计方法 Fisher 检验（目前产品在用）参考文献[1] Hansen N F , Gartner J J , Lan M , et al. Shimmer: detection of genetic alterations in tumors using next-generation sequence data.[J]. Bioinformatics, 2013(12):1498-1503.[2] Koboldt D C , Zhang Q , Larson D E , et al. VarScan 2: Somatic mutation and copy number alteration discovery in cancer by exome sequencing[J]. Genome Research, 2012, 22(3):568-576. 介绍以Varscan为例，基于Fisher检验是否存在显著性（Pvalue &lt; 0.1) ，及双端情况将数据划分为3类（LOH、Germline、Somatic）。 补充说明因为目前监测体系变异检测，会涉及到一些低频检测需求(1%甚至更低的频率检测需求)，在临床使用中，发现一个难以避免的问题，会存在由于对照深度不足导致的P值永远难以显著（即使对照纯阴性也无法存在显著性差异）。模拟统计计算如下：组织深度1200x； 对照纯阴性。（对照阈值300x）对于检测限 1% 的突变（组织/血浆测序深度1200x时)，对照只有达到 389x 以上时，才可能有显著性。对于一个 3% 的突变（组织/血浆测序深度1200x时)，对照只有达到 127x 以上时，才可能有显著性。对于一个 0.5% 的突变（组织/血浆测序深度1200x时)，对照只有达到 843x 以上时，才可能有显著性。 WES产品 500x；对照纯阴性。（对照阈值200x）针对检测限 3% 的突变，纯阴对照需要达到 133x 才能存在显著性。针对一个 1% 的突变，纯阴对照需要达到 506x 才能存在显著性。 做减法参考文献[1] A comparative analysis of algorithms for somatic SNV detection in cancer.[J]. Bioinformatics, 2013.[2] GATK mutect2 介绍以GATK为例 A variant allele in the case sample is not called if the site is variant in controls.We explain an exception for GATK4 Mutect2 in a bit.Historically, somatic callers have called somatic variants at the site-level. That is, if a variant site in the case is also variant in the matched control or in a population resource, e.g. dbSNP, even if the variant allele is different than the control or resource it is discounted from the somatic callset. This practice stems in part from cancer study designs where the control normal sample is sequenced at much lower depth than the case tumor sample. Because of the assumption mutations strike randomly, cancer geneticists view mutations at sites of common germline variation with skepticism. Remember for humans, common germline variant sites occur roughly on average one in a thousand reference bases. So if a commonly variant site accrues additional mutations, we must weigh the chance of it having arisen from a true somatic event or it being something else that will likely not add value to downstream analyses. For most sites and typical analyses, the latter is the case. The variant is unlikely to have arisen from a somatic event and more likely to be some artifact or germline variant, e.g. from mapping or cross-sample contamination.GATK4 Mutect2 still applies this practice in part. The tool discounts variant sites shared with the panel of normals or with a matched normal control’s unambiguously variant site. If the matched normal’s variant allele is supported by few reads, at low allele fraction, then the tool accounts for the possibility of the site not being a germline variant.When it comes to the population germline resource, GATK4 Mutect2 distinguishes between the variant alleles in the germline resource and the case sample. That is, Mutect2 will call a variant site somatic if the allele differs from that in the germline resource. [1] A comparative analysis of algorithms for somatic SNV detection in cancer.[J]. Bioinformatics, 2013.[2] GATK mutect2 体细胞变异调用者使用贝叶斯方法参考文献[1] Cibulskis K , Lawrence M S , Carter S L , et al. Sensitive detection of somatic point mutations in impure and heterogeneous cancer samples[J]. Nature Biotechnology, 2013, 31(3):213-219.[2] Christopher, T, Saunders, et al. Strelka: accurate somatic small-variant calling from sequenced tumor-normal sample pairs.[J]. Bioinformatics, 2012.[3] Yuichi S , Yusuke S , Kenichi C , et al. An empirical Bayesian framework for somatic mutation detection from cancer genome sequencing data[J]. Nucleic Acids Research, 2013(7):e89-e89.[4] SomaticSniper[J]. Bioinformatics, 2012.[5] Identification of somatic mutations in cancer through Bayesian-based analysis of sequenced genome pairs[J]. Bmc Genomics, 2013, 14.]]></content>
      <categories>
        <category>NGS</category>
        <category>标准化</category>
        <category>变异检测</category>
      </categories>
      <tags>
        <tag>变异检测</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[解读癌症研究的里程碑文献：Hallmarks of Cancer]]></title>
    <url>%2F01.%E7%9F%A5%E8%AF%86-05.Paper%2FSeries-Hallmarks_of_cancer%2F</url>
    <content type="text"><![CDATA[概述2000年1月7日，瑞士Agora转化癌症研究中心的Douglas Hanahan教授和麻省理工学院的Robert A Weinberg教授在Cell杂志合作发表了一篇开创性综述“Hallmarks of Cancer”[1]，该综述的发表改变了以往对癌症研究的散乱认知，将众多艰涩难懂的概念归纳到可以简短表述的特征中来。自此，癌症特征（Hallmarks of Cancer）形成一种启发式工具，为人们理解和探索癌症浩如烟海的表型与机制提供了基本的逻辑框架。更为可贵的是，“Hallmarks of Cancer”与时俱进，后又经历了2次“十年磨一剑”的更新，每次更新版本均可谓癌症研究领域的里程碑。在首版“Hallmarks of Cancer”综述中，充分梳理了过去近百年癌症研究的进展和发现，提炼出了癌症的6个特征，其成为Cell杂志历史上被引用次数最多（超过37 800次）的综述之一[1]。“Hallmarks of Cancer”中提出的恶性肿瘤细胞的6个特征分别是：自给自足的生长信号（self-sufficiency in growth signals）、对生长抑制信号不敏感（insensitivity to anti-growth signals）、逃避细胞凋亡（evading apoptosis）、无限复制的潜力（limitless replicative potential）、持续的血管新生（sustained angiogenesis）及组织侵袭转移（tissue invasion &amp; metastasis）。癌症的发病机制是多步骤的，正常细胞转变为肿瘤的过程中会获得这些特征性功能并最终发展成恶性病变。几乎所有的癌症都包括这6个特征，而这些特征在不同肿瘤中的优先级又因肿瘤的阶段和类型而不同，体现出肿瘤的异质性。 2011年，Douglas Hanahan和Robert A Weinberg 2位教授再次在Cell上发表了“Hallmarks of cancer：the next generation”[2]，将首版综述中的6个特征的概念描述进行了些许调整，同时在原有基础上增加了4个特征，分别是：细胞能量代谢的失控（deregulating cellular energetics）、逃避免疫清除（avoiding immune destruction）、肿瘤促炎症作用（tumor-promoting inflammation）及基因组的不稳定性和突变（genome instability and mutation）；此外，进一步明确了一个观点，即肿瘤不仅是肿瘤细胞数量上的增加，而且需要围绕肿瘤微环境进行理解。在该版本中，2位教授用10类药物作为例子佐证了肿瘤10个标志性特征具有的现实意义，如表皮生长因子受体抑制剂对抗“持续的增殖信号”、细胞周期依赖性激酶抑制剂对抗“逃避生长抑制”、血管内皮生长因子抑制剂对抗“持续的血管新生”、抗细胞毒性T淋巴细胞相关蛋白4单克隆抗体对抗“逃避免疫清除”等。 2022年1月，Douglas Hanahan教授在Cancer Discovery杂志发表了第3版“Hallmarks of Cancer：new dimensions”[3]，再增加了4个新的肿瘤标志性特征，分别是：解锁表型可塑性（unlocking phenotypic plasticity）、衰老细胞（senescent cells）、非突变表观遗传重编程（non-mutational epigenetic reprogramming）及多态性的微生物组（polymorphic microbiomes），这4个新增加特征中的前2个被定义为“新出现的特征”（emerging hallmarks），指仍需进一步研究和验证；后2个被定义为“赋能特征”（enabling hallmarks），表明其是肿瘤发展和其他标志性特征获得的驱动性因素。 Hallmarks of Cancer: New Dimensions参考资料原文链接Hallmarks of CancerHallmarks of Cancer：the next generationHallmarks of Cancer: New Dimensions PDFHallmarks of Cancer: New Dimensions.pdf 解析资料http://www.hxyx.com/article/10.7507/1007-9424.202202033]]></content>
      <categories>
        <category>NGS</category>
        <category>文献</category>
      </categories>
      <tags>
        <tag>肿瘤</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[NGS检测相关缩略语说明表]]></title>
    <url>%2F01.%E7%9F%A5%E8%AF%86-01.NGS%2F2022-01-10.NGS%E6%A3%80%E6%B5%8B%E7%9B%B8%E5%85%B3%E7%BC%A9%E7%95%A5%E8%AF%AD%E8%AF%B4%E6%98%8E%E8%A1%A8%2F</url>
    <content type="text"><![CDATA[NGS检测相关实验方向 缩略语 全称 含义说明 PEC 延伸型探针捕获(Probe Extension Capture) 一种先使探针与目标区域上游结合，然后以探针为引物通过PCR延伸获取目标区域DNA序列的捕获技术。 PE测序 双端(Pair-End)测序 - 。 信息分析方向 缩略语 全称 含义说明 VAF 突变频率(Variant Allele Frequency) 基因组某个位点支持突变的reads覆盖深度占这个位点总reads覆盖深度的比例。 Dup Duplication 为了提高丰度在实验前期进行PCR方法对模板进行扩增，扩增后一个模板可能会有多个不同的PCR产物被测到，一个模板被重复检测的测序数据成为Dup 肿瘤检测相关 缩略语 全称 含义说明 UMI Unique molecular identifiers 对原始样本基因组打断后的每一个片段都加上一段特有的标签序列，用于区分同一样本中成千上万的不同的片段，在后续的数据分析中可以通过这些标签序列来排除由于 DNA 聚合酶和扩增以及测序过程中所引入的错误。 MSI Microsatellite Instability 与正常组织相比，肿瘤中某个微卫星位点由于重复单元的插入或缺失而出现新的微卫星等位基因的现象。 MSI的发生是由于肿瘤组织的DNA错配修复出现功能性缺陷导致。 伴随着DNA错配修复缺陷的MSI现象是临床上的一项重要的肿瘤标志物。 临床医学相关 缩略语 全称 含义说明 DLT 剂量限制性毒性 是基于系统性抗癌症治疗在第一个周期中出现严重毒性来定义的。 此类毒性是根据美国国家癌症研究所的不良事件通用术语标准（CTCAE）进行评估的，通常涵盖所有3级或更高的毒性，定义时一般会将3级非发热性中性粒细胞减少症和脱发作为例外。 MTD 最大耐受剂量 RP2D II期推荐剂量 DOR 缓解持续时间 CR 完全缓解(complete response) 所有靶病灶消失，无新病灶出现，且肿瘤标志物正常，至少维持4周。 PR 部分缓解(partial response) 靶病灶最大径之和减少≥30%，至少维持4周。 PD 疾病进展(progressive disease) 靶病灶最大径之和至少增加≥20%，或出现新病灶。 SD 疾病稳定(stable disease) 靶病灶最大径之和缩小未达PR，或增大未达PD。 OS 总生存期(overall survival) 从随机化（random assignment）开始至因任何原因引起死亡的时间（失访患者为最后一次随访时间；研究结束时仍然存活患者，为随访结束日）。 MST 中位生存期(Median Survival Time) 又称为半数生存期，表示有且只有50%的个体可以活过这个时间。评估某个癌种的中位生存期，一般从发现该肿瘤开始计算；如果是评估某项临床试验的中位生存期，一般从给药或随机开始。 DFS 无病生存期/无疾病生存时间 (Disease Free Survival) 指从随机化开始至第一次肿瘤复发/转移或由于任何原因导致受试者死亡的时间(失访患者为最后一次随访时间；研究结束时仍然存活患者，为随访结束日)。 中位DFS 中位DFS 又称半数无病生存期，表示恰好有50％的个体未出现复发/转移的时间。 TTP 疾病进展时间 (，Time To Progression) 指从随机分组开始到第一次肿瘤客观进展的时间。(TTP与PFS唯一不同在于PFS包括死亡，而TTP不包括死亡。因此PFS更能预测和反应临床受益，与OS一致性更好；而TTP在预测临床受益方面则较差，因其仅考虑抗肿瘤活性，在分析时较早时期的死亡情况被删失，导致一些重要信息的丢失。在导致死亡的非肿瘤原因多于肿瘤原因的情况下，TTP是一个合适的指标。TTP同样也需有明确的定义。) PFS 无进展生存期(Progress Free Survival) 指从随机分组开始到第一次肿瘤进展或死亡时间。通常作为晚期肿瘤疗效评价的重要指标。 ORR 客观缓解率(Objective Response Rate) 是指肿瘤缩小达到一定量并且保持一定时间的病人的比例(主要针对实体瘤)，包含完全缓解(CR，Complete Response)和部分缓解(PR，Partial Response)的病例。(客观缓解率是II期试验的主要疗效评价指标，可提供药物具有生物活性的初步证据。但一般不作为III期临床试验的主要疗效指标。) DCR 疾病控制率 (DCR，Disease Control Rate) 是指肿瘤缩小或稳定且保持一定时间的病人的比例(主要针对实体瘤)，包含完全缓解(CR，Complete Response)、部分缓解(PR，Partial Response)和稳定(SD，Stable Disease)的病例 DOR 缓解持续时间 (DOR，Duration of Response) 是指肿瘤第一次评估为CR或PR开始到第一次评估为PD(Progressive Disease)或任何原因死亡的时间。 TTF 治疗失败时间(TTF，Time To Failure) 是指从随机化开始至治疗中止/终止的时间，包括任何中止/终止原因，如疾病进展、死亡、由于不良事件退出、受试者拒绝继续进行研究或者使用了新治疗的时间。(TTF综合了有效性与毒性的评价，是一个具有综合特性的指标，不推荐作为单独支持药物批准的疗效指标。) DDC 疾病控制时间 (DDC，duration of disease control) 是指肿瘤第一次评估为CR、PR或SD开始到第一次评估为PD(Progressive Disease)或任何原因死亡的时间。 OS 总生存期（OS，overall survival） 从随机化（random assignment）开始至因任何原因引起死亡的时间（失访患者为最后一次随访时间；研究结束时仍然存活患者，为随访结束日）。 DOR 总缓解期（Duration of overall response） 从第一次出现CR或PR，到第一次诊断PD或复发的时间。 DSD 疾病稳定期（duration of stable disease） 是指从治疗开始到评价为疾病进展时的这段时间。 ORR 总缓解率 （ORR，overall response rate） 经过治疗CR+PR病人总数占对于总的可评价病例数的比例。 RR 缓解率（RR, response rate） 达到CR、PR的病人占同期病人总数的百分比。 CBR 临床获益率（CBR，clinical benefit rate） CR+PR+SD。 一线治疗 是指诊断以后的首轮治疗，这时的治疗方案效果最好、副作用最小，也称为基本治疗或疗法。一线治疗的目的是在可能的情况下“治愈”癌症。 二线治疗 指的是在一线治疗后，患者再次出现疾病进展，对一线治疗方案产生耐药，需要更换抗癌机理不同的方案。和一线相比，二线治疗方案或疗效劣于一线，或副作用偏大，或价格偏高。 三线治疗 指的是二线治疗失败后，再次换用的其它治疗方案。一般到三线治疗时，可选择的药物和有效的治疗方案就会越少。]]></content>
      <categories>
        <category>NGS</category>
        <category>知识沉淀</category>
        <category>肿瘤检测</category>
        <category>缩略语</category>
      </categories>
      <tags>
        <tag>数据积累</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[实验原理-Barcode]]></title>
    <url>%2F01.%E7%9F%A5%E8%AF%86-01.NGS%2F%E5%AE%9E%E9%AA%8C%E5%8E%9F%E7%90%86-Barcode%2F</url>
    <content type="text"><![CDATA[barcode是做什么的$\qquad$前NGS检测在精准医疗领域广泛应用，但是目前主要测序机型和实际样本数据需求量之前仍然存在较大的距离，以MGISeq-2000为例，一次测序下机数据量约320G（80G*4Lane），但是目前临床产品所需的数据量普遍达不到这样的需求（一般在15G~40G不等），有一些小的靶向捕获芯片，所需数据量甚至不足1G。 $\qquad$在这种测序仪器的测序能力远大于单一测试样本需求数据量的情况下，为避免仪器浪费，一个lane同时测定多个样品成为很自然的思路。然而为了区分多种样品的序列，就必须要给不同样品加上特定的“标签”，从而可以在后续数据分析时将不同样品数据分开，而这个“标签”就是barcode。 $\qquad$简言之，barcode就是测序中混合样品的”身份证“，用于区分不同样品。 如何选择好的barcodebarcode的选择有两个原则：碱基平衡和激光平衡。所有的原则，都是尽可能保证数据的分布均匀，不会给测序过程带来严重的干扰。 碱基平衡碱基平衡是指的需要兼顾barcode序列的平衡度与复杂度，平衡度是指的碱基的比例是均衡的（1:1是最均衡的），而复杂度是指的碱基的种类是多样的（四种碱基同时存在是最多样的）。 所以最好的barcode序列应该是同时有A、T、G、C四种碱基，且各碱基所占比例近似均为25%。 此处所说的碱基平衡是指的多个barcode之间的平衡，并非一个barcode内部的碱基平衡。举例来说，有12个转录组样品需要测定，那么就需要12个barcode（假定每个barcode长度为6位），根据碱基平衡原则，第一位barcode碱基应该尽量同时存在A、T、G、C四种碱基，且各碱基所占比例近似均为25%，也就是这12个barcode序列最佳情况应该是以A、T、G、C开头各3个。剩余5个碱基位的barcode以此类推。 激光平衡在illumina测序仪中，A和C两种碱基共用一种激光，由波长660nm的红激光激发；G和T共用一种激光，由波长532 nm的绿激光激发。因此假使不能满足碱基平衡的情况下，可以退而求其次，尽量满足激光平衡。 简单来说，激光平衡就是尽量在使用的一组barcode中满足每个碱基位都是A+C=G+T。 既不满足碱基平衡，又不满足激光平衡的barcode将会有很大的数据分离隐患，或者无法分离开样品，或者无法识别某些测序片段。]]></content>
      <categories>
        <category>NGS</category>
        <category>肿瘤检测</category>
      </categories>
      <tags>
        <tag>实验原理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[顺反式突变]]></title>
    <url>%2F01.%E7%9F%A5%E8%AF%86-01.NGS%2F%E9%A1%BA%E5%8F%8D%E5%BC%8F%E7%AA%81%E5%8F%98%2F</url>
    <content type="text"></content>
      <categories>
        <category>NGS</category>
        <category>肿瘤检测</category>
        <category>标准化</category>
      </categories>
      <tags>
        <tag>数据积累</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[靶向测序策略-扩增法和杂交捕获]]></title>
    <url>%2F01.%E7%9F%A5%E8%AF%86-01.NGS%2F%E9%9D%B6%E5%90%91%E6%B5%8B%E5%BA%8F%E7%AD%96%E7%95%A5-%E6%89%A9%E5%A2%9E%E6%B3%95%E5%92%8C%E6%9D%82%E4%BA%A4%E6%8D%95%E8%8E%B7%2F</url>
    <content type="text"><![CDATA[NGS技术正逐年成熟，这使得全基因组测序的成本越来越低，但是对全基因组进行测序后得到的极其庞大、繁杂的数据量的分析工作并没有随之一起变得更加简单，恰恰相反，更高的测序深度反而导致最后的数据量变得更加庞大了,分析工作也变得更加困难。于是，测序技术的发展出现了两个极端的方向：一种是大而全的全基因组测序，一种是小而精的靶向重测序。 相比于全基因组测序（WGS），靶向重测序技术直接从样品中对感兴趣的基因组区域进行分离测序。这种方式能够更加高效且经济地发挥NGS技术的优势，后续的分析速度也会有跨越性的提升。比如外显子组仅占基因组的1%左右，但却包含了绝大部分的已知致病突变，将外显子区域分离出来后单独进行测序，后续的分析就能降低99%的工作量，极大的加快了分析的速度。 在遗传突变、肿瘤筛查等领域，靶向重测序所能达到的灵敏度也是全基因组测序完全无法实现的。由于靶向重测序在测序前就对基因的目标区域进行了分离与富集，目标区域的大幅减少可实现5000×甚至更高的测序深度。测序深度的提高意味着更高的灵敏度（能够检测低频率的变异），其检测极限低至0.1%。 杂交捕获技术通过设计与目标片段互补的生物素化探针，使其与含目的基因的片段进行杂交，以达到将目的基因片段富集后进行高通量测序的目的。根据支持物的不同，探针杂交捕获技术分为液相杂交与固相杂交两种。固相杂交由于其在花费与操作上的劣势，已基本被淘汰；液相杂交是在溶液中，目标片段和带有生物素标记的探针直接杂交，然后利用被链霉亲和素包裹的磁珠对杂交了生物素探针的片段进行吸附。洗去游离DNA后，将富集得到的DNA进行扩增，构建高通量测序文库。 优势 对目标序列有一定的容错率，75%的相似度就可以捕获。（doi:10.1128/mBio.01491-15）； 对DNA 完整性要求低，捕获法基于接头互补链接的方式构建文库，理论上可以容忍DNA片段长度比扩增法短； 可以处理cfDNA；劣势 捕获法对DNA起始量的要求是相对较高的，原因是需要先打断，而这个打断的过程通常有损失。损失最大的应该是超声法，其次是酶切法，损失最小的是转座酶法。扩增子捕获技术 扩增子捕获测序技术是一种目标区域高通量测序技术，利用特异性引物来对感兴趣的DNA区域进行PCR扩增，形成高度富集的DNA库，将PCR产物纯化后再进行文库构建和高通量测序。篇首所述的Ion Ampliseq技术便归属于扩增子捕获技术。 优势 PCR扩增这个灵敏度很高，对DNA起始量需求低；劣势 CNV、SV的检测受限（PCR效率会淹没基因组本身的情况） 对序列的相似度要求高，尤其是3’端，必须100%匹配否则无法扩增。 对片段完整性要求高，DNA过度片段化，会导致没有设计扩增子的空间。cfDNA（170bp左右）的设计已经困难； 不适用于甲基化（亚硫酸氢盐处理签不能PCR)参考参考信息参考信息]]></content>
      <categories>
        <category>NGS</category>
        <category>肿瘤检测</category>
      </categories>
      <tags>
        <tag>实验原理</tag>
        <tag>靶向测序</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[肿瘤产品资质相关概念]]></title>
    <url>%2F01.%E7%9F%A5%E8%AF%86-01.NGS%2F%E6%A6%82%E5%BF%B5-%E4%BA%A7%E5%93%81%E8%B5%84%E8%B4%A8%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5%2F</url>
    <content type="text"><![CDATA[资质各阶段环节 Reseach Use Only (RUO) products :Investigational Use Only (IUO) Products :General Purpose Reagent (GPR) Products :Analyte Specific Reagent (ASR) Products :ReferenceRegulatory &amp; More]]></content>
      <categories>
        <category>NGS</category>
      </categories>
      <tags>
        <tag>基础概念</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[靶向捕获相关概念]]></title>
    <url>%2F01.%E7%9F%A5%E8%AF%86-01.NGS%2F%E6%A6%82%E5%BF%B5-%E9%9D%B6%E5%90%91%E6%8D%95%E8%8E%B7%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5%2F</url>
    <content type="text"><![CDATA[cfDNA片段长度cfDNA片段长度约170bp 参考文献 Lengthening and shortening of plasma DNA in hepatocellular carcinoma patients 中靶率（on target rate）液相杂交捕获是一种容错型富集方法，无法做到100%中靶（On target）。在测序下机数据中，总存在一定比例的脱靶数据（Off target），即非靶向区域数据，若该部分数据占比过多，会造成有效利用数据减少。通常使用碱基或者读长片段来计算中靶率，中靶率代表了靶向捕获区域范围内的碱基数或读长片段数占有效测序下机数据中碱基总数或读长片段总数的比例。本次评估中使用碱基数作为评估标准，即%中靶=中靶的碱基数/总有效碱基数。 覆盖度（coverage）覆盖度评估包括读长深度（Read depth）以及完整性（completeness）。需要评估并明确平均以及最小覆盖深度、覆盖均一性以及超过最小覆盖深度的目标区域碱基比例的阈值。需要评估产品宣传检测区域的覆盖度或者完整度比例。平均覆盖深度（average coverage depth）覆盖深度指被测序的DNA片段比对（mapping）到基因组靶向区域的次数，平均覆盖深度指整个检测区域中，各靶向区域覆盖深度的均值，靶向区域被覆盖的越深，其测序结果的可靠性和灵敏度越高。当评估检测所使用的适合的覆盖深度，可以使用标准品或者前期特征化的样本来进行深度定义，即适当平均深度条件下，额外的测序深度覆盖度不能显著的提高测序的准确性。 覆盖均一性（coverage uniformity）在液相杂交捕获体系中，针对不同GC含量、碱基重复性区域等，探针经过捕获以及PCR扩增后会产生不同程度的数据偏好性，体现在不同目标区域检测到的覆盖深度不完全相同，标准差与平均数的比值越小，说明探针的捕获均一性越好。ps:GC含量高的区域经常会降低覆盖均一性。 Fold80芯片捕获区 80%的区域可以达到平均深度的所需要增加的数据量； 一般经验上要求Fold-80要小于2。一个比较直观的示意图如下：Evenness of coverage can be evaluated by the fold80 measure which represents the amount of additional sequencing needed to have 80% of all targets covered at the currently observed mean. It is computed as the mean coverage divided by the 20th percentile. Smaller values indicate tighter coverage distributions. Left, large fold80 values correspond to a wide distribution and uneven coverage; Right, small values correspond to a narrow distribution and even coverage. 同样Twist也提供了一个比较详细的介绍说明CD: desired coverageCM: the mean coverage actually observed in the experiment.引自： TWIST: The Importance of Coverage Uniformity Over On-Target Rate for Efficient Targeted NGS]]></content>
      <categories>
        <category>NGS</category>
      </categories>
      <tags>
        <tag>基础概念</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[肿瘤相关融合人群检出率报道]]></title>
    <url>%2F01.%E7%9F%A5%E8%AF%86-01.NGS%2F2022-01-07.%E8%82%BF%E7%98%A4%E7%9B%B8%E5%85%B3%E8%9E%8D%E5%90%88%E4%BA%BA%E7%BE%A4%E6%A3%80%E5%87%BA%E7%8E%87%E6%8A%A5%E9%81%93%2F</url>
    <content type="text"><![CDATA[NTRK 相关融合参考文献Genomic context of NTRK1/2/3 fusion-positive tumours from a large real-world population 内容本研究旨在查询综合基因组分析数据的大型真实世界数据库，以描述NTRK基因融合的基因组景观和流行情况。NTRK融合阳性肿瘤是从超过 295,000 名癌症患者的 FoundationCORE ®数据库中确定的。我们调查了NTRK融合的患病率和伴随的基因组景观，预测了患者血统，并将 FoundationCORE 队列与 entrectinib 临床试验队列进行了比较 (ALKA-372-001 [EudraCT 2012-000148-88]; STARTRK-1 [NCT02097810-2]; [NCT02568267]）。整体NTRK在具有 88 个独特融合伴侣对的 45 种癌症中，融合阳性肿瘤的患病率为 0.30%，其中 66% 以前未报告过。在所有病例中，≥18 岁和 &lt;18 岁患者的患病率分别为 0.28% 和 1.34%；5 岁以下患者的患病率最高 (2.28%)。在唾液腺肿瘤中观察到NTRK融合的最高流行率(2.62%)。存在NTRK基因融合并没有与其他临床生物标志物可操作相关成分; 在乳腺癌或结直肠癌 (CRC) 中没有与已知的致癌驱动因素同时发生。然而，在 CRC 中，NTRK融合阳性与自发性微卫星不稳定性 (MSI) 相关；在这个 MSI CRC 子集中，与BRAF互斥观察到突变。NTRK融合阳性肿瘤类型在 FoundationCORE 和 entrectinib 临床试验中具有相似的频率。NTRK基因融合患病率因年龄、癌症类型和组织学而异。询问大型数据集有助于更好地了解癌症非常罕见的分子亚群的特征，并允许识别基因组模式和以前未报告的融合伙伴，在较小的数据集中不明显。 数据展示：]]></content>
      <categories>
        <category>NGS</category>
        <category>知识沉淀</category>
        <category>肿瘤检测</category>
        <category>数据积累</category>
      </categories>
      <tags>
        <tag>数据积累</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[肿瘤相关融合人群检出率报道]]></title>
    <url>%2F01.%E7%9F%A5%E8%AF%86-01.NGS%2F%E6%A6%82%E5%BF%B5-%E8%82%BF%E7%98%A4-%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5%2F</url>
    <content type="text"><![CDATA[概念tumorTumors 更多的是指肿块，但这些肿块并非全部都是恶性的，其中存在一些是良性，所以需要记住 tumors 和 cancers 并非同义词。 cancers是tumors中的一部分（恶性部分）。 cancer是tumor中的一种，特指恶性肿瘤（malignant tumor），癌症是用来形容具有异常的失控的细胞分裂，并能侵入其他组织的疾病的一个术语。癌细胞可通过血液和淋巴系统扩散到身体的其他部位。cancer本身也不是一个单一的疾病而是100多种癌症的总称。大多数癌症以其出现的器官或细胞类型命名， 例如，始发于结肠的癌症，被称为结肠癌，始发于皮肤基底细胞的癌症称为基底细胞癌。 癌症类型可归为广义范畴。癌症的种类主要包括： Carcinomacancer that begins in the skin or in tissues that line or cover internal organs. 癌-起始于皮肤或组织，呈线条状或覆于内脏的癌症。 Sarcoma（肉瘤）cancer that begins in bone, cartilage, fat, muscle, blood vessels, or other connective or supportive tissue.起始于骨，软骨，脂肪，肌肉，血管，或其他结缔组织或支持组织的癌症。 Leukemia（白血病）cancer that starts in blood-forming tissue such as the bone marrow and causes large numbers of abnormal blood cells to be produced and enter the blood.起始于造血组织，如骨髓，可生成大量异常血细胞并进入血液的癌症。 Lymphoma and myeloma（淋巴瘤和骨髓瘤）cancers that begin in the cells of the immune system.起始于免疫系统细胞的癌症 Central nervous system cancers（中枢神经系统癌症）cancers that begin in the tissues of the brain and spinal cord.起始于大脑和脊髓组织的癌症。 癌症的起源All cancers begin in cells, the body’s basic unit of life. To understand cancer, it’s helpful to know what happens when normal cells become cancer cells. 所有的癌症起始于细胞---人体基本的生命单位。为了理解癌症，了解正常细胞何时会变为癌细胞非常有用。 The body is made up of many types of cells. These cells grow and divide in a controlled way to produce more cells as they are needed to keep the body healthy. When cells become old or damaged, they die and are replaced with new cells. 身体是由许多类型的细胞构成。因为他们需要保持身体健康，这些细胞在一个可控制的方式下生长和分化，以产生更多的细胞。当细胞衰老或损坏时，细胞死亡并被新的细胞所取代。 However, sometimes this orderly process goes wrong. The genetic material (DNA) of a cell can become damaged or changed, producing mutations that affect normal cell growth and division. When this happens, cells do not die when they should and new cells form when the body does not need them. The extra cells may form a mass of tissue called a tumor. 但是，有时这种有序的过程也会出现错误。一个细胞的遗传物质（DNA），可受损或改变，产生基因突变，从而影响正常的细胞生长和分裂。在这种情况下，细胞应该死亡但并未死亡，身体并不需要时新细胞形成。额外的细胞可能形成一个所谓的肿瘤组织肿块。 Not all tumors are cancerous; tumors can be benign or malignant. 并非所有的肿瘤都是癌症性的，肿瘤也可是良性或恶性。 Benign tumors aren’t cancerous. They can often be removed, and, in most cases, they do not come back. Cells in benign tumors do not spread to other parts of the body. 良性肿瘤不是癌性的。它们通常可以切除，并且，在大多数情况下，不会复发。良性肿瘤的细胞不会扩散到身体的其他部位。 Malignant tumors are cancerous. Cells in these tumors can invade nearby tissues and spread to other parts of the body. The spread of cancer from one part of the body to another is called metastasis. 恶性肿瘤是癌性的。这些肿瘤细胞可侵入附近组织并扩散到身体的其他部位。癌症从身体的一部分扩散至另一部分称为转移。 Some cancers do not form tumors. For example, leukemia is a cancer of the bone marrow and blood. 有些癌症不形成肿瘤。例如，白血病是骨髓和血液的一种癌症。 理解癌症Cancer begins in cells, the building blocks that form tissues. Tissues make up the organs of the body. 癌症细胞起始于细胞，并形成肿块。 Normally, cells grow and divide to form new cells as the body needs them. When cells grow old, they die, and new cells take their place. 通常情况下，细胞生长、分裂，形成新的细胞，这是人体所需要的。当细胞衰老时，细胞死亡，新细胞取代其位置。 Sometimes, this orderly process goes wrong. New cells form when the body does not need them, and old cells do not die when they should. These extra cells can form a mass of tissue called a growth or tumor. 有时，这种有序的过程中出现错误。当身体并不需要时，新细胞形成，衰老的细胞应该死亡但并未死亡。这些额外的细胞可以形成一个组织团块称为增生或肿瘤。 Tumors can be benign or malignant: 肿瘤可以是良性或恶性： Benign tumors are not cancer: 良性肿瘤不是癌性的： Benign tumors are rarely life-threatening. 良性肿瘤很少危及生命。 Generally, benign tumors can be removed, and they usually do not grow back. 一般来说，良性肿瘤可以切除，而他们通常不会复发。 Cells from benign tumors do not invade the tissues around them. 良性肿瘤细胞不侵入周围组织。 Cells from benign tumors do not spread to other parts of the body. 良性肿瘤细胞不会扩散到身体的其他部位。 Malignant tumors are cancer: 恶性肿瘤是癌性的： Malignant tumors are generally more serious than benign tumors. They may be life-threatening. 恶性肿瘤一般都比良性肿瘤严重。他们可能会危及生命。 Malignant tumors often can be removed, but sometimes they grow back. 恶性肿瘤往往可以切除，但有时它们重新生长出来（复发）。 Cells from malignant tumors can invade and damage nearby tissues and organs. 恶性肿瘤细胞可侵入和破坏邻近组织和器官。 Cells from malignant tumors can spread (metastasize) to other parts of the body. Cancer cells spread by breaking away from the original (primary) tumor and entering the bloodstream orlymphatic system. The cells can invade other organs, forming new tumors that damage these organs. The spread of cancer is called metastasis. 恶性肿瘤细胞可以扩散（转移）到身体的其他部位。癌细胞经脱离原（主）肿瘤并进入血液系统或淋巴系统发生扩散。细胞可以侵入其他器官，形成新的肿瘤并损害这些器官。癌症的扩散称为转移。 Most cancers are named for where they start. For example, lung cancer starts in the lung, and breast cancer starts in the breast. Lymphoma is cancer that starts in the lymphatic system. And leukemia is cancer that starts in white blood cells (leukocytes). 大多数癌症以其起始部位命名。例如，肺癌起始于肺，乳腺癌起始于乳腺。淋巴瘤是起始于淋巴系统的癌症。白血病是起始于白细胞的癌症。 When cancer spreads and forms a new tumor in another part of the body, the new tumor has the same kind of abnormal cells and the same name as the primary tumor. For example, if prostate cancer spreads to the bones, the cancer cells in the bones are actually prostate cancer cells. The disease is metastatic prostate cancer, not bone cancer. For that reason, it is treated as prostate cancer, not bone cancer. Doctors sometimes call the new tumor “distant” or metastatic disease. 当癌细胞扩散，并在身体的其他部位形成一种新的肿瘤时，新的肿瘤具有同一种异常细胞，名称与原发肿瘤相同。例如，如果前列腺癌扩散到骨，骨头的癌细胞其实是前列腺癌细胞。这种疾病是转移性前列腺癌，而不是骨癌。出于这个原因，被视为前列腺癌进行治疗，而不是骨癌。医生有时称之为“远部”新肿瘤或转移性疾病。 参考tumor，cancer，carcinoma…你弄清楚了吗？]]></content>
      <categories>
        <category>NGS</category>
      </categories>
      <tags>
        <tag>基础概念</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Fish验证简介]]></title>
    <url>%2F01.%E7%9F%A5%E8%AF%86-01.NGS%2F2022-01-05-Fish%E9%AA%8C%E8%AF%81%E7%AE%80%E4%BB%8B%2F</url>
    <content type="text"><![CDATA[By 赵东晓 NGS检测过程中经常遇到需要第三方验证的需求，不管是为了证明检测结果准确性，还是前期方法学检测的需求，都不可避免的需要面对各种第三方验证。而Fish检验就是一个常用的用来检验融合的方法。最近同事对Fish进行了一些调研，简单梳理以备后用。 Fish调研NGS报告融合推药逻辑 1类融合多见，为常见伴侣融合，是指南里面提到的，且有功能证据证明激活的融合亚型； 针对1类融合，相同基因，不同断点和亚型，现在药物推荐还没有区分差别。 2类融合少见，是指那些没有指南支持，仅有少量文献证据的罕见融合； 3类融合不推药； 通过FISH（荧光原位杂交）检测融合，目前有2种方法： 第1种（应用较广）：可以设计已知融合基因的探针，比如针对ALK的，用红色和黄色覆盖ALK基因的上下游区域，正常就是红黄在一起，融合就是ALK发生了断裂就是红黄分开了。此方法检出的融合，不能确定是何种融合亚型，推药只能按照广义层面上的融合大类来推荐用药。 第2种（不经济实用）：可以设计已知融合基因和伴侣基因的探针，比如针对ALK和EML4的，ALK全部用红色探针覆盖，EML4用黄色探针覆盖，正常就是红黄分开，融合就是红黄在一起。此方法检出的融合，为常见伴侣基因的融合，为1类融合，推药逻辑没有差异。 以ALK探针为例：Vysis ALK break-apart probe (Abbott Molecular)美国食品药品监督管理局(FDA)已批准的FISH分离探针试剂盒(Vysis ALK Break Apart FISH Probe Kit; Abbott Molecular, Inc.)可用于检测ALK融合基因的表达。该试剂盒设计的两种探针分别标记ALK基因第20号外显子断裂点的两端，在5 ‘(着丝粒)侧有一个约442 kb的绿色探针，在3 ‘(端粒)侧有一个约300 kb的橙色探针，橙色区域包括了ALK激酶活性区。 ALK基因：chr2:29,415,640-30,144,452 (GRCh37/hg19 by Entrez Gene)，Size:728,813 bases. 结果判读阴性信号： 一个癌细胞核内至少有一个橙色和一个绿色信号，橙色信号与绿色信号相互邻近或叠加，其问距小于两个信号直径。 一个癌细胞核内至少有一个橙色和一个绿色信号，有单独的绿色信号，但无相应的橙色信号。 阳性信号： 一个癌细胞核内至少有一个橙色和一个绿色信号，橙色和绿色信号的间距大于两个信号直径。 一个癌细胞核内至少有一个橙色和一个绿色信号，有单独的橙色信号，但无相应的绿色信 阳性判定： 计数50个肿痛细胞，若阳性肿指细胞数多于25个，该样本为阳性； 若阳性肿指细胞数小于5个，该样本为阴性； 阳性肿指细胞数介于5-25个，为可疑阳性。需要另计数50个肿指细胞，将前后两次的合计100个肿瘤细胞的信号状况汇总。 若阳性肿痛细胞比例少于1598(15/100),该样本为阴性。 着阳性肿痛细胞比例多于159(15/100,该样本为阳性。 该方法只能判断ALK基因是否断裂，可以检测所有的融合型，但不能区分与其发生融合的基因是什么。阳性细胞为存在橙绿信号分离或单独橙色信号的细胞。 ALK基因的检测方法ALK基因的检测方法有荧光原位杂交（FISH）、显色原位杂交（CISH）、免疫组化（IHC）、基于PCR的各种方法、NGS二代测序等。 检测ALK融合的各种技术的特点总结参考：中国间变性淋巴瘤激酶(ALK)阳性非小细胞肺癌诊断专家共识(2013版) 参考：Int J Mol Sci. 2019 Aug 13;20(16):3939. doi: 10.3390/ijms20163939. 总结ALK基因融合的检测要尽可能采用两种以上的方法相互印证，以免出现漏检或假阴性结果，使得部分病友失去从靶向药中获益的机会。检测实验室应该根据组织标本类型选择合适的检测技术。当怀疑一种技术的可靠性时（如FISH的肿瘤细胞融合率接近15％时），可以考虑采用另一种技术加以验证。]]></content>
      <categories>
        <category>NGS</category>
        <category>肿瘤检测</category>
      </categories>
      <tags>
        <tag>肿瘤</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Jbrowse部署安装]]></title>
    <url>%2F03.%E5%BA%94%E7%94%A8-01.software_in_NGS%2FSoftware-%E5%8F%AF%E8%A7%86%E5%8C%96-Jbrowse%E9%83%A8%E7%BD%B2%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[腾讯云服务如果服务不存在，登陆腾讯云服务器执行如下命令12cd /var/www/html/jbrowsenohup npx serve . &amp; 前言JBrowse 是今天要介绍的主角。它是GMOD( Generic Model Organism Database) 开源项目的一部分,这个项目收集了非常多的开源软件工具，用来管理、可视化、存储、展示基因组学数据。 the Generic Model Organism Database project, a collection of open source software tools for managing, visualising, storing, and disseminating genetic and genomic data. ---GMOD官网在GMOD的众多项目中最出名的应该是Galaxy，而JBrowse则是GMOD之前一款基因浏览器JGBrowse的继任者。 JBrowse资料JBrowse官网GMOD-JBrowse WIkiJBrowse2 Github项目 JBrowse 部署安装镜像调整设置yum镜像123456789cd /etc/yum.repos.d/mkdir centos.backmv *.repo centos.back# 获取新镜像wget http://mirrors.aliyun.com/repo/Centos-7.repo 阿里的源wget http://mirrors.163.com/.help/CentOS7-Base-163.repo 163的源yum clean all # 清空缓存yum makecache # 生成缓存 设置npm镜像123456789# 设置npm源为淘宝NPM镜像npm config set registry https://registry.npm.taobao.org# 查看是否设置成功npm config get registry# 直接使用cnpm命令行工具代替默认的npmnpm install -g cnpm --registry=https://registry.npm.taobao.org# 设置回默认的官方源npm config set registry https://registry.npmjs.org/ 环境需求Node version must be &gt;=12.0.0 to use this CLI需要安装 redhat-lsb1yum -y install redhat-lsb 升级Node版本安装npm的n模块(专门用来管理nodejs的版本)12npm install -g n # 安装n模块n stable # 升级到最新的稳定版本 安装jbrowse123### operate under a normal user so this guide does not use thisnpm install -g @jbrowse/clijbrowse create /var/www/html/jbrowse2 也可以直接下载软件包： 下载最新版JBrowse2软件包 选择其中的Web版本(jbrowse-web-v1.5.3.zip)，解压后进入目录,运行npm服务即可 123456789101112131415unzip jbrowse-web-v1.5.3.zipcd jbrowsenpx serve .弹出如下信息，则证明服务已经成功启动。 ┌──────────────────────────────────────────────────┐ │ │ │ Serving! │ │ │ │ - Local: http://localhost:39963 │ │ - On Your Network: http://172.21.0.8:39963 │ │ │ │ This port was picked because 3000 is in use. │ │ │ └──────────────────────────────────────────────────┘ 配置jbrowse完成了jbrowse以后，下一步就是Jbrowse的配置，有两种方案可以进行配置 命令行的方法 图形界面 使用命令行添加数据添加基因组12# 添加Hg19的基因组，由于添加过程会在执行目录更新config.json文件，因此记得在jbrowse目录下执行！！jbrowse add-assembly /root/Database/hg19.fa --load copy --displayName Hg19 添加 bam 文件123jbrowse add-track /data/volvox.bam --load copy# 如果bam缺少索引文件，需要建立索引。samtools index volvox.bam 添加 BigWig/BigBed 文件因为jbrowse添加的bed文件必须是BigBed文件(二进制的bed文件)，因此bed文件添加前要先通过bedToBigBed 进行处理12345sort -k1,1 -k2,2n unsorted.bed &gt; input.bed #bed文件必须先排序bedToBigBed input.bed chrom.sizes myBigBed.bb## Download bigwig or bigbed filejbrowse add-track volvox-sorted.bam.coverage.bw --load copy 除了bed文件外，jbrowse还支持 BigWig有多种方式可以生成BigWig文件以下介绍的方式有基于wiggle (wig) 格式的文件，通过 软件：wigToBigWig 生成）， BigWig(.bw)的生成需要准备一个BedGraph文件格式如下： 123456#Chr Start End Value(可以自行定义)chr1 10270744 10270745 0.278208333333333chr1 10270745 10270746 0.275666666666667chr1 10270746 10270747 0.276833333333333chr1 10270747 10270748 0.27675chr1 10270748 10270749 0.279083333333333 排序后，通过bedGraphToBigWig工具，将BedGraph文件转换成.bw文件，直接加载12sort -k1,1 -k2,2n $file.bedGraph &gt; $file.sorted.bedGraph bedGraphToBigWig $file.sorted.bedGraph hg19.fa.fai $file.bw Adding a variant track12345678#首先对vcf进行排序bcftools sort file.vcf &gt; file.sorted.vcf#vcf需要进行压缩和建立索引bgzip yourfile.vcftabix yourfile.vcf.gzjbrowse add-track /data/yourfile.vcf.gz --load copy Adding a GFF3 file with GFF3Tabix1234gt gff3 -sortlines -tidy -retainids yourfile.gff &gt; yourfile.sorted.gffbgzip yourfile.sorted.gfftabix yourfile.sorted.gff.gzjbrowse add-track yourfile.sorted.gff.gz --load copy]]></content>
      <categories>
        <category>software</category>
        <category>可视化</category>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>可视化</tag>
        <tag>Jbrowse</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[芯片设计(Panel-Design)]]></title>
    <url>%2F01.%E7%9F%A5%E8%AF%86-01.NGS%2F2021-12-12-Panel-Design%2F</url>
    <content type="text"><![CDATA[前言目前探针的靶向捕获测序，已经发展临床检测应用的常规技术手段。因此在进行靶向捕获测序时，我们需要面临和解决的第一个问题，就是我们应该如何设计芯片探针。2014年，发表的一篇文章An ultrasensitive method for quantitating circulating tumor DNA with broad patient coverage为我们进行靶向捕获区域设计提供了一个可参考的方案CAPP-Seq Selector。 名词概念 RI值 ：Recurrence Index (RI), is defined as the number of unique patients (i.e., tumors) with somatic mutations per kilobase of a given genomic unit (here, exon)。RI =（n×1000）÷L，其中n为所述外显子区间的患者数目，L为外显子区间的序列长度（bp）。 方法介绍 本方法是针对NSCLC设计的，但是也可以推广应用到其他高频突变已经明确的癌种。 首先，我们挑选出一些重要的外显子，这些外显子包含了COSMIC和其他来源（Somatic mutations affect key pathways in lung adenocarcinoma.[PubMed: 18948947]；Identifying cancer driver genes in tumor genome sequencing studies. [PubMed: 21169372]）的潜在驱动基因中反复出现的突变。 然后使用TCGA数据库，获取NSCLC的407例样本的WES测序数据。 最后应用了一种迭代算法来最大化每个患者的错义突变数量，同时最小化整个芯片大小。 Most human cancers are relatively heterogeneous for somatic mutations in individual genes. Specifically, in most human tumors, recurrent somatic alterations of single genes account for a minority of patients, and only a minority of tumor types can be defined using a small number of recurrent mutations (&lt;5-10) at predefined positions. Therefore, the design of the selector is vital to the CAPP-Seq method because (1) it dictates which mutations can be detected with high probability for a patient with a given cancer, and (2) the selector size (in kb) directly impacts the cost and depth of sequence coverage. For example, the hybrid selection libraries available in current whole exome capture kits range from 51-71 Mb, providing ~40-60 fold maximum theoretical enrichment versus whole genome sequencing. The degree of potential enrichment is inversely proportional to the selector size such that for a ~100 kb selector, &gt;10,000 fold enrichment should be achievable. We employed a six-phase design strategy to identify and prioritize genomic regions for the CAPP-Seq NSCLC selector as detailed below. Three phases were used to incorporate known and suspected NSCLC driver genes, as well as genomic regions known to participate in clinically actionable fusions (phases 1, 5, 6), while another three phases employed an algorithmic approach to maximize both the number of patients covered and SNVs per patient (phases 2–4). The latter relied upon a metric that we termed “Recurrence Index” (RI), defined as the number of NSCLC patients with SNVs that occur within a given kilobase of exonic sequence (i.e., No. of patients with mutations / exon length in kb). RI thus serves to measure patient-level recurrence frequency at the exon level, while simultaneously normalizing for gene or exon size. As a source of somatic mutation data uniformly genotyped across a large cohort of patients, in phases 2–4, we analyzed non-silent SNVs identified in TCGA whole exome sequencing data from 178 patients in the Lung Squamous Cell Carcinoma dataset (SCC)10 and from 229 patients in the Lung Adenocarcinoma (LUAD) datasets (TCGA query date was March 13, 2012). Thresholds for each metric (i.e. RI and patients per exon) were selected to statistically enrich for known/suspected drivers in SCC and LUAD data (Supplementary Fig. 1). RefSeq exon coordinates (hg19) were obtained via the UCSC Table Browser (query date was April 11, 2012) The following algorithm was used to design the CAPP-Seq selector (parenthetical descriptions match design phases noted in Fig. 1b). Phase 1 (Known drivers) Initial seed genes were chosen based on their frequency of mutation in NSCLCs. Analysis of COSMIC (v57) identified known driver genes that are recurrently mutated in ≥9% of NSCLC (denominator ≥500 cases). Specific exons from these genes were selected based on the pattern of SNVs previously identified in NSCLC. The seed list also included single exons from genes with recurrent mutations that occurred at low frequency but had strong evidence for being driver mutations, such as BRAF exon 15, which harbors V600E mutations in &lt;2% of NSCLC. Phase 2 (Max. coverage) For each exon with SNVs covering ≥5 patients in LUAD and SCC, we selected the exon withhighest RI that identified at least 1 new patient when compared to the prior phase. Amongexons with equally high RI, we added the exon with minimum overlap among patients alreadycaptured by the selector. This was repeated until no further exons met these criteria. Phase 3 (RI ≥ 30) For each remaining exon with an RI ≥ 30 and with SNVs covering ≥3 patients in LUAD andSCC, we identified the exon that would result in the largest reduction in patients with only 1SNV. To break ties among equally best exons, the exon with highest RI was chosen. This wasrepeated until no additional exons satisfied these criteria. Phase 4 (RI ≥ 20) Same procedure as phase 3, but using RI ≥ 20. Phase 5 (Predicted drivers) We included all exons from additional genes previously predicted to harbor driver mutations inNSCLC12,13. Phase 6 (Add fusions) For recurrent rearrangements in NSCLC involving the receptor tyrosine kinases ALK, ROS1,and RET, the introns most frequently implicated in the fusion event and the flanking exons wereincluded. All exons included in the selector, along with their corresponding HUGO gene symbols andgenomic coordinates, as well as patient statistics for NSCLC and a variety of other cancers, areprovided in Supplementary Table 1, organized by selector design phase. 参考资料文献An ultrasensitive method for quantitating circulating tumor DNA with broad patient coverage $\color{red}{ed}$]]></content>
      <categories>
        <category>NGS</category>
        <category>肿瘤检测</category>
      </categories>
      <tags>
        <tag>芯片设计</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python包-pyechart画web版图片]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-02.python%2FPython-%E5%8C%85-pyechart%E7%94%BBweb%E7%89%88%E5%9B%BE%E7%89%87%2F</url>
    <content type="text"><![CDATA[pyechart quickstart 12345678from pyecharts.charts import Barbar = Bar()bar.add_xaxis([&quot;衬衫&quot;, &quot;羊毛衫&quot;, &quot;雪纺衫&quot;, &quot;裤子&quot;, &quot;高跟鞋&quot;, &quot;袜子&quot;])bar.add_yaxis(&quot;商家A&quot;, [5, 20, 36, 10, 75, 90])# render 会生成本地 HTML 文件，默认会在当前目录生成 render.html 文件# 也可以传入路径参数，如 bar.render(&quot;mycharts.html&quot;)bar.render()]]></content>
      <categories>
        <category>编程拾慧</category>
        <category>python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[本地构建control集合进行过滤]]></title>
    <url>%2F01.%E7%9F%A5%E8%AF%86-01.NGS%2F2021-12-08.Local_control%2F</url>
    <content type="text"><![CDATA[各个流程的处理过程大同小异 GATK PONA Panel of Normal or PON is a type of resource used in somatic variant analysis. Depending on the type of variant you’re looking for, the PON will be generated differently. What all PONs have in common is that : they are made from normal samples (in this context, $\color{red}{“normal”}$ means derived from healthy tissue that is believed to not have any somatic alterations) their main purpose is to capture recurrent technical artifacts in order to improve the results of the variant calling analysis. As a result, the most important selection criteria for choosing normals to include in any PON are the technical properties of how the data was generated. It’s very important to use normals that are as technically similar as possible to the tumor (same exome or genome preparation methods, sequencing technology and so on). Additionally, the samples should come from subjects that were young and healthy to minimize the chance of using as normal a sample from someone who has an undiagnosed tumor. Normals are typically derived from blood samples. There is no definitive rule for how many samples should be used to make a PON (even a small PON is better than no PON) but in practice we recommend aiming for a minimum of 40. At the Broad Institute, we typically make a standard PON for a given version of the pipeline (corresponding to the combination of all protocols used in production to generate the sequence data, starting from sample preparation and including the analysis software) and use it to process all tumor samples that go through that version of the pipeline. Because we process many samples in the same way, we are able to make PONs composed of hundreds of samples. MSKFiltering for high confidence mutations: Raw SNV and indel calls are subjected to a series of filtering steps to ensure only high-confidence calls are admitted to the final step of manual review. These parameters include (1) evidence of it being a somatic mutation (i.e., ratio between mutation frequencies in the tumor and normal samples to be ≥ 5.0); (2) whether the mutation is a known hotspot mutation (refer to Appendix 1a for details); (3) reference on in house ‘standard normal’ based on common artifacts; (4) technical characteristics that use coverage depth (DP), number of mutant reads (AD), mutation frequency (VF). The filtering scheme and threshold are shown in Figure 1 below. The threshold values for the filtering criteria were established based on paired-sample mutation analysis on replicates of normal FFPE samples, and optimized to reject all false positive SNVs and almost all false positive indel calls from the reference dataset. BGI 目前策略Control集合构建SOP reference GATK:PON EVALUATION OF AUTOMATIC CLASS III DESIGNATION FOR MSK-IMPACT]]></content>
      <categories>
        <category>NGS</category>
        <category>肿瘤检测</category>
      </categories>
      <tags>
        <tag>NGS</tag>
        <tag>肿瘤</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Pseudogene]]></title>
    <url>%2F01.%E7%9F%A5%E8%AF%86-01.NGS%2F2023-06-09.pseudogene_in_NGS%2F</url>
    <content type="text"><![CDATA[什么是假基因假基因是与功能基因具有显着同源性的 DNA 序列，但它们缺乏用于转录的启动子序列或包含其他阻止功能产物形成的突变，不能产生功能性蛋白的序列[1]。同时由于假基因通常不面临选择压力，因此更容易出现变异信号的积累。根据 GENCODE统计数据，人类基因组中有14737个假基因（编码基因19393个），对应3391个 “parent gene”。当然其实截止目前，人类基因组的基因总数目其实也尚未在业界达成一个统一性的共识。但是通过GENCODE的数据，至少我们可以看到，在基因组中，假基因的数目占比非常巨大。 假基因的起源提到假基因的产生，其实就绕不过新基因的产生（假基因只是一类不能产生功能性蛋白的特殊新基因）。而新基因的产生过程会涉及到各种分子事件，同时这些事件必须发生在种系中才能遗传给下一代。当新产生的基因能进行遗传后，变回参与到和环境的互作（压力选择）中，最终取得优势得以保存，或者存在劣势被淘汰。假基因最初被定义为类似于已知基因但不能产生功能性蛋白质的序列，对假基因的研究不仅揭示了基因退化的频率，而且还揭示了许多曾经被认为是退化蛋白质编码基因的序列实际上是功能性 RNA 基因。多年来，科学家提出了几种产生新基因的机制。这些包括基因复制、转座子蛋白驯化、横向基因转移、基因融合、基因裂变和从头起源[4]。 Gene Duplication 基因复制 Transposable Element Protein Domestication 转座子蛋白驯化 Lateral Gene Transfer 横向基因转移 Gene Fusion and Fission 基因融合与裂变 De Novo Gene Origination 从头基因起源 假基因起源于与蛋白质编码基因相同的机制，随后是随后破坏阅读框或导致过早终止密码子插入的致残突变（例如，核苷酸插入、缺失和/或替代）的积累[2]。假基因可大致分为两类： - 未加工的假基因通常包含内含子，并且它们通常位于其旁系同源基因的旁边。 - 加工过的假基因被认为起源于逆转录转座；因此，它们缺少内含子和启动子区域，但它们通常包含聚腺苷酸化信号，并且两侧是同向重复序列。逆转录错误和缺乏适当的监管环境通常会导致基因转录的退化。 给定基因组中假基因的丰度通常取决于基因复制和丢失的速率。哺乳动物似乎有大量经过处理的假基因——大约 8,000 个[3]。 假基因对分析带来的影响基于假基因的产生机制，我们不难发现，假基因，尤其是通过基因复制、横向基因转移、基因融合等原因产生的假基因会和基因组上其他区域（假基因产生的母本）存在序列的高度同源性。而这些高度同源性的序列存在会使得NGS分析过程变得复杂化： 使用短读长（75-300 bp ），则序列同源性会导致假基因和母本片段区域难以进行区分。 序列同源使准确的reads比对（映射）变得复杂，如下图所示。映射到多个基因组位置的序列读数在分析中被丢弃，这会导致序列覆盖率出现缺口。差异加大时，可以进行有效的区分，但是随着序列同源性增加时，会出现假基因的错误比对（假基因序列变动会被错误识别为变异）。当序列读数与几个基因组位置对齐得同样好时，它们将被丢弃。而在NGS的靶向捕获中，假基因的存在还会影响杂交捕获过程。 如前所述，人类有上万个假基因（GENCODE 项目），如果我们的检测范围中存在假基因或者假基因对应的编码基因/同源区域，则会影响对应区域的检测准确性，这类区域的准确性会低于没有假基因影响的区域。 有什么方式可以减少假基因的影响 最简单直接的，是在进行芯片涉及阶段，尽可能剔除掉假基因和对应的同源区域及编码基因。从根本上消除假基因的干扰。想获取具体的假基因清单，可以在Ensemble中下载基因结构文件gff,其中基因的biotype 标签会记录基因是否属于假基因。 尽可能采用长读长测序（PE250&gt;PE100&gt;PE50)，长度长测序比短读长能更有效的区分进行同源区域的识别和划分[5]。 提高比对质量值的阈值，仅保留高比对质量的数据进行变异的检出。 涉及长Long-Range PCR和Sanger 引物进行验证，确定分析结果。 进行生物信息流程的定制化，以消除影响。 Reference[1]. Wilde CD. Pseudogenes. CRC Crit Rev Biochem. 1986;19(4):323-352.[2]. D’Errico I, Gadaleta G, Saccone C. Pseudogenes in metazoa: origin and features. Brief Funct Genomic Proteomic. 2004;3(2):157-167. doi:10.1093/bfgp/3.2.157[3]. Zhang Z, Carriero N, Gerstein M. Comparative analysis of processed pseudogenes in the mouse and human genomes. Trends Genet. 2004;20(2):62-67. doi:10.1016/j.tig.2003.12.005[4]. Chitra Chandrasekaran, Esther Betrán .Origins of New Genes and Pseudogenes[5]. Vahid Bahrambeigi and others, An Approach for Accurate Molecular Diagnosis of Highly Homologous SDHA Gene, American Journal of Clinical Pathology, Volume 146, Issue suppl_1, September 2016, 214[6]. ensembl]]></content>
      <categories>
        <category>NGS</category>
        <category>肿瘤检测</category>
      </categories>
      <tags>
        <tag>NGS</tag>
        <tag>分子诊断</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[NGS检测原理-实验]]></title>
    <url>%2F01.%E7%9F%A5%E8%AF%86-01.NGS%2F2021-12-07.NGS%E6%A3%80%E6%B5%8B%E5%8E%9F%E7%90%86-%E5%AE%9E%E9%AA%8C%2F</url>
    <content type="text"><![CDATA[NGS在数据下机前需要对数据进行一些列的实验处理后才能进行上机测序，因此了解实验原理才能更好的从根源了解数据处理过程，及各个环节的不同处理方案可能带来的优劣影响。在下游数据处理过程中，更有针对性的对数据处理过程进行优化提升。 DNA的提取细胞裂解预处理加入保护液（如TE buffer）来溶解DNA并防止其降解；在破胞前加入去垢剂来去除杂质并破坏细胞（如SDS是一种表面活性剂，能破坏细胞膜上的脂质，并在低温下使其沉淀） 主要方法机械破碎法（振荡珠磨、液氮研磨、反复冻融等）和酶解法（如溶菌酶溶解细菌细胞壁） 去除杂质杂质如细胞碎片、蛋白质、RNA、腐殖酸等 方法化学法（加入抑制因子沉淀杂质、使用氯仿等有机溶剂溶解杂质）、酶解法（例如蛋白酶K、RNA酶降解蛋白质、RNA） 回收DNA主要方法 醇沉淀法：DNA不溶解于异丙醇、乙醇等 过柱收集法：DNA在高盐环境下可以吸附在硅胶滤膜上，这是大部分试剂盒采用的方法 磁珠吸附法：DNA分子通过氢键吸附到具有磁性的磁珠上，然后在磁场中分离磁珠，常见于自动提取仪 清洗溶解DNA方法 对沉淀的DNA、收集柱DNA以及磁珠吸附的DNA使用70%乙醇清洗； 待乙醇挥发后使用无菌水溶解DNA。 DNA提取原则 保证核酸一级结构的完整性； 核酸样品中不应存在对酶有抑制作用的有机溶剂和过高浓度的金属离子； 其他生物大分子如蛋白质、多糖和脂类分子的污染应降低到最低程度； 其他核酸分子，如RNA，也应尽量去除。 文库制备末端修饰 使用Taq聚合酶补齐不平的末端； 并在两个末端添加突出的碱基A，从而产生粘性末端（若使用Taq酶扩增，则无需末端修饰）； 产生粘性末端的片段可以添加接头（Adaptor）。 添加接头 经过末端修饰后的PCR片段末端具有突出的A尾，而接头具有突出的T尾，可以使用连接酶将接头添加到DNA片段两端。 NEB的接头为特殊的碱基U连接的环状结构（可以增强稳定性），因此连接接头后，还需要将碱基U删除从而形成“Y”形接头。 上一步添加的接头主要是为了后续PCR中作为引物扩增继续添加文库index和与测序平台互补的寡核苷酸序列（此外还作为测序引物Rd1 SP/Rd2 SP）。 之所以为“Y”型开叉结构，是因为每一端接头是两条不互补的序列（每一端都是Rd1 SP与Rd2 SP交错），连接酶没有选择性，每个接头都是只靠突出的T来与DNA连接，“Y”接头保证了每条单序列两端均为不同的测序引物，从而在后续PCR中可以连接不同的寡核苷酸序列（P5/P7）。 过程示意图如下： 磁珠纯化目的添加接头后的文库体系中含有聚合酶、连接酶等各种酶以及辅助物质，接头的添加也是过量的，而且由于末端的不稳定性，容易形成自连片段，鸟枪法打断的片段中也可能有大片段存在，所以需要特殊磁珠（AMPure XP Beads）纯化来去除大片段以及各种杂质，从而获得成功添加接头的文库片段。 原理磁珠可以通过氢键等作用力来吸附DNA片段，磁珠本身不具有片段大小选择的能力，但其储存的buffer里面含有20%的PEG 8000，PEG浓度越大则可以吸附的DNA片段越小。 注意事项磁珠纯化的时候要根据文库片段不同严格控制磁珠添加量（其实是PEG添加量）来实现片段选择。 PCR扩增 添加了接头的DNA片段，可以使用与接头互补的引物来扩增。 此外，片段还需要添加用于区分不同文库的特异性index，以及与测序仪芯片互补的两种寡核苷酸序列（P5/P7）。 第二次磁珠纯化 PCR后需要将产物DNA片段与聚合酶等杂质分离，因此再次进行磁珠纯化。 之后进行质量检测，包括DNA浓度检测、琼脂糖凝胶电泳和片段长度检测，完成建库。 NGS测序仪上机待补充 参考来源 https://mp.weixin.qq.com/s/zNFvod8B-VhX7Kq7OgoRMA ClarkeA C, Prost S, Stanton J a L, et al. From cheek swabs to consensus sequences: anA to Z protocol for high-throughput DNA sequencing of complete humanmitochondrial genomes[J]. Bmc Genomics, 2014, 15(1): 1-12. BowmanS K, Simon M D, Deaton A M, et al. Multiplexed Illumina sequencing librariesfrom picogram quantities of DNA[J]. Bmc Genomics, 2013, 14(1): 135-143. MardisE R. Next-Generation DNA Sequencing Methods[J]. Annual Review of Genomics &amp;Human Genetics, 2008, 9(9): 387-402]]></content>
      <categories>
        <category>NGS</category>
        <category>肿瘤检测</category>
        <category>标准化</category>
      </categories>
      <tags>
        <tag>NGS</tag>
        <tag>肿瘤</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DNA-Damage]]></title>
    <url>%2F01.%E7%9F%A5%E8%AF%86-01.NGS%2F2021-12-07.DNA-Damage%2F</url>
    <content type="text"><![CDATA[背景概念生物学真实损伤和实验引入损伤间的差异。DNA易受多种损伤，其中之一就是氧化损伤。随着时间的推移，这种类型的损害会逐渐累积，破坏修复系统，导致健康问题，最终导致疾病。 在生理环境下，一旦某一个碱基发生改变如果没有被错配修复蛋白处理掉，这种错配可以传递给子代，形成突变，一个真实的突变正模板链和对应负模板链应同时被替换。 如果碱基改变是发生在实验阶段，那么其对应链不会发生改变，正链发生G&gt;8-oxoG的改变时，由于其可与A配对，易被测序仪读成T，但对应的负链C碱基，仍会被读为C，而不是A 正负链 真实突变 假突变 正链：5’-3’ 5’- ATC$\color{red}{G}$ATCG-3 5’- ATC$\color{red}{G}$ATCG-3 负链：3’-5’ 3’- TAG$\color{red}{A}$TAGC-5 3’- TAG$\color{red}{C}$TAGC-5 NGS识别氧化损伤的技术基础 处理方式参考文献[16] 参考资料文献 Characterization of background noise in capture-based targeted sequencing data FIREVAT: finding reliable variants without artifacts in human cancer samples using etiologically relevant mutational signatures Sequence Neighborhoods Enable Reliable Prediction of Pathogenic Mutations in Cancer Genomes Needlestack: an ultra-sensitive variant caller for multi-sample next generation sequencing data Location analysis of 8-oxo-7,8-dihydroguanine in DNA by polymerase-mediated differential coding Targeted Single Primer Enrichment Sequencing with Single End Duplex-UMI Analysis of error profiles in deep next-generation sequencing data The use of technical replication for detection of low-level somatic mutations in next-generation sequencing Allele balance bias identifies systematic genotyping errors and false disease associations Overview of Next-Generation Sequencing Technologies Detecting Somatic Mutations in Normal Cells Detecting Rare Mutations and DNA Damage with Sequencing-Based Methods IMPUTOR: Phylogenetically Aware Software for Imputation of Errors in Next-Generation Sequencing UDiTaS™, a genome editing detection method for indels and genome rearrangements Reference standards for next-generation sequencing Discovery and characterization of artifactual mutations in deep coverage targeted capture sequencing data due to oxidative DNA damage during sample preparation 网站 Science Direct: Oxidative Damage exploredna: DNA and Oxidative Damage Oxidative DNA damage: mechanisms, mutation, and disease The genomics of oxidative DNA damage, repair, and resulting mutagenesis]]></content>
      <categories>
        <category>NGS</category>
        <category>肿瘤检测</category>
      </categories>
      <tags>
        <tag>NGS</tag>
        <tag>肿瘤</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TMB简介及华大相关进展]]></title>
    <url>%2F01.%E7%9F%A5%E8%AF%86-01.NGS%2F2021-11-29-TMB%2F</url>
    <content type="text"><![CDATA[TMB的出现及发展TMB 基本现状定义 WES： TMB是指肿瘤基因组内存在的体细胞突变位点数量，可以间接反映肿瘤产生新生抗原的能力。由于早期研究多基于WES检测，因此TMB通常是指单位基因组外显子编码区域（外显子组，exome）的突变数量（mutations, muts），单位为muts/exome。 Panel： 虽然WES是检测TMB的金标准，但WES时间成本和分析成本较高。经过多项大样本研究验证后，TMB检测从WES扩展到了更切合临床实际的靶向二代测序（next-generation sequencingpanel, NGS panel）。靶向测序的基因检测位点比外显子组少，由于不同平台检测方法和测序覆盖的外显子区域长度不同，TMB也被定义为肿瘤基因组区域中每兆碱基（megabase, Mb）发生的碱基替换突变和插入缺失突变的数量总和，单位为muts/Mb。 纳入计算的突变范围 WES TMB被定义为通过WES测序肿瘤组织样本中体细胞非同义突变数量的总和。 Panel 在NGSpanel检测TMB的研究中，纳入TMB计算的是体细胞编码区中碱基替换突变和插入缺失突变，部分NGS panel计算TMB也同时纳入了同义突变，而胚系变异、核苷酸多态性位点、明确的抑癌基因及驱动基因热点突变则不计算在内.(Analysis of 100,000 human cancer genomes reveals the landscape of tumor mutational burden、The COSMIC (Catalogue of Somatic Mutations in Cancer) database and website、ExAC:Analysis of protein-coding genetic variation in 60,706 humans ) TMB计算过程中分母的选择-2023年9月补充实际在进行TMB计算过程中，尤其是使用Panel进行捕获的产品，分母的选择也是成为了困扰。部分机构会直接使用Panel的全长（涵盖内含子之类的区域）作为分母进行计算，但是还有一些机构会选择使用编码区域全长作为分母。所以在这里也记录看到的一些处理方式以供参考。 使用编码区长度 FoundationOne®CDx June 7, 2022 技术文档中提到，The resulting mutation number is then divided by the coding region corresponding to the number of total variants counted; 其实略尴尬，TMB共识普遍建议应该使用的Panel（当然未明确说明编码区和内含子区域的比例）在1MB以上，但是FoundationOne的编码区大小仅753kb。 肿瘤突变负荷应用于肺癌免疫治疗的专家共识 TMB 定义中提到，由于早期研究多基于WES检测，因此TMB通常是指单位基因组外显子编码区域（外显子组，exome）的突变数量（mutations, muts），单位为muts/exome。虽然WES是检测TMB的金标准，但WES时间成本和分析成本较高。经过多项大样本研究验证后，TMB检测从WES扩展到了更切合临床实际的靶向二代测序（next-generation sequencing panel, NGS panel）。靶向测序的基因检测位点比外显子组少，由于不同平台检测方法和测序覆盖的外显子区域长度不同，TMB也被定义为肿瘤基因组区域中每兆碱基（megabase, Mb）发生的碱基替换突变和插入缺失突变的数量总和，单位为muts/Mb。，明确说明了时编码区的变异数目，虽然对应区域同理应该选用编码区长度，但是共识未明确体积分母的计算方式。 肿瘤突变负荷检测及临床应用中国专家共识（2020年版） 中提到 以数百个基因的肿瘤靶向 Panel 检测是通过计算覆盖在产品外显子上的体细胞突变数目除以产品的外显子覆盖区域得到基于 Panel 的 TMB 结果，然而其计算得到的 TMB值与 WES 检测得到的 TMB 值存在偏差,说明了以产品的外显子覆盖区域作为分母。 [TMB标准化项目蓝皮书]在中检院组织的TMB标准化项目中，针对WES的计算提供了分母的计算方式 位于全外显子捕获区间区域上下游扩展50bp 与 CDS 区域上下游扩展 2bp 的交集区域。 ，分母对应的范围时CDS区域上方下游2bp区域。 其实目前国内大部分检测公司并没有开展临床试验，而目前评估Panel的性能也多为和WES结果进行相关性比较，但是显然，分母作为一个常数值，不管如何选择，都不会影响相同计算体系下的样本排序。以下是一些国际检测产品的计算方式 Panel大小有一些证据[23,24]发现检测的基因数越多，TMB的检测结果与WES的一致性越高，NGS大panel可能更适合评估TMB。对于TMB较高的样本，不同大小的NGS panel检测结果差异可能不显著；而对于TMB较低的样本，检测结果的不一致性显著增加。目前一般认为NGS panel ≥0.8 Mb可以较好地评估肿瘤组织TMB水平(Implementing TMB measurement in clinical practice: considerations on assay requirements)。 参考材料：来源：历史中检院宣讲PPT材料 Transl Lung Cancer Res 2018;7(6):703-715Genes Chromosomes Cancer. 2019;58:578–588. 影响因素样本收集阶段、DNA处理阶段、测序阶段、生物信息分析阶段和报告生成阶段均会影响TMB检测的可靠性。样本收集阶段主要包括样本类型、肿瘤类型、肿瘤异质性和克隆进化等影响因素；DNA处理阶段包括DNA质量和数量、文库构建等影响因素；测序阶段包括DNA捕获区域、测序深度、覆盖读长、测序平台等影响因素；生物信息分析阶段包括突变类型、胚系突变过滤、等位基因突变频率等影响因素；报告生成阶段包括瘤种分类、患者人群、患者数量、TMB排序标准等影响因素。除了考虑技术因素，流程监管、样本收集和处理质控以及样本运送时长等因素可能也会影响样本质量，从而影响检测结果。 关联指标在一定程度上，TMB 水平反映的是肿瘤细胞内DNA 的修复损伤情况，与产生肿瘤新抗原能力密切相关。DNA错配修复基因（mismatch repair genes，MMR）负责修正DNA 复制错误，若MMR 存在突变往往会导致微卫星不稳定（microsatellite instability，MSI），因此高微卫星不稳定（MSI high，MSI鄄H）常作为MMR 功能缺陷（mismatch repair deficient，dMMR）的替代指标［6］。 此外，DNA 聚合酶着（DNA polymerase 着，POLE）和DNA 聚合酶啄1（polymerase delta 1,POLD1）对DNA复制的校对和保真至关重要，POLE/POLD1 基因突变（特别是外切酶活性域突变）也会导致肿瘤的高突变或超突变（TMB&gt;100 个突变/Mb）(Tumor and Microenvironment Evolution during Immunotherapy with Nivolumab) TMB发展重要节点历程TMB概念起源于2013年Nature发表的一项研究(Signatures of mutational processes in human cancer)。在30个癌种7,000多个标本中，研究者通过全基因组测序（whole genome sequencing,WGS）和全外显子测序（whole exome sequencing, WES）技术分析了突变图谱，描述了不同癌种样本中每百万碱基（megabase, Mb）的突变数量。 2014年的一项黑色素瘤研究（Genetic Basis for Clinical Response to CTLA-4 Blockade in Melanoma）发现，免疫治疗的响应率与肿瘤突变数目有一定的相关性，通过WES检出错义突变数量大于100的患者接受免疫治疗后具有更长的总生存期（overall survival, OS），这是首个验证TMB和免疫治疗疗效相关性的研究。 2015年，首个TMB与NSCLC免疫治疗疗效的研究（Mutational landscape determines sensitivity to PD-1 blockade in non–small cell lung cancer）发表于Science，该研究发现高于中位TMB的NSCLC患者具有更长的无进展生存期（progression-free survival, PFS）。 此后，CheckMate-026、CheckMate-227等多项大型研究证实了TMB对NSCLC免疫治疗疗效的预测作用。 2017年，Genome Medicine发表的一项10万例实体瘤患者研究(Analysis of 100,000 human cancer genomes reveals the landscape of tumor mutational burden)，探索了靶向捕获测序panel与WES检测TMB的相关性，证实了panel检测TMB的可行性与可靠性。 2019年中国临床肿瘤学会（Chinese Society of Clinical Oncology, CSCO）指南和美国国家综合癌症网络（National Comprehensive Cancer Network, NCCN）指南均将TMB纳入晚期肺癌的分子病理检测范围。 2020年，美国食品药品监督管理局（Food and Drug Administration, FDA）批准Pembrolizumab单药用于治疗高TMB且既往接受治疗后病情进展的不可手术或转移性实体瘤患者。Pembrolizumab成为全球首个以TMB作为标志物而获批的抗肿瘤药物。但临床实践中TMB的检测和评估缺乏统一的标准，这极大限制了其临床应用。虽然《肿瘤突变负荷检测及临床应用中国专家共识（2020年版）》已发布，但TMB在肺癌免疫治疗临床应用中的相关规范仍有待统一。 2021年，为促进TMB在肺癌免疫治疗中应用的规范化，协作组组织国内肺癌领域权威专家，综合国内外高质量文献，形成《肿瘤突变负荷应用于肺癌免疫治疗的专家共识》，对TMB的定义、临床意义和临床应用给出指导性建议。 2024年，AMP联合CAP等联合发布TMB检测指南 Recommendations for Tumor Mutational Burden Assay Validation and Reporting00115-6/fulltext) pdf TMB 重要资讯TMB是肿瘤NGS检测 TMB相关文章指南共识信息 发布时间 简介 机构 文章材料 2019.10.17 肿瘤突变负荷检测国家参考品说明书公示 中检院 肿瘤突变负荷检测国家参考品说明书公示 2020.06 肿瘤突变负荷检测国家参考品 中检院 肿瘤突变负荷检测国家参考品 2020.10 肿瘤突变负荷检测及临床应用中国专家共识（2020 年版） 中国癌症防治杂志 肿瘤突变负荷检测及临床应用中国专家共识（2020年版） 2021.11 TMB国内共识 - 面向肺癌 中国临床肿瘤学会 肿瘤突变负荷应用于肺癌免疫治疗的专家共识 2024.6 Recommendations for Tumor Mutational Burden Assay Validation and Reporting AMP &amp; CAP &amp; SITC Recommendations for Tumor Mutational Burden Assay Validation and Reporting TMB阈值确定方法标准化的阈值，应该使用临床疗效数据进行阈值的判断！ 目前部分文章是使用的人群比例占比进行的TMB High/Low的划分。 阈值确定参考素材 index TMB-High TMB-intermedia TMB-L 参考文献 1 Top 10% Top 10%-20% else https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7210182/ 2 Top 10% Top 10%-50% else https://mct.aacrjournals.org/content/molcanther/16/11/2598.full.pdf 免疫治疗响应疗效报道 BGI的TMB工作TMB计算逻辑1.5% 以上的SNV（含同义突变)和InDel的总突变数目除以芯片大小(2.79M)其中包含同义突变的参考依据如下： Chalmers ZR, et al. Analysis of 100,000 human cancer genomes reveals the landscape of tumor mutational burden. Genome Med. 2017;9:34. doi: 10.1186/s13073-017-0424-2. Rosenberg JE, et al. Atezolizumab in patients with locally advanced and metastatic urothelial carcinoma who have progressed following treatment with platinum-based chemotherapy: a single-arm, multicentre, phase 2 trial. Lancet. 2016;387(10031):1909–1920 Han Chang, Ariella Sasson, Sujaya Srinivasan, Ryan Golhar, Danielle M. Greenawalt, William J. Geese, George Green, Kim Zerba, Stefan Kirov, Joseph Szustakowski bioRxiv 626143; doi: https://doi.org/10.1101/626143 FM 原文参考: TMB by F1CDx is defined based on counting the total number of all synonymous and non-synonymous variants present at 5% allele frequency or greater (after filtering) and reported as mutations per megabase (mut/Mb) unit. The clinical validity of TMB defined by this panel has not been established TMB标准化项目整体时间规划 第一阶段结果基于华大数据进行Panel和WES的相关性拟合基于TCGA公共数据，进行拟合，获取各公司Panel计算TMB值和原WES水平TMB的回归拟合公式。y=1.371559x-0.735414 （x：Panel检测的TMB值； y：经过矫正后，对应WES水平的TMB值。） 使用该公式，对PanCancer（688芯片）检测临床样本获得的TMB值进行校正，并利用校正后的数据和WES检测TMB结果进行相关性比较，结果如下： 第一阶段总结TMB标准化第一阶段发布会-内容存档 华大数据基于三分段确定的阈值BGI历史数据阈值（三分段）基于该标准（index1）和华大历史样本确定的整体阈值（数据截止2020.8） 癌症类型 TMB high TMB median TMB low 组织泛癌种 ≥8.6Mut/Mb ＞5.02Mut/Mb且＜8.6Mut/Mb ≤5.02Mut/Mb 基于该标准（index1）和华大历史样本确定的各个癌种阈值（数据截止2021.9） 癌症类型 TMB high TMB median TMB low 胆管癌 Cholangiocarcinoma ≥5.73Mut/Mb ＞3.94Mut/Mb且＜5.73Mut/Mb ≤3.94Mut/Mb 胆囊癌 Carcinoma of Gallbladder ≥8.6Mut/Mb ＞4.66Mut/Mb且＜8.6Mut/Mb ≤4.66Mut/Mb 非小细胞肺癌 Non-Small Cell Lung Cancer ≥9.68Mut/Mb ＞6.09Mut/Mb且＜9.68Mut/Mb ≤6.09Mut/Mb 肝细胞癌 Hepatocellular Carcinoma ≥7.89Mut/Mb ＞5.38Mut/Mb且＜7.89Mut/Mb ≤5.38Mut/Mb 宫颈癌 Cervical Cancer ≥16.49Mut/Mb ＞8.96Mut/Mb且＜16.49Mut/Mb ≤8.96Mut/Mb 黑色素瘤 Melanoma ≥8.24Mut/Mb ＞3.94Mut/Mb且＜8.24Mut/Mb ≤3.94Mut/Mb 结直肠癌 Colorectal Cancer ≥27.24Mut/Mb ＞6.45Mut/Mb且＜27.24Mut/Mb ≤6.45Mut/Mb 卵巢癌 Ovarian Cancer ≥5.73Mut/Mb ＞3.94Mut/Mb且＜5.73Mut/Mb ≤3.94Mut/Mb 膀胱癌 Bladder Cancer ≥20.07Mut/Mb ＞10.75Mut/Mb且＜20.07Mut/Mb ≤10.75Mut/Mb 前列腺癌 Prostate Cancer ≥8.6Mut/Mb ＞6.09Mut/Mb且＜8.6Mut/Mb ≤6.09Mut/Mb 乳腺癌 Breast Cancer ≥6.81Mut/Mb ＞5.38Mut/Mb且＜6.81Mut/Mb ≤5.38Mut/Mb 软组织肉瘤 Soft Tissue Sarcoma ≥4.66Mut/Mb ＞2.87Mut/Mb且＜4.66Mut/Mb ≤2.87Mut/Mb 肾癌 Kidney Cancer ≥6.45Mut/Mb ＞3.94Mut/Mb且＜6.45Mut/Mb ≤3.94Mut/Mb 头颈癌 Head and Neck Cancer ≥8.6Mut/Mb ＞5.73Mut/Mb且＜8.6Mut/Mb ≤5.73Mut/Mb 胃癌 Gastric Cancer ≥12.19Mut/Mb ＞7.17Mut/Mb且＜12.19Mut/Mb ≤7.17Mut/Mb 胰腺癌 Pancreatic Adenocarcinoma ≥3.94Mut/Mb ＞2.51Mut/Mb且＜3.94Mut/Mb ≤2.51Mut/Mb 子宫肿瘤 Uterine Neoplasms ≥26.88Mut/Mb ＞17.92Mut/Mb且＜26.88Mut/Mb ≤17.92Mut/Mb 基于该标准（index1）和华大历史样本确定的各个癌种阈值（数据截止2022.04，质评用）| 癌症类型 | TMB high | TMB median | TMB low || ——————– | ———— | ————————— | ———– || 泛癌种(15917例) | ≥9.68Mut/Mb | ＞5.73Mut/Mb且＜9.68Mut/Mb | ≤5.73Mut/Mb || 肺腺癌(2733例) | ≥8.24Mut/Mb | ＞5.02Mut/Mb且＜8.24Mut/Mb | ≤5.02Mut/Mb || 非小细胞肺癌(4249例) | ≥10.04Mut/Mb | ＞6.45Mut/Mb且＜10.04Mut/Mb | ≤6.45Mut/Mb |]]></content>
      <categories>
        <category>NGS</category>
        <category>肿瘤检测</category>
        <category>标准化</category>
      </categories>
      <tags>
        <tag>Standards and guidelines</tag>
        <tag>Biomarker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据分析模型 - 波士顿矩阵]]></title>
    <url>%2F01.%E7%9F%A5%E8%AF%86-09.%E4%BA%A7%E5%93%81%E6%A8%A1%E5%9E%8B%2F01.%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E6%A8%A1%E5%9E%8B-%E6%B3%A2%E5%A3%AB%E9%A1%BF%E7%9F%A9%E9%98%B5%2F</url>
    <content type="text"><![CDATA[波士顿矩阵（BCG Matrix），又称市场增长率-相对市场份额矩阵，由美国著名的管理学家、波士顿咨询公司创始人布鲁斯·亨德森于1970年首创，它是通过销售增长率（反应市场引力的指标）和市场占有率（反应企业实力的指标）来分析决定企业的产品结构。 波士顿矩阵将产品类型分为四种： 1，明星类产品：高增长且高市占，发展前景好，竞争力强，需加大投资以支持其发展； 2，问题类产品：高增长但低市占，发展前景好但市场开拓不足，需谨慎投资； 3，现金牛产品：低增长但高市占，成熟市场的领导者，应降低投资，维持市占并延缓衰退； 4，瘦狗类产品：低增长且低市占，理论率低甚至亏损，应采取撤退战略。 参考链接： https://zhuanlan.zhihu.com/p/67544309]]></content>
      <categories>
        <category>知识沉淀</category>
        <category>数据分析模型</category>
      </categories>
      <tags>
        <tag>数据分析模型</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[随机森林（Forest Tree)]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-20.MachineLearn%2F1204.%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0-1.bagging-%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97%2F</url>
    <content type="text"><![CDATA[介绍随机森林 (Random Forests) 是一种利用决策树作为基学习器的 Bagging 集成学习算法。随机森林模型的构建过程如下： 数据采样作为一种 Bagging 集成算法，随机森林同样采用有放回的采样，对于总体训练集 (T)，抽样一个子集 (T_{sub}) 作为训练样本集。除此之外，假设训练集的特征个数为 (d)，每次仅选择 (k\left(k &lt; d\right)) 个构建决策树。因此，随机森林除了能够做到样本扰动外，还添加了特征扰动，对于特征的选择个数，推荐值为 (k = \log_2 d) 。 树的构建每次根据采样得到的数据和特征构建一棵决策树。在构建决策树的过程中，会让决策树生长完全而不进行剪枝。构建出的若干棵决策树则组成了最终的随机森林。 随机森林在众多分类算法中表现十分出众 7，其主要的优点包括： 由于随机森林引入了样本扰动和特征扰动，从而很大程度上提高了模型的泛化能力，尽可能地避免了过拟合现象的出现。 随机森林可以处理高维数据，无需进行特征选择，在训练过程中可以得出不同特征对模型的重要性程度。 随机森林的每个基分类器采用决策树，方法简单且容易实现。同时每个基分类器之间没有相互依赖关系，整个算法易并行化。 安装1pip install -U scikit-learn 安装 graphviz 一个对dot文件进行绘图的软件，可以用于对模型进行可视化。 数据准备数据准备包括训练集和测试集两部分的数据准备（也可以是一批数据按比例进行拆分，一部分作为训练一部分作为测试）这里可以下载一份测试数据数据格式如下（前30行）：1234567891011121314151617181920212223242526272829303132id,N_Days,Drug,Age,Sex,Ascites,Hepatomegaly,Spiders,Edema,Bilirubin,Cholesterol,Albumin,Copper,Alk_Phos,SGOT,Tryglicerides,Platelets,Prothrombin,Stage,Status0,999,D-penicillamine,21532,M,N,N,N,N,2.3,316.0,3.35,172.0,1601.0,179.8,63.0,394.0,9.7,3.0,D1,2574,Placebo,19237,F,N,N,N,N,0.9,364.0,3.54,63.0,1440.0,134.85,88.0,361.0,11.0,3.0,C2,3428,Placebo,13727,F,N,Y,Y,Y,3.3,299.0,3.55,131.0,1029.0,119.35,50.0,199.0,11.7,4.0,D3,2576,Placebo,18460,F,N,N,N,N,0.6,256.0,3.5,58.0,1653.0,71.3,96.0,269.0,10.7,3.0,C4,788,Placebo,16658,F,N,Y,N,N,1.1,346.0,3.65,63.0,1181.0,125.55,96.0,298.0,10.6,4.0,C5,703,D-penicillamine,19270,F,N,Y,N,N,0.6,227.0,3.46,34.0,6456.2,60.63,68.0,213.0,11.5,3.0,D6,1300,Placebo,17703,F,N,N,N,N,1.0,328.0,3.35,43.0,1677.0,137.95,90.0,291.0,9.8,3.0,C7,1615,Placebo,21281,F,N,Y,N,N,0.6,273.0,3.94,36.0,598.0,52.7,214.0,227.0,9.9,3.0,C8,2050,D-penicillamine,20684,F,N,N,N,N,0.7,360.0,3.65,72.0,3196.0,94.55,154.0,269.0,9.8,2.0,C9,2615,D-penicillamine,15009,F,N,N,N,N,0.9,478.0,3.6,39.0,1758.0,171.0,140.0,234.0,10.6,2.0,C10,3581,Placebo,25772,F,N,N,N,N,0.5,252.0,3.6,26.0,377.0,56.76,185.0,336.0,10.0,2.0,C11,1614,Placebo,14106,F,N,N,N,N,0.9,328.0,3.61,62.0,1105.0,137.95,95.0,145.0,9.5,3.0,C12,1847,Placebo,12279,F,N,N,N,N,0.6,232.0,3.68,38.0,1029.0,128.65,99.0,273.0,10.7,2.0,C13,1153,D-penicillamine,22347,F,N,Y,N,N,0.6,232.0,3.83,24.0,678.0,65.1,99.0,248.0,10.4,3.0,C14,904,D-penicillamine,22388,F,N,Y,N,N,3.9,304.0,3.2,13.0,1440.0,153.45,169.0,156.0,10.0,3.0,D15,1212,Placebo,15112,F,N,N,N,N,0.7,335.0,3.54,44.0,1345.0,137.95,145.0,244.0,10.6,3.0,C16,1967,Placebo,17884,F,N,N,N,N,0.7,328.0,3.58,39.0,1065.0,98.0,78.0,259.0,11.7,2.0,C17,1592,D-penicillamine,14872,F,N,Y,N,N,1.1,392.0,3.43,39.0,1395.0,184.45,133.0,328.0,11.2,2.0,C18,1481,Placebo,18302,F,N,N,N,N,1.0,259.0,3.85,67.0,936.0,134.85,139.0,341.0,9.6,3.0,C19,3358,Placebo,17031,F,N,N,N,N,0.6,322.0,3.77,52.0,834.0,60.45,214.0,153.0,11.0,3.0,C20,3092,Placebo,15612,F,N,Y,N,N,0.6,303.0,3.1,70.0,1032.0,56.76,154.0,336.0,10.6,4.0,C21,326,D-penicillamine,18199,F,N,Y,Y,S,6.6,244.0,3.02,199.0,1819.0,170.5,91.0,132.0,12.1,4.0,D22,2363,Placebo,17703,F,N,N,N,N,1.0,215.0,3.95,58.0,645.0,97.65,71.0,233.0,10.1,4.0,C23,1152,D-penicillamine,16736,F,N,Y,N,N,1.1,373.0,3.9,69.0,1353.0,116.25,139.0,268.0,10.0,4.0,C24,3577,D-penicillamine,27398,F,N,N,N,N,0.6,253.0,4.03,38.0,642.0,41.85,112.0,227.0,9.9,2.0,C25,799,Placebo,27220,F,N,Y,N,N,1.3,325.0,3.6,81.0,2065.0,232.5,100.0,277.0,11.1,4.0,C26,1832,Placebo,17442,F,N,Y,N,N,2.0,328.0,3.35,76.0,2276.0,114.7,104.0,518.0,10.0,4.0,D27,4467,D-penicillamine,12398,F,N,Y,N,N,1.2,414.0,3.43,41.0,876.0,84.0,110.0,385.0,11.0,3.0,C28,2301,D-penicillamine,15105,F,N,Y,Y,N,2.3,528.0,3.34,173.0,1282.0,120.9,55.0,123.0,10.7,4.0,D29,943,Placebo,19002,F,N,N,N,N,28.0,556.0,3.26,39.0,1713.0,171.0,171.0,348.0,10.2,3.0,D30,1882,Placebo,15265,F,N,Y,Y,N,1.1,316.0,3.35,67.0,1353.0,137.95,137.0,273.0,9.6,3.0,C 打眼一看，我们可以看到数据中有很多文本/类别的信息是使用字符串进行表示的，所以在进行文本读取后，我们需要对数据进行一个整体的预处理过程。12345678910111213141516171819202122232425262728import pandas as pdtrains_data = pd.read_csv("train.csv",sep=",")test_data = pd.read_csv("test.csv",sep=",")# 删除一些不会考虑作为训练指标的信息，例如Status是最终结果，Edema指标是分散文本。Trains_input= trains_data.drop(["Status","Edema"],axis=1)Trains_target= trains_data["Status"]# 对其他一些分类指标构建数据清洗映射关系Drug_mapping = &#123;"D-penicillamine":1,"Placebo":2&#125;sex_mapping = &#123;'F': -1, 'M': 1&#125;Ascites_mapping = &#123;"N":0,"Y":1&#125;Hepatomegaly_mapping = &#123;"N":0,"Y":1&#125;Spiders_mapping = &#123;"N":0,"Y":1&#125;# 对文本信息进行更新迭代Trains_input["Drug"]=Trains_input["Drug"].replace(Drug_mapping)Trains_input["Sex"]=Trains_input["Sex"].replace(sex_mapping)Trains_input["Ascites"]=Trains_input["Ascites"].replace(Ascites_mapping)Trains_input["Hepatomegaly"]=Trains_input["Hepatomegaly"].replace(Hepatomegaly_mapping)Trains_input["Spiders"]=Trains_input["Spiders"].replace(Spiders_mapping)Trains_input.head()# 结果示例 id N_Days Drug Age Sex Ascites Hepatomegaly Spiders Bilirubin Cholesterol Albumin Copper Alk_Phos SGOT Tryglicerides Platelets Prothrombin Stage0 0 999 1 21532 1 0 0 0 2.3 316.0 3.35 172.0 1601.0 179.80 63.0 394.0 9.7 3.01 1 2574 2 19237 -1 0 0 0 0.9 364.0 3.54 63.0 1440.0 134.85 88.0 361.0 11.0 3.02 2 3428 2 13727 -1 0 1 1 3.3 299.0 3.55 131.0 1029.0 119.35 50.0 199.0 11.7 4.0 数据概览123import sweetviz my_report = sweetviz.analyze(trains_data)my_report.show_notebook(w=&quot;100%&quot;, h=&quot;full&quot;) 模型训练1234567891011# 模型(也可用单个决策树)from sklearn.ensemble import RandomForestClassifier# 决策树数量model = RandomForestClassifier(n_estimators=10) # random_state：目的让模型稳定下来。# n_estimators：控制森林中数目的数量。# bootstrap：默认True，代表采用这种有放回的随机抽样技术# oob_score：默认True，是否用袋外数据来测试我们的模型。# 训练model.fit(Trains_input, Trains_target) 基于此，我们就完成了一个最基本的模型构建，也可以通过这个模型进行简单的数据分析/预测，完成了入门，当然这个随便确定参数的模型性能可能并不完善，所以接下来我们就要看看怎么进行调参优化。 指标筛选指标权重1234567891011from sklearn.ensemble import RandomForestClassifierimport numpy as np# 决策树数量model = RandomForestClassifier(n_estimators=10) model.fit(Trains_input, Trains_target)importances = model.feature_importances_indices = np.argsort(importances)[::-1] #[::-1]表示将各指标按权重大小进行排序输出feat_labels = Trains_input.columns[0:] #直接提取列明作为训练集指标名称# feat_labels = ["A","B","C","D"] # 自定义训练集指标的名称for f in range(Trains_input.shape[1]): print("%2d) %-*s %f" % (f + 1, 30, feat_labels[indices[f]], importances[indices[f]])) 就可以得到每个指标在最终建模时的权重（权重综合为1），格式如下1234567891011121314151617 1) Bilirubin 0.195250 2) N_Days 0.149384 3) Copper 0.091069 4) Prothrombin 0.085256 5) Age 0.064164 6) SGOT 0.063270 7) Platelets 0.057314 8) Cholesterol 0.054671 9) Albumin 0.05288210) Alk_Phos 0.05220511) Tryglicerides 0.04850012) Stage 0.03157013) Hepatomegaly 0.02086614) Spiders 0.01348615) Drug 0.00980416) Sex 0.00666417) Ascites 0.003645 当变量参数非常多的时候，我们可以基于每个指标的权重进行筛选，对输入参数进行精简。比如在我们这个数据集中，Bilirubin，N_Days等指标重要性比较高，而对应的Ascites、Sex、Drug等指标重要性就相对较低。 参数调优逐步调优借助交叉验证得分确定最有决策树数目。在随机森林里面，决策树越多，在对训练集的建模表现越好，但是对应的资源需求也就越大。所以有时候，我们需要进行评估，构建多少颗决策树更合适。这时候，我们可以使用 cross_val_score 进行不同梯度交叉验证123456789101112superpa=[]#for i in [1,10,20,30,40,50,60,70,80,90,100,110,120,130,140,150,160,170,180,190,200]:for i in range(20): #rfc = RandomForestClassifier(max_depth=i+1,n_jobs=-1,random_state=1,n_estimators=i+1) rfc = RandomForestClassifier(n_estimators=i+1,random_state=1) rfc_s = cross_val_score(rfc,Trains_input,Trains_target,cv=10).mean() superpa.append(rfc_s)# 打印分数最高时的分数和max_depth的值print(max(superpa),superpa.index(max(superpa))+1)plt.figure(figsize=[20,5])plt.plot(range(1,201),superpa)plt.show() 确定决策树数目后，确定决策树深度基于上述确定的决策数数目，对决策树的最大深度进行遍历，确定最大决策树深度12345678910superpa=[]for i in range(20): rfc = RandomForestClassifier(max_depth=i+1,n_jobs=-1,random_state=1,n_estimators=70) rfc_s = cross_val_score(rfc,Trains_input,Trains_target,cv=10).mean() superpa.append(rfc_s)# 打印分数最高时的分数和max_depth的值print(max(superpa),superpa.index(max(superpa))+1)plt.figure(figsize=[20,5])plt.plot(range(1,21),superpa)plt.show() 网格调优12345678910111213141516171819from sklearn.ensemble import RandomForestClassifierfrom sklearn.model_selection import cross_val_score #K折交叉验证from sklearn.model_selection import GridSearchCV # 网格调参from sklearn.model_selection import RandomizedSearchCV#分类问题，先建立一个分类器clf = RandomForestClassifier(n_estimators=20)#给定参数搜索范围param_test=&#123;'max_depth':[i for i in range(1, 25, 2)], 'n_estimators':[i for i in range(1, 150, 5)], "min_samples_split": [i for i in range(1, 10, 2)]&#125;#RandomSearch+CV选取超参数random_search = RandomizedSearchCV(clf,param_distributions =param_test,n_iter=20,cv=5)random_search.fit(xtrain_vec,sentiment_train)print("随机搜索最优得分：",random_search.best_score_)print("随机搜索最优参数组合：\n",random_search.best_params_) 模型可视化1234567891011121314151617181920# 提取一个决策树model = RandomForestClassifier(max_depth=13,n_jobs=-1,random_state=1,n_estimators=70)model.fit(Trains_input, Trains_target)estimator = model.estimators_[0]# 导出为dot 文件from sklearn.tree import export_graphvizexport_graphviz(estimator, out_file='tree.dot', feature_names = Trains_input.columns[0:], rounded = True, proportion = False, precision = 2, filled = True)# 用系统命令转为PNG文件(需要 Graphviz)from subprocess import callcall(['D:\Software\Graphviz\bin\dot.exe', '-Tpng', 'tree.dot', '-o', 'tree.png', '-Gdpi=600'])# 在jupyter notebook中展示from IPython.display import ImageImage(filename = 'tree.png') 得到的随机森林某一个决策树判断逻辑如下： 模型应用通过上述步骤，我们已经成功的得到我们所需要的一个随机森林模型 model ，杰西莱我们就需要使用我们的模型，对于全新的测试数据进行处理。123456789101112131415161718192021# 因为一般训练集和测试集的数据格式是一样的，所以我们对训练集进行的数据处理过程需要同时对测试集进行处理。Drug_mapping = &#123;"D-penicillamine":1,"Placebo":2&#125;sex_mapping = &#123;'F': -1, 'M': 1&#125;Ascites_mapping = &#123;"N":0,"Y":1&#125;Hepatomegaly_mapping = &#123;"N":0,"Y":1&#125;Spiders_mapping = &#123;"N":0,"Y":1&#125;test_data["Drug"]=test_data["Drug"].replace(Drug_mapping)test_data["Sex"]=test_data["Sex"].replace(sex_mapping)test_data["Ascites"]=test_data["Ascites"].replace(Ascites_mapping)test_data["Hepatomegaly"]=test_data["Hepatomegaly"].replace(Hepatomegaly_mapping)test_data["Spiders"]=test_data["Spiders"].replace(Spiders_mapping)test_data= test_data.drop(["Edema","id"],axis=1)test_data.head()#直接返回预测结果model.predict(test_data)# 返回每类的预测概率result = model.predict_proba(test_data)result_df =pd.DataFrame(result,columns = ['Status_C', 'Status_CL', 'Status_D']) 自定义优化预测结果随机森林本身事基于多颗决策树的预测结果。由于每颗树所以来的输入信息事不完全一致的，因此不同的数据集，每颗决策树的表现也会有所不同。如果偏差不大，一般我们会直接使用每颗树的结果计算均值作为最终结果。但是有时候我们也可以通过自定义函数，对预测结果进行优化。 比如下面的这个示例，我们首先获得每颗决策树的预测结果，然后对所有决策树预测结果进行排序，然后剔除前5%和后5%的数据，然后求平均值作为最终预测结果。相比直接计算均值，可以具有更好的鲁棒性。123456789101112def predict_fit(model,data,feature_columns): trees = model.estimators_ tree_columns = [] for index in range(0,len(trees)): tag="pre_"+str(index) tree_columns.append(tag) data.loc[:,tag] = trees[index].predict(data[feature_columns].values).round(0) predict_list = data[tree_columns].apply(lambda row: ';'.join(row.values.astype(str)), axis=1) list=[float(num) for num in predict_list.split(';')].sort() filt_num=int(len(in_list)*0.05+1) filt_list=list[filt_num:-filt_num] return (sum(filt_list)/len(filt_list))]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>Machine-Learning</tag>
        <tag>集成学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[docker - 挂载目录]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-04.pipeline%2Fpipeline-container-docker-%E9%95%9C%E5%83%8F%E4%BD%BF%E7%94%A8-%E6%8C%82%E8%BD%BD%E7%9B%AE%E5%BD%95%2F</url>
    <content type="text"><![CDATA[我们可以在创建容器的时候，将宿主机的目录与容器内的目录进行映射，这样我们就可以实现宿主机和容器目录的双向数据自动同步 挂载一个容器启动容器时挂载一个目录（推荐）Docker容器启动的时候，如果要挂载宿主机的一个目录，可以用-v参数指定。 譬如我要启动一个centos容器，宿主机的/test目录挂载到容器的/soft目录，可通过以下方式指定：1# docker run -it -v /test:/soft centos /bin/bash 这样在容器启动后，容器内会自动创建/soft的目录。通过这种方式，我们可以明确一点，即-v参数中，冒号”:”前面的目录是宿主机目录，后面的目录是容器内目录（ 容器目录不可以是相对目录，必须是绝对目录 ）。 默认挂载的目录是可以进行双向同步的，但是有些数据库文件，一般在镜像中我们只会进行数据的读取，可以挂载为只读目录。1docker run -it -v /宿主机目录:/容器目录:ro 镜像名 其中进行目录挂载时的几个执行的逻辑情况如下： 可以挂载多个目录（-v 参数可以重复使用） 1docker run -it -v /宿主机目录:/容器目录 -v /宿主机目录2:/容器目录2 镜像名 容器目录不可以是相对目录，必须是绝对目录 宿主机目录如果不存在，则会自动生成 宿主机目录如果是相对目录，则会基于/var/lib/docker/volumes/ 生成相对目录，与执行目录无关。（可以通过 docker inspect 查看） -v挂载时，只指定一个目录，则会在宿主机/var/lib/docker/volumes/下生成一个随机目录名，指定的目录为容器内的目录。 容器内如果对挂载目录的属主和属组进行了修改，会同步修改宿主对应目录的属主和属组信息，但是更改的是用户的UID，由于容器内和宿主机中UID标识的实际用户一般不一样，因此属主和属组会发生改变，但是用户会存在差异。 容器销毁后，挂载目录内的更改仍会保留，不会还原。 通过使用数据容器挂载 首先创建一个数据卷 命令: docker run -v 需挂载目录的路径:容器挂载路径 –name 数据卷名字 容器名字 /bin/bash 1docker run -v /home/dock/Database:/Database --name Database ubuntu64 /bin/bash 再创建一个新的容器，来使用这个数据卷(通过数据卷的名字 Database。 1docker run -it --volumes-from Database ubuntu64 /bin/bash 通过dockerfile创建挂载点上面介绍的通过docker run命令的-v标识创建的挂载点只能对创建的容器有效。 通过dockerfile的 VOLUME 指令可以在镜像中创建挂载点，这样只要通过该镜像创建的容器都有了挂载点。 还有一个区别是，通过 VOLUME 指令创建的挂载点，无法指定主机上对应的目录，是自动生成的 容器共享卷（挂载点）1docker run --name test1 -it myimage /bin/bash 上面命令中的 myimage是用前面的dockerfile文件构建的镜像。 这样容器test1就有了 /data1 和 /data2两个挂载点。 下面我们创建另一个容器可以和test1共享 /data1 和 /data2卷 ，这是在 docker run中使用 –volumes-from标记，如：12345可以是来源不同镜像，如：docker run --name test2 -it --volumes-from test1 ubuntu /bin/bash也可以是同一镜像，如：docker run --name test3 -it --volumes-from test1 myimage /bin/bash 上面的三个容器 test1 , test2 , test3 均有 /data1 和 /data2 两个目录，且目录中内容是共享的，任何一个容器修改了内容，别的容器都能获取到。 常见异常情况及处理挂载宿主机目录后，无操作权限挂载宿主机已存在目录后，在容器内对其进行操作，报“Permission denied”。 解决方式 关闭selinux。 临时关闭：# setenforce 0 永久关闭：修改/etc/sysconfig/selinux文件，将SELINUX的值设置为disabled。 以特权方式启动容器指定–privileged参数如：# docker run -it –privileged=true -v /test:/soft centos /bin/bash]]></content>
      <categories>
        <category>pipeline</category>
        <category>镜像</category>
        <category>container</category>
      </categories>
      <tags>
        <tag>流程开发</tag>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[docker - 常用命令 - 运行任务是参数的传递]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-04.pipeline%2Fpipeline-container-docker-%E8%BF%90%E8%A1%8C%E4%BC%A0%E5%8F%82%2F</url>
    <content type="text"><![CDATA[运行容器时传参ENTRYPOINT 和 CMD我们在运行 docker 镜像时希望能用下面的命令向容器传递命令行参数1docker run &lt;image-name&gt; &lt;command&gt; arg1 arg2 如果要向 docker 容器传递参数时，Dockerfile 该如何写，这就有必要稍稍了解一下 Dockerfile 中 CMD 和 ENTRYPOINT这两个指令，并且它们有 exec 和 shell 两种格式的写法。 对于一个 docker 镜像，我们可以这么来理解 ENTRYPOINT 与 CMD 的关系如果没有定义 ENTRYPOINT， CMD 将作为它的 ENTRYPOINT 定义了 ENTRYPOINT 的话，CMD 只为 ENTRYPOINT 提供参数 CMD 可由 docker run \&lt;image> 后的命令覆盖，同时覆盖参数 对于 #1 和 #2 更精致的理解是容器运行的最终入口由 ENTRYPOINT 和实际的 CMD 拼接而成。ENTRYPOINT 和 CMD 需转换为实际镜像中的 exec 格式来拼接，合并后的第一个元素是命令，其余是它的参数。 举四个例子进行说明 一, 未定义 ENTRYPOINT, 定义了 CMD 12#ENTRYPOINT []CMD [&quot;echo&quot;, &quot;hello&quot;] 实际入口是它们拼接后还是 CMD 本身，[“echo”, “hello”] 二, 定义了 ENTRYPOINT 和 CMD 12ENTRYPOINT [&quot;echo&quot;, &quot;hello&quot;]CMD [&quot;echo&quot;, &quot;world&quot;] 实际入口是它们拼接起来，形成 [“echo”, “hello”, “echo”, “world”], 执行 docker run test 显示为 hello echo world 三, 定义了 ENTRYPOINT, CMD 由 docker run 提供 1ENTRYPOINT [&quot;echo&quot;, &quot;hello&quot;] 执行命令 docker run \ rm -rf /, 实际入口是由 [“echo”, “hello”] 与 [“rm”, “-rf”, “/“] 拼接而成的 [“echo”, “hello”, “rm”, “-rf”, “/“], 输出为 hello rm -rf /。看到 rm -rf / 也不用担心，用 ENTRYPOINT 就是让人放心 注：ENTRYPOINT 同样可以被覆盖，如 docker run –entrypoint ls test -l /，将会执行 ls -l / 命令。 四, 如果 ENTRYPOINT 用 shell 格式定义的 1234567891011121314151617181920212223 ENTRYPOINT java -jar /app.jar CMD [&quot;hello&quot;, &quot;world&quot;] ``` 通过 docker inspect 命令看到镜像中实际的 ENTRYPOINT 是 ENTRYPOINT [&quot;/bin/sh&quot;, &quot;-c&quot;, &quot;java -jar /app.jar&quot;] 所以与 CMD 连接起来的入口就是 [&quot;/bin/sh&quot;, &quot;-c&quot;, &quot;java -jar /app.jar&quot;, &quot;hello&quot;, &quot;world&quot;]，&quot;bin/sh&quot; 直接忽略掉后面的 &quot;hello&quot; 与 &quot;world&quot;，这就是为什么shell 命令方式无法获取参数。有了以上几点概念，以及四个实例作为感观认识后，想要怎么往容器传递参数应该很容易确定了。## 传参示例- 未定义 ENTRYPOINT没有定义 ENTRYPOINT 的镜像想怎么来就怎么来，docker run &lt;image&gt; 后面的输入你自己作主。- 有定义 ENTRYPOINT 定义了 ENTRYPOINT 的镜像，则是 CMD 或 docker run &lt;image&gt; 后的输入作为 ENTRYPOINT 中命令的附加参数。再次提醒 shell 格式的 ENTRYPOINT 和 CMD 务必要转换为相应 exec 格式来理解。- shell 格式的 ENTRYPOINT 如果是复杂的 shell 命令不容易拆解出一个个参数，而希望用 shell 格式来定义 ENTRYPOINT 的话，也有办法。shell 格式的 ENTRYPOINT 是由 &quot;/bin/sh -c&quot; 启动的，而它是可以解析变量的。另一方面 CMD 或 docker run &lt;image&gt; 的输入第一个元素存成了 $0，其他剩余元素存为 $@, 所以 shell 格式的 ENTRYPOINT 可以这么写## 使用环境变量传参### 使用ENTRYPOINT对于 shell 格式的 ENTRYPOINT, 或者显式由 &quot;/bin/sh -c&quot; 来启动的命令，可以通过环境变量传入参数 ENTRYPOINT java $JAVA_OPTS -jar app.jar $0 $@ #或显式的 ENTRYPOINT [“/bin/sh”, “-c”, “java $JAVA_OPTS -jar /app.jar $0 $@”]1启动容器时的命令用 docker run -e JAVA_OPTS=”-Xmx5G -Xms2G” aa bb1那么实际执行 java 的完整命令就是 java -Xmx5G -Xms2G -jar /app.jar aa bb12### 使用CMD- dockerfile 示例 FROM docker.io/python:3.6MAINTAINER tianye 设置容器时间 RUN cp /usr/share/zoneinfo/Asia/Shanghai /etc/localtime &amp;&amp; echo ‘Asia/Shanghai’ &gt;/etc/timezone ENV LANG C.UTF-8 # 设置编码 ENV PATH=$PATH:/usr/local/lib/python3.6/ ENV PYTHONPATH $PATH # 配置环境变量 ENV PARAMS=”” # 给我们要传的参数一个初始值 #代码添加到code文件夹 ADD ./tttt/ /test/code/tttt/ #设置code文件夹为工作目录 WORKDIR /test/code/tttt/ CMD python3 ttt.py $PARAMS 1- 创建镜像并启动容器 docker build -t my_image . docker run -it -d –name my_container -e PARAMS=”hahaha” my_image # my_image 放最后 这里hahaha 加不加引号 无所谓 docker logs -f –tail 200 my_container ` Dockerfile中 最后一行 $PARAMS 会解析为一个变量获取其值，也就是 docker run传入的参数 “hahaha”， 在python程序中通过 argv[1] 就可以获取到我们传入的”hahaha” ! 需要注意的一点是Dockerfile 中CMD的用法，如果我们不传参那么写法有很多如： CMD [“python3”, “ttt.py”] CMD [python3, ttt.py] CMD “python3” “ttt.py” CMD python3 ttt.py 但是要传参的话： 我们的参数 \$PARAMS 是万万不能用 “ “ 的，不然Dockerfile会认为是普通字符串 CMD [“python”, “ttt.py”, \$PARAMS] (×) 原因可能是字符串和变量放到一个列表时，字符串优先级高，直接将 $PARAMS当作一个字符串处理 CMD [python3, ttt.py, $PARAMS] (×) CMD “python3” “ttt.py” $PARAMS (√，推荐) CMD python3 ttt.py $PARAMS (√)CMD [] 形式，中括号中 必须用逗号分割； 如果不用中括号，不能用逗号分割！传参机制 容器运行的最终入口由 ENTRYPOINT 和实际的 CMD 拼接而成。 ENTRYPOINT 和 CMD 合并前需转换为 exec 格式(用 docker inspect 查看)，合并后(相当于数组) 第一个元素是命令，其他都为参数 CMD 可在 Dockerfile 中配置，在启动容器时会被 docker run 后的参数覆盖 CMD 的 exec 格式中，第一个元素是 shell 的 $0, 其余元素是 shell 的 $@。当 ENTRYPOINT 中用 shell 格式或显式的 sh(bash等)就可以引用 $0, $@ 环境变量的解析是通过 sh(bash 等) 来解析的，所以 ENTRYPOINT [“echo”, “$name”] 中的 $name 是不被解析的最能说明问题的是 docker inspect 看个究竟，Path 和 Args 说明了一切 参考资料 CSDN博客 博客]]></content>
      <categories>
        <category>pipeline</category>
        <category>镜像</category>
        <category>container</category>
      </categories>
      <tags>
        <tag>流程开发</tag>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[docker - 常用命令]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-04.pipeline%2Fpipeline-container-docker-02.%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[常用命令docker相关常用命令123456789sudo systemctl start docker #启动dockersystemctl restart docker.service #重启dockerdocker version #查看 docker 版本docker info #显示 docker 系统信息（容器情况、镜像情况等 ）docker –help #显示所有命令docker login/logout # 登录、退出Docker账号docker ps -a # 查看所有容器；docker search keyword # 从Docker Hub中搜索相关镜像docker top # 查看容器中正在运行的进程 镜像相关常用命令123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120===== 查看镜像 # 查看本地镜像列表docker images # 查看镜像制作过程docker history ===== 获取镜像 # 从配置的仓库下载镜像 docker pull &lt;name:tag&gt; # 获取指定版本的镜像eg: docker pull centos:centos7 # 默认获得最新版本（latest）eg: docker pull centos ===== 创建镜像(DockerFile)docker build &lt;Path&gt; \ # Path为包含DokcerFile的目录-f python.dockerFile #若DockerFile的文件名不是默认名称则需要说明===== 创建镜像(快照)docker load # 通过tar包，导入镜像# 通过镜像tar（save创建）包导入镜像库eg: docker load -i python.dockerimage.tar # 从容器快照文件(export创建的tar) 导入到镜像库docker import ubuntu.tar ubuntu:v1 ===== 重命名镜像# 对镜像进行从命名docker tag &lt;镜像ID&gt; &lt;newname:newtag&gt; ===== 保存镜像docker save &lt;镜像ID&gt; # 将镜像保存到一个tar包eg: docker save python:latest -o python.tar===== 上传镜像docker push &lt;name:tag&gt; # 推送镜像到远程仓库eg: eg: docker push benair/pancancer:v0 ===== 删除镜像docker rmi &lt;镜像ID&gt;eg: docker rmi -f ae9660359c2ad``` ## 容器相关常用命令```shell===== 查看容器$ docker ps #查看运行中的容器CONTAINER-ID IMAGE COMMAND CREATED STATUS PORTS NAMES5917eac21c36 ubuntu:15.10 "/bin/bash" 52 minutes ago Up 52 minutes sweet_agnesi# 查看容器的内容变更docker diff &lt;CONTAINERID&gt; # 查看特定任务的日志文件$ docker logs &lt;CONTAINERID&gt; # 用于查看容器的配置信息，包含容器名、环境变量、运行命令、主机配置、网络配置和数据卷配置等。docker inspect &lt;CONTAINERID&gt; ===== 启动容器docker run &lt;镜像ID&gt;/&lt;name:tag&gt; # 通过特定镜像启动一个容器eg: docker run \-d \ # -d 后台启动一个容器 -it \ # -it 交互式启动容器 -v /share:/share \ # -v 将宿主机的目录挂载的容器内部（实现文件的交互）； -u root \ # -u 指定登陆容器后的用户名称 -p 80:80 \ # -p 对宿主机和容器的端口建立映射-H hostname \ # -H 制定容器的主机名称--name centos \ # --name 制定容器的名称centos:centos7 \ # 启动的容器镜像/bin/bash # 启动容器后，执行的命令eg: docker run -it===== 进入容器docker attach/exec # attach 进入容器后，如果退出容器会导致容器停止eg: docker attach &lt;镜像ID&gt;/&lt;name:tag&gt; # exec 进入容器后，如果退出，容器不会停止eg: docker exec -it &lt;镜像ID&gt;/&lt;name:tag&gt; /bin/bash ===== 退出容器# 交互状态中镜像中运行exit 或者使用 ctrk+d 退出容器$ exit===== 容器内文件交互# 将文件从容器拷贝到宿主机docker cp &lt;CONTAINERID&gt;:/FilePath /FilePath # 将文件从宿主机拷贝到容器docker cp /FilePath &lt;CONTAINERID&gt;:/FilePath===== 删除容器 docker rm# 删除指定容器,删除容器前需要确定容器是关闭的。docker rm -f 1e560fca3906# 批量删除某个镜像衍生的容器docker ps -a | grep &lt;镜像ID&gt; | awk -F ' ' '&#123;print "docker rm "$1&#125;' | sh # 清理掉所有处于终止状态的容器。docker container prune ===== 保存容器（快照）# docker export 基于当前版本建立快照（之前的层不保存）docker export &lt;CONTAINERID&gt; &gt; ubuntu.tardocker export &lt;CONTAINERID&gt; -o ubuntu.tar# docker commit 当前容器内容新建一层（在原有启动镜像基础上新建一层）docker commit &lt;CONTAINERID&gt; &lt;name:tag&gt; # 如果容器需要上传到远程仓库，在保存时还需要指定用户docker commit &lt;CONTAINERID&gt; usr/&lt;name:tag&gt; ===== 关启容器# 启动容器（不能是运行状态的容器）docker start &lt;镜像ID&gt;/&lt;name:tag&gt;# 停止容器 docker stop &lt;镜像ID&gt;/&lt;name:tag&gt;# 重启容器docker restart &lt;镜像ID&gt;/&lt;name:tag&gt;# 终止容器docker kill &lt;镜像ID&gt;/&lt;name:tag&gt;===== 其他命令docker rename # 重命名容器 参考资料官方文档Docker:Guides:Get started 生信常用镜像清单bwavepGATK 官方介绍]]></content>
      <categories>
        <category>pipeline</category>
        <category>镜像</category>
        <category>container</category>
      </categories>
      <tags>
        <tag>流程开发</tag>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[docker - 镜像管理]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-04.pipeline%2Fpipeline-container-docker-%E9%95%9C%E5%83%8F%E7%AE%A1%E7%90%86-%E4%B8%8B%E8%BD%BD%E4%B8%8A%E4%BC%A0%2F</url>
    <content type="text"><![CDATA[docker产生的本质是为了让开发环境更好的进行迁移和分享，因此我们需要一个远程仓库可以保存我们的镜像，以便进行镜像的分享，和不同环境的迁移。同时在此介绍一些常用的docker仓库和配置方式，供大家参考 下载镜像12345# 下载一个公共镜像$ docker pull centos:centos7.6.1810 # 从私有仓库下载一个镜像$ docker pull registry.cn-shenzhen.aliyuncs.com/ben*/ubuntu:[镜像版本号] 官方镜像不定期抽风，可以在国内镜像 中查看使用可用的本地镜像 上传镜像针对没指定远程的镜像，需要先进行标记（docker tag），将本地镜像和远程仓库进行关联。粗暴一点理解就是在镜像前面加上自己的docker hub的Docker ID，如果是第三方仓库或私有仓库还需要标注注册的主机名/IP和端口 123456789101112131415161718#docker tag [OPTIONS] IMAGE[:TAG] [REGISTRYHOST/][USERNAME/]NAME[:TAG]#可以使用镜像ID进行标记docker tag 0e5574283393 ben*/ubuntu:version1.0# 可以使用本地的镜像名称进行标记docker tag ubuntu ben*/ubuntu:version1.0# 也可以使用镜像的本地名称和tag进行标记docker tag ubuntu:test ben*/ubuntu:version1.0.test# 如果推送到私有仓库，则需要在目标仓库中明文标记主机名（默认推送的docker hub）# 示例docker tag 0e5574283393 myregistryhost:5000/ben*/ubuntu:version1.0# 以阿里云ACR服务为例docker tag 0e5574283393 registry.cn-shenzhen.aliyuncs.com/ben*/ubuntu:version1.0# 如果推送到私有仓库docker tag 0e5574283393 localhost:5000/ubuntu:version1.0 完成仓库的关联后，直接通过docker push命令进行推送1234567891011121314# docker push [OPTIONS] NAME[:TAG]OPTIONS说明：--disable-content-trust :忽略镜像的校验,默认开启# 请确定已经参考前文进行了账户信息的配置$ docker push ben*/ubuntu:version1.0The push refers to repository [docker.io/ben*/pancancer]3c8d9fba298a: Pushedf9a878ee0ce7: Pushed58f7c9d6536a: Pushed89df9bf0321e: Pushedf925ccf73093: Pushedf8b471c2b6cb: Pushed89169d87dbe2: Mounted from library/centos# 会基于镜像的层数，逐层进行推送， 至此，你的本地镜像就推送到了远程，就可以非常方便的进行共享了。]]></content>
      <categories>
        <category>pipeline</category>
        <category>镜像</category>
        <category>container</category>
      </categories>
      <tags>
        <tag>流程开发</tag>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[docker - 常用命令 - 运行任务是参数的传递]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-04.pipeline%2Fpipeline-container-docker-%E5%BC%82%E5%B8%B8%E6%B8%85%E5%8D%95%2F</url>
    <content type="text"><![CDATA[环境问题交互式启动容器（-it)，进入容器后可以执行安装的软件（以perl为例），但是通过 -i 运行容器找不到安装的软件。 启动容器时不会运行bash 文件，需要将软件链接到 Usr/bin 中。 通过 -it运行示例如下1234$ docker run -v /home/gedongcen/WDL/:/home/gedongcen/WDL/ -it aio_dev:1.0.0.0#进入环境root@caf48909e8b5[Wed Feb 08]$ perl -v# This is perl 5, version 22, subversion 0 (v5.22.0) built for x86_64-linux-thread-multi 通过 -i 运行示例如下12345$ docker run -v /home/gedongcen/WDL/:/home/gedongcen/WDL/ -i aio_dev:1.0.0.0 perl -v# docker: Error response from daemon: failed to create shim task: OCI runtime create failed: runc create failed: unable to start container process: exec: "perl": executable file not found in $PATH: unknown.$ docker run -v /home/gedongcen/WDL/:/home/gedongcen/WDL/ -i aio_dev:1.0.0.0 /bin/bash perl -v# /bin/bash: perl: No such file or directory]]></content>
      <categories>
        <category>pipeline</category>
        <category>镜像</category>
        <category>container</category>
      </categories>
      <tags>
        <tag>流程开发</tag>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[docker - 基于commit方法的镜像制作]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-04.pipeline%2Fpipeline-container-docker-%E9%95%9C%E5%83%8F%E5%88%B6%E4%BD%9C-commit%E4%BF%9D%E5%AD%98%2F</url>
    <content type="text"><![CDATA[参考资料官方指南知乎 制作自己的Docker镜像制作自己的Docker镜像主要有如下两种方式： 在运行一个已有镜像后，我们进入对应容器，进行了相关的操作（安装、配置等），想要保存相关的操作记录，则可以通过 docker commit 将容器保存为一个新的镜像. 构建一个DockerFile（记录了镜像创建步骤），然后使用docker build创建一个新的镜像。 本文主要介绍通过commit进行镜像的创建，也就是我们基于一个已有镜像启动一个容器，然后再容器中进行相应的环境配置、软件安装，完成相关工作后，重新将容器打包成一个镜像。DockerFile的创建可以参考另外一篇文章。 使用docker commit 命令来扩展镜像docker commit的方案相对比较简单,而且在镜像创建过程中，允许通过交互式shell来进行容器的调试；但是仍然推荐使用DockerFile进行镜像的管理；docker commit 不会打包容器上挂载卷的数据；正在提交的容器和其进程将在提交镜像时暂停（关闭 –pause false）。 既然以容器为单位，所以首先我们需要获得一个运行状态的容器 12# 通过docker run 命令启动容器docker run -it -v /etc:/etc_test python:latest /bin/bash 进入容器，对容器内的环境（软件、文件、环境、用户等）进行更改。实际和正常使用容器的过程一样 12345docker run -it -v /etc:/etc_test ae9660359c2a /bin/bashroot@1fb4f27a9174:/# pip install pandasCollecting pandas Downloading pandas-2.0.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.3 MB) ━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.3/12.3 MB 72.4 kB/s eta 0:02:46 docker commit 提交修改的镜像1234567891011121314151617181920212223# docker commit [OPTIONS] CONTAINER [REPOSITORY[:TAG]] # 从容器创建一个新的镜像。-a :提交的镜像作者；-c :使用Dockerfile指令来创建镜像,更改dockerFile的元数据；-m :提交时的说明文字；-p :在commit时，将容器暂停。eg: docker commit -m "this is a test" --author='Ben' bbcad8aaf6f1 test:v1$ docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESc3f279d17e0a ubuntu:22.04 /bin/bash 7 days ago Up 25 hours desperate_dubinsky197387f1b436 ubuntu:22.04 /bin/bash 7 days ago Up 25 hours focused_hamiltondocker inspect -f "&#123;&#123; .Config.Env &#125;&#125;" c3f279d17e0a[HOME=/ PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin]# 更改DockerFile的 "ENV" 指令，制定DEBUG的参数为truedocker commit --change "ENV DEBUG=true" c3f279d17e0a svendowideit/testimage:version3f5283438590ddocker inspect -f "&#123;&#123; .Config.Env &#125;&#125;" f5283438590d[HOME=/ PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin DEBUG=true] 使用 docker commit 完成登记以后，新的镜像就创建完成了。 可以看到，通过在容器中进行操作，然后commit保存镜像的方法非常便捷，尤其是针对镜像环境/软件依赖没有明确的情况下，可以灵活的进行环境的调整和测试。而且不需要额外的去熟悉学习DockerFile的相关命令和特性，对刚接触Docker的人来说相对友好。 但是另一个方面，每一个运行的容器中的所有操作统一保存到镜像，一方面难免会存在一些冗余操作，另一方面，不如dockerFile方便进行共享。]]></content>
      <categories>
        <category>pipeline</category>
        <category>镜像</category>
        <category>container</category>
      </categories>
      <tags>
        <tag>流程开发</tag>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[docker - 常用命令 - 运行任务是参数的传递]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-04.pipeline%2Fpipeline-container-docker-%E7%94%A8%E6%88%B7%E7%BB%84%E7%AE%A1%E7%90%86%2F</url>
    <content type="text"><![CDATA[docker 以进程为核心, 对系统资源进行隔离使用的管理工具. 隔离是通过 cgroups (control groups 进程控制组) 这个操作系统内核特性来实现的. 包括用户的参数限制、 帐户管理、 资源(cpu,内存,磁盘i/o,网络)使用的隔离等. docker 在运行时可以为容器内进程指定用户和组. 没有指定时默认是 root .但因为隔离的原因, 并不会因此丧失安全性. 传统上, 特定的应用都以特定的用户来运行, 在容器内进程指定运行程序的所属用户或组并不需要在 host 中事先创建. Docker容器的权限管理默认使用的root权限不管是以root用户还是以普通用户（有启动docker容器的权限）启动docker容器，容器进程和容器内的用户权限都是root！新建了sleep用户，以sleep用户权限启动容器并在有root权限的磁盘进行权限测试。1docker run -v /data/sleep:/sleep -d --name sleep-1 ubuntu sleep infinity 在宿主机中/data/sleep路径新建了leo_zhou文件并在文件写入“docker”，然后进入sleep-1容器1docker exec -it sleep-1 bash 依然可以正常操作拥有root权限的文件。 限制Docker容器启动的用户新增–user参数，使容器启动用户变成指定的sleep用户，发现并不能操作拥有root权限的文件了。会发现容器中的uid号和实际主机中的uid号一样，也验证了docker容器使用宿主机的内核。可以一定程度进行权限管理。 使用namespace隔离技术namespace是一种隔离技术，docker就是使用隔离技术开启特定的namespace创建出一些特殊的进程，不过使用namespace是有条件的。系统会创建dockremap，通过/etc/subuid和/etc/subuid对应的id值，映射到容器中去；实际情况还是使用的是dockremap普通权限，达到自动隔离的效果。①开启Centos内核中关闭的user namespace的功能。12grubby --args=&quot;namespace.unpriv_enable=1 user_namespace.enable=1&quot; --update-kernel=&quot;$(grubby --default-kernel)&quot;echo &quot;user.max_user_namespaces=15076&quot; &gt;&gt; /etc/sysctl.conf ②修改/etc/docker/daemon.json配置，新增”userns-remap”: “default”选项，default默认就是docker自动创建的用户dockremap，然后重启docker。修改此项配置需要慎重，如果是已经部署了一套docker环境，启用此选项后，会切换到隔离环境，以前的docker容器将无法使用！ ③Centos需要手动输入id值映射范围最后systemctl restart docker后再次测试效果，发现文件权限已经变成nobody，但docker容器内部依然是以”root”的权限管理，但实际只有普通用户的权限，从而达到权限隔离的效果。 进程控制组cgroups主要可能做以下几件事: 资源限制 组可以设置为不超过配置的内存限制, 其中还包括文件系统缓存 优先级 某些组可能会获得更大的 cpu 利用率份额或磁盘 i/o 吞吐量 帐号会计 度量组的资源使用情况, 例如, 用于计费的目的 控制 冻结组进程, 设置进程的检查点和重新启动 与 cgroups(控制进程组) 相关联的概念是 namespaces (命令空间)。命名空间主要有六种名称隔离类型: pid 命名空间为进程标识符 (pids) 的分配、进程列表及其详细信息提供了隔离。 虽然新命名空间与其他同级对象隔离, 但其 “父 “ 命名空间中的进程仍会看到子命名空间中的所有进程 (尽管具有不同的 pid 编号)。 网络命名空间隔离网络接口控制器 (物理或虚拟)、iptables 防火墙规则、路由表等。网络命名空间可以使用 “veth “ 虚拟以太网设备彼此连接。 uts 命名空间允许更改主机名。 mount(装载)命名空间允许创建不同的文件系统布局, 或使某些装入点为只读。 ipc 命名空间将 system v 的进程间通信通过命名空间隔离开来。 用户命名空间将用户 id 通过命名空间隔离开来。 普通用户 docker run 容器内 root如 busybox, 可以在 docker 容器中以 root 身份运行软件. 但 docker 容器本身仍以普通用户执行.考虑这样的情况1echo test | docker run -i busybox cat 前面的是当前用户当前系统进程,后面的转入容器内用户和容器内进程运行. 当在容器内 pid 以1运行时, linux 会忽略信号系统的默认行为, 进程收到 sigint 或 sigterm 信号时不会退出, 除非你的进程为此编码. 可以通过 dockerfile stopsignal signal指定停止信号. 普通用户 docker run 容器内指定不同用户 demo_user1docker run --user=demo_user:group1 --group-add group2 &lt;image_name&gt; &lt;command&gt; 这里的 demo_user 和 group1(主组), group2(副组) 不是主机的用户和组, 而是创建容器镜像时创建的. 当dockerfile里没有通过user指令指定运行用户时, 容器会以 root 用户运行进程. docker 指定用户的方式dockerfile 中指定用户运行特定的命令12user &lt;user&gt;[:&lt;group&gt;] #或user &lt;uid&gt;[:&lt;gid&gt;] docker run -u(–user)[user:group] 或 –group-add 参数方式12345678$ docker run busybox cat /etc/passwdroot:x:0:0:root:/root:/bin/sh...www-data:x:33:33:www-data:/var/www:/bin/falsenobody:x:65534:65534:nobody:/home:/bin/false$ docker run --user www-data busybox iduid=33(www-data) gid=33(www-data) docker 容器内用户的权限对比以下情况, host 中普通用户创建的文件, 到 docker 容器下映射成了 root 用户属主:123$ mkdir test &amp;&amp; touch test/a.txt &amp;&amp; cd test$ docker run --rm -it -v `pwd`:/mnt -w /mnt busybox /bin/sh -c &apos;ls -al /mnt/*&apos; -rw-r--r-- 1 root root 0 oct 22 15:36 /mnt/a.txt 而在容器内卷目录中创建的文件, 则对应 host 当前执行 docker 的用户:1234$ docker run --rm -it -v `pwd`:/mnt -w /mnt busybox /bin/sh -c &apos;touch b.txt&apos;$ ls -al-rw-r--r-- 1 xwx staff 0 10 22 23:36 a.txt-rw-r--r-- 1 xwx staff 0 10 22 23:54 b.txt docker volume 文件访问权限创建和使用卷, docker 不支持相对路径的挂载点, 多个容器可以同时使用同一个卷.1234567891011121314$ docker volume create hello #创建卷hello$ docker run -it --rm -v hello:/world -w /world busybox /bin/sh -c &apos;touch /world/a.txt &amp;&amp; ls -al&apos; #容器内建个文件total 8drwxr-xr-x 2 root root 4096 oct 22 16:38 .drwxr-xr-x 1 root root 4096 oct 22 16:38 ..-rw-r--r-- 1 root root 0 oct 22 16:38 a.txt$ docker run -it --rm -v hello:/world -w /world busybox /bin/sh -c &apos;rm /world/a.txt &amp;&amp; ls -al&apos; #从容器内删除total 8drwxr-xr-x 2 root root 4096 oct 22 16:38 .drwxr-xr-x 1 root root 4096 oct 22 16:38 .. 外部创建文件, 容器内指定用户去删除12$ touch c.txt &amp;&amp; sudo chmod root:wheel c.txt$ docker run -u 100 -it --rm -v `pwd`:/world -w /world busybox /bin/sh -c &apos;rm /world/c.txt &amp;&amp; ls -al&apos; 实际是可以删除的123456rm: remove &apos;/world/c.txt&apos;? ytotal 4drwxr-xr-x 4 100 root 128 oct 23 16:09 .drwxr-xr-x 1 root root 4096 oct 23 16:09 ..-rw-r--r-- 1 100 root 0 oct 22 15:36 a.txt-rw-r--r-- 1 100 root 0 oct 22 15:54 b.txt docker 普通用户的1024以下端口权限123456 $ docker run -u 100 -it --rm -p 70:80 busybox /bin/sh -c &apos;nc -l -p 80&apos;nc: bind: permission denied #用户id 100 时, 不能打开80端口 $ docker run -u 100 -it --rm -p 70:8800 busybox /bin/sh -c &apos;nc -l -p 8800&apos; #容器端口大于1024时则可以... $ docker run -it --rm -p 70:80 busybox /bin/sh -c &apos;nc -l -p 80&apos; #容器内是 root 也可以...]]></content>
      <categories>
        <category>pipeline</category>
        <category>镜像</category>
        <category>container</category>
      </categories>
      <tags>
        <tag>流程开发</tag>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[docker - 常用命令 - 运行任务是参数的传递]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-04.pipeline%2Fpipeline-container-docker-%E7%BC%96%E7%A0%81%E8%A7%84%E8%8C%83%2F</url>
    <content type="text"><![CDATA[运行容器时传参ENTRYPOINT 和 CMD我们在运行 docker 镜像时希望能用下面的命令向容器传递命令行参数1docker run &lt;image-name&gt; &lt;command&gt; arg1 arg2 如果要向 docker 容器传递参数时，Dockerfile 该如何写，这就有必要稍稍了解一下 Dockerfile 中 CMD 和 ENTRYPOINT这两个指令，并且它们有 exec 和 shell 两种格式的写法。 对于一个 docker 镜像，我们可以这么来理解 ENTRYPOINT 与 CMD 的关系如果没有定义 ENTRYPOINT， CMD 将作为它的 ENTRYPOINT 定义了 ENTRYPOINT 的话，CMD 只为 ENTRYPOINT 提供参数 CMD 可由 docker run \&lt;image> 后的命令覆盖，同时覆盖参数 对于 #1 和 #2 更精致的理解是容器运行的最终入口由 ENTRYPOINT 和实际的 CMD 拼接而成。ENTRYPOINT 和 CMD 需转换为实际镜像中的 exec 格式来拼接，合并后的第一个元素是命令，其余是它的参数。 举四个例子进行说明 一, 未定义 ENTRYPOINT, 定义了 CMD 12#ENTRYPOINT []CMD [&quot;echo&quot;, &quot;hello&quot;] 实际入口是它们拼接后还是 CMD 本身，[“echo”, “hello”] 二, 定义了 ENTRYPOINT 和 CMD 12ENTRYPOINT [&quot;echo&quot;, &quot;hello&quot;]CMD [&quot;echo&quot;, &quot;world&quot;] 实际入口是它们拼接起来，形成 [“echo”, “hello”, “echo”, “world”], 执行 docker run test 显示为 hello echo world 三, 定义了 ENTRYPOINT, CMD 由 docker run 提供 1ENTRYPOINT [&quot;echo&quot;, &quot;hello&quot;] 执行命令 docker run \ rm -rf /, 实际入口是由 [“echo”, “hello”] 与 [“rm”, “-rf”, “/“] 拼接而成的 [“echo”, “hello”, “rm”, “-rf”, “/“], 输出为 hello rm -rf /。看到 rm -rf / 也不用担心，用 ENTRYPOINT 就是让人放心 注：ENTRYPOINT 同样可以被覆盖，如 docker run –entrypoint ls test -l /，将会执行 ls -l / 命令。 四, 如果 ENTRYPOINT 用 shell 格式定义的 1234567891011121314151617181920212223 ENTRYPOINT java -jar /app.jar CMD [&quot;hello&quot;, &quot;world&quot;] ``` 通过 docker inspect 命令看到镜像中实际的 ENTRYPOINT 是 ENTRYPOINT [&quot;/bin/sh&quot;, &quot;-c&quot;, &quot;java -jar /app.jar&quot;] 所以与 CMD 连接起来的入口就是 [&quot;/bin/sh&quot;, &quot;-c&quot;, &quot;java -jar /app.jar&quot;, &quot;hello&quot;, &quot;world&quot;]，&quot;bin/sh&quot; 直接忽略掉后面的 &quot;hello&quot; 与 &quot;world&quot;，这就是为什么shell 命令方式无法获取参数。有了以上几点概念，以及四个实例作为感观认识后，想要怎么往容器传递参数应该很容易确定了。## 传参示例- 未定义 ENTRYPOINT没有定义 ENTRYPOINT 的镜像想怎么来就怎么来，docker run &lt;image&gt; 后面的输入你自己作主。- 有定义 ENTRYPOINT 定义了 ENTRYPOINT 的镜像，则是 CMD 或 docker run &lt;image&gt; 后的输入作为 ENTRYPOINT 中命令的附加参数。再次提醒 shell 格式的 ENTRYPOINT 和 CMD 务必要转换为相应 exec 格式来理解。- shell 格式的 ENTRYPOINT 如果是复杂的 shell 命令不容易拆解出一个个参数，而希望用 shell 格式来定义 ENTRYPOINT 的话，也有办法。shell 格式的 ENTRYPOINT 是由 &quot;/bin/sh -c&quot; 启动的，而它是可以解析变量的。另一方面 CMD 或 docker run &lt;image&gt; 的输入第一个元素存成了 $0，其他剩余元素存为 $@, 所以 shell 格式的 ENTRYPOINT 可以这么写## 使用环境变量传参### 使用ENTRYPOINT对于 shell 格式的 ENTRYPOINT, 或者显式由 &quot;/bin/sh -c&quot; 来启动的命令，可以通过环境变量传入参数 ENTRYPOINT java $JAVA_OPTS -jar app.jar $0 $@ #或显式的 ENTRYPOINT [“/bin/sh”, “-c”, “java $JAVA_OPTS -jar /app.jar $0 $@”]1启动容器时的命令用 docker run -e JAVA_OPTS=”-Xmx5G -Xms2G” aa bb1那么实际执行 java 的完整命令就是 java -Xmx5G -Xms2G -jar /app.jar aa bb12### 使用CMD- dockerfile 示例 FROM docker.io/python:3.6MAINTAINER tianye 设置容器时间 RUN cp /usr/share/zoneinfo/Asia/Shanghai /etc/localtime &amp;&amp; echo ‘Asia/Shanghai’ &gt;/etc/timezone ENV LANG C.UTF-8 # 设置编码 ENV PATH=$PATH:/usr/local/lib/python3.6/ ENV PYTHONPATH $PATH # 配置环境变量 ENV PARAMS=”” # 给我们要传的参数一个初始值 #代码添加到code文件夹 ADD ./tttt/ /test/code/tttt/ #设置code文件夹为工作目录 WORKDIR /test/code/tttt/ CMD python3 ttt.py $PARAMS 1- 创建镜像并启动容器 docker build -t my_image . docker run -it -d –name my_container -e PARAMS=”hahaha” my_image # my_image 放最后 这里hahaha 加不加引号 无所谓 docker logs -f –tail 200 my_container ` Dockerfile中 最后一行 $PARAMS 会解析为一个变量获取其值，也就是 docker run传入的参数 “hahaha”， 在python程序中通过 argv[1] 就可以获取到我们传入的”hahaha” ! 需要注意的一点是Dockerfile 中CMD的用法，如果我们不传参那么写法有很多如： CMD [“python3”, “ttt.py”] CMD [python3, ttt.py] CMD “python3” “ttt.py” CMD python3 ttt.py 但是要传参的话： 我们的参数 \$PARAMS 是万万不能用 “ “ 的，不然Dockerfile会认为是普通字符串 CMD [“python”, “ttt.py”, \$PARAMS] (×) 原因可能是字符串和变量放到一个列表时，字符串优先级高，直接将 $PARAMS当作一个字符串处理 CMD [python3, ttt.py, $PARAMS] (×) CMD “python3” “ttt.py” $PARAMS (√，推荐) CMD python3 ttt.py $PARAMS (√)CMD [] 形式，中括号中 必须用逗号分割； 如果不用中括号，不能用逗号分割！传参机制 容器运行的最终入口由 ENTRYPOINT 和实际的 CMD 拼接而成。 ENTRYPOINT 和 CMD 合并前需转换为 exec 格式(用 docker inspect 查看)，合并后(相当于数组) 第一个元素是命令，其他都为参数 CMD 可在 Dockerfile 中配置，在启动容器时会被 docker run 后的参数覆盖 CMD 的 exec 格式中，第一个元素是 shell 的 $0, 其余元素是 shell 的 $@。当 ENTRYPOINT 中用 shell 格式或显式的 sh(bash等)就可以引用 $0, $@ 环境变量的解析是通过 sh(bash 等) 来解析的，所以 ENTRYPOINT [“echo”, “$name”] 中的 $name 是不被解析的最能说明问题的是 docker inspect 看个究竟，Path 和 Args 说明了一切 参考资料 CSDN博客 博客]]></content>
      <categories>
        <category>pipeline</category>
        <category>镜像</category>
        <category>container</category>
      </categories>
      <tags>
        <tag>流程开发</tag>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[docker - 基于DockerFile的镜像制作]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-04.pipeline%2Fpipeline-container-docker-%E9%95%9C%E5%83%8F%E5%88%B6%E4%BD%9C-DockerFile%2F</url>
    <content type="text"><![CDATA[制作自己的Docker镜像主要有如下两种方式： 运行一个已有的镜像，启动镜像后，进入容器，进行相关的操作（安装、配置等）然后将容器保存为一个新的镜像 构建一个DockerFile（记录了镜像创建步骤），然后使用docker build创建一个新的镜像。 本文主要介绍通过DockerFile进行镜像的创建，commit的创建可以参考另外一篇文章。 Dockerfile是一个文本文档，其中包含用户可以在命令行上运行调用以生成镜像的所有命令。获得 Dockerfile 文件后，我们可以使用 docker build 来完成镜像的创建。要创建一个 Dockerfile 文件，我们首先介绍一下 DockerFile 文件支持的指令和文件格式。 获得一个DokcerFile通过已知的镜像获取DockerFiledfimage 是一款第三方工具，可用来从镜像中提取 Dockerfile12alias dfimage="docker run -v /var/run/docker.sock:/var/run/docker.sock --rm alpine/dfimage"dfimage -sV=1.36 test:v1.0 如果我们有一个基本满足我们需求的镜像，但是可能这个镜像相对我们有一些冗余的功能，我们可以通过已知的镜像生成一个DockerFile进行微调。当然也可以通过这种方式了解一些镜像的构建过程，相比于 docker history 使用dfimage得到的镜像构建信息会更全面。 从头写一个 DockerFile要自己写 DockerFile， 我们先来简单的了解一下DockerFile的语法。 Dockerfile 中使用#来进行注释，同时Dockerfile支持多种不同的命令分别用以指定初始镜像（FROM）、工作目录（WORKDIR）、运行数据挂载和网络设置（RUN）、命令行执行（CMD）、设置环境变量（ENV）、添加文件到镜像文件系统中（ADD）、添加文件到容器文件系统中（COPY）、创建挂载点（VOLUME）、设置用户（USER）、变量（ARG）、镜像添加元数据（LABEL）、监听网络（EXPOSE）、入口点（ENTRYPOINT）等等。现在为大家介绍一些镜像构建过程中会用到的基本命令，一些指令在现在的个人进行镜像构建的过程中并未用到，暂时不进行扩展，大家有需要的时候可以参考官方文档DockerFile reference DockerFile 指令说明 DockerFile指令 作用 示例 FROM 初始基础镜像，后续所有操作都是以基础镜像为初始环境 FROM ubuntu:14.04 RUN 用于执行后面跟着的命令行命令。有以下俩种格式。 RUN &lt;命令行命令&gt; COPY 复制指令，从上下文目录中复制文件或者目录到容器里指定路径。 COPY [--chown=&lt;user&gt;:&lt;group&gt;] &lt;源路径1&gt;... &lt;目标路径&gt; ADD 添加本地或远程文件和目录。 ARG 使用构建时变量。 CMD 指定默认命令。 ENTRYPOINT 指定默认可执行文件。 ENV 设置环境变量。 EXPOSE 描述您的应用程序正在侦听哪些端口。 HEALTHCHECK 在启动时检查容器的运行状况。 LABEL 将元数据添加到镜像。 MAINTAINER 指定图像的作者。 ONBUILD 指定在构建中使用映像时的说明。 SHELL 设置图像的默认外壳。 STOPSIGNAL 指定退出容器的系统调用信号。 USER 设置用户和组 ID。 VOLUME 创建卷挂载。 WORKDIR 更改工作目录。 解析器指令顾名思义，解析器指令和之前列的这些镜像操作指令不一样，解析器指令是用来配置DockerFile解释器如何解析文件的，会影响 Dockerfile 中后续行的处理方式。解释器指令不是必须的，只有需要对解析逻辑进行调整时，需要配置。解析器指令不会向构建添加层，也不会显示为构建步骤。解析器指令以 # directive=value 形式编写为特殊类型的注释。单个指令只能使用一次。解析器指令位于 Dockerfile 的顶部，否则都会视为注释。 12345# 指定DockerFile的版本# syntax=docker/dockerfile:1 # 指定命令续航符（默认的"\"),比如windwos系统，默认续航符会和目录间隔符存在冲突，我们需要进行更改。# escape=` ENV环境变量（使用 ENV 语句声明）也可以在某些指令中用作由 Dockerfile 解释的变量。还可以处理转义以将类似变量的语法按字面意思包含到语句中。 123456789101112131415161718192021222324$&#123;variable:-word&#125; #表示如果设置了 variable ，则结果将是该值。如果 variable 未设置，则结果为 word 。$&#123;variable:+word&#125; #表示如果设置了 variable ，则结果为 word ，否则结果为空字符串。#$&#123;variable#pattern&#125; 从 variable 中删除 pattern 的最短匹配，从字符串的开头查找。str=foobarbaz echo $&#123;str#f*b&#125; # arbaz#$&#123;variable##pattern&#125; 从 variable 中删除 pattern 的最长匹配项，从字符串的开头查找。str=foobarbaz echo $&#123;str##f*b&#125; # az#$&#123;variable%pattern&#125; 从 variable 中删除 pattern 的最短匹配，从字符串末尾向后查找。string=foobarbaz echo $&#123;string%b*&#125; # foobar#$&#123;variable%%pattern&#125; 从 variable 中删除 pattern 的最长匹配项，从字符串末尾向后查找。string=foobarbaz echo $&#123;string%%b*&#125; # foo#$&#123;variable/pattern/replacement&#125; 将 variable 中第一次出现的 pattern 替换为 replacementstring=foobarbaz echo $&#123;string/ba/fo&#125; # fooforbaz#$&#123;variable//pattern/replacement&#125; 将 variable 中出现的所有 pattern 替换为 replacementstring=foobarbaz echo $&#123;string//ba/fo&#125; # fooforfoz#pattern 是一个全局模式，其中 ? 匹配任何单个字符， * 匹配任意数量的字符（包括零）。要匹配文字 ? 和 * ，请使用反斜杠转义： \? 和 \* word 可以是任何字符串，包括其他环境变量。123456# 示例（解析后的表示显示在 # 之后）：FROM busyboxENV FOO=/barWORKDIR $&#123;FOO&#125; # WORKDIR /barADD . $FOO # ADD . /barCOPY \$FOO /quux # COPY $FOO /quux 在 ADD、COPY、ENV、EXPOSE、FROM、LABEL、STOPSIGNAL、USER、VOLUME、WORKDIR中都支持使用环境变量。** 环境变量替换在整个指令中对每个变量使用相同的值。更改变量的值仅在后续指令中生效。 From1234# 三种语法示例FROM [--platform=&lt;platform&gt;] &lt;image&gt; [AS &lt;name&gt;]FROM [--platform=&lt;platform&gt;] &lt;image&gt;[:&lt;tag&gt;] [AS &lt;name&gt;]FROM [--platform=&lt;platform&gt;] &lt;image&gt;[@&lt;digest&gt;] [AS &lt;name&gt;] FROM 初始化一个新的构建阶段并为后续指令设置基础镜像。因此，有效的 Dockerfile 必须以 FROM 开头。该镜像可以是任何有效的镜像（尽可能选择满足需求的最小镜像）。 ARG是唯一可以位于FROM之前的指令 FROM 可以在单个 Dockerfile 中出现多次，以创建多个镜像或使用一个构建阶段作为另一个构建阶段的依赖项。前面的FROM及对应的构建阶段都不会保存，只有最后一个阶段构建的镜像会被直接保存。 tag 或 digest 值是可选的。如果省略其中任何一个，构建器默认采用 latest 标记。 RUNRUN 指令将执行任何命令以在当前图像之上创建新层。添加的层将在 Dockerfile 的下一步中使用。 RUN 有两种形式：1234567#shell格式，&lt;命令行命令&gt; 等同于，在终端操作的 shell 命令。RUN &lt;命令行命令&gt;#exec 格式：RUN ["可执行文件", "参数1", "参数2"]# 例如：# RUN ["./test.php", "dev", "offline"] 等价于 RUN ./test.php dev offline shell 形式是最常用的，它允许您将较长的指令分成多行，可以使用换行符转义，也可以使用下属方法：12345#shell格式，&lt;命令行命令&gt; 等同于，在终端操作的 shell 命令。相当于命令行续航符"\"RUN &lt;&lt;EOFapt-get updateapt-get install -y curlEOF 前面的RUN构建过程相对容易，比较复杂的时相关文件系统的处理，因为可能在构建过程中，我们需要使用到一些特定的数据，这些数据我们需要在执行某些特定命令的时候，挂载到镜像上。 RUN –mount 允许您创建构建可以访问的文件系统挂载。这可以用于： 创建到主机文件系统或其他构建阶段的绑定挂载 访问构建机密或 ssh-agent 套接字 使用持久的包管理缓存来加快构建速度 类型 说明 bind(default) 用于挂载一个上下文目录（不能挂载宿主机的目录）到容器中 ,默认时只读的 cache 主要用于挂载一个临时目录来缓存编译器和包管理器的目录。 tmpfs 主要用于挂载一个tmpfs secret 允许构建容器访问诸如私钥之类的安全文件，并且此类文件不会出现在构建好的镜像中，避免密钥外泄。 ssh 允许构建容器通过SSH代理访问SSH密钥，并支持密码短语 这部分使用因为在自己业务场景中没有太大需求，这里介绍一些简单的示例。知道有这个功能就好，就不进行展开了。12345RUN --mount=[type=TYPE][,option=&lt;value&gt;[,option=&lt;value&gt;]...]# 挂载一个大小为100MB的临时文件系统到 /mnt 目录，并在其中执行命令RUN --mount=type=tmpfs,size=100m,uid=1000,gid=1000,mode=0755 \ touch /mnt/test_file CMDCMD 指令设置从映像运行容器时执行的命令。Dockerfile 中只能有一条 CMD 指令。如果列出多个 CMD ，则只有最后一个生效。CMD 的目的是为执行容器（创建镜像阶段不会执行）提供默认值。这些默认值可以包含可执行文件，也可以省略可执行文件，在这种情况下，您还必须指定 ENTRYPOINT 指令。 LABEL1LABEL &lt;key&gt;=&lt;value&gt; &lt;key&gt;=&lt;value&gt; &lt;key&gt;=&lt;value&gt; ... LABEl支持将元数据添加到镜像中，LABEL的格式是key=value的键值对。如果value中涉及到空格的话，参考命令行使用引号和反斜杠来处理。示例：12345LABEL "com.example.vendor"="ACME Incorporated"LABEL org.opencontainers.image.authors="liubo4@genomics.com"LABEL version="1.0"LABEL description="This text illustrates \that label-values can span multiple lines." 一个镜像可能会有多个标签，针对早期的Docker（Docker 1.10之前）写成一行可以有效的降低最终的镜像大小；新版本的Docker（Docker 1.10之后）两种写法是类似的。12345LABEL multi.label1="value1" multi.label2="value2" other="value3"LABEL multi.label1="value1" \ multi.label2="value2" \ other="value3" 父镜像（From的镜像）中的标签将会子镜像继承，在子镜像重新赋值的可以覆盖父镜像的标签。查看镜像的标签使用 docker image inspect $imageid ADDADD 还可以复制压缩（tarball）文件并在目的地自动提取内容。此功能仅适用于本地存储的压缩文件和目录。但 ADD 可以提取压缩文件并通过URL从远程位置复制文件。 1234ADD [source] … [destination]# 要将本地可用文件添加到图像目录，请键入：ADD /home/test/myapp/code.php /root/myapp COPY示例Docker 引入了 COPY 作为复制内容的附加命令，以解决 ADD 命令的一些功能问题。 ADD 的主要问题是压缩文件的自动提取，这可能会导致 Docker 镜像损坏。如果用户没有预料到该命令的行为，就会发生这种情况。COPY 命令的唯一指定功能是以现有格式复制指定的文件和目录。此限制会影响按原样复制的压缩文件，即不解压。此外， COPY 仅适用于本地存储的文件。它不能与 URL 一起使用来将外部文件复制到镜像。需要拷贝的目录需要在dockerFIle目录中保存，测试使用中，只能访问该目录下的文件，其他目录进行拷贝时，会存在无法访问相关文件的问题。 123COPY [source] … [destination] # 例如，键入以下命令将 index.html 从主构建上下文目录复制到映像中的 /usr/local/apache2/htdocs：COPY index.html /usr/local/apache2/htdocs 除了用户想要提取本地压缩文件外，在所有情况下都不鼓励使用 ADD 命令。对于复制远程文件，run命令结合wget或curl更安全、更高效。此方法避免创建额外的图像层并节省空间。 SHELLSHELL 指令用于更改执行命令的默认 shell。Linux 上的默认 shell 是 [“/bin/sh”, “-c”] ，Windows 上的默认 shell 是 [“cmd”, “/S”, “/C”] 。SHELL 指令必须以JSON形式写入 Dockerfile 中。 SHELL 指令在 Windows 上特别有用，其中有两种常用且截然不同的本机 shell： cmd 和 powershell ，以及可用的备用其它 shell。 123456789101112131415FROM microsoft/windowsservercore# Executed as cmd /S /C echo defaultRUN echo default# Executed as cmd /S /C powershell -command Write-Host defaultRUN powershell -command Write-Host default# Executed as powershell -command Write-Host helloSHELL ["powershell", "-command"]RUN Write-Host hello# Executed as cmd /S /C echo helloSHELL ["cmd", "/S", "/C"]RUN echo hello 当 Dockerfile 中使用 shell 形式时，以下指令可能会受到 SHELL 指令的影响： RUN 、 CMD 和 ENTRYPOINT . ARGARG 指令定义了一个变量，用户可以在构建时使用 –build-arg = 标志通过 docker build 命令将其传递给构建器。ARG 变量定义从 Dockerfile 中定义它的行开始生效，而不是从命令行或其他地方使用参数开始生效。所以尽可能紧跟FROM 完成需要传递变量的定义，从而避免引用变量时，还未进行定义。 123456FROM busybox# 定义一个变量ARG user1 # 定义一个变量并设置默认值ARG buildno=1 # ... 每个构建阶段单独声明ARG 指令在定义它的构建阶段结束时超出范围。要在多个阶段使用参数，每个阶段必须包含 ARG 指令。1234567FROM busyboxARG SETTINGSRUN ./run/setup $SETTINGSFROM busyboxARG SETTINGSRUN ./run/other $SETTINGS VOLUME撰写DockerFile了解了DockerFile的语法之后，我们可以利用对应的指令编写我们需要环境对应的一个 DockerFile。再编写DockerFile的时候，我们需要注意几个点： 规范要求 将以 # 开头的行视为注释，除非该行是有效的解析器指令。行中任何其他位置的 # 标记都被视为参数。这允许这样的语句： 123RUN echo hello \# commentworld 约定俗成 该指令不区分大小写。然而，惯例是它们是大写的，以便更容易地将它们与参数区分开来。 注意事项 编写Dockerfile，Dockerfile中每一条/行指令都创建镜像的一层，所以尽可能合并无意义的层，避免镜像过大，例如：这时候，也许会想，我们把所有环境安装都放到一层不就可以了么。创建镜像的时候还要考虑其他问题：1. 那就是镜像创建环境的网络稳定性。如果只有一层，而网络又不稳定的话，那么可能每次构建都会中途中断，然后需要从头重构。进行合理的分层，相当于设置一些构建过程中的一些重要节点（层），这样在进行build的时候，会记录保存中间层的构建，二次构建的时候，可以直接引用已有层的结果，即使中途中断，也可以从对应节点继续。 2. 如果需要使用的镜像环境非常多，我们需要构建多个镜像，但是他们有一些相同的底层镜像依赖。那么我们可以通过镜像分层构建共同的底座，然后通过继承的方式来构建不同的镜像。 不建议的写法（命令拆分产生无意义的多层镜像）：12345678# 这里是注释# 设置继承自哪个镜像FROM ubuntu:14.04# 下面是一些创建者的基本信息MAINTAINER liubo (liubo4@genomics.com)# 在终端需要执行的命令RUN apt-get install -y openssh-server # 创建第一层RUN mkdir -p /var/run/sshd # 创建第二层 建议的写法（如非必要只创建一层）123456# 设置继承自哪个镜像FROM ubuntu:14.04# 下面是一些创建者的基本信息MAINTAINER birdben (191654006@163.com)# 在终端需要执行的命令RUN apt-get install -y openssh-server &amp;&amp; mkdir -p /var/run/sshd # 创建第一层 通过DockerFile生成镜像编写完成 Dockerfile 后可以使用 docker build 来生成镜像。首先创建一个新的目录，将 DockerFile 文件存档道目录中，执行 docker build 命令来生成镜像。 1234567891011121314151617181920212223$ sudo docker build -t=&quot;birdben/ubuntu:v1&quot; .# 下面是一堆构建日志信息############我是日志############# 参数：# -t 标记来添加 tag，指定新的镜像的用户和镜像名称信息。 # “.” 是 Dockerfile 所在的路径（当前目录），也可以替换为一个具体的 Dockerfile 的路径。# 以交互方式运行docker$ docker run -it birdben/ubuntu:v1 /bin/bash# 运行docker时指定配置$ sudo docker run -d -p 10.211.55.4:9999:22 ubuntu:tools &apos;/usr/sbin/sshd&apos; -D# 参数：# -i：表示以“交互模式”运行容器，-i 则让容器的标准输入保持打开# -t：表示容器启动后会进入其命令行，-t 选项让Docker分配一个伪终端（pseudo-tty）并绑定到容器的标准输入上# -v：表示需要将本地哪个目录挂载到容器中，格式：-v &lt;宿主机目录&gt;:&lt;容器目录&gt;，-v 标记来创建一个数据卷并挂载到容器里。在一次 run 中多次使用可以挂载多个数据卷。# -p：指定对外80端口# 不一定要使用“镜像 ID”，也可以使用“仓库名:标签名” 在build阶段设置参数 Dockerfile 最后一行如下： 12345678910111213[root@fangjike temp]# cat Dockerfile FROM python:2.7-slimMAINTAINER yellowtailCOPY startup.sh /optRUN chmod +x /opt/startup.shARG envType=xxxENV envType $&#123;envType&#125;CMD /opt/startup.sh $&#123;envType&#125; build 1docker build -t yellow:4.0 --build-arg envType=dev . run 12[root@fangjike temp]# docker run -ti --rm=true yellow:4.0in startup, args: dev 参考资料官方指南知乎]]></content>
      <categories>
        <category>pipeline</category>
        <category>镜像</category>
        <category>container</category>
      </categories>
      <tags>
        <tag>流程开发</tag>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[云计算技术及性能优化]]></title>
    <url>%2F01.%E7%9F%A5%E8%AF%86-06.cloud_computing%2F%E4%BA%91%E8%AE%A1%E7%AE%97-%E4%B9%A6_%E4%BA%91%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E5%8F%8A%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%9601%2F</url>
    <content type="text"><![CDATA[写在前面，最近一些业务工作，涉及到整体的云计算业务平台开发。之前虽然都有一些零散的涉猎和了解，但是知识框架不成体系，零散的认知在紧迫的业务面前捉襟见肘。所以趁着双十一，买了一些教材构建一个针对云计算的系统认知。各种博客、课程、教材看的再多，可能都不如自己亲自体验经历一遍，因此也会借此机会在系列课程的学习过程中，利用一些现有条件进行实践。 - 纸上得来终觉浅，绝知此事要躬行。 云图 – 云计算图志 云计算的产生与发展1.1 云计算的产生20世纪60年代，John McCarthy提到“计算迟早有一天会变成一种公用基础设施”。 2007年10月，IBM和GOogle宣布在云计算领域的合作，云计算开始吸引了众多的关注迅速发展。 21世纪初，Web2.0 的流程让网络迎来了新的发展高峰。 技术上，分布式计算(Distributed Computing)技术的日益成熟和应用，特别是网络计算的发展通过Internet把分散在各处的硬件、软件、信息资源链接成为一个巨大的整体，是的人们能够利用地理上分散的资源，完成大规模、复杂的计算和数据处理任务。 数据存储的快速增长产生了以谷歌文件系统(Google File System, GFS)、存储域网络(Storage Area Network, SAN)为代表的的高性能存储技术。另外服务器整合需求推动了虚拟化技术的进步，这些技术的发展未构建更强大的计算能力和服务平台提供了可能。云计算应运而生。 1.2 云计算发展历程1.2.1 计算模式演进云计算是在并行计算(Parallel Computing)、分布式计算、网格计算(Grid Computing)和效用计算(Utility Computing)的基础上发展起来的，经过持续烟花和融合改进逐步形成目前留下的云计算模型，云计算的演化过程如下所述： 1. 并行计算包括空间并行、基于流水线技术的时间并行，以及基于优化算法的数据并行和任务并行等。是对穿行计算的单指令流单数据流做出优化，以及通过采用多指令流多数据流的并行计算的大幅度提升系统的处理能力。 2. 分布式计算分布式计算模式是在处理庞大的计算请求时，将需要解决的问题分解成细小的组成部分，然后将这些组成部分分散给众多的计算机进行处理，处理完成后将结果进行汇总，形成最终结果。 3. 网格计算网格计算是一种无缝、集成的计算和协作环境。安装网格提供的功能，网格可分为两类：计算网格和存储网格。计算网格可以提供虚拟的、无限制的计算和分布数据资源，存储网格则提供一个合作环境。 4. 效用计算效用计算的具体目标是结合分散各地的服务器、存储系统及应用程序来立即提供需求数据的技术。效用这个词是指为客户提供个性化的服务，并且可以满足不断变化的客户需求，可以基于实际占用的资源进行收费。 5. 云计算云计算强调所有资源均以服务的形态出现，包括基础设施即服务、平台即服务、软件即服务、数据即服务、知识即服务、存储即服务、安全即服务等。 1.2.2 云计算发展大事记 1959年6月，Christopher Strachey发表虚拟化论文，虚拟化是今天云计算基础架构的基石。 1961年，John McCarthy提出计算力和通过公用事业销售计算机应用的思想。 1962年，J.C.R. Licklider提出“星际计算机网络”设想。 1965年 美国电话公司Western Union一位高管提出建立信息公用事业的设想。 1984年，Sun公司的联合创始人John Gage说出了“网络就是计算机”的名言，用于描述分布式计算技术带来的新世界，今天的云计算正在将这一理念变成现实。 1996年，网格计算Globus开源网格平台起步。 1997年，南加州大学教授Ramnath K. Chellappa提出云计算的第一个学术定义”，认为计算的边界可以不是技术局限，而是经济合理性。 1998年，VMware(威睿公司)成立并首次引入X86的虚拟技术。 1999年，Marc Andreessen创建LoudCloud，是第一个商业化的IaaS平台。 1999年，salesforce.com公司成立，宣布“软件终结”革命开始。 2000年，SaaS兴起。 2004年，Web 2.0会议举行，Web 2.0成为技术流行词，互联网发展进入新阶段。 2004年，Google发布MapReduce论文。Hadoop就是Google集群系统的一个开源项目总称，主要由HDFS、MapReduce和Hbase组成，其中HDFS是Google File System(GFS)的开源实现;MapReduce是Google MapReduce的开源实现;HBase是Google BigTable的开源实现。 2004年，Doug Cutting 和 Mike Cafarella实现了Hadoop分布式文件系统(HDFS)和Map-Reduce，Hadoop并成为了非常优秀的分布式系统基础架构。 2005年，Amazon宣布Amazon Web Services云计算平台。 2006年，Amazon相继推出在线存储服务S3和弹性计算云EC2等云服务。 2006年，Sun推出基于云计算理论的“BlackBox”计划。 2007年，Google与IBM在大学开设云计算课程。 2007年3月，戴尔成立数据中心解决方案部门，先后为全球5大云计算平台中的三个(包括Windows Azure、Facebook和Ask.com)提供云基础架构。 2007年7月，亚马逊公司推出了简单队列服务(Simple Queue Service，SQS)，这项服务使托管主机可以存储计算机之间发送的消息。 2007年11月，IBM首次发布云计算商业解决方案，推出“蓝云”(Blue Cloud)计划。 2008年1月，Salesforce.com推出了随需应变平台DevForce,Force.com平台是世界上第一个平台即服务的应用。 2008年2月，EMC中国研发集团云架构和服务部正式成立，该部门结合云基础架构部、Mozy和Pi两家公司共同形成EMC云战略体系。 2008年2月，IBM宣布在中国无锡太湖新城科教产业园为中国的软件公司建立第一个云计算中心。 2008年4月，Google App Engine发布。 2008年中，Gartner发布报告，认为云计算代表了计算的方向。 2008年5月，Sun在2008JavaOne开发者大会上宣布推出“Hydrazine”计划。 2008年6月，EMC公司中国研发中心启动“道里”可信基础架构联合研究项目。 2008年6月，IBM宣布成立IBM大中华区云计算中心。 2008年7月，HP、Intel和Yahoo联合创建云计算试验台Open Cirrus。 2008年8月3日，美国专利商标局(以下简称“SPTO”)网站信息显示，戴尔正在申请“云计算”(Cloud Computing)商标，此举旨在加强对这一未来可能重塑技术架构的术语的控制权。戴尔在申请文件中称，云计算是“在数据中心和巨型规模的计算环境中，为他人提供计算机硬件定制制造”。 2008年9月 Google公司推出Google Chrome浏览器，将浏览器彻底融入云计算时代。 2008年9月，甲骨文和亚马逊AWS合作，用户可在云中部署甲骨文软件、在云中备份甲骨文数据库。 2008年9月，思杰公布云计算战略，并发布新的思杰云中心(Citrix Cloud Center，C3)产品系列。 2008年10月，微软发布其公共云计算平台——Windows Azure Platform，由此拉开了微软的云计算大幕。 2008年12月，Gartner披露十大数据中心突破性技术，虚拟化和云计算上榜。 2008年，亚马逊、Google和Flexiscale的云服务相继发生宕机故障，引发业界对云计算安全的讨论。 2009年，思科先后发布统一计算系统(UCS)、云计算服务平台，并与EMC、Vmware建立虚拟计算环境联盟。 2009年1月，阿里软件在江苏南京建立首个“电子商务云计算中心”。 2009年4月，VMware推出业界首款云操作系统VMware vSphere 4。 2009年7月 Google宣布将推出Chrome OS操作系统。 2009年7月，中国首个企业云计算平台诞生(中化企业云计算平台)。 2009年9月，VMware启动vCloud计划 构建全新云服务。 2009年11月，中国移动云计算平台“大云”计划启动。 2010年1月，HP和微软联合提供完整的云计算解决方案。 2010年1月，IBM与松下达成迄今为止全球最大的云计算交易。 2010年1月，Microsoft正式发布Microsoft Azure云平台服务。 2010年4月，英特尔在IDF上提出互联计算，图谋用X86架构统一嵌入式、物联网和云计算领域。 2010年，微软宣布其90%员工将从事云计算及相关工作。 2010年4月，戴尔推出源于DCS部门设计的PowerEdgeC系列云计算服务器及相关服务。 1.2.3 云计算时代企业的云化IT设施建设过程可以分为三个阶段，如下图 第一个阶段：大集中过程。这一过程将企业分散的数据资源、IT 资源进行了物理集中，形成了规模化的数据中心基础设施。在数据集中过程中，不断实施数据和业务的整合，大多数企业的数据中心基本完成了自身的标准化，使得既有业务的扩展和新业务的部署能够规划、可控，并以企业标准进行IT 业务的实施，解决了数据业务分散时期的混乱无序问题。在这一阶段中，很多企业在数据集中后期也开始了容灾建设，特别是在雪灾、大地震之后，企业的容灾中心建设普遍受到重视，以金融为热点行业几乎开展了全行业的容灾建设热潮，并且金融行业的大部分容灾建设的级别都非常高，面向应用级容灾（数据零丢失为目标）。总的来说，第一阶段过程解决了企业IT 分散管理和容灾的问题。 第二个阶段：实施虚拟化的过程。在数据集中与容灾实现之后，随着企业的快速发展，数据中心IT 基础设施扩张很快，但是系统建设成本高、周期长，即使是标准化的业务模块建设（哪怕是系统的复制性建设），软硬件采购成本、调试运行成本与业务实现周期并没有显著下降。标准化并没有给系统带来灵活性， 集中的大规模IT 基础设施出现了大量系统利用率不足的问题，不同的系统运行在独占的硬件资源中，效率低下而数据中心的能耗、空间问题逐步突显出来。因此，以降低成本、提升 IT 运行灵活性、提升资源利用率为目的的虚拟化开始在数据中心进行部署。虚拟化屏蔽了不同物理设备的异构性，将基于标准化接口的物理资源虚拟化成逻辑上也完全 标准化和一致化的逻辑计算资源（虚拟机）和逻辑存储空间。虚拟化可以将多台物理服务器整合成单台，每台服务器上运行多种应用的虚拟机，实现物理服务器资源 利用率的提升，由于虚拟化环境可以实现计算与存储资源的逻辑化变更，特别是虚拟机的克隆，使得数据中心IT 实施的灵活性大幅提升，业务部署周期可用数月缩小到一天以内。虚拟化后，应用以VM 为单元部署运行，数据中心服务器数量可大为减少且计算能效提升，使得数据中心的能耗与空间问题得到控制。 总的来说，第二阶段过程提升了企业IT 架构的灵活性，数据中心资源利用率有效提高，运行成本降低。 第三个阶段：云计算阶段。对企业而言，数据中心的各种系统（包括软硬件与基础设施）是一大笔资源投入。新系统（特别是硬件）在建成后一般经历3-5 年即面临逐步老化与更换，而软件技术则不断面临升级的压力。另一方面，IT 的投入难以匹配业务的需求，即使虚拟化后，也难以解决不断增加的业务对资源的变化需求，在一定时期内扩展性总是有所限制。于是企业IT 产生新的期望蓝图：IT 资源能够弹性扩展、按需服务，将服务作为IT 的核心，提升业务敏捷性，进一步大幅降低成本。因此，面向服务的IT 需求开始演化到云计算架构上。云计算架构可以由企业自己构建，也可采用第三方云设施，但基本趋势是企业将逐步采取租用IT 资源的方式来实现业务需要，如同水力、电力资源一样，计算、存储、网络将成为企业IT 运行的一种被使用的资源，无需自己建设，可按需获得。从企业角度，云计算解决了IT 资源的动态需求和最终成本问题，使得IT 部门可以专注于服务的提供和业务运营。 这三个阶段中，大集中与容灾是面向数据中心物理组件和业务模块，虚拟化是面向数据中心的计算与存储资源，云计算最终面向IT 服务。这样一个演进过程，表现出IT 运营模式的逐步改变，而云计算则最终根本改变了传统IT 的服务结构，它剥离了IT 系统中与企业核心业务无关的因素(如IT 基础设施)，将IT 与核心业务完全融合，使企业IT 服务能力与自身业务的变化相适应。在技术变革不断发生的过程中，网络逐步从基本互联网功能转换到WEB 服务时代(典型的WEB2.0 时代)，IT 也由企业网络互通性转换到提供信息架构全面支撑企业核心业务。技术驱动力也为云计算提供了实现的客观条件，如图2 所示，在关键领域云计算技术已经就绪： 标准化：公共技术的长期发展，使得基础组件的标准化非常完善，硬件层面的互通已经没有阻碍（即使是非常封闭的大型机目前也开始支持对外直接出IP 接口），大规模运营的云计算能够极大降低单位建设成本。 虚拟化与自动化：虚拟化技术不断纵深发展，IT 资源已经可以通过自动化的架构提供全局动态调度能力，自动化提升了IT 架构的伸缩性和扩展性。 并行/分布式架构：大规模的计算与数据处理系统已经在分布式、并行处理的架构上得到广泛应用，计算密集、数据密集、大型数据文件系统成为云计算的实现基础，从而要求整个基础架构具有更高的弹性与扩展性。 带宽：大规模的数据交换需要超高带宽的支撑，网络平台在40G/100G 能力下可具备更扁平化的结构，使得云计算的信息交互以最短快速路径执行。 因此，从传统WEB 服务向云计算服务发展已经具备技术基础，而企业的IT 从信息架构演进到弹性的IT 服务也成为必然。]]></content>
      <categories>
        <category>云计算</category>
      </categories>
      <tags>
        <tag>流程开发</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[云服务器(Elastic Compute Service, ECS)]]></title>
    <url>%2F01.%E7%9F%A5%E8%AF%86-06.cloud_computing%2F%E4%BA%91%E8%AE%A1%E7%AE%97-%E6%A6%82%E5%BF%B5-ECS-%E4%BA%91%E6%9C%8D%E5%8A%A1%E5%99%A8%2F</url>
    <content type="text"><![CDATA[概念云服务器(Elastic Compute Service, ECS)是一种简单高效、安全可靠、处理能力可弹性伸缩的计算服务。其管理方式比物理服务器更简单高效。用户无需提前购买硬件，即可迅速创建或释放任意多台云服务器。 云服务器是云计算服务的重要组成部分，是面向各类互联网用户提供综合业务能力的服务平台。平台整合了传统意义上的互联网应用三大核心要素：计算、存储、网络，面向用户提供公用化的互联网基础设施服务。 实例类型示例规格可以分为三个大类，企业级实例，弹性裸金属，共享型实例。 BGP(Border Gateway Protocol)BGP全称是Border Gateway Protocol, 对应中文是边界网关协议。这个名字比较抽象，而维基中文的解释我觉得比较好（维基英文甚至都没有类似的解释）。BGP是互联网上一个核心的去中心化自治路由协议。从这个解释来看，首先这是一个用于互联网（Internet）上的路由协议。它的地位是核心的（目前是最重要的，互联网上唯一使用的路由协议），它的目的是去中心化，以达到各个网络自治。 参考资料 镜像和快照关系自定义镜像必须通过已创建成功的快照而进行创建，同时快照被用来创建自定义镜像后，在自定义镜像删除前，该快照不能被删除。自定义镜像适用范围：仅系统盘快照适用范围：数据盘、系统盘 不同 镜像可直接用来创建ECS实例，而快照不可以。 快照可以是ECS实例系统盘或数据盘的数据备份，而镜像一定包含ECS实例系统盘的数据。 快照只能用于当前ECS实例磁盘的数据恢复，而镜像可用于当前ECS实例及其他实例更换系统盘或创建新的ECS实例。 快照不可以跨地域使用。若您需要在其他地域恢复实例数据，可使用自定义镜像，详情请参见复制镜像。 应用场景不同。由于您只能使用自定义镜像备份数据，这里仅列举快照和自定义镜像的一些应用场景。 快照存放的位置与您自建的OSS Bucket相互独立，您无需为快照创建新的Bucket。 镜像导出时，可以指定导出到Bucket中。 镜像应用场景复制镜像可用于跨地域部署ECS实例， 例如跨地域或跨账号的ECS示例部署。因为镜像所在对象存储是不可以跨地域的，可以通过复制镜像（仅限自定义镜像，其他镜像需要先创建实例，导出为自定义镜像）到不同的地域/账户下，然后进行实例创建。 镜像类型 自定义镜像（自己上传的镜像也需要先制作成自定义镜像后才可以使用） 公共镜像 共享镜像 云市场镜像：收费镜像，弹性伸缩中不可用 部署集部署集是控制ECS实例分布的策略，使您能在创建ECS实例的时候就设计容灾能力和可用性。 部署策略 高可用策略采用高可用策略后，部署集内所有ECS实例会在指定地域内严格分散在不同的物理服务器上。适用于需要将几台ECS实例相互隔离的应用架构，大幅降低服务不可用的几率。 部署集组高可用策略该策略支持将部署集划分为最多7个分组，多台ECS实例可以根据实际需要分散部署在不同的分组当中。不同分组的ECS实例会在指定地域内严格分散在不同的物理服务器上；相同分组的ECS实例不保障严格分散部署。 网络低时延策略采用网络低时延策略后，部署集内所有ECS实例会集中部署到所在可用区内同一个网络拓扑范围内，降低网络互通的时延。此策略下可能会出现多台ECS实例调度到同一台物理服务器上的情况，低时延策略下，无法保证高可用。 使用限制 部署集之间不支持相互合并。 部署集内不能创建抢占式实例。 部署集不支持创建专有宿主机。 部署集内能创建的实例数量与部署策略有关 高可用策略：在部署集内创建ECS实例时，一个可用区内最多能创建20台ECS实例，一个阿里云地域下能创建的ECS实例数量为20 * 可用区数量。 部署集组高可用策略：在部署集内创建ECS实例时，一个可用区内最多能创建7个组，每个组最多创建20台ECS实例，一个阿里云地域下能创建的ECS实例数量为7 20 可用区数量。 网络低时延策略：在部署集内创建ECS实例时，所有实例必须在同可用区，同可用区最多创建20台ECS实例。 块存储块存储是阿里云为云服务器ECS提供的块设备产品，具有高性能和低时延的特点，支持随机读写，满足大部分通用业务场景下的数据存储需求。您可以像使用物理硬盘一样格式化并建立文件系统来使用块存储。 计费 包年包月 按量付费 存储容量单位包SCU 按量付费+节省计划预留实例券可以作为弹性资源的节省计划方案。 硬盘挂载 AttachDisk一个ECS最多能挂载16块云盘作数据盘用，且云盘只能挂载到同一地域下统一可用区内的示例上。一块全新的Windows数据盘挂载到ECS实例后，还不能直接存储数据，通常您需要完成磁盘联机、新建分区、格式化等初始化操作后，才能供系统读写数据。 调用该接口时要求： 云盘的状态必须为待挂载（Available）。 挂载数据盘时： 目标ECS实例必须处于运行中（Running）或者已停止（Stopped）状态。 如果是您单独购买的云盘，计费方式必须是按量付费。 从ECS实例上卸载的系统盘作为数据盘挂载时，不限制计费方式。 挂载系统盘时： 目标ECS实例必须是卸载系统盘时的源实例。 目标ECS实例必须处于已停止（Stopped）状态。 您必须设置实例登录凭证。 查询ECS实例信息时，如果返回数据中包含{“OperationLocks”: {“LockReason” : “security”}}，则禁止一切操作。 开启多重挂载特性的云盘，只能挂载到支持NVMe协议的实例上。 挂载节点系统盘：/dev/xvda数据盘：/dev/xcdb,/dev/xcdc……/dev/xcdz 数据恢复在使用云服务器ECS的过程中，有时难免会出现数据被误删除的情况，在阿里云上恢复被误删除的数据有多种方式，比如： 通过ECS管理控制台回滚已创建的快照 通过ECS管理控制台恢复该磁盘对应的数据块副本 使用开源工具Extundelete恢复误删的数据 ACP考试相关外延知识点API相关 ECS服务的API服务地址：ecs.aliyuncs.com 可以在传入参数时，指定返回的数据格式，默认是XML格式 配置 每个ECS至少加入一个安全组，最多加入5个安全组。 Linux开启NetWorkManager（网络配置和管理的服务）服务后，会和系统内部网络服务出现冲突导致网络异常。 一个地域通常由多个可用区组成。只有同一地域内的不同可用区之间内网互通，且使用低时延链路相连。不同地域之间的可用区完全隔离。 ECS配置升级后，必须通过管理控制台重启ECS实例才能生效，ECS内重启无效；变更配置后，实例的公网和内网IP不会改变。 带宽的变配后，即时生效。 安全组配置 ECS安全组授权仅支持基于 IP、端口和安全组，不支持基于MAC地址。 应用场景多媒体、大流量的APP或网站；简单的Web应用；企业应用开发环境；访问量波动大的APP或网站；企业官网 是云服务的典型应用场景。]]></content>
      <categories>
        <category>云计算</category>
      </categories>
      <tags>
        <tag>基础概念</tag>
        <tag>阿里云</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[云计算-基础概念-私有云公有云混合云]]></title>
    <url>%2F01.%E7%9F%A5%E8%AF%86-06.cloud_computing%2F%E4%BA%91%E8%AE%A1%E7%AE%97-%E6%A6%82%E5%BF%B5-%E7%A7%81%E6%9C%89%E4%BA%91%E5%85%AC%E6%9C%89%E4%BA%91%E6%B7%B7%E5%90%88%E4%BA%91%2F</url>
    <content type="text"><![CDATA[概念公有云概念公有云是部署云计算最常见的方式。公有云资源（如服务器和存储空间）由第三方云服务提供商拥有和运营，这些资源通过 Internet 提供。在公有云中，所有硬件、软件和其他支持性基础结构均为云提供商所拥有和管理。在公有云中，你与其他组织或云“租户”共享相同的硬件、存储和网络设备。你可以使用 Web 浏览器访问服务和管理帐户。公有云部署通常用于提供基于 Web 的电子邮件、网上办公应用、存储以及测试和开发环境。 优势 成本更低 — 无需购买硬件或软件，仅对使用的服务付费。 无需维护 — 维护由服务提供商提供。 近乎无限制的缩放性 — 提供按需资源，可满足业务需求。 高可靠性 — 具备众多服务器，确保免受故障影响。劣势 保密数据的安全性差、网络性能和匹配性问题。 但是同时，很多人担心公有云的安全性、私密性等问题。于是就有了私有云。 私有云概念私有云是云计算的另一种形式。它为一个企业或组织提供专用的云环境。私有云可以由企业或组织内部的IT团队在该组织的防火墙后面进行内部操作，因此组织可以更好地控制其计算资源。私有云主要由企业使用，因此它也被视为一种企业云。私有云可在物理上位于组织的现场数据中心，也可由第三方服务提供商托管。私有云中，服务和基础结构始终在私有网络上进行维护，硬件和软件专供组织使用。私有云可使组织更加方便地自定义资源，从而满足特定的 IT 需求。私有云的使用对象通常为政府机构、金融机构以及其他具备业务关键性运营且希望对环境拥有更大控制权的中型到大型组织。 优势 灵活性更高 — 组织可自定义云环境以满足特定业务需求。 安全性更高 — 资源不与其他组织共享，从而可实现更高控制性和安全性级别。 缩放性更高 — 私有云仍然具有公有云的缩放性和效率。劣势 比公共云的成本更昂贵。 但是私有云的费用相对较高， 并且维护成本也不低。于是有的厂商结合了公有云和私有云推出了混合云。 混合云概念混合云是一种云计算模型，它通过安全连接（如V**连接或租用线路）组合一个或多个公有云和私有云环境，从而允许在不同云环境之间共享数据和应用程序。当在私有云上运行的应用程序遇到使用高峰时，它们可以自动“突发”到公有云环境以获得额外的按需容量。这被称为“云爆发”。由于额外的需求将在公有云上，因此无需担心提前配置硬件以满足高峰需求。连接公有云和私有云有两种方法：V**和点对点专用连接。混合云通常被认为是“两全其美”，它将本地基础架构或私有云与公有云相结合，组织可利用这两者的优势。在混合云中，数据和应用程序可在私有云和公有云之间移动，从而可提供更大灵活性和更多部署选项。 优势 控制性 — 组织可针对敏感资产维持私有基础结构。 灵活性 — 需要时可利用公有云中的其他资源。 成本效益 — 具备扩展至公有云的能力，因此可仅在需要时支付额外的计算能力。 容易轻松 — 无需费时费力即可转换至云，因为可根据时间按工作负荷逐步迁移。 混合云整合了公有云和公有云的优势。它提供高可扩展性，几乎无限的存储空间，灵活的支付模式，并且与公有云一样具有成本效益。混合云也非常安全；它为您提供了更多的灵活性和对云资源的控制。 社群云社群云，也称社区云，是由几个组织共享的云端基础设施，它们支持特定的社群，有共同的关切事项，例如使命任务、安全需求、策略与法规遵循考量等。管理者可能是组织本身，也能是第三方；管理位置可能在组织内部，也可能在组织外部。 社群云在模式和机制、可靠性、安全、组织管理等方面面临挑战，有待进一步解决。社群云与私有云、公有云相比模式上复杂一些，由多个组织共同构建和共享云设施。 这四种云的主要区别就是使用的人群和使用的方式不一样： 公有云由公众开放使用 私有云由单一组织独占使用 混合云则是前述的两种以上模式的混合 社群云是由一个特定社区独占使用，该社区由具有共同关切 (如使命、安全要求、政策等) 的多个组织组成 本地化部署概念本地化部署(On-Premises)，是指运行在个人或组织所在的现实位置计算机内的高级数据处理硬件，目前大多数都是在自己的经营场地上部署并操作运行的一套系统。 优势一次购买，永久使用，且数据完全掌握在企业手里，数据信息更安全。由于是通过内联网，集成相对容易。数据在系统之间的传输会更快。 劣势企业承担硬件、软件费用和实施费用，以及后期维护费用，投入成本高，部署周期长，适合大中型企业使用。 不同方式的优缺点经济性公有云 &lt; 私有云 &lt; 本地化部署 公有云：购买服务的费用 私有云：机房、设备、运行维护费用 本地化部署：搭建、机房、设备、运行维护费用 从上面的比较我们可以明显看出，本地化部署成本是远远高于公有云和私有云服务的。 安全性公有云 &lt; 私有云 &lt; 本地化部署 公有云：通过运营商网络访问，可以通过算法对数据加密 私有云：数据由内部网络获取，第三方很难获取 本地化部署：数据储存在本地，第三方无法获取 管理性公有云：无需运营，但是灵活性有些受限 私有云：自定义程度高 本地化部署：可以本地开发 本地化部署、私有云、公有云如何选择网上比较多讨论是私有化部署和本地部署，但其实他们有没有绝对的好与坏之分，像有些企业为什么选择私有化部署？其实这不是一个技术决策，而是业务决策。具体在选择方面，可以从这几点考虑！（1）成本。私有化部署可以通过企业规模来选择，定制个性使用方案，成本较低。本地化部署则企业承担硬件、软件费用和实施费用，以及后期维护费用，成本较高。（2）功能使用不同。企业私有化部署可以根据自己的需求和情况，定制使用功能，拓展性强，在原有功能上二次开发进行自主升级，让产品更好的服务于企业。本地化部署功能定制需要专业人员进行开发，开发也需要二次付费。不过大部分企业都主要会考虑成本和使用方面，两者比较多会选择私有化部署。但企业往往也会有一些顾虑，比如担心自己的网络环境和基础设施不支持私有化部署，或者担心私有化部署无法满足自己企业的特殊需求。其实，这些担心都是多余的，私有化部署的特点就在于私有化、个性化，能够根据不同企业的需要定制不同的功能。 参考资料microsoftalibabacloud]]></content>
      <categories>
        <category>云计算</category>
      </categories>
      <tags>
        <tag>基础概念</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[云计算安全相关概念]]></title>
    <url>%2F01.%E7%9F%A5%E8%AF%86-06.cloud_computing%2F%E4%BA%91%E8%AE%A1%E7%AE%97-%E6%A6%82%E5%BF%B5-%E5%AE%89%E5%85%A8%E7%9B%B8%E5%85%B3%2F</url>
    <content type="text"><![CDATA[安全防护相关概念DDos防护DDos攻击的概念分布式拒绝服务（Distributed Denial of Service，简称DDoS）将多台计算机联合起来作为攻击平台，通过远程连接，利用恶意程序对一个或多个目标发起DDoS攻击，消耗目标服务器性能或网络带宽，从而造成服务器无法正常地提供服务。 攻击原理攻击者使用一个非法账号将DDoS主控程序安装在一台计算机上，并在网络上的多台计算机上安装代理程序。在所设定的时间内，主控程序与大量代理程序进行通讯，代理程序收到指令时对目标发动攻击，主控程序甚至能在几秒钟内激活成百上千次代理程序的运行。 常见的DDoS攻击类型 畸形报文 畸形报文攻击指通过向目标系统发送有缺陷的IP报文，使得目标系统在处理这样的报文时出现崩溃，从而达到拒绝服务的攻击目的。 传输层DDoS攻击 以Syn Flood攻击为例，它利用了TCP协议的三次握手机制，当服务端接收到一个Syn请求时，服务端必须使用一个监听队列将该连接保存一定时间。因此，通过向服务端不停发送Syn请求，但不响应Syn+Ack报文，从而消耗服务端的资源。当监听队列被占满时，服务端将无法响应正常用户的请求，达到拒绝服务攻击的目的。 DNS DDoS攻击 以DNS Query Flood攻击为例，其本质上执行的是真实的Query请求，属于正常业务行为。但如果多台傀儡机同时发起海量的域名查询请求，服务端无法响应正常的Query请求，从而导致拒绝服务。 连接型DDoS攻击 以Slowloris攻击为例，其攻击目标是Web服务器的并发上限。当Web服务器的连接并发数达到上限后，Web服务即无法接收新的请求。Web服务接收到新的HTTP请求时，建立新的连接来处理请求，并在处理完成后关闭这个连接。如果该连接一直处于连接状态，收到新的HTTP请求时则需要建立新的连接进行处理。而当所有连接都处于连接状态时，Web将无法处理任何新的请求。 Slowloris攻击利用HTTP协议的特性来达到攻击目的。HTTP请求以\r\n\r\n标识Headers的结束，如果Web服务端只收到\r\n，则认为HTTP Headers部分没有结束，将保留该连接并等待后续的请求内容。 Web应用层DDoS攻击 通常应用层攻击完全模拟用户请求，类似于各种搜索引擎和爬虫一样，这些攻击行为和正常的业务并没有严格的边界，难以辨别。 Web服务中一些资源消耗较大的事务和页面。例如，Web应用中的分页和分表，如果控制页面的参数过大，频繁的翻页将会占用较多的Web服务资源。尤其在高并发频繁调用的情况下，类似这样的事务就成了早期CC攻击的目标。 由于现在的攻击大都是混合型的，因此模拟用户行为的频繁操作都可以被认为是CC攻击。例如，各种刷票软件对网站的访问，从某种程度上来说就是CC攻击。 CC攻击瞄准的是Web应用的后端业务，除了导致拒绝服务外，还会直接影响Web应用的功能和性能，包括Web响应时间、数据库服务、磁盘读写等。 跨站脚本 (XSS:Cross Site Scripting)### DDos基础防护每日上限5G的基础DDos共计防护 事态感知态势感知具备异常登录检测、网站后门查杀、网站后门查杀、进程异常行为、敏感文件算改、异常网络连接、Linuⅸ软件漏洞、Windows系统漏洞、Web-CMS漏洞、应急漏洞、Web漏洞扫描、主机基线、云产品基线、资产指纹、AK和账号密码泄露、数据大屏、日志检索、全量日志分析。 维护Web应用防火墙可以提供一下攻击类型的防护： 跨站脚本 SQL注入 CSRF（Cross-site Request Forgery) 本地文件包含 WebShell 安骑士云安全中心Agent 一键自动安装Agent（仅适用阿里云ECS服务器） 服务端检测到和Agent客户端的通信异常，包括但不限于网络异常、客户端进程被异常结束、客户端被卸载等，会将客户端的状态更改为离线。 如果服务端在10个小时内没有收到Agent客户端登录、采集到的数据等信息，会将客户端状态更改为离线。 Agent卸载后会有一段保护期(24小时)，在保护期内重新安装的Agent会被自动卸载。 云安全中心Agent可以在Linux、Windows系统中使用。6.安全管家 内容安全 内容审核（机审） 人工审核 图片OCR识别 人脸识别]]></content>
      <categories>
        <category>云计算</category>
      </categories>
      <tags>
        <tag>基础概念</tag>
        <tag>阿里云</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[存储类型]]></title>
    <url>%2F01.%E7%9F%A5%E8%AF%86-06.cloud_computing%2F%E4%BA%91%E8%AE%A1%E7%AE%97-%E6%A6%82%E5%BF%B5-%E5%AD%98%E5%82%A8%E7%B1%BB%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[存储分类阿里云存储服务| 存储类型 | 简介 || ————– | ————————————————————————————————————————————————————————————————— || 块存储 | 块存储是阿里云为云服务器ECS提供的块设备，高性能、低时延、随机读写。您可以像使用物理硬盘一样格式化并建立文件系统来使用块存储。 || 文件存储NAS | 阿里云文件存储NAS（Network Attached Storage）是一款面向阿里云ECS实例、E-HPC和容器服务等计算节点的高可靠、高性能的分布式文件系统，可共享访问、弹性扩展。NAS基于POSIX文件接口，天然适配原生操作系统。 || 对象存储OSS | 对象存储OSS（Object Storage Service）是一款海量、安全、低成本、高可靠的云存储服务，其容量和处理能力弹性扩展，提供多种存储类型供选择，覆盖从热到冷的各种数据存储场景，帮助您全面优化存储成本。 || 文件存储CPFS | 文件存储CPFS（Cloud Paralleled File System）是一款并行文件系统，其数据存储在集群中的多个数据节点，多个客户端可以同时访问，满足大型高性能计算机集群的高IOPS、高吞吐、低时延的数据存储需求。 || 文件存储HDFS版 | 文件存储HDFS版（Apsara File Storage for HDFS）是一款面向阿里云ECS实例及容器服务等计算资源的文件存储服务，满足以Hadoop为代表的分布式计算业务类型对分布式存储性能、容量和可靠性的多方面要求。 || 表格存储 | 表格存储（Tablestore）是阿里云自研的结构化数据存储，提供海量结构化数据存储以及快速的查询和分析服务，具备PB级存储、千万TPS以及毫秒级延迟的服务能力。 || 云存储网关 | 云存储网关（Cloud Storage Gateway）是一款可以部署在用户IDC和阿里云上的网关产品，以阿里云对象存储OSS为后端存储，为云上和云下应用提供业界标准的文件服务（NFS和SMB）和块存储服务（iSCSI）。 | 对象存储OSSOSS使用基于纠删码、多副本的数据冗余存储机制，将每个对象的不同冗余存储在同一个区域内的多个设施的多个设备上。 基本概念单一账号存储空间不能超过30个，存储空间一旦成功，其名称、地域、存储类型不能修改，单个存储空间的容量无限制。存储空间Bucket权限包括：私有、公共读、公共读写 。 对象（Object） ：在IMG中，用户操作图片的基本数据单元是Object，也称为对象或文件。单个Object（每张图片）允许的大小最大不超过20 MB。频道（Channel） ：Channel是IMG上的命名空间，也是计费、权限控制、日志记录等高级功能的管理实体。IMG名称在整个图片处理服务中具有全局唯一性，且不能修改。每个用户最多可创建10个Channel，但每个Channel中存放的object的数量没有限制。样式（Style） : 图片处理服务支持您将图片的处理操作和参数保存为一个别名，即样式。您可以在一个样式中包含多个图片处理参数，快速实现复杂的图片处理操作。 Bucket命名规则同一阿里云账号在同一地域内创建的Bucket总数不能超过100个。Bucket创建后，其名称无法修改。Bucket命名规则如下： Bucket名称在OSS范围内必须全局唯一。 只能包括小写字母、数字和短划线（-）。 必须以小写字母或者数字开头和结尾。 长度为3~63个字符。 存储类型 标准存储类型高持久、高可用、高性能的对象存储服务，支持频繁的数据访问。是各种社交、分享类的图片、音视频应用、大型网站、大数据分析的合适选择。 低频访问存储类型适合长期保存不经常访问的数据（平均每月访问频率1到2次）。存储单价低于标准类型，适合各类移动应用、智能设备、企业数据的长期备份，支持实时数据访问。（最少30天） 归档存储类型适合需要长期保存（建议半年以上）的归档数据，在存储周期内极少被访问，数据进入到可读取状态需要1分钟的解冻时间。适合需要长期保存的档案数据、医疗影像、科学资料、影视素材。（最少60天，使用前需要进行解冻；读取前需要一分钟的解冻时间，读取收费） 冷归档存储类型适合需要超长时间存放的极冷数据。例如因合规要求需要长期留存的数据、大数据及人工智能领域长期积累的原始数据、影视行业长期留存的媒体资源、在线教育行业的归档视频等。 日志记录服务器端加密当您在设置了服务器端加密的存储空间（Bucket）中上传文件（Object）时，OSS对收到的文件进行加密，再将得到的加密文件持久化保存。当您通过GetObject请求下载文件时，OSS自动将加密文件解密后返回给用户，并在响应头中返回x-oss-server-side-encryption，用于声明该文件进行了服务器端加密。 使用KMS托管密钥进行加解密（SSE-KMS）使用KMS托管的默认CMK（Customer Master Key）或指定CMK进行加解密操作。数据无需通过网络发送到KMS服务端进行加解密。需要使用自管理、可指定的密钥 使用OSS完全托管密钥进行加解密（SSE-OSS）使用OSS完全托管的密钥加密每个Object。为了提升安全性，OSS还会使用定期轮转的主密钥对加密密钥本身进行加密。 延展概念存储单元对象是对象存储的基本单元，也被称为OSS文件。对象由元信息（Object Meta）、用户数据（Data）、和文件名（key）组成，对象由存储空间内部唯一的key标识，元信息是一组键值对，记录了对象的一些属性（如修改时间、大小等信息，也可以进行自定义）。 每个文件有一个Object Key（文件名），作为该文件在该存储空间中的唯一标识。OSS没有文件夹的概念，所有元素都是以文件来存储，但是您可以通过创建以正斜线（/）结尾的文件名（如folder1/folder2/file）来模拟文件夹。 图片处理功能OSS除了存储外，还提供部分图片处理功能包括： 图片缩放、添加水印 ； 图片格式转换(例如：由jpg转换成png) ； 图片360度范围内旋转。 存储挂载OSS存储可以通过使用ossfs挂载到本地系统 传输加速 传输加速开启及关闭操作会在30分钟内全网生效。 开启传输加速后必须使用OSS的传输加速域名才会提升访问速度。 开启传输加速功能后，OSS提供的其他Endpoint仍可正常使用。在不需要传输加速的场景中，您可以使用默认Endpoint以减少传输加速的费用。 传输加速Endpoint仅支持HTTP/HTTPS协议的API接入，不支持RTMP协议等非HTTP/HTTPS协议的API接入。 开启传输加速后，Bucket会在保留默认Endpoint的基础上，新增以下两种传输加速Endpoint。 全球加速Endpoint：地址为oss-accelerate.aliyuncs.com。传输加速接入点分布在全球各地，全球各地的Bucket均可以使用该域名进行传输加速。 非中国内地加速Endpoint：地址为oss-accelerate-overseas.aliyuncs.com。传输加速接入点分布在除中国内地以外的各地域，仅在中国香港及海外各地域Bucket绑定未备案的域名做CNAME指向时使用。 文件存储CPFS文件存储CPFS（Cloud Parallel File Storage）是阿里云推出的全托管、可扩展并行文件系统，满足高性能计算场景的需求。通过标准的POSIX文件协议挂载即可以使用。阿里云CPFS特有的数据流动功能可以实现将对象存储OSS中的数据合并入CPFS，进行统一命名空间的元数据管理。 文件存储HDFS版文件存储 HDFS 版（Apsara File Storage for HDFS）是面向阿里云ECS实例及容器服务等计算资源的文件存储服务。适用于互联网行业、金融行业等有大数据计算与存储分析需求的行业客户，进行海量数据存储和离线计算的业务场景，充分满足以Hadoop为代表的分布式计算业务类型对分布式存储性能、容量和可靠性的多方面要求。 NAS 存储类型NFS v3.0 ：在NFS v2版本进行了一些bug修复。 NFS v4.0 ：支持文件锁 NFS v4.1 ：支持并行存储 ACP考试相关外延知识点 除了通过PUT Object接口上传文件到OSS以外，OSS还提供了另外一种上传模式——Multipart Upload。以下情况（不仅限于此）建议使用 Multipart Upload 需要支持断点上传。 上传超过100MB大小的文件。 网络条件较差，和OSS的服务器之间的链接经常断开。 上传文件之前，无法确定上传文件的大小。 OSS收费包含三个部分，存储空间费用、流量费用和OSS API的费用， OSS API请求付费，目前仅支持按量付费。 存储费用支持包年包月 和 按量付费。 仅公网下行收费，公网上传/内网上下行不好收费。 OSS免费支持 DDos攻击、自动流量清洗和黑洞功能。 OSS 提供多种鉴权和授权机制及白名单、防盗链、主子账号功能。]]></content>
      <categories>
        <category>云计算</category>
      </categories>
      <tags>
        <tag>基础概念</tag>
        <tag>阿里云</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[云服务器(Elastic Compute Service, ECS)]]></title>
    <url>%2F01.%E7%9F%A5%E8%AF%86-06.cloud_computing%2F%E4%BA%91%E8%AE%A1%E7%AE%97-%E5%9F%BA%E5%9B%A0%E5%88%86%E6%9E%90%E5%B9%B3%E5%8F%B0%2F</url>
    <content type="text"><![CDATA[基因分析平台部署过程可能的坑docker镜像基因分析平台默认使用 wdl版本基因分析平台默认支持 v1.0 的wdl语法， runtime的个性化配置基因分析平台上有一些独立的资源配置例如 ：disks 默认是40G，instanceType 可以直接指定节点类型（而不是内存和cpu数）software 可以调用一些软件模块 数据访问类型 oss路径中，除了开始的 oss:// 外，后续目录在进行拼接时，需要规避 // 的出现，基因分析平台的数据访问都是基于oss挂载到容器中进行访问，oss中不支持后续路径中 //的存在。 所有oss中的文件，以文件名结尾，但是所有的目录必须以 / 结尾。 以 / 结尾在分析中会将整个目录挂载到镜像中进行分析，]]></content>
      <categories>
        <category>云计算</category>
        <category>pipeline</category>
        <category>framework</category>
      </categories>
      <tags>
        <tag>流程开发</tag>
        <tag>阿里云</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[边缘计算 - 概念]]></title>
    <url>%2F01.%E7%9F%A5%E8%AF%86-06.cloud_computing%2F%E8%BE%B9%E7%BC%98%E8%AE%A1%E7%AE%97-%E6%A6%82%E5%BF%B5%2F</url>
    <content type="text"><![CDATA[什么是边缘计算？边缘计算是在靠近物或数据源头的网络边缘侧，通过融合网络、计算、存储、应用核心能力的分布式开放平台，就近提供边缘智能服务。简单点讲，边缘计算是将从终端采集到的数据，直接在靠近数据产生的本地设备或网络中进行分析，无需再将数据传输至云端数据处理中心。 为什么需要边缘计算？物联网技术的快速发展，使得越来越多具备独立功能的普通物体实现互联互通，实现万物互联。得益于物联网的特征，各行各业均在利用物联网技术快速实现数字化转型，越来越多的行业终端设备通过网络联接起来。 然而，物联网作为庞大而复杂的系统，不同行业应用场景各异，据第三方分析机构统计，到2025年将有超过千亿的终端设备联网，终端数据量将达300ZB，如此大规模的数据量，按照传统数据处理方式，获取的所有数据均需上送云计算平台分析，云计算平台将面临着网络时延高、海量设备接入、海量数据处理难、带宽不够和功耗过高等高难度挑战。 为了解决传统数据处理方式下时延高、数据实时分析能力匮乏等弊端，边缘计算技术应运而生。边缘计算技术是在靠近物或数据源头的网络边缘侧，通过融合网络、计算、存储、应用核心能力的分布式开放平台，就近提供边缘智能服务。简单点讲，边缘计算是将从终端采集到的数据，直接在靠近数据产生的本地设备或网络中激进型分析，无需再将数据传输至云端数据处理中心。 边缘计算 VS 云计算边缘计算的概念是相对于云计算而言的，云计算的处理方式是将所有数据上传至计算资源集中的云端数据中心或服务器处理，任何需要访问该信息的请求都必须上送云端处理。 因此，云计算面对物联网数据量爆发的时代，弊端逐渐凸显： 云计算无法满足爆发式的海量数据处理诉求。 随着互联网与各个行业的融合，特别是在物联网技术普及后，计算需求出现爆发式增长，传统云计算架构将不能满足如此庞大的计算需求。 云计算不能满足数据实时处理的诉求。 传统云计算模式下，物联网数据被终端采集后要先传输至云计算中心，再通过集群计算后返回结果，这必然出现较长的响应时间，但一些新兴的应用场景如无人驾驶、智慧矿山等，对响应时间有极高要求，依赖云计算并不现实。 边缘计算的出现，可在一定程度上解决云计算遇到的这些问题。如下图所示，物联终端设备产生的数据不需要再传输至遥远的云数据中心处理，而是就近即在网络边缘侧完成数据分析和处理，相较于云计算更加高效和安全。 项目 边缘计算 云计算 计算方式 分布式计算，聚焦实时、短周期数据的分析 集中式计算，依赖云端数据中心 处理位置 靠近产生数据的终端设备或物联网关 云端数据中心 延时性 低延时 高延时 数据存储 只向远端传输有用的处理信息，无冗余信息 采集到的所有信息 部署成本 低 高 隐私安全 隐私性和安全性较高 隐私性和安全性相对低，需要高度关注 边缘计算是如何工作的？边缘计算架构如下图所示，尽可能靠近终端节点处处理数据，使数据、应用程序和计算能力远离集中式云计算中心。 边缘计算架构 终端节点：由各种物联网设备（如传感器、RFID标签、摄像头、智能手机等）组成，主要完成收集原始数据并上报的功能。在终端层中，只需提供各种物联网设备的感知能力，而不需要计算能力。 边缘计算节点：边缘计算节点通过合理部署和调配网络边缘侧节点的计算和存储能力，实现基础服务响应。 网络节点：负责将边缘计算节点处理后的有用数据上传至云计算节点进行分析处理。 云计算节点：边缘计算层的上报数据将在云计算节点进行永久性存储，同时边缘计算节点无法处理的分析任务和综合全局信息的处理任务仍旧需要在云计算节点完成。除此之外，云计算节点还可以根据网络资源分布动态调整边缘计算层的部署策略和算法。 边缘计算的典型应用正是基于这种更实时处理数据的能力、特性，更快的响应时间，边缘计算非常适合被应用于物联网领域，通过具有边缘计算能力的物联网关就近（网络边缘节点）提供设备管理控制等服务，解决物联网通信“最后一公里”的问题，最终实现物联网设备的智慧连接和高效管理。 边缘计算网联网架构如下图所示，它聚焦于工业物联网领域，不仅支持丰富的工业协议和物联接口，可以广泛适应不同行业设备的联接场景，而且通过开放的边缘计算能力和云管理架构，快速满足不同行业边缘智能数据处理诉求： 联接：实现海量终端设备接入物联网络，主要通过边缘计算网关支持的各种物联接口（IP化PLC/RF/RS485/RS232等）连接各种传感器和终端，实现终端设备接入。 云管理：通过物联网平台，应用云计算技术，实现边缘物联设备（如网络、设备、容器及应用）的统一云化管理，同时北向支持与其他行业应用系统灵活对接。 行业应用：物联网平台提供标准的开放接口与不同合作伙伴的行业应用系统开放对接，构建广泛的行业适应性，可开发更多契合行业场景，深度定制化物联网行业应用。]]></content>
      <categories>
        <category>边缘计算</category>
      </categories>
      <tags>
        <tag>基础概念</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[内容分发（Content Delivery Network，CDN）]]></title>
    <url>%2F01.%E7%9F%A5%E8%AF%86-06.cloud_computing%2F%E4%BA%91%E8%AE%A1%E7%AE%97-%E6%A6%82%E5%BF%B5-CDN-%E5%86%85%E5%AE%B9%E5%88%86%E5%8F%91%2F</url>
    <content type="text"><![CDATA[概念CDN=更智能的镜像+缓存+流量导流。 内容分发网络（英语：Content Delivery Network或Content Distribution Network，缩写：CDN）是指一种透过互联网互相连接的电脑网络系统，利用最靠近每位用户的服务器，更快、更可靠地将音乐、图片、视频、应用程序及其他文件发送给用户，来提供高性能、可扩展性及低成本的网络内容传递给用户。 (From wiki) 阿里云内容分发网络CDN（Content Delivery Network）是建立并覆盖在承载网之上，由遍布全球的边缘节点服务器群组成的分布式网络。阿里云CDN能分担源站压力，避免网络拥塞，确保在不同区域、不同场景下加速网站内容的分发，提高资源访问速度。(From aliyun) CDN分发，仅支持静态数据，不支持动态数据。 原理 当终端用户向www.aliyundoc.com下的指定资源发起请求时，首先向Local DNS（本地DNS）发起请求域名www.aliyundoc.com对应的IP。 Local DNS检查缓存中是否有www.aliyundoc.com的IP地址记录。如果有，则直接返回给终端用户；如果没有，则向网站授权DNS请求域名www.aliyundoc.com的解析记录。 当网站授权DNS解析www.aliyundoc.com后，返回域名的CNAME www.aliyundoc.com.example.com。 Local DNS向阿里云CDN的DNS调度系统请求域名www.aliyundoc.com.example.com的解析记录，阿里云CDN的DNS调度系统将为其分配最佳节点IP地址。 Local DNS获取阿里云CDN的DNS调度系统返回的最佳节点IP地址。 Local DNS将最佳节点IP地址返回给用户，用户获取到最佳节点IP地址。 用户向最佳节点IP地址发起对该资源的访问请求。 如果该最佳节点已缓存该资源，则会将请求的资源直接返回给用户（步骤8），此时请求结束。 如果该最佳节点未缓存该资源或者缓存的资源已经失效，则节点将会向源站发起对该资源的请求。获取源站资源后结合用户自定义配置的缓存策略，将资源缓存到CDN节点并返回给用户（步骤8），此时请求结束。 CND由 缓存系统、调度系统、链路质量系统和支撑系统 这四大系统组成。DNS服务要了解cdn就先要了解一下dns。当我们在浏览器中输入一个域名时，首先需要将域名转换为ip地址，再将ip地址转换为mac地址，这样才能在网络上找到该服务器。我们先不看ip转换mac地址的过程，先来看看是怎么将一个域名转换为ip的。 当我们向dns服务器发起解析域名的请求时，dns服务器首先会查询自己的缓存中有没有该域名，如果缓存中存在该域名，则可以直接返回ip地址。如果缓存中没有，服务器则会以递归的方式层层访问。例如，我们要访问www.baidu.com，首先我们会先向全球13个根服务器发起请求，询问com域名的地址，然后再向负责com域名的名称服务器发送请求，找到baidu.com，这样层层递归，最终找到我们需要的ip地址。 刷新和预热资源刷新把CDN所有节点上对应的缓存资源标记为失效，当用户再次请求时，CDN会直接回源站获取对应的资源并返回给用户，同时将资源重新缓存到CDN节点。刷新功能会降低缓存命中率。CDN刷新缓存的 方式有三种： URL刷新： 用户可以在刷新任务中提交含有正则表达式的URL，阿里云CDN会对匹配该正则表达式的所有URL进行批量刷新，这样可以更有针对性地刷新URL。 目录刷新 URL预热预热源站主动将对应的资源缓存到CDN节点，当您首次请求资源时，即可直接从CDN节点获取到最新的资源，无需再回源站获取。预热功能会提高缓存命中率。 缓存方式cdn中缓存了服务器上的部分资源。那么服务器怎么去更新cdn节点的缓存呢？这里有两种方式， 一种是服务器主动去更新缓存，cdn节点被动接受。 另一种方式是当用户请求的资源不存在时，cdn服务器向上游服务器发起请求，更新缓存，然后将数据返回给用户，这种方式是cdn服务器主动，源站服务器被动。 显然第一种方式存在很多问题，例如很容易产生404等，所以一般采用第二种缓存方式。 服务端控制台OCSP Stapling可实现由CDN预先缓存在线证书验证结果并下发给客户端，无需浏览器直接向CA站点查询证书状态，从而减少用户验证时间。 一些优势 体验方面，加速了网站的访问——用户与内容之间的物理距离缩短，用户的等待时间也得以缩短；分发至不同线路的缓存服务器，也让跨运营商之间的访问得以加速。 安全方面，内容进行分发后，源服务器的IP被隐藏，受到攻击的概率会大幅下降。而且，当某个服务器故障时，系统会调用临近的健康服务器，进行服务，避免对用户造成影响。 对运营商，能以存储换带宽——通过服务“下沉”，减轻上层骨干网络的流量压力，避免硬件扩容，降低网络建设成本。 内容分发网络节点会在多个地点，多个不同的网络上摆放。这些节点之间会动态的互相传输内容，对用户的下载行为优化，并借此减少内容供应者所需要的带宽成本，改善用户的下载速度，提高系统的稳定性。 内容分发网络所需要的节点数量随着需求而不同，依照所需要服务的对象大小，有可能有数万台服务器。 CDN分类 DCDN（Dynamic Route for Content Delivery Network）:全速加速（DCDN）是在CDN加速的基础上升级的产品，智能区分访问的是动态还是静态，静态直接调用阿里云CDN加速，动态会通过协议优化等快速回源拉取内容数据。相比于CDN加速只能静态加速资源，DCDN可同时进行静态和动态的加速。 SCDN（Secure Content Delivery Network）:SCDN是有安全防护的CDN服务，体现在能够提前预判外界攻击行为，将恶意请求切换到高防IP，无需人工操作，目的是通过数据清洗将恶意流量移除，而让真实用户正常打开页面，这样就可以做到智能加速和安全运行。 PCDN（P2P CDN）:直播视频等业务适用，可有效降低成本，其它场景忽略。 ACP考试相关外延知识点 CDN实际产生的流量比应用层统计到的流量高7%-15% ， 是由于会出现TCP重传，和TCP/IP包头的消耗。 Range回源，指DCDN节点在回源的HTTP请求里面携带了Range信息，源站在收到DCDN节点的回源请求时，根据HTTP请求头中的Range信息返回指定范围的内容数据给DCDN节点。Range回源可有效提高文件分发效率，可以提高缓存命中率，减少回源流量消耗和源站压力，并且提升资源响应速度。 背景补充上世纪80年代，互联网技术刚刚走入民用领域。人们主要通过拨号来访问网络，带宽很低，用户也很少，所以，没有对骨干网以及服务器带来压力。随着互联网的爆炸式发展，用户越来越多，加上宽带接入网的出现，内容源服务器和骨干网络的压力越来越大，无法及时响应用户的访问需求。 1995年，麻省理工学院教授、互联网的发明者之一，Tim Berners-Lee博士发现，网络拥塞越来越严重，将会成为互联网发展的最大障碍。于是，他提出一个学术难题，希望有人能发明一种全新的、从根本上解决问题的方法，来实现互联网内容的无拥塞分发。 当时Tim Berners-Lee博士的隔壁，是Tom Leighton教授的办公室。他是一位麻省理工学院应用数学教授。Tom Leighton 他被Berners-Lee的挑战激起了兴趣，于是他请研究生Danny C. Lewin和其他几位顶级研究人员一起破解这个技术难题。 最终，他们开发了利用数学运算法则来处理内容的动态路由算法技术，有效地解决了这个难题。这个技术，就是CDN。他们还为此专门成立了公司，发挥其商业价值。这个公司，就是后来鼎鼎大名的CDN服务鼻祖——Akamai公司。 参考资料阿里云官方文档内容分发网络到底什么是CDN？]]></content>
      <categories>
        <category>云计算</category>
      </categories>
      <tags>
        <tag>基础概念</tag>
        <tag>阿里云</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[负载均衡(SLB:Server Load Balance) 介绍]]></title>
    <url>%2F01.%E7%9F%A5%E8%AF%86-06.cloud_computing%2F%E4%BA%91%E8%AE%A1%E7%AE%97-%E6%A6%82%E5%BF%B5-SLB-%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1%2F</url>
    <content type="text"><![CDATA[SLB（Server Load Balance）服务通过设置虚拟服务地址（IP），将位于同一地域（Region）的多台云服务器（Elastic Compute Service，简称ECS）资源虚拟成一个高性能、高可用的应用服务池；再根据应用指定的方式，将来自客户端的网络请求分发到云服务器池中。负载均衡（Server Load Balancer，下文简称 SLB）的引入，可以降低单台云服务器 ECS（下文简称 ECS）出现异常时对业务的冲击，提升业务的可用性。同时，结合弹性伸缩服务，通过动态调整后端服务器，可以快速对业务进行弹性调整（扩容或缩容），以快速应对业务的发展。 SLB 基本概念参考资料SLB由 LoadBalancer、Listener、Backend Server 三个部分组成。LoadBalancer：负载均衡实例。Listener：用户定制的监听器，定义了负载均衡策略和转发规则。BackendServer：后端的一组ECS（云服务器）。 负载均衡实例性能保障型实例的三个关键指标： 最大连接数-Max Connection最大连接数定义了一个负载均衡实例能够承载的最大连接数量。当实例上的连接超过规格定义的最大连接数时，新建连接请求将被丢弃。 每秒新建连接数-Connection Per Second（CPS）每秒新建连接数定义了新建连接的速率。当新建连接的速率超过规格定义的每秒新建连接数时，新建连接请求将被丢弃。 每秒查询数-Query Per Second（QPS）每秒请求数是七层监听特有的概念，指的是每秒可以完成的HTTP或HTTPS的查询（请求）的数量。当请求速率超过规格所定义的每秒查询数时，新建连接请求将被丢弃。 SLB架构整个SLB架构分成三种：四层负载均衡，七层负载均衡 和 控制系统（用于 配置和监控 负载均衡系统），如下图所示； 四层负载均衡（传输层四层负载均衡，采用开源软件LVS（linux virtual server），并根据云计算需求对其进行了定制化。适合无状态的 TCP建议的应用场景注重可靠性，对数据准确性要求高，速度可以相对较慢的场景；示例：文件传输、发送或接收邮件、远程登录、无特殊要求的 Web 应用 特性 面向连接的协议； 在正式收发数据前，必须和对方建立可靠的连接； 基于源地址会话保持，在网络层可直接看到来源地址； 监听支持 TCP 和 HTTP 两种方式进行健康检查； 转发路径短，所以性能比 7 层更好，数据传输速度更快 UDP建议的应用场景 关注实时性而相对不注重可靠性的场景； 示例：视频聊天、金融实时行情推送特性 面向非连接的协议； 在数据发送前不与对方进行三次握手，直接进行数据包发送，不提供差错恢复和数据重传； 可靠性相对低；数据传输快 通过UDP报文探测来获取状态信息。 实现 LVS（Linux Virtual Server）：即Linux虚拟服务器 基于NAT的LVS模式负载均衡 基于TUN的LVS负载均衡 基于DR的LVS负载均衡 LVS的组成 负载调度器(load balancer/ Director)，它是整个集群对外面的前端机，负责将客户的请求发送到一组服务器上执行，而客户认为服务是来自一个IP地址(我们可称之为虚拟IP地址)上的。 服务器池(server pool/ Realserver)，是一组真正执行客户请求的服务器，执行的服务一般有WEB、MAIL、FTP和DNS等。 共享存储(shared storage)，它为服务器池提供一个共享的存储区，这样很容易使得服务器池拥有相同的内容，提供相同的服务。 七层负载均衡（应用层）七层负载均衡，采用开源软件Tengine HTTP建议的应用场景 需要对数据内容进行识别的应用，如 Web 应用、小的手机游戏等特性 应用层协议，主要解决如何包装数据； 基于 Cookie 会话保持；使用 X-Forward-For 获取源地址； 监听只支持 HTTP 方式健康检查 HTTPs建议的应用场景 有更高的安全性需求，需要加密传输的应用特性 加密传输数据，可以阻止未经授权的访问； 统一的证书管理服务，用户可以将证书上传到负载均衡，解密操作直接在负载均衡上完成 实现 通过Tengine实现 会话保持四层TCP同一P地址的请求持续发往一台服务器 七层HTTP相同cookiel同的请求发往一台服务器 负载均衡的调度算法1.轮询调度轮询调度（Round Robin 简称’RR’）算法就是按依次循环的方式将请求调度到不同的服务器上，该算法最大的特点就是实现简单。轮询算法假设所有的服务器处理请求的能力都一样的，调度器会将所有的请求平均分配给每个真实服务器。 2.加权轮询调度加权轮询（Weight Round Robin 简称’WRR’）算法主要是对轮询算法的一种优化与补充，LVS会考虑每台服务器的性能，并给每台服务器添加一个权值，如果服务器A的权值为1，服务器B的权值为2，则调度器调度到服务器B的请求会是服务器A的两倍。权值越高的服务器，处理的请求越多。 3.最小连接调度最小连接调度（Least Connections 简称’LC’）算法是把新的连接请求分配到当前连接数最小的服务器。最小连接调度是一种动态的调度算法，它通过服务器当前活跃的连接数来估计服务器的情况。调度器需要记录各个服务器已建立连接的数目，当一个请求被调度到某台服务器，其连接数加1；当连接中断或者超时，其连接数减1。 （集群系统的真实服务器具有相近的系统性能，采用最小连接调度算法可以比较好地均衡负载。) 4.加权最小连接调度加权最少连接（Weight Least Connections 简称’WLC’）算法是最小连接调度的超集，各个服务器相应的权值表示其处理性能。服务器的缺省权值为1，系统管理员可以动态地设置服务器的权值。加权最小连接调度在调度新连接时尽可能使服务器的已建立连接数和其权值成比例。调度器可以自动问询真实服务器的负载情况，并动态地调整其权值。 5.基于局部的最少连接基于局部的最少连接调度（Locality-Based Least Connections 简称’LBLC’）算法是针对请求报文的目标IP地址的 负载均衡调度，目前主要用于Cache集群系统，因为在Cache集群客户请求报文的目标IP地址是变化的。这里假设任何后端服务器都可以处理任一请求，算法的设计目标是在服务器的负载基本平衡情况下，将相同目标IP地址的请求调度到同一台服务器，来提高各台服务器的访问局部性和Cache命中率，从而提升整个集群系统的处理能力。LBLC调度算法先根据请求的目标IP地址找出该目标IP地址最近使用的服务器，若该服务器是可用的且没有超载，将请求发送到该服务器；若服务器不存在，或者该服务器超载且有服务器处于一半的工作负载，则使用’最少连接’的原则选出一个可用的服务器，将请求发送到服务器。 6.带复制的基于局部性的最少连接带复制的基于局部性的最少连接（Locality-Based Least Connections with Replication 简称’LBLCR’）算法也是针对目标IP地址的负载均衡，目前主要用于Cache集群系统，它与LBLC算法不同之处是它要维护从一个目标IP地址到一组服务器的映射，而LBLC算法维护从一个目标IP地址到一台服务器的映射。按’最小连接’原则从该服务器组中选出一一台服务器，若服务器没有超载，将请求发送到该服务器；若服务器超载，则按’最小连接’原则从整个集群中选出一台服务器，将该服务器加入到这个服务器组中，将请求发送到该服务器。同时，当该服务器组有一段时间没有被修改，将最忙的服务器从服务器组中删除，以降低复制的程度。 7.目标地址散列调度目标地址散列调度（Destination Hashing 简称’DH’）算法先根据请求的目标IP地址，作为散列键（Hash Key）从静态分配的散列表找出对应的服务器，若该服务器是可用的且并未超载，将请求发送到该服务器，否则返回空。 8.源地址散列调度U源地址散列调度（Source Hashing 简称’SH’）算法先根据请求的源IP地址，作为散列键（Hash Key）从静态分配的散列表找出对应的服务器，若该服务器是可用的且并未超载，将请求发送到该服务器，否则返回空。它采用的散列函数与目标地址散列调度算法的相同，它的算法流程与目标地址散列调度算法的基本相似。 9.最短的期望的延迟最短的期望的延迟调度（Shortest Expected Delay 简称’SED’）算法基于WLC算法。举个例子吧，ABC三台服务器的权重分别为1、2、3 。那么如果使用WLC算法的话一个新请求进入时它可能会分给ABC中的任意一个。使用SED算法后会进行一个运算 A：（1+1）/1=2 B：（1+2）/2=3/2 C：（1+3）/3=4/3 就把请求交给得出运算结果最小的服务器。 10.最少队列调度最少队列调度（Never Queue 简称’NQ’）算法，无需队列。如果有realserver的连接数等于0就直接分配过去，不需要在进行SED运算。 错误码503没有配置后端服务器，或配置的后端服务器的权重为0会导致SLB返回状态码503。 ACP考试相关外延知识点 后端服务器的健康状况为normal,abnormal和unavailable三种。其中unavailable表示这个负载均衡实例没有配置健康检查，无法获取后端服务器的健康状况。 负载均衡目前仅支持阿里云云服务器（ECS）实例。 开启负载均衡后，请求分布不均匀的可能原因 总体的请求数较少。 后端服务器组中ECS实例的权重不一致。 后端服务器的健康状态异常。 负载均衡SLB实例开启了会话保持功能。 后端服务器组中仅部分ECS实例开启了TCP的Keepalive保持长连接特性。 四层网络是基于 源IP实现， 7层网络是基于Cookie实现的 健康检查过程中，4层服务健康检查基于端口，七层服务检查是基于返回的健康码。 共享实例带宽, 如果给某个监听设置带宽峰值，则会再总带宽中剥离出对应的贷款作为该监控的独享（其他监听任务无法使用） SLB 四层：支持轮询、加权轮询和一致性哈希调度；SLB 七层：支持轮询、加权轮询调度算法。]]></content>
      <categories>
        <category>云计算</category>
      </categories>
      <tags>
        <tag>基础概念</tag>
        <tag>阿里云</tag>
        <tag>负载均衡</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[云计算-基础概念]]></title>
    <url>%2F01.%E7%9F%A5%E8%AF%86-06.cloud_computing%2F%E4%BA%91%E8%AE%A1%E7%AE%97-%E6%A6%82%E5%BF%B5-%E5%B8%B8%E8%A7%81%E6%A6%82%E5%BF%B5%E6%80%BB%E8%A7%88%2F</url>
    <content type="text"><![CDATA[服务器相关ECS (Elastic Compute Service, ECS) 云服务器云服务器(Elastic Compute Service, ECS)是一种简单高效、安全可靠、处理能力可弹性伸缩的计算服务。其管理方式比物理服务器更简单高效。用户无需提前购买硬件，即可迅速创建或释放任意多台云服务器。 云服务器是云计算服务的重要组成部分，是面向各类互联网用户提供综合业务能力的服务平台。平台整合了传统意义上的互联网应用三大核心要素：计算、存储、网络，面向用户提供公用化的互联网基础设施服务。 VPS (Virtual Private Server) 虚拟专用服务器是将一台服务器分割成多个虚拟专享服务器的优质服务。实现VPS的技术分为容器技术，和虚拟化技术。在容器或虚拟机中，每个VPS都可选配独立公网IP地址、独立操作系统、实现不同VPS间磁盘空间、内存、CPU资源、进程和系统配置的隔离，为用户和应用程序模拟出“独占”使用计算资源的体验。VPS可以像独立服务器一样，重装操作系统，安装程序，单独重启服务器。 IDC (Internet Data Center) 互联网数据中心IDC（Internet Data Center）互联网数据中心，是集中计算、存放数据的地方。是一个集中式收集、存储、处理和发送数据的设备提供运行维护的设施以及相关的服务体系。其实就是大型机房。IDC提供的主要业务包括主机托管(机位、机架、VIP机房出租)、资源出租(如虚拟主机业务、数据存储服务)、系统维护(系统配置、数据备份、故障排除服务)、管理服务(如带宽管理、流量分析、负载均衡、入侵检测、系统漏洞诊断)，以及其他支撑、运行服务等。 对于IDC的概念，目前还没有一个统一的标准，但从概念上可以将其理解为公共的商业化的Internet“机房”，同时它也是一种IT专业服务，是IT工业的重要基础设施。IDC不仅是一个服务概念，而且是一个网络的概念，它构成了网络基础资源的一部分，就像骨干网、接入网一样，提供了一种高端的数据传输(DataDelivery)的服务和高速接入服务。 资源扩展 水平扩展 (scale out)，针对于实例数目的增减。 垂直扩展 (scal up)，即单个实例可以使用的资源的增减, 比如增加cpu和增大内存。 网络相关VPC (Virtual Private Cloud) 虚拟私有云从服务的角度来看，VPC指的是一种云（Cloud），这与它的字面意思相符。对于基础架构服务（IaaS），云就是指资源池。你或许听过公有云（Public Cloud），私有云（Private Cloud），混合云（Hybrid Cloud）。不过，VPC不属于这三种云中任一种。这是一种运行在公有云上，将一部分公有云资源为某个用户隔离出来，给这个用户私有使用的资源的集合。 VBR（Virtual border router）边界路由器VBR是CPE（Customer-premises equipment）设备和专有网络VPC（Virtual Private Cloud）之间的一个路由器，作为数据从VPC到本地数据中心IDC（Internet Data Center）的转发桥梁。 VBR提供以下功能： 作为VPC和本地IDC的中间路由器，负责交换数据包。 决定物理专线端口模式为三层路由接口或基于VLAN的三层子接口。 在三层子接口模式下，可以识别或附加VLAN（Virtual Local Area Network）标签。 支持边界路由协议BGP（Border Gateway Protocol）。 BGP是一种基于TCP协议的动态路由协议，主要应用于不同自治域间交换路由信息和网络可达信息。在物理专线接入的过程中，您可以使用BGP实现本地IDC与VBR之间的内网互连，实现更高效、灵活且可靠地搭建混合云。 VBR支持IPv4和IPv6 BGP。 VBR使用限制 不支持源地址策略路由。 每个VBR有且只有一张路由表。 VBR支持的BGP版本为BGP-4。 每个VBR下最多建立8个BGP邻居。 每个BGP邻居的动态路由条数上限为110条。超过上限的路由将被丢弃，无法接收。 通过BGP协议连接VPC时，您需要为阿里云侧分配独立的AS号，不能和云平台内部交换机的AS号重复。 BGP（Border Gateway Protocol）边界路由协议BGP是一种基于TCP协议的动态路由协议，主要应用于不同自治域间交换路由信息和网络可达信息。在物理专线接入的过程中，您可以使用BGP实现本地IDC与VBR之间的内网互连，实现更高效、灵活且可靠地搭建混合云。 OSI（Open System Interconnection）七层网络模型参考模型是国际标准化组织（ISO）制定的一个用于计算机或通信系统间互联的标准体系，一般称为OSI参考模型或七层模型。它是一个七层的、抽象的模型体，不仅包括一系列抽象的术语或概念，也包括具体的协议。 SLB（Server Load Balance） 负载均衡 应用型负载均衡ALB（Application Load Balancer）：专门面向七层，提供超强的业务处理性能，例如HTTPS卸载能力。单实例每秒查询数QPS（Query Per Second）可达100万次。同时ALB提供基于内容的高级路由特性，例如基于HTTP报头、Cookie和查询字符串进行转发、重定向和重写等，是阿里云官方云原生Ingress网关。更多信息，请参见什么是应用型负载均衡ALB。 网络型负载均衡NLB（Network Load Balancer）：面向万物互联时代推出的新一代四层负载均衡，支持超高性能和自动弹性能力，单实例可以达到1亿并发连接，帮您轻松应对高并发业务。NLB面向海量终端上连、高并发消息服务、音视频传输等业务场景针对性地推出了TCPSSL卸载、新建连接限速、多端口监听等高级特性，在物联网MQTTS加密卸载、抗洪峰上联等场景为用户提供多种辅助手段，是适合IoT业务的新一代负载均衡。更多信息，请参见什么是网络型负载均衡NLB。 传统型负载均衡CLB（Classic Load Balancer）：支持TCP、UDP、HTTP和HTTPS协议，具备良好的四层处理能力，以及基础的七层处理能力。更多信息，请参见什么是传统型负载均衡CLB。 相关概念说明 术语 全称 中文 说明 负载均衡 Server Load Balancer 负载均衡服务，简称 阿里云计算提供的一种网络负载均衡服务，可以结合阿里云提供的负载均衡服务.ECS服务为用户提供基于ECS实例的TCP、UDP与HTTP负载均衡服务。 Region Region 地域 代表资源所在并有效的地域，每个地域包含一组数据中心。 Zone Zone 可用区 代表负载均衡所在的Zone LoadBalancer Load Balancer 简称负载均衡实例 负载均衡实例可以理解为负载均衡服务的一个运行实例，用户要使用负载均衡服务，就必须先创建一个负载均衡实例，LoadBalancerld是识别用户负载均衡实例的唯一标识。 Listener Listener 负载均衡服务监听 负载均衡服务监听，包括监听端口、负载均衡策略和健康检查配置等。 BackendServer Backend Server 后端服务器 接受负载均衡分发请求的一组云服务器，负载均衡服务将外部的访问请求按照用户设定的规侧转发到这一组后端服务器上进行处理。 Address Address 服务地址 系统分配的服务地址，当前为P地址。用户可以选择该服务地址是否对外公开，来分别创建公网和私网类型的负载均衡服务。 ### OSSRDSCDN其他CAPCAP原则又称为CAP理论，主要思想是在任何一个分布式系统中都无法同时满足CAP。 C（Consistency）：表示一致性，所有的节点同一时间看到的是相同的数据。 A（Avaliablity）：表示可用性，不管是否成功，确保一个请求都能接收到响应。 P（Partion Tolerance）：分区容错性，系统任意分区后，在网络故障时，仍能操作。 Region]]></content>
      <categories>
        <category>云计算</category>
      </categories>
      <tags>
        <tag>基础概念</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[网络七层模型 (OSI:Open System Interconnection）介绍]]></title>
    <url>%2F01.%E7%9F%A5%E8%AF%86-06.cloud_computing%2F%E4%BA%91%E8%AE%A1%E7%AE%97-%E6%A6%82%E5%BF%B5-OSI-%E7%BD%91%E7%BB%9C%E4%B8%83%E5%B1%82%E6%A8%A1%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[七层网络参考模型介绍各层网络概念 1. 物理层：解决两个硬件之间怎么通信的问题，常见的物理媒介有光纤、电缆、中继器等。它主要定义物理设备标准，如网线的接口类型、光纤的接口类型、各种传输介质的传输速率等。它的主要作用是传输比特流（就是由1、0转化为电流强弱来进行传输，到达目的地后在转化为1、0，也就是我们常说的数模转换与模数转换）。这一层的数据叫做比特。 2. 数据链路层：在计算机网络中由于各种干扰的存在，物理链路是不可靠的。该层的主要功能就是：通过各种控制协议，将有差错的物理信道变为无差错的、能可靠传输数据帧的数据链路。它的具体工作是接收来自物理层的位流形式的数据，并封装成帧，传送到上一层；同样，也将来自上层的数据帧，拆装为位流形式的数据转发到物理层。这一层的数据叫做帧。 3. 网络层：计算机网络中如果有多台计算机，怎么找到要发的那台？如果中间有多个节点，怎么选择路径？这就是路由要做的事。该层的主要任务就是：通过路由选择算法，为报文（该层的数据单位，由上一层数据打包而来）通过通信子网选择最适当的路径。这一层定义的是IP地址，通过IP地址寻址，所以产生了IP协议。 4. 传输层：定义传输数据的协议端口号，以及流控和差错校验，监控数据传输服务的质量，保证报文的正确传输。当发送大量数据时，很可能会出现丢包的情况，另一台电脑要告诉是否完整接收到全部的包。如果缺了，就告诉丢了哪些包，然后再发一次，直至全部接收为止。 5. 会话层：建立、管理和终止应用程序之间的通信 （在五层模型里面已经合并到了应用层）。虽然已经可以实现给正确的计算机，发送正确的封装过后的信息了。但我们总不可能每次都要调用传输层协议去打包，然后再调用IP协议去找路由，所以我们要建立一个自动收发包，自动寻址的功能(一个会话）。 6. 表示层：表示层负责数据格式的转换，将应用处理的信息转换为适合网络传输的格式，或者将来自下一层的数据转换为上层能处理的格式。（在五层模型里面已经合并到了应用层） 7. 应用层：应用层是计算机用户，以及各种应用程序和网络之间的接口，其功能是直接向用户提供服务，完成用户希望在网络上完成的各种工作。前端同学对应用层肯定是最熟悉的。 各层相关协议网络层：ICMP IGMP IP（IPV4 IPV6）传输层：TCP UDP，数据包一旦离开网卡即进入网络传输层应用层：HTTP FTP TFTP SMTP SNMP DNS TELNET HTTPS POP3 DHCP 路由器和交换机两者的区别 交换机：交换机 工作在第二层（链路层）（目前有更加高级的三层交换机，四层交换机，甚至还有七层交换机）交换机的主要功能是组织局域网，经过交换机内部处理解析信息之后，讲信息以点对点的形式发送给固定端 路由器路由器的主要功能：进行跨网段进行数据传输，路由选择最佳路径。每个路由器关联为唯一的路由表。 ex:如果你需要要多台电脑连接到一根网线，用交换机即可如果你只用一个外网IP，但是你有好多台电脑需要上网，用路由器即可 两者的原理路由器：寻址，转发（依靠 IP 地址）交换机：过滤，转发（依靠 MAC 地址） 我们可以看出这两者的主要工作就是转发数据，但是不同之处是，依靠的地址不同，这是一个根本区别！路由器内有一份路由表，里面有它的寻址信息（就像是一张地图），它收到网络层的数据报后，会根据路由表和选路算法将数据报转发到下一站（可能是路由器、交换机、目的主机） 路由器的主要功能：进行跨网段进行数据传输，路由选择最佳路径。寻址，转发（依靠 IP 地址） 交换机交换机内有一张MAC表，里面存放着和它相连的所有设备的MAC地址，它会根据收到的数据帧的首部信息内的目的MAC地址在自己的表中查找，如果有就转发，如果没有就放弃。交换机的主要功能是组织局域网，经过交换机内部处理解析信息之后，讲信息以点对点的形式发送给固定端作用主要是过滤，转发（依靠 MAC 地址） 简单理解7层模型 应用层 人做好信息，往下发 表示层 翻译一下 会话层 打包 传输层 把包发给下层 网络层 报文：给包贴个ip地址的标签 数据链路层 帧：查表ip转mac，然后转成电信号 物理层 定义好各种信号的意思，线路和插口的格式，发送吧 其他网络模型五层网络模型会话层和表示层，都整合到应用层中，从而通过剩余的5层网络进行通信。TCP/IP 层级模型结构，应用层之间的协议通过逐级调用传输层（Transport layer）、网络层（Network Layer）和物理数据链路层（Physical Data Link）而可以实现应用层的应用程序通信互联。 两种网络模型的关系 参考资料[https://www.jianshu.com/p/9b9438dff7a2][https://blog.csdn.net/yaopeng_2005/article/details/7064869][https://blog.csdn.net/superjunjin/article/details/7841099][https://blog.csdn.net/a369189453/article/details/81193661]]]></content>
      <categories>
        <category>云计算</category>
      </categories>
      <tags>
        <tag>基础概念</tag>
        <tag>阿里云</tag>
        <tag>网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[关系型数据库RDS（Relational Database Service）]]></title>
    <url>%2F01.%E7%9F%A5%E8%AF%86-06.cloud_computing%2F%E4%BA%91%E8%AE%A1%E7%AE%97-%E6%A6%82%E5%BF%B5-RDS-%E4%BA%91%E6%95%B0%E6%8D%AE%E5%BA%93%2F</url>
    <content type="text"><![CDATA[阿里云关系型数据库RDS（Relational Database Service） 是一种稳定可靠、可弹性伸缩的在线数据库服务。基于阿里云分布式文件系统和SSD盘高性能存储，RDS支持MySQL、SQL Server、PostgreSQL和MariaDB TX引擎，并且提供了容灾、备份、恢复、监控、迁移等方面的全套解决方案，帮助您解决数据库运维的烦恼。 相关概念 产品系列云数据库RDS实例包括四个系列：基础版、高可用版、集群版和三节点企业版（原金融版） 存储类型云数据库RDS提供三种存储类型：本地SSD盘、ESSD云盘和SSD云盘 规格族云数据库RDS根据CPU、内存、连接数和IOPS，提供多种实例规格族，一种实例规格族又包括多个实例规格。 慢日志数据节点慢日志 数据节点慢日志中统计的命令执行时间仅包含命令在数据节点中的执行时间，不包含数据节点与代理或客户端的通信时间以及命令在单线程队列上的排队延迟等。 数据节点慢日志的保留时间为72小时，无数量限制。 由于Redis性能出色，通常情况下，数据节点慢日志的数量较少。 代理慢日志 代理慢日志中统计的命令执行时间从代理向数据节点发出请求开始，到代理从数据节点收到相应的回复为止，包含了命令在数据节点中的执行时间、数据在网络中的传输时间以及命令的排队延迟等。 代理慢日志的保留时间为72小时，无数量限制。 由于代理慢日志反映的延迟与您在应用端感受到的延迟更相近，在排查Redis服务超时问题时，建议多关注此类日志。 实例变更注意事项 变更配置可能会进行数据迁移，迁移完成后根据您选择的切换时间进行切换（期间保持增量同步），切换过程中会出现一次约30秒的闪断，而且与数据库、账号、网络等相关的大部分操作都无法执行，请尽量在业务低峰期执行变配操作，或确保您的应用有自动重连机制。 变更配置后无需您手动重启实例。 由于基础系列只有一个数据库节点，没有备节点作为热备份，因此当该节点意外宕机或者执行变更配置、版本升级等任务时，会出现较长时间的不可用。如果业务对数据库的可用性要求较高，不建议使用基础系列，可选择其他系列（如高可用系列）。 如果RDS实例下已创建只读实例，则在扩容存储空间时，请确保只读实例的存储空间大于等于主实例的存储空间，否则将扩容失败。建议您先扩容只读实例的存储空间，所有只读实例扩容完成后，再扩容主实例存储空间。 对实例规格或者存储空间进行配置变更，说明如下： 实例为运行中。 实例当前无正在运行的备份。 实例规格和存储空间必须指定一项。 降低磁盘空间配置，输入的磁盘空间不能小于实际使用空间大小的1.1倍。 当前只支持常规实例、只读实例变更配置，不支持灾备实例、临时实例。-变更说明|变更项|说明||-|-||版本| 部分版本实例支持升级到更高版本||系列|支持基础系列升级到高可用系列。更多详情，请参见基础系列升级为高可用系列。||规格|所有实例类型都支持变更规格。||存储类型| 存储类型为ESSD版本的实例支持升级PL等级（不支持降级），例如将ESSD PL1升级为ESSD PL2。非集群系列的SSD云盘实例支持升级为ESSD云盘，但是不支持再降级回SSD||存储空间|所有实例仅支持增加存储空间，不支持减少存储空间。在增加存储空间时，最小增加量为5 GB，最大不能超过当前实例规格的存储空间限制。说明:如果主实例有只读实例，由于只读实例存储空间不能小于主实例存储空间，因此需要先增加只读实例存储空间，才能增加主实例的存储空间。 ps:高可用云盘版实例增加存储空间时，会出现一次约30秒的闪断。在此期间，数据库、账号、网络等相关的大部分操作都无法执行，请尽量在业务低峰期执行变配操作，并确保您的应用有自动重连机制。| 库表恢复RDS MySQL支持常规和极速级别的库表恢复功能，您可将指定的库、表按备份集或时间点恢复至原实例或新实例，无需恢复全部数据，可用于误操作后的快速订正，以及分析历史数据等场景。]]></content>
      <categories>
        <category>云计算</category>
      </categories>
      <tags>
        <tag>基础概念</tag>
        <tag>阿里云</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[专有网络（VPC：Virtual Private Cloud）介绍]]></title>
    <url>%2F01.%E7%9F%A5%E8%AF%86-06.cloud_computing%2F%E4%BA%91%E8%AE%A1%E7%AE%97-%E6%A6%82%E5%BF%B5-%E5%AD%90%E7%BD%91%E6%8E%A9%E7%A0%81%2F</url>
    <content type="text"><![CDATA[当下网络技术的发展使得全球范围内的计算机能够相互连接和通信。然而，为了实现有效的数据传输和网络管理，需要将IP地址划分为网络和主机两个部分。子网掩码应运而生，它是一种用于确定IP地址的网络和主机部分的掩码。本文将介绍子网掩码的背景、概念、作用、规范和使用示例。 网络（Network）：网络是指由一组连接在一起的计算机和其他设备组成的通信系统。它可以是局域网（LAN）、广域网（WAN）或互联网（Internet）等。网络提供了连接和通信的基础设施，允许计算机和设备之间进行数据传输、资源共享和通信。 主机（Host）：主机是指连接到网络并具有自己唯一标识（如IP地址）的计算机或网络设备。主机可以是个人计算机、服务器、路由器、交换机、物联网设备等。它们在网络中扮演着发送和接收数据的角色，可以是数据的源或目的地。 在IP网络中，主机通常被视为具有唯一IP地址的终端设备，而网络则是这些主机之间的连接和通信基础设施。IP地址由网络部分和主机部分组成，通过子网掩码将其划分为网络地址和主机地址，以便确定数据在网络中的传输路径和目的地。 背景在网络通信中，IP地址是唯一标识网络上的设备的方式。然而，一个IP地址本身并不能提供足够的信息来确定网络和主机的边界。为了解决这个问题，需要引入子网掩码。 概念子网掩码是一个32位的二进制数，与IP地址进行逻辑与（AND）操作，用于划分IP地址的网络和主机部分。子网掩码中的1表示网络位，0表示主机位。通过确定哪些位是网络位和主机位，子网掩码定义了网络的规模和主机的容量。 作用子网掩码的主要作用是确定IP地址的网络地址和主机地址。通过与IP地址进行逻辑与操作，子网掩码将网络位设为1，主机位设为0，从而确定IP地址所属的网络地址。同时，子网掩码将网络位设为0，主机位保留不变，从而确定IP地址所属的主机地址。 此外，子网掩码还可以划分子网，将一个网络进一步划分为多个子网，以满足不同子网的需求。它还支持网络管理和路由的实施，提供了一种有效的方式来控制网络中不同子网的访问和通信。 规范子网掩码通常与IPv4地址一起使用。IPv4地址由32位二进制数组成，用四个十进制数表示，如192.168.0.1。子网掩码也是32位的二进制数，由四个八进制数（也可表示为十进制数或十六进制数）表示，如255.255.255.0。 使用示例假设有一个IP地址为192.168.0.1，子网掩码为255.255.255.0的网络。子网掩码中前24位（即前三个八进制数）均为1，表示前24位是网络位，后8位是主机位。因此，这个网络可以容纳256个主机（2^8-2，减去网络地址和广播地址）。 通过子网划分，可以将一个网络划分为多个子网，每个子网可以有不同的IP地址范围和主机容量。这样可以更好地管理网络资源和控制网络通信。 总结子网掩码是一种用于确定IP地址的网络和主机部分的掩码。它通过与IP地址进行逻辑与操作，确定网络地址和主机地址，实现网络划分和管理。子网掩码的作用包括确定网络地址、确定主机地址、划分子网和支持网络管理与路由。在IPv4中，子网掩码与IP地址一起使用，由32位二进制数表示。通过子网划分，可以将一个网络划分为多个子网，每个子网具有不同的IP地址范围和主机容量。了解和正确使用子网掩码对于构建有效的网络架构和实现网络通信至关重要。]]></content>
      <categories>
        <category>云计算</category>
      </categories>
      <tags>
        <tag>基础概念</tag>
        <tag>阿里云</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[网络相关 - 网关]]></title>
    <url>%2F01.%E7%9F%A5%E8%AF%86-06.cloud_computing%2F%E4%BA%91%E8%AE%A1%E7%AE%97-%E6%A6%82%E5%BF%B5-VPN%2F</url>
    <content type="text"><![CDATA[VPN网关VPN网关提供IPsec-VPN和SSL-VPN两种网络连接方式，不同的网络连接方式适用于不同的应用场景。 IPsec-VPNIPsec-VPN是一种基于路由的网络连接技术，提供灵活的流量路由方式，方便您配置和维护VPN策略，适用于在企业本地数据中心或企业办公网络与VPC之间建立网络连接。 注意事项 本地数据中心和VPC间互通的网段没有重叠 本地数据中心的网关设备必须配置静态公网IP地址 需要在本地数据中心的网关设备中加载阿里云上VPN网关的配置， SSL-VPNSSL VPN是采用SSL（Security Socket Layer）/TLS（Transport Layer Security）协议来实现远程接入的一种轻量级VPN技术。SSL-VPN是一种基于OpenVPN架构的网络连接技术，适用于在互联网客户端与VPC之间建立网络连接。部署后，仅需要在互联网客户端中加载证书并发起连接，互联网客户端便可与VPC互通。 SSL-VPN仅支持绑定国际标准商用密码算法的公网VPN网关实例。 VPC网关IPv4网关IPv4网关是连接专有网络VPC（Virtual Private Cloud）和公网的网络组件。VPC访问IPv4公网的流量经过IPv4网关，由IPv4网关实现路由转发以及私网地址到公网地址的转换，最终实现对公网的访问。 功能 作为VPC路由表中的路由下一跳，控制VPC访问公网的目的地址范围。 为VPC中分配了IPv4公网地址的网络资源（例如弹性网卡、ECS等），提供网络地址转换功能。使用限制 IPv4网关是地域级别的资源，只能在同一个地域中使用。 一个VPC下只支持创建一个IPv4网关，且一个IPv4网关仅能关联一个VPC。 只有激活成功的IPv4网关才具有公网访问能力。 一个IPv4网关仅能绑定一张网关路由表。 IPv4网关不能绑定系统路由表。 已绑定交换机的路由表不能与IPv4网关绑定。 网关路由表不支持配置自定义路由条目，但可以修改网关路由表中路由条目的下一跳类型。 VPC内包含特定资源（VPC内存在网卡可见模式的EIP资源）时，不支持创建IPv4网关。 NAT网关(Network Address Translation)网络地址转换，是将IP数据包头中的IP地址转换为另一个IP地址的过程。在实际的应用中，NAT主要用于实现私有网络访问公共网络的功能。这种通过使用少量的公网IP地址代表较多的私有IP地址的方式，将有助于减缓可用IP地址空间的枯竭。 公网NAT网关公网NAT网关是一款针对公网访问的企业级安全网关产品，提供NAT代理功能（SNAT和DNAT），具有100 Gbps的转发能力及跨可用区的容灾能力。公网NAT网关具有高性能、自动弹性、灵活计费、精细化运维等特性，可以帮助您更好地管理公网访问流量。 业务场景需求： 如果您的云上网络只希望主动访问公网上的业务，而不希望云上的业务直接暴露在公网上从而有被攻击的风险，您可以选用公网NAT网关为业务提供安全防护能力。 如果您的业务具有突增的访问公网的流量需求，您可以选用公网NAT网关为您提供灵活和弹性的扩容能力，并且只需要按使用量付费，节省企业成本。 如果您有大量访问公网的机器，您可以通过公网NAT网关统一公网出口，并通过公网NAT网关准确和精细化的运维监控能力管理企业访问公网的流量。功能类型SNAT(Source Network Address Translation)为专有网络VPC（Virtual Private Cloud）内无公网IP的ECS实例提供访问公网的代理服务。改变原来IP实现对外部资源的访问DNAT(Destination Network Address Translation)将公网NAT网关上绑定的弹性公网IP（Elastic IP Address，简称EIP）映射给VPC内的ECS实例使用，使ECS实例可以面向公网提供服务。改变外部访问的目标IP（改为私网IP）实现业务的对外服务 私网NAT网关功能类型SNAT(Source Network Address Translation)通过NAT IP地址为VPC内的ECS实例提供访问外部私有网络代理服务。 DNAT(Destination Network Address Translation)通过将NAT IP地址和端口映射转换为VPC内ECS实例的IP和端口，使ECS实例对外提供私网访问服务。]]></content>
      <categories>
        <category>云计算</category>
      </categories>
      <tags>
        <tag>基础概念</tag>
        <tag>阿里云</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[专有网络（VPC：Virtual Private Cloud）介绍]]></title>
    <url>%2F01.%E7%9F%A5%E8%AF%86-06.cloud_computing%2F%E4%BA%91%E8%AE%A1%E7%AE%97-%E6%A6%82%E5%BF%B5-VPC%2F</url>
    <content type="text"><![CDATA[概念VPC不属于公有云（Public Cloud），私有云（Private Cloud），混合云（Hybrid Cloud）这三种云中任一种。这是一种运行在公有云上，将一部分公有云资源为某个用户隔离出来，给这个用户私有使用的资源的集合。 服务角度VPC是这么一种云，它由公有云管理，运行在公共资源上，但是保证每个用户之间的资源是隔离，用户在使用的时候不受其他用户的影响，感觉像是在使用自己的私有云一样 从这种意义上看，VPC不是网络，我们可以对比VPC和它一个字面上相近的概念：VPN（Virtual Private Network）。VPN在公共的网络资源上虚拟隔离出一个个用户网络，例如IPsec VPN可以是在互联网上构建连接用户私有网络的隧道，MPLS VPN更是直接在运营商的PE设备上划分隔离的VRF给不同的用户。从提供服务的角度来，说如果VPC指的只是网络的话，那它跟VPN的概念是重复的。所以，从公有云所提供的服务来说，VPC应该理解成，向用户提供的隔离资源的集合。 VPC最早是由AWS在2009年提出[1]，不过VPC的一些组成元素在其提出之前就已经存在。VPC只是将这些元素以私有云的视角重新包装了一下。在VPC之后，云主机只能使用VPC内部的对应的元素。从这个角度看，VPC更像是公有云服务商以打包的形式提供服务。 用户可以在公有云上创建一个或者多个VPC，每个部门一个VPC。对于需要连通的部门创建VPC连接。 技术角度VPC是用户专属的一个二层网络。 VPC的网络链接组件IPv4IPv4网关是连接专有网络VPC（Virtual Private Cloud）和公网的网络组件。VPC访问IPv4公网的流量经过IPv4网关，由IPv4网关实现路由转发以及私网地址到公网地址的转换，最终实现对公网的访问。 功能 作为VPC路由表中的路由下一跳，控制VPC访问公网的目的地址范围。 为VPC中分配了IPv4公网地址的网络资源（例如弹性网卡、ECS等），提供网络地址转换功能。使用场景对公网访问进行集中控制 使用限制 IPv4网关是地域级别的资源，只能在同一个地域中使用。 一个VPC下只支持创建一个IPv4网关，且一个IPv4网关仅能关联一个VPC。 一个IPv4网关仅能绑定一张网关路由表。 IPv4网关不能绑定系统路由表。 已绑定交换机的路由表不能与IPv4网关绑定。 HaVip（High-Availability Virtual IP Address）高可用虚拟IPHaVip是一种可以独立创建和释放的私网IP资源，具备与ECS实例主私网IP地址一样的网络接入能力，可以与高可用软件，例如Keepalived配合使用，搭建高可用主备服务，提高业务的可用性。HaVip支持绑定一个弹性公网IP（EIP）、多个ECS实例或多个ECS实例的主网卡或辅助网卡，以实现同可用区、多服务器高可用架构下的IP漂移，确保对外提供服务的私网IP始终不变。 HaVip资源限制 资源 默认限制 支持创建高可用虚拟IP（HaVip）的网络类型 VPC类型 单个ECS实例支持同时绑定的HaVip数量 5个 单个HaVip支持同时绑定的EIP数量 1个 单个HaVip支持同时绑定的ECS实例或弹性网卡的数量 10个HaVip只能绑定ECS实例和弹性网卡中的一种，不能跨类别绑定；HaVip具有子网属性，只能绑定同一交换机下的实例或弹性网卡 HaVip是否支持广播和组播通信 不支持 网络ACL网络ACL是VPC中的网络访问控制功能。您可以自定义设置网络ACL规则，并将网络ACL与交换机绑定，实现对交换机中ECS实例的流量的访问控制。 ECS安全组安全组是一种虚拟防火墙，具备状态检测和数据包过滤能力，用于在云端划分安全域。通过配置安全组规则，您可以控制安全组内一台或多台ECS实例的入流量和出流量。安全组和交换机不是绑定的。 经典网络ACP考试相关外延知识点 VPC中使用的弹性公网IP是收费服务。 参考资料[https://zhuanlan.zhihu.com/p/33658624]]]></content>
      <categories>
        <category>云计算</category>
      </categories>
      <tags>
        <tag>基础概念</tag>
        <tag>阿里云</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基因分析平台 - 超大数据分析技巧]]></title>
    <url>%2F01.%E7%9F%A5%E8%AF%86-06.cloud_computing%2F%E5%9F%BA%E5%9B%A0%E5%88%86%E6%9E%90%E5%B9%B3%E5%8F%B0%20-%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[之前也一直在使用阿里云的基因分析平台，但是更多是作为解决方案的技术储备，而且其实更多的是平台迁移，数据都比较常规。而最近接手到一些样本量非常大的非常规项目（单样本有接近2T的数据），这个数据量，不管是计算资源还是存储资源，其实本地IDC都不太能支持，所以将视角转向通过基因分析平台进行实现。由于数据规模超大，也因此遇到许多新的问题，所以趁此机会介绍一下基因分析平台的使用方法和一些隐藏的坑。 基因分析平台简介适用场景只要涉及数据，没有合规性限制，上云可能都整体是一个更稳定更省心的解决方案。首先自建机房，往往在资源上需要进行过饱和的投入，以应对业务峰值和未来的业务成长场景，而这也就带来了低谷期的业务浪费。简单说，要么一直有资源浪费，要么经常资源不足。这也是本地IDC搭建在底层逻辑上无法解决的问题。 而且自建机房，除了硬件资源外，还需要机房（静电地板、空调、备用发电机、安保、网络、电力）、各类运维管理人员投入、设备的阶段性维护投入等。对于小型企业来讲，自建集群在前期需要一笔不菲的投入。针对业务量稳定的企业，使用本地自建IDC集群，对集群有更高的可控性，资源可以稳定在饱和运行的水平，本地IDC的性价比会比较高。但是现实中这类情况非常少，甚至一些稳定业务也是周期性稳定，业务低谷无法避免会存在资源的浪费。所以其实现在针对大型企业（有资源使用量的基本盘，但是依然存在顶峰波动的情况）混合云是一个比较不错的选择，业务执行逻辑优先充分利用本地资源，资源不足时，进行云商资源的弹性扩展应对峰值需求。 使用方式基因分析平台就是阿里云针对基因检测领域开发的云计算平台，面向基因检测的垂直细分领域进行了一些适配，如果大家本地使用WDL和docker的话，可以实现分析流程的无缝迁移。基因分析平台工作模式如下：基因分析平台本身采用的模式是，存算分离，按需分配资源的模式。每个任务的计算资源、存储资源、执行环境都是作为独立的资源项，进行单独的配置。每个资源项支持的上限(64核心 512G内存，512T存储)还是蛮高的。然后再任务执行阶段，想一块块积木进行组装，然后进行相关的分析。 基因分析平台，是工作空间作为一个整体管理一个独立完整的项目，以下是工作空间主要包含的几个内容，如果大家第一次接触，主要关注文件、应用、运行即可，另外的表格、模板、投递三部分属于批量任务投递的部分，可以放到后面进行了解，| 类别 | 名称 | 用途 | 含义 || ——– | —- | ——– | ————————————————————– || 数据管理 | 文件 | OSS存储 | 账号可以访问的OSS存储，可以直接查看OSS的文件 || 应用配置 | 应用 | 分析流程 | 记录了流程的wdl文件和对应的说明文档、版本信息等 || 分析任务 | 运行 | 流程执行 | 启动一个分析流程，执行分析任务，查看任务进度和状态 || 数据管理 | 表格 | 表格文件 | 用户按特定表格模板上传的表格文件，用于批量进行任务投递 || 应用配置 | 模板 | 投递模板 | 基于应用和表格文件，配置好的任务投递模板，可以直接用于任务投递 || 分析任务 | 投递 | 流程投递 | 基于应用和表格，启动一批分析任务，执行分析 | 因此如果我们想使用基因分析平台进行任务分析，就需要做一些准备工作： 需要将我们要分析的数据存储到OSS上，所有输入输出数据的长期保存都依赖于OSS，分析中挂载的存储上产生的中间数据均不可见。 搭建一个分析流程，分析流程目前仅支持 WDL ，调用的分析环境（docker镜像或阿里提供的software）、计算资源（最小 1c2Gb 到最大 64c512Gb ）、分析存储（HDD和SSD）都是在WDL中指定的。 性能监控云平台 注意事项 流程部署过程中，WDL语法中所有的File类型会作为文件关联到分析容器内（/cromwell_inputs/ 目录下），所以一定要明确数据的真实类型，错误的类型使用，会导致分析过程中出现文件缺失。 由于流程运行环境依赖于容器，因此需要注意相关的容器需要开放访问权限。 wdl本身提供了scatter语法，以便进行一些遍历/并行操作，但是在基因分析平台任务监控中，只能进行2层深度的scatter解析（多深度可以进行任务的分析，但是并没有对应颗粒度的任务状态监控），所以为了更好的查看所有任务的运行情况和资源使用情况，最好控制wdl的数据深度。比如最高维度可以借助投递逻辑来实现（比如下述示例中的Sample_info层)，流程只是针对一个样本,批次多样本的投递依赖于表格和模板结合进行投递的方式实现。 1234567scatter (Sample_info in Samples_info)&#123; scatter (sample_lib in Sample_info)&#123; scatter (Fastq_info in sample_lib.fastqlist)&#123; call xxxx ; &#125; &#125;&#125; 历史的一些隐藏小坑 基因分析平台对 wdl 文件大小有一个非明文限制（100kb以下），23年2月初和阿里反馈调整过一次，目前我们没有出现由于文件大小导致的问题，但是限制可能依然存在。不过常规流程可能不会遇到文件大小瓶颈问题。 系统盘（local-disk）的存储范围 40G~500G，超出后无法启动任务，可以不配置（默认40G），如果为了规避资源浪费设置了10G资源，会导致任务无法启动。 数据盘官网提示可以挂载 16*32T，但是自定义挂载目录（例如说明文档提供的 /mnt1 、/mnt2），那么生成结果无法正常导入OSS，数据盘挂载目录只有使用 /cromwell_root 时且output中文件名不指定根目录时，生成的数据才能正常导出OSS，并被作为正常输入被下游任务调用。 如果使用根目录，会导出到OSS，但是文件路径会有问题影响下游调用。为方便理解，提供一个示例供大家参考。 12345678910111213141516task test_run &#123; input &#123; String text &#125; command &#123; echo $&#123;text&#125; &gt; "out1.txt" &#125; output &#123; File out1 = "out1.txt" &#125; runtime &#123; cpu: 1 memory: "2G" disks: "/cromwell_root 20 cloud_ssd" &#125;&#125;]]></content>
      <categories>
        <category>基因分析平台</category>
      </categories>
      <tags>
        <tag>基础概念</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[software-ossutil-阿里云数据传输]]></title>
    <url>%2F03.%E5%BA%94%E7%94%A8-01.software_in_NGS%2Fsoftware-%E5%AF%B9%E8%B1%A1%E5%AD%98%E5%82%A8-ossutil-%E9%98%BF%E9%87%8Capi%2F</url>
    <content type="text"><![CDATA[数据上传阿里云对象存储文档 配置12345/jdfstj1/B2C_COM_P1/Research_and_Development/Software/ossutil64 -i LTAI5tCC7sn** # 对象存储账号 -k bEawRWhj9dY**-e oss-cn-shenzhen.aliyuncs.com cp -r /share/nastj8/B2C_RD_P1/Liubo/WGS-菁良 oss://canseq-3/Pancancer688_LDT/jingliang-A_GW-WES/ 上述明文会导致整个账号存在潜在的风险,同时每次的重复输入也会导致更高的工作成本，可以使用 ossutil64 生成配置文件 123456789101112131415161718$ /jdfstj1/B2C_COM_P1/Research_and_Development/Software/ossutil64 config后通过交互作业分别输入下列相关信息生成config# 输入配置文件存储目录Please enter the config file name,the file name can include path(default /home/liubo4/.ossutilconfig, carriage return will use the default file. If you specified this option to other file, you should specify --config-file option to the file when you use other commands): */path/liubo/.aliClodu.configPlease enter language(CH/EN, default is:EN, the configuration will go into effect after the command successfully executed):Please enter endpoint: oss-cn-shenzhen-internal.aliyuncs.comPlease enter accessKeyID:LTAI5*Please enter accessKeySecret:bEawRW*Please enter stsToken: # 可不填写# 非交互生成配置文件./ossutil64 config -e oss-cn-beijing.aliyuncs.com -i LTAIbZcdVCmQ**** -k D26oqKBudxDRBg8Wuh2EWDBrM0**** -L CH -c /myconfig# 使用配置文件/jdfstj1/B2C_COM_P1/Research_and_Development/Software/ossutil64 \-c ~/.aliCloud.config -u --maxupspeed=100000 \cp -r LocalPath oss://canseq-3/CloudPath cp参数必须参数 选项名称 描述 -c，–config-file ossutil工具的配置文件路径，ossutil启动时将从配置文件读取配置。当您需要管理多个账号下的Bucket时，可以生成多个配置文件，并将其中一个指定为默认配置文件。当您需要管理其他账户下的Bucket时，请通过-c指定正确的配置文件。 -e，–endpoint 指定Bucket对应的Endpoint，当您需要管理多个地域的Bucket时，可以通过此选项指定多个Endpoint。各地域Endpoint详情请参见访问域名和数据中心。 -i，–access-key-id 指定访问OSS使用的AccessKey ID，当您需要管理多个账号下的Bucket时，可通过此选项指定对应的AccessKey ID。 -k，–access-key-secret 指定访问OSS使用的AccessKey Secret，当您需要管理多个账号下的Bucket时，可通过此选项指定对应的AccessKey Secret。 -p， –password 指定访问OSS使用的AccessKey Secret，输入该选项时会提示用户从键盘输入AccessKey Secret，ossutil工具以从键盘读取的AccessKey Secret为准，并忽略通过其他方式配置的AccessKey Secret。 –loglevel 在当前工作目录下输出ossutil日志文件ossutil.log。该选项默认为空，表示不输出日志文件。 重要参数 选项名称 描述 –start-time 值为Linux或Unix系统下面的时间戳，如果输入这个选项，最后更新时间早于该时间的Object会被忽略。 –maxupspeed 最大上传速度，单位为KB/s，默认值为0（不受限制）。 –maxdownspeed 最大下载速度，单位为KB/s，默认值为0（不受限制）。 –bigfile-threshold 开启大文件断点续传的文件大小阈值，单位为Byte，默认值为100 MByte，取值范围为0~9223372036854775807。 –type 数据校验的方式。取值如下： crc64（默认值）：数据CRC64校验。 md5：数据MD5校验。 -u， –update 更新操作。 -j， –jobs 多文件操作时的并发任务数，默认值为3，取值范围为1~10000。 –output-dir 指定输出文件所在的目录，输出文件目前包含cp命令批量拷贝文件出错时所产生的report文件。默认值：当前目录下的ossutil_output目录。 –meta 设置Object的meta，格式为[header：value#header：value…]，如：Cache-Control：no-cache#Content-Encoding：gzip。 –end-time Linux或Unix系统下的时间戳。如果使用该选项，则最后更新时间晚于通过此选项指定时间的Object会被忽略。 –start-time 值为Linux或Unix系统下面的时间戳，如果输入这个选项，最后更新时间早于该时间的Object会被忽略。 –storage-class 设置Object的存储方式。取值如下：Standard（默认值）：适用于频繁的数据访问。IA：适用于较低访问频率（平均每月访问频率1到2次）的业务场景，有最低存储时间（30天）和最小计量单位（64 KB）要求。支持数据实时访问，访问数据时会产生数据取回费用。Archive：适用于数据长期保存的业务场景，有最低存储时间（60天）和最小计量单位（64 KB）要求。数据需解冻（约1分钟）后访问，解冻会产生数据取回费用。ColdArchive：适用于需要超长时间存放的极冷数据，有最低存储时间（180天）和最小计量单位（64 KB）要求。数据需解冻后访问，解冻时间根据数据大小和选择的解冻模式决定，解冻会产生数据取回费用。 –disable-all-symlink 上传时忽略所有的符号链接子文件以及符号链接子目录。 –tagging 上传或复制文件时设置文件的对象标签，格式为”abc=1&amp;bcd=2&amp;……”。 rsync12345678910111213141516171819202122232425./ossutil64 sync file_url cloud_url[-f --force][-u --update][--delete][--enable-symlink-dir][--disable-all-symlink][--disable-ignore-error][--only-current-dir][--output-dir &lt;value&gt;][--bigfile-threshold &lt;value&gt;][--part-size &lt;value&gt;][--checkpoint-dir &lt;value&gt;][--encoding-type &lt;value&gt;][--snapshot-path &lt;value&gt;][--include &lt;value&gt;][--exclude &lt;value&gt;][--meta &lt;value&gt;][--acl &lt;value&gt;][--maxupspeed &lt;value&gt;][--disable-crc64][--payer &lt;value&gt;][-j, --job &lt;value&gt;][--parallel &lt;value&gt;][--retry-times &lt;value&gt;][--tagging &lt;value&gt;] 重要参数 配置项 说明 file_url 待同步的本地文件夹路径。例如Linux系统文件路径/localfolder/，Windows系统文件路径D:\localfolder\。 cloud_url OSS文件夹路径。格式为oss://bucketname/path/。例如oss://examplebucket/exampledir/。如果输入的cloud_url没有以正斜线（/）结尾，ossutil会自动在结尾处添加一个正斜线（/）。 -u，–update 只有当目标文件不存在，或源文件的最后修改时间晚于目标文件时，ossutil才会执行同步操作。 -f –force 强制操作，不进行询问提示。 –only-current-dir 仅同步当前目录下的文件，忽略子目录及子目录下的文件。 Reference官方参数说明]]></content>
      <categories>
        <category>云计算</category>
      </categories>
      <tags>
        <tag>基础概念</tag>
        <tag>对象存储</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[弹性伸缩 (Auto Scaling)]]></title>
    <url>%2F01.%E7%9F%A5%E8%AF%86-06.cloud_computing%2F%E4%BA%91%E8%AE%A1%E7%AE%97-%E6%A6%82%E5%BF%B5-AS-%E5%BC%B9%E6%80%A7%E4%BC%B8%E7%BC%A9%2F</url>
    <content type="text"><![CDATA[概念跟进业务需求和策略设置伸缩规则，在业务新一区一增长时自动增加ECS实例，以保证计算能力；在业务需求下降时自动是减少ECS实例以节省成本。 适用场景 无规律的业务量波动某新闻网站播出了热点新闻，访问量突增，新闻的时效性降低后，访问量回落。由于该新闻网站的业务量波动无规律，访问量突增和回落的具体时间难以预测，所以手动调整实例很难做到及时性，而且调整数量也不确定。您可以利用弹性伸缩的报警任务，由阿里云自动根据CPU使用率等衡量指标进行弹性伸缩。 有规律的业务量波动某游戏公司每天18:00业务需求急速增长进入高峰期，到22:00业务需求降低，高峰期结束。该游戏公司的业务量波动有规律，但是每天手动调整计算能力浪费人力和时间成本。您可以利用弹性伸缩的定时任务，由阿里云定时自动进行弹性伸缩。您可以设置两个定时任务，报警任务执行的伸缩规则是简单规则类型。一个定时任务用于在每天17:55自动为您增加3台实例，另一个定时任务用于在每天22:05自动为您减少3台实例。该方式可以很好地应对每天18:00~22:00高峰期的业务量，且在高峰期结束后及时释放实例，不浪费多余的实例资源和成本。 无明显的业务量波动某通信公司的业务支撑系统需要全天运作，业务量一段时间内无明显波动。如果现有计算资源突然出现故障，会导致业务受到影响，很难及时进行故障修复或者替换。您可以利用弹性伸缩的高可用优势，开启健康检查模式。阿里云会自动检查实例的健康状态，当发现存在实例不健康时，自动增加实例替换不健康的实例，确保故障的计算资源及时得到修复。而且伸缩组必须设置最小实例数，确保无论在哪种情况下，伸缩组内的实例数量都至少等于下限，确保业务可以运作。 混合型的业务场景如果某公司的业务场景比较复杂，日常业务量波动不明显，且在某个时间段内，业务量是在一定基础上波动的，您已经订购了一部分包年包月的实例，只是想针对波动的业务量合理调整实例数量。您可以手动将已订购的包年包月实例加入伸缩组，再结合弹性伸缩的报警任务，由阿里云自动根据CPU使用率等衡量指标进行弹性伸缩，更经济、稳定地管理业务的计算能力。 除手动调整实例数量和报警任务，弹性伸缩还支持定时任务、健康检查等。您可以根据业务场景灵活组合以上功能，从而在使用弹性伸缩的时候获得更丰富灵活的使用体验。 工作原理 支持的伸缩模式固定数量模式如果您创建伸缩组时设置期望实例数，伸缩组会自动将ECS实例数量维持在期望实例数 健康模式如果您在伸缩组开启健康检查功能，伸缩组会定期检查ECS实例的运行状态，如果发现一台ECS实例未处于运行中状态，则判定该ECS实例为不健康实例并移出。 定时模式您可以创建定时任务，在指定时间执行指定伸缩规则。 动态模式您可以基于云监控性能指标（例如CPU使用率）创建报警任务，当伸缩组的指标数据满足您指定的报警条件（例如伸缩组内所有ECS实例的CPU平均值大于60%）时，触发报警并执行您指定的伸缩规则。 自定义模式您可以手动进行弹性伸缩，包括手动执行伸缩规则，或者手动添加、移出或者删除已有的ECS实例。 弹性伸缩组件伸缩组概述伸缩组是弹性伸缩的核心单元，用来管理一组具有相同应用场景和相同实例类型的实例。 伸缩配置定义了弹性伸缩的ECS示例的配置信息。在一个伸缩组中，您可以创建多个伸缩配置，且最多可以拥有70个伸缩配置。但一个伸缩组中只能有一个伸缩配置处于生效状态，选用一个新的伸缩配置后，原生效中的伸缩配置会进入未生效状态。必须配置：付费模式（按量付费、抢占式实例）、实例和镜像（实例规格、镜像）、存储（系统盘、数据盘）、网络（公网IP、安全组）；可选配置：登录凭证（SSH秘钥）、高级设置（资源组、实例名称、主机名、RAM角色、负载均衡权重）、实例自定义数据、标签。 伸缩触发任务用于触发伸缩规则的任务，如定时任务、云监控的报警任务。 报警任务支持：CPU,内存，系统平均负载，内外网出和入流量，TCP总连接数和已建立连接数。系统盘读和写BPS,系统盘读和写IOPS,内网网卡收包数和发包数。磁盘IOPS不是具体的确定的属性，具体的要区分具体的系统盘读和写BPS,系统盘读和写IOPS。 伸缩规则伸缩规则的作用由伸缩规则的类型来决定，可用于触发伸缩活动或者智能设置伸缩组边界值。创建好伸缩组后，您需要设置伸缩规则来实现手动或自动伸缩ECS实例或者ECI实例资源。 通过设置伸缩规则，您可以触发伸缩组的伸缩活动或者智能设置伸缩组边界值。在计算和执行过程中，伸缩规则可以根据伸缩组的最小实例数、最大实例数或者期望实例数自动调整增加或减少的ECS实例数。例如，伸缩规则中指定将伸缩组的ECS实例数调整至50台，但伸缩组最大实例数只支持45台，则整个伸缩规则会按调整至45台来计算和执行。 执行伸缩规则主要有以下几种方式： 手动执行。 通过定时任务执行。更多信息，请参见配置定时任务。 通过报警任务执行。更多信息，请参见报警任务概述。 伸缩活动在执行伸缩规则、手动添加或移出已有ECS实例时，均会触发伸缩活动实现扩张或收缩操作，用来描述伸缩组内实例的变化情况。伸缩活动的生命周期为判断伸缩组健康状态和边界条件和启动cooldown步骤间的所有活动，步骤及顺序如下： 判断伸缩组的健康状态、边界条件和ECS实例的状态、类型。 分配Activityld和执行伸缩活动。 加入ECS实例。 修改Total Capacity. 添加RDS白名单。 挂载负载均衡，将权重设为当前伸缩组中已激活的伸缩配置上指定的“负载均衡权重”。此处使用了伸缩配置上指定的负载均衡权重”。 伸缩活动完成，启动cooldown。 冷却时间自动触发的伸缩活动有冷却时间，冷却时间指伸缩组成功执行伸缩活动后的一段锁定时间。在冷却时间内，伸缩组会拒绝由报警任务触发的伸缩活动请求。但非报警任务（手动执行任务、定时任务、健康检查等）触发的伸缩活动可以立即执行，绕过冷却时间。 冷却时间规则 冷却时间不能为空，如果不配置使用默认冷却时间。 如果在伸缩活动中，多台ECS实例加入或者移出伸缩组，则从最后一台ECS实例成功加入或者移出伸缩组后，弹性伸缩服务开始计算冷却时间。如果在伸缩活动中，没有ECS实例成功加入或者移出伸缩组，则弹性伸缩服务不会开始计算冷却时间。 如果您停用再启用伸缩组，伸缩组启用后的首次伸缩活动可以立即执行，不会受冷却时间影响。当伸缩组启用后首次成功执行伸缩活动，弹性伸缩服务才开始计算冷却时间。 生命周期挂钩生命周期挂钩是一个管理伸缩组内ECS实例或ECI实例生命周期的工具。弹性伸缩会自动触发扩缩容活动，并触发生命周期挂钩使伸缩活动中的ECS实例或ECI实例处于挂起中的状态（即等待的状态），为您保留一段自定义操作的时间，直至生命周期挂钩超时结束。 最佳实践生命周期挂钩和OOS模板通过生命周期挂钩，您能够在扩缩容流程中挂起ECS实例，执行自定义操作后再使用或者释放ECS实例。运维编排服务（OOS）是阿里云提供的云上自动化运维服务，能够自动化管理和执行任务。您可以通过模板定义执行任务、执行顺序、执行输入和输出，然后执行模板完成一组运维操作。 使用限制功能限制 部署在伸缩组内ECS实例或ECI实例上的应用必须是无状态并且可横向扩展的。 伸缩组内ECS实例或ECI实例可能会被自动释放，因此不适合保存会话记录、应用数据、日志等信息。如有需要，您可以将会话记录等状态信息保存到独立的状态服务器，并将应用数据保存到云数据库RDS，将日志存储到日志服务。 弹性伸缩不支持自动将ECS实例或ECI实例添加到Memcache实例的访问白名单，您需要自行添加。 如果某个伸缩组关联的RDS实例、ALB服务器组、CLB实例或者CLB实例的后端服务器组被删除，则伸缩组自动解除与该资源的关联。 如果某个伸缩组内自动触发的伸缩活动连续失败超过30天，弹性伸缩系统巡检会暂停该伸缩组自动触发伸缩活动的功能，并通过短信或者邮件向您发送通知。 数量限制ACP考试相关外延知识点 弹性伸缩只支持ECS的伸缩，不支持其他服务的伸缩 用户手动添加到伸缩组中的ECS实例不会停止和释放，弹性伸缩只会停止和释放自动创建的ECS. 参考资料首页&gt;弹性伸缩&gt;用户指南]]></content>
      <categories>
        <category>云计算</category>
      </categories>
      <tags>
        <tag>基础概念</tag>
        <tag>阿里云</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[WDL - 变量的数据结构]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-04.pipeline%2Fpipeline-frameworks-WDL-4.%E5%8F%98%E9%87%8F%E7%9A%84%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%2F</url>
    <content type="text"><![CDATA[版本在之前的介绍中，我们介绍了wdl的整体框架，和任务调度过程中涉及的执行逻辑，同时我们也了解了如何投递一个简单的任务。但是实际肯定不只是hello word。而在面对实际应用过程中，我们往往需要进行一些更复杂的数据处理和业务逻辑的实现。而为了提高整体代码的可理解性，更好的标识一些特定关联数据之间的逻辑关系和数据类型，所以和其他编程语言一样，我们需要用到一些数据结构当然也伴随会使用到一些基础的数据处理。当然同样的和其他编程语法一样，WDL也存在不同版本间的语法差异，因此在使用WDL进行流程撰写时，需要声明版本。本文中若无特殊说明，均是基于 WDL v1.0 版本进行的介绍。 截至目前几个主要的版本分别是：draft-3v1.0v1.1 截止202301，cromwell还不支持该版本。 语法规则变量变量类型基础变量(primitive types )这个比较好理解，就是基本在所有编程语言中都会涉及的一些基础类型，比如整数型、浮点数、布尔值、字符串。但是除了这四种比较常见的，还有一个相对比较特殊的类型 File类型，这是因为wdl在后期任务执行过程中，会涉及到容器的使用，容器使用过程中，就会涉及到一些数据文件的挂载（宿主机和虚拟机的数据不再可以直接进行访问）。另一方面如果输出类型是File的话，调度系统会对校验文件是否存在。 12345Int i = 0 # An integer valueFloat f = 27.3 # A floating point numberBoolean b = true # A boolean true/falseString s = &quot;hello, world&quot; # A string valueFile f = &quot;path/to/file&quot; # A file 复合变量(compound types)符合变量，也会有比较多，比如 Array类似其他编程语言中的属组/序列， Map类似其他编程语言中的字典/哈希，Object就相当于其他语言中的对象（有属性，但是没方法┑(￣Д ￣)┍）。其中比较特殊的类型是Pair，可以理解成只有两个元素的Array，也可以理解成直接两个隐藏key（left，right）的 Map，在NGS领域的话，倒是很适合存储成对的Fastq数据分别对应Fq1和Fq2。 1234567891011121314# An array of XsArray[X] xs = [x1, x2, x3] # A map from Ps to YsMap[P,Y] p_to_y = &#123; p1: y1, p2: y2, p3: y3 &#125; # Object keys are always `String`sObject o = &#123; &quot;field1&quot;: f1, &quot;field2&quot;: f2 &#125; # A pair of one X and one YPair[X,Y] x_and_y = (x, y) # 符合变量的套娃使用Array[Array[X]] xs = [[x1, x2, x3],[y1, y2, y3]] Arrays数组 数组的定义和访问可以参考类似python的语法，例如： 123456# An array of XsArray[String] a = [&quot;a&quot;, &quot;b&quot;, &quot;c&quot;]Array[Int] b = [0,1,2]# 访问属组中的指定元素String x = a[2] # 通过索引访问数组特定位置的值，所以索引必需是整数 Map字典 字典的定义和访问可以参考类似python的语法，例如： 1234567# A map from Ps to YsMap[String, Int] = &#123;&quot;a&quot;: 1, &quot;b&quot;: 2&#125;Map[Int, Int] = &#123;1: 10, 2: 11&#125;Map[P,Y] p_to_y = &#123; p1: y1, p2: y2, p3: y3 &#125; # key 和value的类型可以是声明的自定义结构。# 访问字典中的指定元素value = p_to_y[p1] # 通过key访问字典对应key的值，key必须是map中存在的 Object对象Object的定义和map比较类似，数据的访问可以通过 x.y 的方式访问， 其中 x 必须是一个对象或者是一个 workflow中的 task 。一个Task可以被视为一个对象，而Task的属性就是一个Task中的 output的内容，例如： 123456789101112131415161718 Object o = &#123; &quot;field1&quot;: f1, &quot;field2&quot;: f2 &#125; # Object keys are always `String`s workflow wf &#123; input &#123; Object obj Object foo &#125; call foo &#123; input: var=obj.attr # 通过对象 obj 的属性attr，访问获得对象的特定属性值。 &#125; call foo as foo2 &#123; input: var=foo.out # foo.out 通过一个call生成的对象访问对应task执行阶段生成的output中的值 &#125;&#125; Pair配对配对的类型其他语言比较少见，是一个固定格式的，传入两个值的类型，同时可以通过 x.left and x.right 获取左侧和右侧的值，例如 1234Pair[Struct_X,Struct_Y] x_and_y = (x, y) # A pair of one X and one YStruct_X x = x_and_y.left # 获取Pair的左侧/第一个值Struct_Y y = x_and_y.right # 获取Pair的右侧/第二个值 自定义结构（Struct Definition）struct 是一种类似c的构造，它允许用户创建由先前存在的类型组成的新的复合类型。然后，可以在Task或Workflow定义中使用struct作为声明来代替任何其他常规类型。在许多情况下，结构体替代了Object类型，并允许对其成员进行适当的类型设置。 1struct SampleData&#123;&#125; 复合类型还可以在结构中使用，以便轻松地将它们封装在单个对象中。 变量的调用访问Objectwdl本身提供了多种读取文件信息生成Object的接口（read_tsv、read_json、read_map)等。部分示例如下：Config_File_software：123pipeline_root /home/liubo4/Project/aio.wdl/Gdc/aio.NewAnnotation/java bin/java_v1.8tabix bin/tabix 要在wdl中提取上述文件信息123456789101112workflow wf_echo &#123; input&#123; String Config_File_software Object software = read_map(Config_File_software)# 方式一：直接通过Config_File_software文件的第一列值作为属性直接提取； pipeline_root = software.pipeline_root# 方式二： 通过中间变量提取，这种方式可以高效的处理一些差异化配置需求； String tmp="pipeline_root" pipeline_root = software[tmp] &#125;&#125; 在一些复杂业务中，可以使用json格式的输入文件，提供丰富的数据结构。包括将json文件内容解析为我们自己定义的 struct 类型 遍历类型由于WDL语言本身的应用场景，遍历使用的环境并不多，基本上和 scatter 函数绑定使用了，从而实现分析任务的拆解和并行处理。对应的支持遍历的类型主要是两种，分别是Array和Map类型。遍历Array的时候，每次返回一个Array内部的元素类型，和其他编程语言类似，比较好理解。而遍历Map不是和其他语言一样分别返回key和value。而是会返回一个wdl特有的Pair类型的数据[key,value]对，然后我们可以通过 Pair.left 或 Pair.left 实现key、value获取。参考12345678910111213141516171819202122 # 遍历Map类型数据 Map[String, Int] map scatter (pair in map) &#123; String key = pair.left Int value = pair.right &#125; output &#123; # Automatically gathered from inside the scatter: Array[String] keys = key Array[Int] values = value &#125; # 遍历属组 Array[String] ArrayList scatter (Str_info in ArrayList) &#123; String value = Str_info &#125; output &#123; # Automatically gathered from inside the scatter: Array[Int] values = value &#125;&#125; 变量的可选参数Optional Parameters &amp; Type Constraints？？ 表示该参数是可选的。 用户无需指定参数值即可满足工作流的所有输入。可以实现一些可选参数的配置12345678910111213task test &#123; input &#123; String? val # 设置了一个可选参数 &#125; command &#123; # python script.py --val=$&#123;val&#125; # 错误的示例，这个示例会执行 python script.py --val= 导致以外错误 python script.py $&#123;&quot;--val=&quot; + val&#125; # 可选参数不传入时，则运行阶段也不会使用该参数的配置 &#125;&#125; ++ 仅适用于Array类型，它表示数组值必须包含一个或多个元素的约束。比如我们的下机数据，下机数据文件时不确定的，可能有多对Fastq，可能只有一对Fastq，但是不能没有。1Array[Pair[Fq1,Fq2]]+ SequenceData 关于每一种变量的使用，以及 WDL 的更多使用技巧，请参考官方规范文档。 表达式(Expressions) LHS Type Operators RHS Type Result Semantics Boolean == Boolean Boolean Boolean != Boolean Boolean Boolean &gt; Boolean Boolean Boolean &gt;= Boolean Boolean Boolean &lt; Boolean Boolean Boolean &lt;= Boolean Boolean Boolean `\ \ ` Boolean Boolean Boolean &amp;&amp; Boolean Boolean File + File File Append file paths File == File Boolean File != File Boolean File + String File File == String Boolean File != String Boolean Float + Float Float Float - Float Float Float * Float Float Float / Float Float Float % Float Float Float == Float Boolean Float != Float Boolean Float &gt; Float Boolean Float &gt;= Float Boolean Float &lt; Float Boolean Float &lt;= Float Boolean Float + Int Float Float - Int Float Float * Int Float Float / Int Float Float % Int Float Float == Int Boolean Float != Int Boolean Float &gt; Int Boolean Float &gt;= Int Boolean Float &lt; Int Boolean Float &lt;= Int Boolean Float + String String Int + Float Float Int - Float Float Int * Float Float Int / Float Float Int % Float Float Int == Float Boolean Int != Float Boolean Int &gt; Float Boolean Int &gt;= Float Boolean Int &lt; Float Boolean Int &lt;= Float Boolean Int + Int Int Int - Int Int Int * Int Int Int / Int Int Integer division Int % Int Int Integer division, return remainder Int == Int Boolean Int != Int Boolean Int &gt; Int Boolean Int &gt;= Int Boolean Int &lt; Int Boolean Int &lt;= Int Boolean Int + String String String + Float String String + Int String String + String String String == String Boolean String != String Boolean String &gt; String Boolean String &gt;= String Boolean String &lt; String Boolean String &lt;= String Boolean - Float Float + Float Float - Int Int + Int Int ! Boolean Boolean 参考 变量赋值时的逻辑判断有些时候，我们需要根据输入的数据，根据一些逻辑规则完成相应参数的赋值情况，这时候，我们可以使用 if … then … else … 的方式，例如： 1234Int array_length = length(array)runtime &#123; memory: if array_length &gt; 100 then &quot;16GB&quot; else &quot;8GB&quot;&#125; 参考该指标，可以根据数据量动态的给每个任务分配内存。]]></content>
      <categories>
        <category>pipeline</category>
        <category>framework</category>
        <category>任务调度</category>
      </categories>
      <tags>
        <tag>流程开发</tag>
        <tag>WDL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[WDL - 架构简介]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-04.pipeline%2Fpipeline-frameworks-WDL-1.%E5%85%B6%E4%BB%96%E7%BB%84%E4%BB%B6%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[在前面的文章中，我们介绍了 WDL 的5个基本组件，掌握这五个基本组件（workflow、task、call、output、runtime）已经可以帮助我们完成流程的开发了。但是于此同时，wdl还有一些特殊的组件和特性，可以让我们更便捷的实现一些复杂功能。在这里，我们将介绍一些特殊的组件和特性。 commandcommand 组件是 task 必需属性。 command 块的正文指定要运行的命令行指令（基本上是可以在终端 shell 中运行的任何命令），占位符（例如， ~{input_file} ）用于需要填写的命令行的可变部分。请注意，必须在 task 输入定义中定义所有变量占位符。 12345command &lt;&lt;&lt; java -jar myExecutable.jar \ INPUT = ~&#123;input_file&#125; \ OUTPUT = ~&#123;output_basename&#125;.txt&gt;&gt;&gt; meta meta 组件是工作流或任务的可选属性。它旨在存储元数据，例如作者或联系人电子邮件。可以使用任何键/值对。1234meta &#123; author: "Broad Institute Data Science Platform"， pipeline_version: "V1.0.1"&#125; Parameter_metaparameter_meta 组件是工作流或任务的可选属性。它可以用来为工作流或任务中使用的输入参数和输出参数提供元数据信息，如果您的变量名称描述性不够，可以通过 parameter_meta 对参数进行描述说明。因此 parameter_meta 中的任何项目（即 input_file ）都必须与工作流或任务的输入或输出相对应，具体取决于 parameter_meta 组件的使用位置。1234parameter_meta &#123; my_input: "Input file to be analyzed" name: "Name of the sample"&#125; 参考文档WDL参考文档 Broad WDL 官方论坛 WDL项目仓库 OpenWDL WDL-readthedoc 标准流程描述语言 WDL 阿里云最佳实践 在学习编写 WDL 的过程中，可以参考 Broad 官方的一些 GATK工作流 借鉴和学习 WDL 的用法。]]></content>
      <categories>
        <category>pipeline</category>
        <category>framework</category>
        <category>任务调度</category>
      </categories>
      <tags>
        <tag>流程开发</tag>
        <tag>WDL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[WDL - 一些可以复用代码示例]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-04.pipeline%2Fpipeline-frameworks-WDL-6.demo%E7%A4%BA%E4%BE%8B%2F</url>
    <content type="text"><![CDATA[和其他编程语法一样，WDL也存在不同版本间的语法差异，因此在使用WDL进行流程撰写时，需要明确参考的版本规范所以后文语法如无特殊标准，本文想过内容均基于 WDL v1.0 版本。 示例Demo存档可以看到WDL本身提供了比较充足的变量类型，但是由于wdl本身对于文本/变量的处理函数非常匮乏，因此无法像snakemake一样，在读取数据后进行自定义的数据处理构造需要的数据结构。因此针对wdl应用到生信检测过程时，一些复杂的逻辑关系需要提前梳理形成特定的数据结构，本文记录一些学习开发阶段，接触的复杂数据结构和 WDL 解析方式，以备后用。 项目结构1234567docker/ # 镜像文件config/ # 配置文件structs/ # 数据结构subworkflow/ # 子流程main.wdlmain.input.jsonReadme.md 样本文库下机数据多层依赖关系最终生效的输入文件格式（json），通过wdl解析成object + 数组，实现多层结构实现。 单样本解析输入的文本文件.json12345678910111213&#123; "cancer":[&#123; "LibID":"1C-Lib-1", "LibData":[["FastqA1","FastqA2"],["FastqB1","FastqB2"],["FastqC1","FastqC2"],["FastqD1","FastqD2"]] &#125;,&#123; "LibID":"1C-Lib-2", "LibData":[["L2FastqA1","L2FastqA2"],["L2FastqB1","L2FastqB2"],["L2FastqC1","L2FastqC2"]] &#125;], "normal":[&#123; "LibID":"1N-Lib-1", "LibData":[["NFastqA1","NFastqA2"],["NFastqB1","NFastqB2"],["NFastqC1","NFastqC2"]] &#125;]&#125; 解析脚本.wdl 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566version 1.0workflow wf_echo &#123; input&#123; File data &#125; Object All_Sample = read_json(data) scatter ( singleLin in All_Sample.cancer)&#123; String lib = singleLin.LibID Array[Array[String]] FastqList = singleLin.LibData scatter (singleFastq in FastqList)&#123; Array[String] Fastq = singleFastq call echoa as cancer_fastq&#123; input: Fastq=Fastq &#125; &#125; call Singlelib as Cancer_lib&#123; input: lib = lib, Fastq = cancer_fastq.outputa &#125; &#125; call Singlelib as Cancer_sample&#123; input: lib = &quot;All_cancer&quot;, Fastq = Cancer_lib.Tag &#125;&#125;task Singlelib &#123; input&#123; String lib Array[String] Fastq &#125; String out = lib + &quot;.txt&quot; String sample = lib command &lt;&lt;&lt; echo ~&#123;sep=&quot;,&quot; Fastq&#125; &gt; ~&#123;sample&#125;.txt &gt;&gt;&gt; output &#123; String outFile = out String Tag = lib &#125;&#125;task echoa &#123; input&#123; Array[String] Fastq &#125; String Fastq1=Fastq[0] String Fastq2=Fastq[1] command &lt;&lt;&lt; echo ~&#123;Fastq1&#125; ~&#123;Fastq2&#125; &gt;&gt;&gt; output&#123; String outputa=Fastq1+&quot;:&quot;+Fastq2 &#125;&#125; 执行示例脚本12345678910111213141516171819202122232425262728293031323334353637[2023-02-09 20:13:43,87] [info] BackgroundConfigAsyncJobExecutionActor [70365163wf_echo.cancer_fastq:0:1]: echo L2FastqA1 L2FastqA2[2023-02-09 20:13:43,87] [info] BackgroundConfigAsyncJobExecutionActor [3956881dwf_echo.cancer_fastq:3:1]: echo FastqD1 FastqD2[2023-02-09 20:13:43,87] [info] BackgroundConfigAsyncJobExecutionActor [70365163wf_echo.cancer_fastq:2:1]: echo L2FastqC1 L2FastqC2[2023-02-09 20:13:43,87] [info] BackgroundConfigAsyncJobExecutionActor [3956881dwf_echo.cancer_fastq:2:1]: echo FastqC1 FastqC2[2023-02-09 20:13:43,87] [info] BackgroundConfigAsyncJobExecutionActor [3956881dwf_echo.cancer_fastq:0:1]: echo FastqA1 FastqA2[2023-02-09 20:13:43,87] [info] BackgroundConfigAsyncJobExecutionActor [70365163wf_echo.cancer_fastq:1:1]: echo L2FastqB1 L2FastqB2[2023-02-09 20:13:43,87] [info] BackgroundConfigAsyncJobExecutionActor [3956881dwf_echo.cancer_fastq:1:1]: echo FastqB1 FastqB2[2023-02-09 20:13:50,36] [info] 70365163-898e-417f-b7a7-7ce6866b4dae-SubWorkflowActor-SubWorkflow-ScatterAt45_12:1:1 [70365163]: Workflow ScatterAt45_12 complete. Final Outputs:&#123; &quot;cancer_fastq.outputa&quot;: [&quot;L2FastqA1:L2FastqA2&quot;, &quot;L2FastqB1:L2FastqB2&quot;, &quot;L2FastqC1:L2FastqC2&quot;], &quot;Fastq&quot;: [[&quot;L2FastqA1&quot;, &quot;L2FastqA2&quot;], [&quot;L2FastqB1&quot;, &quot;L2FastqB2&quot;], [&quot;L2FastqC1&quot;, &quot;L2FastqC2&quot;]]&#125;[2023-02-09 20:13:50,36] [info] 3956881d-5958-469c-bd04-e4f41509fd51-SubWorkflowActor-SubWorkflow-ScatterAt45_12:0:1 [3956881d]: Workflow ScatterAt45_12 complete. Final Outputs:&#123; &quot;Fastq&quot;: [[&quot;FastqA1&quot;, &quot;FastqA2&quot;], [&quot;FastqB1&quot;, &quot;FastqB2&quot;], [&quot;FastqC1&quot;, &quot;FastqC2&quot;], [&quot;FastqD1&quot;, &quot;FastqD2&quot;]], &quot;cancer_fastq.outputa&quot;: [&quot;FastqA1:FastqA2&quot;, &quot;FastqB1:FastqB2&quot;, &quot;FastqC1:FastqC2&quot;, &quot;FastqD1:FastqD2&quot;]&#125;[2023-02-09 20:14:03,75] [info] BackgroundConfigAsyncJobExecutionActor [00e87cbbwf_echo.Cancer_lib:1:1]: echo L2FastqA1:L2FastqA2,L2FastqB1:L2FastqB2,L2FastqC1:L2FastqC2 &gt; 1C-Lib-2.txt[2023-02-09 20:14:03,75] [info] BackgroundConfigAsyncJobExecutionActor [00e87cbbwf_echo.Cancer_lib:0:1]: echo FastqA1:FastqA2,FastqB1:FastqB2,FastqC1:FastqC2,FastqD1:FastqD2 &gt; 1C-Lib-1.txt[2023-02-09 20:14:13,74] [info] BackgroundConfigAsyncJobExecutionActor [00e87cbbwf_echo.Cancer_sample:NA:1]: echo 1C-Lib-1,1C-Lib-2 &gt; All_cancer.txt[2023-02-09 20:14:18,89] [info] WorkflowExecutionActor-00e87cbb-2f53-4a0e-ae74-4ff2d8eaf88c [00e87cbb]: Workflow wf_echo complete. Final Outputs:&#123; &quot;wf_echo.cancer_fastq.outputa&quot;: [[&quot;FastqA1:FastqA2&quot;, &quot;FastqB1:FastqB2&quot;, &quot;FastqC1:FastqC2&quot;, &quot;FastqD1:FastqD2&quot;], [&quot;L2FastqA1:L2FastqA2&quot;, &quot;L2FastqB1:L2FastqB2&quot;, &quot;L2FastqC1:L2FastqC2&quot;]], &quot;wf_echo.lib&quot;: [&quot;1C-Lib-1&quot;, &quot;1C-Lib-2&quot;], &quot;wf_echo.Cancer_sample.Tag&quot;: &quot;All_cancer&quot;, &quot;wf_echo.Fastq&quot;: [[[&quot;FastqA1&quot;, &quot;FastqA2&quot;], [&quot;FastqB1&quot;, &quot;FastqB2&quot;], [&quot;FastqC1&quot;, &quot;FastqC2&quot;], [&quot;FastqD1&quot;, &quot;FastqD2&quot;]], [[&quot;L2FastqA1&quot;, &quot;L2FastqA2&quot;], [&quot;L2FastqB1&quot;, &quot;L2FastqB2&quot;], [&quot;L2FastqC1&quot;, &quot;L2FastqC2&quot;]]], &quot;wf_echo.Cancer_lib.Tag&quot;: [&quot;1C-Lib-1&quot;, &quot;1C-Lib-2&quot;], &quot;wf_echo.Cancer_lib.outFile&quot;: [&quot;1C-Lib-1.txt&quot;, &quot;1C-Lib-2.txt&quot;], &quot;wf_echo.Cancer_sample.outFile&quot;: &quot;All_cancer.txt&quot;, &quot;wf_echo.FastqList&quot;: [[[&quot;FastqA1&quot;, &quot;FastqA2&quot;], [&quot;FastqB1&quot;, &quot;FastqB2&quot;], [&quot;FastqC1&quot;, &quot;FastqC2&quot;], [&quot;FastqD1&quot;, &quot;FastqD2&quot;]], [[&quot;L2FastqA1&quot;, &quot;L2FastqA2&quot;], [&quot;L2FastqB1&quot;, &quot;L2FastqB2&quot;], [&quot;L2FastqC1&quot;, &quot;L2FastqC2&quot;]]]&#125; 批次多样本解析wdl在解析json时，最外层总会强制解析成一个object（输入 list 也无法识别为Array进行scatter操作），所以顶层必须使用object。12345678910111213141516171819202122232425262728&#123;"All_Sample":[&#123; "sampleID":"SampleA", "cancer":[&#123; "LibID":"1C-Lib-1", "LibData":[["FastqA1","FastqA2"],["FastqB1","FastqB2"],["FastqC1","FastqC2"],["FastqD1","FastqD2"]] &#125;,&#123; "LibID":"1C-Lib-2", "LibData":[["L2FastqA1","L2FastqA2"],["L2FastqB1","L2FastqB2"],["L2FastqC1","L2FastqC2"]] &#125;], "normal":[&#123; "LibID":"1N-Lib-1", "LibData":[["NFastqA1","NFastqA2"],["NFastqB1","NFastqB2"],["NFastqC1","NFastqC2"]] &#125;]&#125;,&#123; "sampleID":"SampleB", "cancer":[&#123; "LibID":"1C-Lib-1", "LibData":[["FastqA1","FastqA2"],["FastqB1","FastqB2"],["FastqC1","FastqC2"],["FastqD1","FastqD2"]] &#125;,&#123; "LibID":"1C-Lib-2", "LibData":[["L2FastqA1","L2FastqA2"],["L2FastqB1","L2FastqB2"],["L2FastqC1","L2FastqC2"]] &#125;], "normal":[&#123; "LibID":"1N-Lib-1", "LibData":[["NFastqA1","NFastqA2"],["NFastqB1","NFastqB2"],["NFastqC1","NFastqC2"]] &#125;]&#125;]&#125; 解析脚本123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081version 1.0workflow wf_echo &#123; input&#123; File data &#125; Object All_Sample = read_json(data) scatter (singlesample in All_Sample.All_Sample)&#123; String sampleID = singlesample.sampleID scatter (singleLin in singlesample.cancer)&#123; String Type = &quot;cancer&quot; String lib = singleLin.LibID Array[Array[String]] FastqList = singleLin.LibData scatter (singleFastq in FastqList)&#123; Array[String] Fastq = singleFastq call echoa as cancer_fastq&#123; input: Fastq=Fastq &#125; &#125; call Singlelib as Cancer_lib&#123; input: sample = sampleID, Type = Type, lib = lib, Fastq = cancer_fastq.outputa &#125; &#125; call Singlelib as Cancer_sample&#123; input: sample = sampleID, Type = &quot;cancer&quot;, lib = &quot;All_lib&quot;, Fastq = Cancer_lib.Tag &#125; &#125; call Singlelib as sample_level&#123; input: sample = &quot;All_sample&quot;, Type = &quot;All_type&quot;, lib = &quot;All_case&quot;, Fastq = Cancer_sample.Tag &#125;&#125;task Singlelib &#123; input&#123; String sample String Type String lib Array[String] Fastq &#125; String out = lib + &quot;.txt&quot; command &lt;&lt;&lt; echo ~&#123;sep=&quot;,&quot; Fastq&#125; &gt; ~&#123;sample&#125;.txt &gt;&gt;&gt; output &#123; String outFile = out String Tag = lib &#125;&#125;task echoa &#123; input&#123; Array[String] Fastq &#125; String Fastq1=Fastq[0] String Fastq2=Fastq[1] command &lt;&lt;&lt; echo ~&#123;Fastq1&#125; ~&#123;Fastq2&#125; &gt;&gt;&gt; output&#123; String outputa=Fastq1+&quot;:&quot;+Fastq2 &#125;&#125;]]></content>
      <categories>
        <category>pipeline</category>
        <category>framework</category>
        <category>任务调度</category>
      </categories>
      <tags>
        <tag>流程开发</tag>
        <tag>WDL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[WDL - 内置函数]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-04.pipeline%2Fpipeline-frameworks-WDL-5.%E5%86%85%E7%BD%AE%E5%87%BD%E6%95%B0%2F</url>
    <content type="text"><![CDATA[版本和其他编程语法一样，WDL也存在不同版本间的语法差异，因此在使用WDL进行流程撰写时，需要明确参考的版本规范draft-3v1.0v1.1 截止202301，cromwell还不支持该版本。所以后文语法如无特殊标准，均基于 WDL v1.0 版本。 通过之前的介绍，我们了解 wdl的整体框架组成、任务调度过程中涉及的执行逻辑、如何投递一个简单的任务以及我们在编写WDL流程过程中涉及的数据结构。基于wdl本身的使用场景（串接一个个数据处理的任务，而不是自己进行数据处理），所以至此，我们已经原则上可以实现各种pipeline的开发了。当然这种可以实现本身是不优雅的，因为我们不可能对所有简单的数据处理，都单独撰写一个task任务，一方面会让代码冗长，另一方面，这些冗余的操作会影响代码的执行逻辑，也会降低执行阶段的执行效率。所以今天我们来了解一些wdl本身支持的内置函数，让我们开发的流程显得尽可能优雅一些。 读取文件pipeline构建完成后，我们可以像流程投递中介绍的，每次分析不同的样本，我们都单独构建生成一个输入的input.json文件。但是这有两个问题，一个是input里面会包含一些非样本的配置信息重复生成很繁琐，另一个方面是手动构造json格式文件并不是一个人类有好的工作。所以我们这时候，就需要做两部分，首先是将每次分析会更换的样本信息独立开，通过一个独立的文件而不是input.json进行提供，另一方面就是结合我们上游数据来源确定一个更人类友好的样本信息配置文件（如果是机器生成可以是json，如果是人准备可以是tsv）。这时候我们就需要在wdl内部进行一些文件读取的操作，wdl提供了几个文件读取的参数，read_tsv、read_json、read_int、read_lines、read_map、read_object、read_float、read_string read_tsvread_tsv()函数用于读取tsv文件，返回一个二维数组 Array[Array[String]]。可以直接使用二维属组的方式进行数值的获取。但是使用tsv有一个比较麻烦的问题，就是不支持title的识别，所以只能按index获取数据。下面是一个示例： 这是我们需要在wdl中进行读取的文件：输入的文本文件.tsv12345sample_id fq1Path fq2Path****059 /V350201891_L01_51_1.fq.gz /V350201891_L01_51_2.fq.gz****571 /V350201891_L01_44_1.fq.gz /V350201891_L01_44_2.fq.gz****851 /V350201891_L01_54_1.fq.gz /V350201891_L01_54_2.fq.gz****853 /V350201891_L01_56_1.fq.gz /V350201891_L01_56_2.fq.gz 那么在wdl中，我们可以通过如下方式进行数据的读取：12345678# 解析tsv并转换成 Array[Array[String]]Array[Array[String]] batch_info = read_tsv("输入的文本文件.tsv")# 按行遍历输入的数据scatter (sample in batch_info)&#123; # 针对每行的数据，通过列索引获取对应的信息。 sample_id =sample[0] fq1=sample[1] fq2=sample[2] read_jsonwdl 中可以通过read_json函数读取json文件。读取后，根据json的数据类型，会保存成对应的wdl结构数据。json类型和wdl类型对应关系如下：| JSON Type | WDL Type || ——— | —————- || object | Map[String, ?] || array | Array[?] || number | Int or Float || string | String || boolean | Boolean || null | ??? || Pair | object || Map | object || Struct | object | 读取json文件的示例如下：input_file.json123456789101112&#123;"cancer":[&#123; "LibID":"1C-Lib-1", "LibData":[["FastqA1","FastqA2"],["FastqB1","FastqB2"],["FastqC1","FastqC2"],["FastqD1","FastqD2"]] &#125;,&#123; "LibID":"1C-Lib-2", "LibData":[["L2FastqA1","L2FastqA2"],["L2FastqB1","L2FastqB2"],["L2FastqC1","L2FastqC2"]] &#125;],"normal":[&#123; "LibID":"1N-Lib-1", "LibData":[["NFastqA1","NFastqA2"],["NFastqB1","NFastqB2"],["NFastqC1","NFastqC2"]] &#125;]&#125; 解析脚本.wdl12345678Object All_Sample = read_json(input_file.json)# All_Sample.cancer 获取json中 cancer对应的列表，并通过scatter对这个列表进行遍历，依次处理每个文库scatter ( singleLin in All_Sample.cancer)&#123; # 对每个文库（json的字典），转换成 object（wdl的结构），通过属性获取文库对应的值（LibID或LibData） String lib = singleLin.LibID # 将 LibData的值[["FastqA1","FastqA2"],["FastqB1","FastqB2"],["FastqC1","FastqC2"],["FastqD1","FastqD2"]]，直接保存成一个wdl的对象结构式 Array[Array[String]] Array[Array[String]] FastqList = singleLin.LibData 其他 read_map 读取两列的tsv文件，并将两列 TSV 解释为 Map[String, String] 1Map[String, String] mapping = read_map( File/String ) read_lines 按行读取，并把多行存储到一个字符串构成的数组中. 1Array[String] matches = read_lines( File/String ) read_float、read_string 读取一个文件路径，预期只有一行和一个值（对应需要读取的类型） 输出文件write_lines给定与 Array[String] 兼容的内容，这会将每个元素写入文件中自己的行。使用换行符 \n 字符作为行分隔符。1234567891011task example &#123; Array[String] array = [&quot;first&quot;, &quot;second&quot;, &quot;third&quot;] command &#123; ./script --file-list=$&#123;write_lines(array)&#125; &#125;&#125;# 生成文件格式如下：firstsecondthird write_tsv给定与 Array[Array[String]] 兼容的内容，这会写入数据结构的 TSV 文件。12345678910task example &#123; Array[String] array = [[&quot;one&quot;, &quot;two&quot;, &quot;three&quot;], [&quot;un&quot;, &quot;deux&quot;, &quot;trois&quot;]] command &#123; ./script --tsv=$&#123;write_tsv(array)&#125; &#125;&#125;# 生成文件文件格式如下：one\ttwo\tthreeun\tdeux\ttrois write_json给定任何类型的数据，write_json 会写入对应结构的json文件。对应关系参见read_json()定义中的表格。1234567891011121314task example &#123; input &#123; Map[String, String] map = &#123;&quot;key1&quot;: &quot;value1&quot;, &quot;key2&quot;: &quot;value2&quot;&#125; &#125; command &#123; ./script --map=$&#123;write_json(map)&#125; &#125;&#125;#生成的json文件示例：&#123; &quot;key1&quot;: &quot;value1&quot;, &quot;key2&quot;: &quot;value2&quot;&#125; write_objects给定任何 Array[Object] ，这将写出一个 2+ 行、n 列的 TSV 文件，其中包含每个对象的属性和值。1234567891011task test &#123; input &#123; Array[Object] in &#125; command &lt;&lt;&lt; /bin/do_work --obj=~&#123;write_objects(in)&#125; &gt;&gt;&gt; output &#123; File results = stdout() &#125;&#125; Attribute 属性 Value 价值 key_1 “value_1” key_2 “value_2” key_3 “value_3” 上述格式的object对象生成文件后，格式如下：12key_1\tkey_2\tkey_3value_1\tvalue_2\tvalue_3 统计大小统计文件容量 size12345678910# 统计文件的大小Float input_file_size = size(input_file)# 统计文件大小指定单位（支持 kB、MB、GB、TB)Float created_file_size_in_KB = size(&quot;created_file&quot;, &quot;K&quot;) # 0.022# 输入的是数组，则返回属组中所有文件大小的总和Float size(Array[File], [String])# 字符串处理 统计属组长度大小1234567Array[Int] xs = [ 1, 2, 3 ]Array[String] ys = [ &quot;a&quot;, &quot;b&quot;, &quot;c&quot; ]Array[String] zs = [ ]Integer xlen = length(xs) # 3Integer ylen = length(ys) # 3Integer zlen = length(zs) # 0 字符串相关判断是否被定义 defined如果参数是未设置的可选值，则此函数将返回 false 。在所有其他情况下它将返回 true 。 字符串基于正则进行替换 subsub(input, pattern, replace) 给定 3 个字符串参数 input 、 pattern 、 replace ，此函数将使用replace的呢日哦那个替换 input 中匹配 pattern 的任何匹配项 。 pattern 应该是一个正则表达式。正则表达式评估的详细信息将取决于运行 WDL 的执行引擎。123456String chocolike = &quot;I like chocolate when it&apos;s late&quot;String chocolove = sub(chocolike, &quot;like&quot;, &quot;love&quot;) # I love chocolate when it&apos;s lateString chocoearly = sub(chocolike, &quot;late&quot;, &quot;early&quot;) # I like chocoearly when it&apos;s earlyString chocolate = sub(chocolike, &quot;late$&quot;, &quot;early&quot;) # I like chocolate when it&apos;s early&#125; 添加字符串前缀 prefix给定一个 String 和一个 Array[X]，其中 X 是基本类型， prefix 函数返回一个字符串数组，该数组由输入数组的每个元素组成，并添加了指定的前缀字符串为前缀。例如：12345Array[String] env = [&quot;key1=value1&quot;, &quot;key2=value2&quot;, &quot;key3=value3&quot;]Array[String] env_param = prefix(&quot;-e &quot;, env) # [&quot;-e key1=value1&quot;, &quot;-e key2=value2&quot;, &quot;-e key3=value3&quot;]Array[Integer] env2 = [1, 2, 3]Array[String] env2_param = prefix(&quot;-f &quot;, env2) # [&quot;-f 1&quot;, &quot;-f 2&quot;, &quot;-f 3&quot;] 文件相关获取文件基本名称 basenamebasename 根据输入文件名（如输入文件名）命名输出文件名（剔除目录信息）的字符串。如果还给它一个字符串（后缀）作为辅助参数，该函数将尝试从文件名剔除后缀后返回剩余的任何内容。 1234File input_file = "/Users/chris/input.bam"String base = basename(input_file) # 返回 input.bamString stripped = basename(input_file, ".bam") # 返回 input 数值相关数组的筛选提取 select_first(Array[X?])给定一个可选值数组， select_first 将选择第一个被定义的值并返回它。请注意，这是一项运行时检查，要求至少存在一个定义的值。 select_all(Array[X?])给定一个可选值数组， select_all 将仅选择那些已定义的元素。 查找符合正则的文件 globGlob 可用于定义可能包含零个、一个或多个文件的输出。因此，glob 函数返回 File 输出的数组：123output &#123; Array[File] output_bams = glob(&quot;*.bam&quot;)&#125; 使用场景:输出文件有多个，文件具体名称不确定，但是知道文件命名的规则（正则表达式） 使用方法:采用 glob 表达式，用 array 方式存储多个输出文件 价值:输出结果支持通配符匹配，简化 WDL 编写，采用数组方式，方便并发处理 类型转换浮点数转整型 Floor(Float)向下舍入到下一个较小的整数 ceil(Float)向上舍入到下一个更大的整数 round(Float)根据标准舍入规则舍入到最接近的整数 zip 构建pair类型给定任意两个对象类型， zip 函数以 Pair 对象的形式返回这些对象类型的点积。123456Pair[Int, String] p = (0, &quot;z&quot;)Array[Int] xs = [ 1, 2, 3 ]Array[String] ys = [ &quot;a&quot;, &quot;b&quot;, &quot;c&quot; ]Array[String] zs = [ &quot;d&quot;, &quot;e&quot; ]Array[Pair[Int, String]] zipped = zip(xs, ys) # i.e. zipped = [ (1, &quot;a&quot;), (2, &quot;b&quot;), (3, &quot;c&quot;) ] 标准输出：stdout, stderr stdout()函数用于捕获command中命令生成的标准输出。 stderr()函数用于捕获command中命令生成的标准报错。stderr比stdout更常用，更多用于捕获warning信息12String out_info = stdout()String err_info = stderr()]]></content>
      <categories>
        <category>pipeline</category>
        <category>framework</category>
        <category>任务调度</category>
      </categories>
      <tags>
        <tag>流程开发</tag>
        <tag>WDL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[WDL - 任务执行路径]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-04.pipeline%2Fpipeline-frameworks-WDL-2.%E4%BB%BB%E5%8A%A1%E6%89%A7%E8%A1%8C%E7%BB%93%E6%9E%84%2F</url>
    <content type="text"><![CDATA[从上一篇文章，我们已经知道怎么构建一个简单的 workflow，接下来我们看看一个个任务在wdl中是如何构建形成一个流程的。以及我们如何让这个流程运行起来。 在 workflow 中，我们通过 call 调用了一系列的 task，但是这些被调用的 task 是如何执行的呢。毕竟当这些任务有些可以同时分析，有些任务则需要在其他任务完成后开始分析。这时候，细心的朋友如果稍微注意，会发现，我们之前在介绍 call 的时候，提到过 call 可以在不同任务之间进行参数的传递，而着就是wdl 内部进行任务连接到依据。 123456workflow myWorkflow &#123; call taskA&#123;...&#125; call taskB&#123; input_file = taskA.output &#125;&#125; 在我们上述示例中，taskB的运行需要taskA的输出作为输入。所以我们基于这样的输入输出关系，就可以构建出 taskA 和 taskB 之间的线性运行关系。当然除了简单的线性关系，我们在实际应用中，还会出现并串联的情况、流程分支选择的情况。这些特性都可以在 wdl 中实现。接下来，我们来了解下wdl支持的管道类型 wdl的管道类型介绍一个 workflow 里面包含多个 task，task 之前的串行或并行关系主要有下面三种情况： Linear Chaining在工作流中将任务链接在一起的最简单的方法就是线性链，我们将一个任务的输出提供给下一个任务的输入，如下所示： 12345678call stepB &#123; input: in = stepA.out &#125;call stepC &#123; input: in = stepB.out &#125; 因为 WDL 允许我们使用语法在另一个任务（实际上是块 workflow 中的其他任何地方）的 call 语句中引用任何任务的输出 task_name.output_variable （在任务 output 块中适当声明）。我们只需在调用中 stepB 时指定使用 stepA 指定的输出结果 stepA.out 作为 stepB 的输入值 in ，并且它与 的规则 stepC 相同。 Multi-input / Multi-output当然有时候，可能我们需要传递的输入/输出值并不是只有一个，这是 task 也可以定义多个输入和输出，比如上面的例子，taskB 有两个输出，作为 taskC 的输入。 12345call stepC &#123; input: in1 = stepB.out1, in2 = stepB.out2 &#125; 分支及合并输出连接到线性链接和多输入/多输出中描述的输入的能力依赖于分层命名，可以进一步扩展以将任务的输出定向到单独的路径，对它们执行某些操作，然后将分支路径合并在一起。12345678910111213call stepB &#123; input: in = stepA.out&#125;call stepC &#123; input: in = stepA.out&#125;call stepD &#123; input: in1 = stepC.out, in2 = stepB.out&#125; 在这里你可以看到，输出的 stepA 分别输入 stepB stepC 后产生不同的输出，然后我们一起输入 stepD 。 条件任务针对不同的数据进行分析时，有些步骤也许我们不想一直运行。这意味着我们需要根据某个判断标准在两条提供的两条路径之间切换（在模式 A 中运行工具与在模式 B 中运行工具）或完全跳过一个步骤（运行工具与不运行工具）。在这种情况下，我们将使用条件语句。123456if (shouldICallStepB) &#123; call stepB &#123; input: in = stepA.out &#125;&#125; 和其他编程语法一样，我们需要使用 if 进行判断，该 if 语句可以显式控制，就像我们在上面的示例中使用布尔变量所做的那样。除了作为开关机制之外，还可以通过测试其他变量的值来隐式控制它，例如：if (myVar&gt;0)。 并行及合并并行性是一种通过并行执行多个操作而不是按顺序执行（即等待每个操作完成然后再开始下一个操作）来使程序更快地完成的方法。分支及合并提供了一种并行方案，但是其针对的时执行不同任务的情况。但是另一种场景时，我们具有一系列相同的输入文件（一个属组类型的输入）需要执行相同的操作。那么我们可以使用 scatter 并行执行。 123456789101112input &#123; Array[File] inputFiles&#125;scatter (oneFile in inputFiles) &#123; call stepA &#123; input: in = oneFile&#125;call stepB &#123; input: files = stepA.out&#125; 场景是用于 task 的并发执行。例如一个 task 有多个样本需要并发处理，可以使用数组的方式将样本传入，然后使用 scatter 并发的处理每个样本，每个执行的单元称为一个 shard。所有的 shard 执行完成，则当前 task 执行完成，所有 shard 的输出，又作为一个数组，可以传递到下一个 task 处理。 利用别名重复调用任务工作流中多次调用任务时，可以使用任务别名。复制粘贴任务的定义并在每次需要在工作流中再次使用它时更改名称会很乏味。这种方法称为复制和粘贴编程，前期足够简单，但从长远来看很难维护。想象一下，您在一项任务中发现了一个错别字 - 您需要在每个粘贴的任务中修复该错别字！但是，使用 WDL 的内置任务别名功能 call taskName as aliasName，您可以调用相同的任务代码并为其分配别名。然后，遵循分层命名的原则，为了访问别名任务的输出，我们使用别名，而不是原始任务名称。12345678910111213141516call stepA as firstInput &#123; input: in = in1&#125;call stepA as secondInput &#123; input: in = in2&#125;call stepB &#123; input: in = firstInput.out&#125;call stepC &#123; input: in = secondInput.out&#125; 参考文档WDL参考文档 Broad WDL 官方论坛 WDL项目仓库 OpenWDL WDL-readthedoc 标准流程描述语言 WDL 阿里云最佳实践 在学习编写 WDL 的过程中，可以参考 Broad 官方的一些 GATK工作流 借鉴和学习 WDL 的用法。]]></content>
      <categories>
        <category>pipeline</category>
        <category>framework</category>
        <category>任务调度</category>
      </categories>
      <tags>
        <tag>流程开发</tag>
        <tag>WDL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[WDL - 投递我们的流程]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-04.pipeline%2Fpipeline-frameworks-WDL-3.%E6%B5%81%E7%A8%8B%E7%9A%84%E6%8A%95%E9%80%92%2F</url>
    <content type="text"><![CDATA[在前面两部分，我们知道了如何撰写wdl流程，同时也对wdl执行每个任务的逻辑有了一个了解，接下来，我们来看看如何真实的投递一个wdl任务。 验证语法没人喜欢投递任务下班回家，第二天上班发现由于脚本中的语法错误，它们在几分钟内都失败了。而针对NGS分析动辄几个小时的分析更是如此。所以我们在进行任务投递前，一定要先验证一下整体的语法逻辑。而这部分工作可以通过 wdl自带的 WOMtools 实现。它提供了语法检查、参数检查、构造流程输入文件等功能。为了验证语法，我们可以使用 validate 功能。1$ java -jar wdltool.jar validate myWorkflow.wdl wdltool 分析 WDL 脚本并提醒我们注意任何语法错误，例如缺少大括号、未定义的变量、缺少逗号等。它将解决导入问题，但请注意，它无法识别错误，例如命令中的拼写错误、不正确的文件名或缺少工作流中运行的程序所需的输入。 生成流程图但语法验证没有问题时，我们肯定想更直观的看看我们的流程执行路径，一方面时帮助我们调试确定流程执行路径没有问题，另一方面也方便我们对外介绍我们的流程。 womtools 也支持这一个功能。123java -jar ../docker/womtool-46.jar graph workflow_test_pipe1.wdl &gt;workflow_test_pipe1.wdl.dot#转换为svg格式dot -Tsvg -o workflow_test_pipe1.wdl.svg workflow_test_pipe1.wdl.dot 针对复杂的流程，生成graph可以非常方便的帮助我们理解流程的执行路径。 指定输入在我们进行真实的流程分析的时候，往往需要进行一些参数的传入，比如样本的数据路径和属性信息等。当然可以硬编码到流程中，但是每次更换数据都重写脚本，显然时非常不优雅也不可取的。而 wdl 提供的解决方案时每次运行的时候可以提供一个json文件，对流程中的变量进行定义，并在执行时，对json文件进行解析。针对复杂流程梳理变量也是一个庞杂的工程，这里我们可以使用 WOMtools 来帮我们生成输入的json文件模板，然后用每次分析的实际值进行填空。 配置文件生成与填写workflow 的输入，比如数据的存储位置、计算软件的命令行参数、计算节点的资源配置、镜像等等任务执行过程需要的参数，基本都可以通过 json 文件的形式来指定。使用 wdltools 工具可以根据 WDL 文件来生成输入模板：1java -jar wdltools.jar inputs myWorkflow.wdl &gt; myWorkflow_inputs.json 这样我们每次分析不同的数据就可以编辑模板文件，将实际的值填入其中。模板格式如下：123&#123; &quot;&lt;workflow name&gt;.&lt;task name&gt;.&lt;variable name&gt;&quot;:&quot;&lt;variable type&gt;&quot;&#125; 当然，如果工作流不是很复杂，也可以按照上面的格式手写 input 文件。下面是一个 GATK 工作流的 input 文件的片段： 执行任务执行任务有许多不同的方式，这里我们暂时介绍最为常用的cromwell Cromwell 是 Broad Institute 开发的工作流管理引擎。具有如下的优势： 支持 WDL 和 CWL 两种工作流描述语言 多平台支持，包括本地服务器、SGE集群、云计算平台等 阿里云批量计算是官方支持的云平台之一 丰富的元数据，展示工作流执行过程 支持多种高级特性，优化 workflow 的执行 本地运行 run 模式 在本地机器上运行是最简单的事情。假设您有一个已验证的 WDL 脚本，称为 myWorkflow.wdl ，并且有一个名为 myWorkflow_inputs.json 的输入的 JSON 文件，您只需调用 Cromwell run 的函数，如下所示：1java -jar Cromwell.jar run myWorkflow.wdl --inputs myWorkflow_inputs.json 运行完成后，Cromwell 会在当前目录生成一个 cromwell-executions 文件夹，里面包含了所有执行过程中产生的文件。然后就可以看到 cromwell 每步的执行日志了。记住这都是 Cromwell 的日志，流程定义的屏幕输出都会保存到执行目录生成的 stderr 中 默认情况下，您可以在以下文件夹中找到所有生成的文件（输出和日志）：12/Users/johnsmith/cromwell-executions/My-Workflows/\&lt;run-id&gt;/all-helloWorld# username workflow-name 随机hash task-name 这里是是最简单的投递方法，而cromwell本身支持非常多的个性化配置，从而使其可以适配SGE、云服务器、容器环境等等多种个性化场景，这部分我们后续在cromwell的配置中在展开进行介绍。 Server 模式12345# 用下面的命令启动一个 HTTP server$ java -Dconfig.file=application.conf -jar cromwell.jar server# 再使用 RESTful API 提交工作流到 server 执行：$ java -jar cromwell.jar submit -t wdl -i input.json -o option.json -h http://localhost:8000 相比 Run 模式，Server 模式有以下优势： 可以并行处理多个 workflow，适用于生产环境 有 Call caching 等高级特性（下文会讲到），优化 workflow 的执行 提供丰富的 workflow metadata，来展示 workflow 的执行过程 注意：不管是使用Run 模式还是 Server模式，要使用批量计算作为后端运行 WDL，都可以通过对应的配置文件支持，提供更多的附加支持，比如使用镜像分析，续跑等特性。配置文件详解请参考批量计算官方文档或Cromwell 官方文档。配置文件的整体说明和原理比较复杂，后面我们单独展开介绍 参考文档WDL参考文档 Broad WDL 官方论坛 WDL项目仓库 OpenWDL WDL-readthedoc 标准流程描述语言 WDL 阿里云最佳实践 在学习编写 WDL 的过程中，可以参考 Broad 官方的一些 GATK工作流 借鉴和学习 WDL 的用法。]]></content>
      <categories>
        <category>pipeline</category>
        <category>framework</category>
        <category>任务调度</category>
      </categories>
      <tags>
        <tag>流程开发</tag>
        <tag>WDL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[WDL - 架构简介 -常用组件介绍]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-04.pipeline%2Fpipeline-frameworks-WDL-1.%E5%B8%B8%E7%94%A8%E7%BB%84%E4%BB%B6%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[WDL 是 Workflow Description Language的缩写，有时也写作 Workflow Definition Language，是美国 Broad Institute 推出的工作流描述语言。从开发者及开发者赋予的名字中，我们就能看出WDL是一个面向生物信息/基因组学领域的专业的工具。 经过几年的发展，WDL 已经是生信行业广泛接受的一种工作流标准，具有下面的优势： Human-readable WDL 作为一种为工作流领域定制的语言，和 Shell、Python 等通用的脚本语言相比，没有过多复杂的概念，对使用者的计算机技能要求不高，对于生信用户容易上手。 Portable Workflow WDL 可以在多个平台执行，比如本地服务器、SGE 集群，云计算平台等，可以做到一次编写多处执行。 Standard 作为GA4GH支持的工作流描述语言之一，已经得到了众多大厂和行业协会的支持，形成了比较完善的生态。 WDL架构结构组件WDL是一种流程编写语言，没有太多复杂的逻辑和语法，入门简单。首先看一个hello world的例子 1234567891011workflow myWorkflow &#123; call myTask&#125;task myTask &#123; command &#123; echo "hello world" &#125; output &#123; String out = read_string(stdout()) &#125;&#125; 对于一个WDL脚本而言，有以下5个重要的核心结构 workflow：工作流定义 task：工作流包含的任务定义 call：调用或触发工作流里面的 task 执行 output：task 或 workflow 的输出定义 runtime：task 的运行环境定义 三个顶级组件 workflow, call and task在顶层，我们定义了一个 workflow ，我们在其中调用一组任务。任务实际上是在工作流定义块之外定义的 task ，而 call 语句则放在工作流定义块内部。工作流块和任务定义在脚本中的排列顺序无关紧要。调用语句的顺序也无关紧要。 call call 组件在工作流定义中用于指定应执行特定任务。在最简单的形式中，调用只需要一个任务名称。 或者，我们可以添加一个代码块来指定任务的输入变量。我们还可以修改 call 语句以别名调用任务，这允许在同一工作流中使用不同的参数多次运行相同的任务。这使得重用代码变得非常容易。当然这也提到了另一个点，通过 call 可以帮助我们将参数从workflow传到task中。123456789101112131415161718# in it's simplest formcall my_task &#123; &#125;# with input variablescall my_task &#123; input: task_var1 = workflow_var1, task_var2 = workflow_var2, ...&#125;# with an alias and input variablescall my_task as task_alias &#123; input: task_var1 = workflow_var1, task_var2 = workflow_var2, ...&#125; 同时通过使用 call 我们可以在不同的任务之间实现输入输出的衔接和调用。1234567891011121314workflow myWorkflow &#123; # 调用一个taskA 并在其中定义了task的输出结果 output call taskA&#123; **** &#125; call taskB&#123; input: # 声明task copy_file的输入参数 input_file = taskA.output &#125; output &#123; # 保存 copy_file的结果文件 output_file File output_file = copy_file.output_file &#125;&#125; tasktask 可以理解成其他语言中的 模块/包，是一个可以完成特定工作的任务单元，一个完整的 task 中会声明执行task所需要的所有信息包括输入文件和需要指定的输入参数(input)，执行的具体命令(command)，以及执行后会得到的结果文件(output)。除此之外还可以通过 runtime、meta 和 parameter_meta 组件提供其他（可选）执行命令所需要的属性信息，比如环境和硬件资源(runtime)。一个 task 中常见的组件如下： task 代表任务，读取输入文件，执行相应命令，然后输出; input 指定task运行时需要提供的输入参数和数据类型，无默认值的在调用task时，需要在call过程中指定。 parameter_meta 可选参数，通过键值对的形式提供每个输入参数的说明 command 对应的就是执行的命令，比如一条具体的gatk的命令; output 指定task的输出值。下游其他task需要调用的结果需要明文输出 runtime task在计算节点上的运行参数，包括 CPU、内存、docker 镜像等 Metadata 记录需要记录到task中的一些元数据信息，比如作者信息。 一个简单的示例123456789101112131415161718192021222324252627282930313233343536373839404142# 最简单的例子task my_task &#123; input &#123; ... &#125; command &lt;&lt;&lt; ... &gt;&gt;&gt; output &#123; ... &#125;&#125;# 一个相对复杂的示例task demo&#123; # 说明执行task所需的输入文件 input &#123; String memory_mb = 1G File input_file &#125; # 进行一些变量的处理和定义 output_file = "output.txt" # 提供每个参数的说明 parameter_meta &#123; memory_mb: "Amount of memory to allocate to the JVM" input_file: "The Filename of the sample in demo task" &#125; # 定义声明具体需要执行的命令 command &lt;&lt;&lt; cp ~&#123;input_file&#125; ~&#123;output_file&#125; &gt;&gt;&gt; # 声明任务执行所需要的相关资源配置 runtime &#123; docker: "ubuntu:latest" &#125; # 任务的元信息说明 meta &#123; author: "Joe Somebody" email: "joe@company.org" &#125; # 任务执行后需要输出的的结果文件 output &#123; #说明输出 File output_file=output_file &#125;&#125; task的执行顺序和调用的顺序、书写顺序无关，但撰写过程应该尽可能保持整个流程的可读性。可以将task理解为编程语言中的函数，每个函数读取输入的参数，执行代码，然后返回，command对应执行的具体代码，output对应返回值。在wdl中，DAG关系就是由input 和 output来构建确认的。 workflowworkflow 组件是 WDL 脚本所需的顶级组件。它包含调用task组件的调用语句，以及工作流级输入定义。 每个wdl脚本都必须有且仅有一个 workflow ，在其中定义了整个流程的输入数据(input)和输出数据(output)。并通过 call 完成相应 task 任务的调用。在执行阶段，wdl解析程序，会根据每个任务的输入和输出构建整个流程的有向无环图并依次进行任务的处理和分析。同时一个wdl文件中定义的 workflow 可以被其他 wdl 文件引用，从而实现 workflow 的复用。 12345678910111213141516171819202122232425262728# 声明workflowworkflow myWorkflow &#123; # 调用一个task copy_file call copy_file&#123; input: # 声明task copy_file的输入参数 input_file = "input.txt" &#125; output &#123; # 保存 copy_file的结果文件 output_file File output_file = copy_file.output_file &#125;&#125;task copy_file&#123; # 说明执行task所需的输入文件 input &#123; File input_file &#125; # 进行一些变量的处理和定义 output_file = "output.txt" # 定义声明具体需要执行的命令 command &lt;&lt;&lt; cp ~&#123;input_file&#125; ~&#123;output_file&#125; &gt;&gt;&gt; # 任务执行后需要输出的的结果文件 output &#123; #说明输出 File output_file=output_file &#125;&#125; 如果我们调用了多个不同的task任务，wdl在解析过程中会根据任务的输入输出文件进行拓扑排序，对独立的任务进行并行处理 其他核心组件input &amp; output在前面的示例中，我们多次看到input和output的声明，不难发现，其实所有输入输出的定义前面都有一个关键字 File或String，这两个是wdl中最基本的变量类型关键字，因为我们在定义一个新的变量时，都需要对变量的类型进行声明，wdl原生支持的类型如下：| 关键字 | 说明 || ———- | ————————————————————- || String | 字符串 || Int | 整数 || Float | 浮点数 || File | 文件（会校验文件是否存在） || Boolean | 布尔型 || Array[T] | 属组（数组的内部也需要指定，例如Array[File] 或 Array[String] || Map[K, V] | 相当于字典 || Pair[X, Y] | 含有两个关键字的特殊属组 || Object | 对象，比如解析json文件时 | 关于每一种变量的使用，以及 WDL 的更多使用技巧，请参考官方规范文档。 再就是我们在 task层的变量可以引用workflow层的变量，也可以直接传参。下面我举个例子来说明一下： 12345678910111213141516171819202122232425262728293031323334353637workflow helloHaplotypeCaller &#123; File Ref String Sample call demoA&#123; input: RefFasta=Ref, sampleName=Sample &#125; call demoB&#123; input: # 使用demoA的结果作为demo的输入 vcf = demoA.rawVCF&#125;task demoA &#123; File GATK File RefFasta String sampleName command &#123; java -jar $&#123;GATK&#125; \ -T HaplotypeCaller \ -R $&#123;RefFasta&#125; \ -I $&#123;sampleName&#125;.bam \ -o $&#123;sampleName&#125;.raw.indels.snps.vcf &#125; output &#123; File rawVCF = "$&#123;sampleName&#125;.raw.indels.snps.vcf" &#125;&#125;task demoA &#123; input&#123; File vcf &#125; command &#123; annotation $&#123;vcf&#125; ****&#125; output &#123; File annotVCF = "$&#123;sampleName&#125;.anno.vcf" &#125; 在workflow层，我们定义了两个变量，Ref和Sample，在call demoA 的过程中，我们用input语句将它们传给了task层的 RefFasta 和 sampleName ，这样的话，提升了传参的复用——只用给Ref和Sample两个变量传参，多个task都可以引用它们。而其它的变量 GATK 则可以由输入文件一起传入。 批量计算 runtime用于配置任务运行时的相关参数。使用批量计算作为后端时，主要的 runtime 参数有：12345678910111213141516171819202122232425262728cluster: 计算集群环境 支持serverless 模式和固定集群模式mounts: 挂载设置 支持 OSS 和 NASdocker: 容器镜像地址 支持容器镜像服务simg: 容器镜像文件 支持singularity 镜像systemDisk: 系统盘设置 包括磁盘类型和磁盘大小dataDisk: 数据盘设置 包括磁盘类型、磁盘大小和挂载点memory: 所需的任务内存cpu: 所需的计算核心数目timeout: 作业超时时间maxRetries: 指令允许定义在发生故障时可以重新提交流程实例的最大次数。continueOnReturnCode: [0,1] 指定判定任务成功的返回码，默认是0成功，非0是任务失败。 具体的参数解释及填写方法，请参考 Cromwell 官方文档 除此之外，还有一些其他的概念 command parameter_meta meta从官方版本45开始，Cromwell 使用批量计算作为后端，支持 glob 和 Call caching 两个高级特性。 参考文档WDL参考文档 Broad WDL 官方论坛 WDL项目仓库 OpenWDL WDL-readthedoc 标准流程描述语言 WDL 阿里云最佳实践 在学习编写 WDL 的过程中，可以参考 Broad 官方的一些 GATK工作流 借鉴和学习 WDL 的用法。]]></content>
      <categories>
        <category>pipeline</category>
        <category>framework</category>
        <category>任务调度</category>
      </categories>
      <tags>
        <tag>流程开发</tag>
        <tag>WDL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[WDL - 开发应用环境配置]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-04.pipeline%2Fpipeline-frameworks-WDL-0.%E5%BC%80%E5%8F%91%E5%BA%94%E7%94%A8%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[工具插件Vim WDL 插件broadinstitute官方提供了在Vim环境下的WDL插件，提供基础的关键词、语法高亮等功能。其安装方式，可参考 https://github.com/broadinstitute/vim-wdl VScode WDL 插件VScode也提供了WDL语言的插件。用户可直接通过VScode插件管理进行安装。 WDL 语法校验工具WOMtool 可以对WDL脚本进行语法校验，其网站为 https://cromwell.readthedocs.io/en/stable/WOMtool/ 12# 通过conda安装conda install -c "bioconda/label/cf201901" womtool CWL 转化 WDL目前，除了WDL之外，也有相当一部分生物信息流程是以CWL语言编写的。用户可以采用dxCompiler将CWL脚本转化为WDL脚本。https://github.com/dnanexus/dxCompiler 调度引擎cromwell12# 通过conda安装conda install -c "bioconda/label/cf201901" cromwell widdlerwiddler GitHubWiddler is a command-line tool for executing WDL workflows on Cromwell servers.Features include: Workflow execution: Execute a workflow on a specified Cromwell server. Workflow restart: Restart a previously executed workflow. Workflow queries: Get the status, metadata, or logs for a specific workflow. Workflow result explanation: Get more detailed information on fails at the command line. Workflow monitoring: Monitor a specific workflow or set of user-specific workflows to completion. Workflow abortion: Abort a running workflow. JSON validation: Validate a JSON input file against the WDL file intended for use. 一个demo示例helloword.wdlhelloword.wdl123456789101112131415161718task echo &#123; String out command &#123; echo Hello World! &gt; $&#123;out&#125; &#125; output &#123; File outFile = "$&#123;out&#125;" &#125;&#125;workflow wf_echo &#123; call echo output &#123; echo.outFile &#125;&#125; 执行操作1234567891011# 生成配置文件（json格式）womtool inputs helloword.wdl &gt; helloword.wdl.input.json# 编辑配置文件vi helloword.wdl.input.json# &#123;# "wf_echo.echo.out": "Demo-Test"# &#125;# 运行任务cromwell run helloword.wdl -i helloword.wdl.input.json]]></content>
      <categories>
        <category>pipeline</category>
        <category>framework</category>
        <category>任务调度</category>
      </categories>
      <tags>
        <tag>流程开发</tag>
        <tag>WDL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[注释软件的转录本选择]]></title>
    <url>%2F01.%E7%9F%A5%E8%AF%86-03.standard%2FStandard-%E8%BD%AC%E5%BD%95%E6%9C%AC%E7%9A%84%E9%80%89%E6%8B%A9%2F</url>
    <content type="text"><![CDATA[简介临床检测过程中，面对的一个比较复杂的问题，就是转录本的选择。 转录本的选择转录本的选择需要结合相关预期临床用途。目前华大相关产品的转录本选择方案参考文档： 肿瘤产品转录本选择管理规程文档仍在准备受控阶段，后续确定终板需要进行更新。 转录本切换评估方案构建结构注释bed文件首先基于新的转录本，构建一个描述基因结构的bed文件。该文件应该包含所关注的转录本结构区域。 在NCBI上下载对应版本的基因结构注释结果文件*gff格式。 使用 /jdfstj1/B2C_COM_P1/Research_and_Development/Database/Transtript_choose/5.1.1NCBI/gff2bed.pl 脚本将将gff格式的结构文件转化为bed文件（NCBI.gff2bed.bed)。 使用bedtools对产品检测范围进行结构注释。需要基于捕获区间分别对新旧转录本获取该文件。 1234567891011# 对原始bed进行排序bedtools sort -i BedPrePare/Pancancer_v2.bed &gt; BedPrePare/Pancancer_v2.sort.bed# 对原始bed区域进行合并bedtools merge -i BedPrePare/Pancancer_v2.sort.bed &gt; BedPrePare/Pancancer_v2.merge.bed# 基于结构文件对原始bed区域进行注释bedtools intersect -a BedPrePare/Pancancer_v2.merge.bed -b NCBI.gff2bed.bed -wb | cut -f1-3,7-10 &gt; BedPrePare/Pancancer_v2.anno.bed# 评估相关基因的覆盖情况bedtools intersect -b BedPrePare/Pancancer_v2.merge.bed -a NCBI.gff2bed.bed -wao &gt; BedPrePare/Pancancer_v2.covercheck.bed 对现有产品的新旧转录本文件进行比较。确定差异检测范围 获得两套转录本区域（该区域指和检测范围区间交集为基础）之间的交集 1bedtools intersect -a Pancancer_v2.anno.Oldtrans.bed -b Pancancer_v2.anno.Newtrans.bed &gt; old_new.overlap.bed 获得切换转录本后减少的区域 1bedtools subtract -a Pancancer_v2.anno.Oldtrans.bed -b Pancancer_v2.anno.Newtrans.bed &gt; old.subtract.new.bed 切换转录本后增加的区域 1bedtools subtract -a Pancancer_v2.anno.Newtrans.bed -b Pancancer_v2.anno.Oldtrans.bed &gt; new.subtract.old.bed 接受替换的标准 针对就转录本进行替换时，需要额外补充如下评估内容 更换转录本后，由于转录本涵盖区间发生改变，需要将更换后，不会提报的范围反馈解读。基于临床历史检测结果的变异进行汇总统计。确认是否会影响致病变异位点的输出。 核查新旧转录本特异性区域，对应的公共数据库交集区域，确定更换转录本对公共数据库变异范围产生的影响。 针对肿瘤检测相关产品可参考数据库如下： ClinVar Cosmic 备注由于部分原始下载文件中，染色体编号常规命名和常规使用命名方式可能会存在出入； 因此在下游处理过程中需要对染色体编号进行转换，转换关系如下： NCBI获取染色体编号和NC编码之间的对应关系|Molecule name | GenBank sequence | |RefSeq sequence|Unlocalized sequences count||-|-|-|-|-||Chromosome 1|CM000663.1|=|NC_000001.10|2||Chromosome 2|CM000664.1|=|NC_000002.11|0||Chromosome 3|CM000665.1|=|NC_000003.11|0||Chromosome 4|CM000666.1|=|NC_000004.11|2||Chromosome 5|CM000667.1|=|NC_000005.9|0||Chromosome 6|CM000668.1|=|NC_000006.11|0||Chromosome 7|CM000669.1|=|NC_000007.13|1||Chromosome 8|CM000670.1|=|NC_000008.10|2||Chromosome 9|CM000671.1|=|NC_000009.11|4||Chromosome 10|CM000672.1|=|NC_000010.10|0||Chromosome 11|CM000673.1|=|NC_000011.9|1||Chromosome 12|CM000674.1|=|NC_000012.11|0||Chromosome 13|CM000675.1|=|NC_000013.10|0||Chromosome 14|CM000676.1|=|NC_000014.8|0||Chromosome 15|CM000677.1|=|NC_000015.9|0||Chromosome 16|CM000678.1|=|NC_000016.9|0||Chromosome 17|CM000679.1|=|NC_000017.10|4||Chromosome 18|CM000680.1|=|NC_000018.9|1||Chromosome 19|CM000681.1|=|NC_000019.9|2||Chromosome 20|CM000682.1|=|NC_000020.10|0||Chromosome 21|CM000683.1|=|NC_000021.8|1||Chromosome 22|CM000684.1|=|NC_000022.10|0||Chromosome X|CM000685.1|=|NC_000023.10|0||Chromosome Y|CM000686.1|=|NC_000024.9|0| 竞品公司的转录本信息世和燃石基因转录本-part1燃石基因转录本-part2]]></content>
      <categories>
        <category>NGS</category>
        <category>肿瘤检测</category>
        <category>数据</category>
      </categories>
      <tags>
        <tag>数据库</tag>
        <tag>Annotation</tag>
        <tag>BGI work</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[免疫组化分析软件 IMonitor]]></title>
    <url>%2F03.%E5%BA%94%E7%94%A8-01.software_in_NGS%2FSoftware-%E5%85%8D%E7%96%AB%E7%BB%84%E5%BA%93-IMonitor%2F</url>
    <content type="text"><![CDATA[Methods核心分析流程共分为4个步骤 basic data processingIn the first step, the readswere checked for inclusion of adaptorsequences. If any adaptor sequence was detected and locatedwithin 50 bp of the 39end of the read, it was deleted from theread. Reads bearing adaptor sequence at the 59 end or .5%“N” bases were discarded. The average base quality of eachreadwas calculated after removing the low-quality bases (basequality,10) at the 39 end. Further filtration left out reads withaverage quality ,15. For Illumina paired-end (PE) sequencing,the PE reads were merged at their overlapping region. ForPE readswith insertion length longer than the length of a singleread, the COPE (Liu et al. 2012) toolwas used; otherwise readswere assembled by an in-house program. Themain parametersfor both tools included themaximumoverlapping length (readlength), minimum overlapping length (10 bp), mismatch rate(10%) at the overlapping region, and ratio V(D)J assignmentThe V/D/J reference sequences were downloaded from the IMGT database, the international ImMunoGeneTics information system (http://www.imgt.org/). Processed sequences were aligned to the V, (D), J references, respectively, by BLAST (Altschul et al. 1990; Zhang et al. 2000; Ye et al. 2006) and specific parameters were applied to accommodate the differences in lengths of V, (D), J segments (BLAST parameters: V, -W 15 -K 3 -v 1 -b 3; D, -W 4 -K 3 -v 3 -b 5; and J, -W 10 -K 3 -v 1 -b 3). The high similarity among the genes and alleles of the germline sequences, along with the diversity of V/D/J gene rearrangement, gave rise to difficulties for accurate alignment. This might eventually lead to an incorrect structural analysis (CDR3 identification, deletion, or insertion). To improve the accuracy, a second alignment procedure was developed to identify exactly the V/D/J genes (Figure 2). First, a global alignment strategy, which attempted to align every base in every sequence, was used for the non-CDR3 region of the sequence. The mapped region generated from BLAST became a new seed and served as starting points for bootstrapping (base-by-base) extension to both directions, until the entire non-CDR3 region in the query was mapped to the target (reference) sequence. The mapping score was calculated according to these rules: reward for a nucleotide match was 5 and penalty for a nucleotide mismatch was 24. Second, the M-mismatch extension model of local alignment strategy was applied to locate the exact end positions of V and J genes during CDR3 region realignment. The procedure began at the CDR3 start position in the V gene or the CDR3 end position in the J gene and continuously extended in one direction until the preset mismatch limit was reached, generating the longest possible interval with the highest score. The mismatch numbers allowed for V/D/J genes were determined based on the analysis result of publicly available rearrangement sequences (http://www.imgt.org/ligmdb/) (Supporting Information, Figure S2A) and adjusted accordingly for different TCR and BCR chains (mismatches allowed: TRBV/J, TRAV/J, 0; IGHV/J, 2; IGKV/J, IGLV/J, 7). As shown in Figure S2A, these mismatch limits took mutations into consideration and covered .99.5% of all defined rearrangement sequences. Because the entire D gene was located within the CDR3 region, only the M-mismatch extension model was used for its realignment (mismatches allowed: TRBD, 0; IGHD, 4). Finally, all data including alignment score, identity, mismatch number, and alignment length were processed, and the alignment with highest score and identity larger than the threshold (.80%) was selected as the best hit. However, there might be several best hits with the same score due to the homology among the germline genes and alleles. In this case, the reference with the fewest deletions was selected, as shorter deletions are more likely to happen according to previous reported results (Warren et al. 2009) and our analysis from actual public rearrangement data (Figure S2B). structural analysisstatistics/visualization]]></content>
      <categories>
        <category>NGS</category>
        <category>software</category>
      </categories>
      <tags>
        <tag>免疫组库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基因注释软件-VEP]]></title>
    <url>%2F03.%E5%BA%94%E7%94%A8-01.software_in_NGS%2FSoftware-%E5%8F%98%E5%BC%82%E6%B3%A8%E9%87%8A-VEP%2F</url>
    <content type="text"><![CDATA[简介在TCGA等大型项目中，也推荐使用了VEP作为注释软件，同时，TCGA项目提供了由VEP向MAF转换的工具 vcf2maf，该工具由MSK长期跟进及维护。VEP 是Ensemble旗下开发的软件，有众多的专业人员进行着更新维护，同时他也符合 CLIA Variant Effect Predictor（VEP）是功能强大的注释、分析工具。它可以对二代测试产生的不同类型变异进行注释，包含SNPs, insertions, deletions, copy number variants和structural variants。也可以依据各种数据库的内容，根据需要，对变异进行过滤和排序。 VEP 输入格式VEP 结果格式VEP Calculated variant consequences 软件问题VEP 本身的对齐，只可以进行全碱基的对齐； 原始变异 ：GAATCATCATCATCATG在vep中 ：GAATCATCATCATCATG而不会拆分碱基进行对齐GAATCATCATCATCATG 安装环境依赖123456conda install -c bioconda perl-DBIconda install -c bioconda perl-Archive-Zipconda install -c bioconda perl-Bio-DB-HTS-Tabix # 使用gff进行注释时，该包必须安装perl -e 'use DBI ; use Archive::Zip ; use Bio::DB:HTS::Tabix'conda install -c bioconda perl-DBD-mysql # 使用离线模式，可以跳过该包 VEP的安装可以直接参考官方说明 通过conda安装VEP本身可以直接通过conda进行安装，由于官方安装文档未提及，在此进行补充说明1conda install -c bioconda ensembl-vep==108 # 如果需要安装新版本需要指定，本文撰写阶段最新版本108，但是默认安装版本是v92. 库文件配置在线版进行数据库下载的速度非常慢，而conda也只会进行vep注释软件的安装，不包含相关库文件，因此需要单独准备库文件，速度会更快。 数据集下载 运行VEP软件自带的 INSTALL.pl 进行下载，非常之慢，具体方式可以参考官方安装说法说明。 从官方ftp进行下载cache文件可以访问官方ftp进行下载https://ftp.ensembl.org/pub/grch37/release-108/variation/vep/homo_sapiens_refseq_vep_108_GRCh37.tar.gz参考基因组的fasta序列，https://ftp.ensembl.org/pub/grch37/release-108/fasta/homo_sapiens/dna/Homo_sapiens.GRCh37.dna_sm.primary_assembly.fa.gz 扩展功能配置VEP本身提供了一个外部扩展功能模块 Plugins可以使用INSTALL.pl 进行安装，也可以直接通过Github仓库进行下载 相关问题及处理记录20221104 安装VEP108时会发发现存在版本兼容性错误因为bioconda中123# 报错信息# Compress::Raw::Zlib version 2.201 required--this is only version 2.105conda install -c conda-forge perl-compress-raw-zlib # 使用使用自己的转录本库（transcript models）VEP本身自带对应的转录本库版本，但是如果遇到默认转录本中缺失了自己需要的转录本信息时，可以构建自己的转录本库，进行注释。参考官方文档：自建注释数据库的参考文档 VEP can use a variety of annotation sources to retrieve the transcript models used to predict consequence types. Cache - a downloadable file containing all transcript models, regulatory features and variant data for a species GFF or GTF - use transcript models defined in a tabix-indexed GFF or GTF file Database - connect to a MySQL database server hosting Ensembl databasesGFF/GTF filesVEP can use transcript annotations defined in GFF or GTF files. The files must be bgzipped and indexed with tabix and a FASTA file containing the genomic sequence is required in order to generate transcript models. 123grep -v "#" data.gff | sort -k1,1 -k4,4n -k5,5n -t$'\t' | bgzip -c &gt; data.gff.gztabix -p gff data.gff.gz./vep -i input.vcf --gff data.gff.gz --fasta genome.fa.gz GFF fileExample of command line with GFF, using of flag –gff :1./vep -i input.vcf --cache --gff data.gff.gz --fasta genome.fa.gz This functionality uses VEP’s custom annotation feature, and the –gff flag is a shortcut to:1--custom data.gff.gz,,gff NOTE: You should use the longer custom annotation form if you wish to customise the name of the GFF as it appears in the SOURCE field and VEP output header. GTF fileExample of command line with GTF, using of flag –gtf :1./vep -i input.vcf --cache --gtf data.gtf.gz --fasta genome.fa.gz This functionality uses VEP’s custom annotation feature, and the –gtf flag is a shortcut to:1--custom data.gtf.gz,,gtf NOTE: You should use the longer custom annotation form if you wish to customise the name of the GTF as it appears in the SOURCE field and VEP output header. 定制注释内容（Custom annotations）VEP 可以使用 --custom 标志将标准格式文件中的自定义注释集成到您的结果中。对应的数据库可以是任意使用 tabix 构建过索引的文件（BED、GFF、GTF、VCF）或 bigWig (文件包含自己的索引)。 Format Type Description Notes GFF/GTF Gene/transcript annotations Formats to describe genes and other genomic features — format specifications: GFF3 and GTF Requires a FASTA file in offline mode or if the desired species or assembly is not part of the Ensembl species list. VCF Variant data A format used to describe genomic variants VEP uses the 3rd column as the identifier. INFO and FILTER fields from records may be added to the VEP output. BED Basic/uninterpreted data A simple tab-delimited format containing 3-12 columns of data. The first 3 columns contain the coordinates of the feature. VEP uses the 4th column (if available) as the feature identifier. bigWig Basic/uninterpreted data A format for storage of dense continuous data. VEP uses the value for the given position as the identifier. BigWig files contain their own indices, and do not need to be indexed by tabix. Requires Bio::DB::BigFile. 定制内容的使用参数说明123456./vep [...] --custom Filename,Short_name,File_type,Annotation_type,Force_report_coordinates,VCF_fields# Filename: 定制注释对应的库文件名称# Short_name：注释后展示的名称# File type： 定制库文件的文件类型# Annotation type： "exact" or "overlap" 当使用“exact”时，仅报告其坐标与变体的坐标完全匹配的注释。这适用于位置特定信息，例如保守分数、等位基因频率或表型信息。使用“overlap”，任何与变体重叠甚至 1bp 的注释都会被报告。# VCF_fields: 您可以指定自定义输入 VCF 的 INFO 字段中存在的任何信息类型（例如“AC”），或为 FILTER 字段指定“FILTER”，以将这些添加为自定义注释： 详细的参数说明和具体使用示例，参考 VEP官方文档 VEP的结果逻辑说明Ensembl Variation - Calculated variant consequences同时由于一个突变可能会存在多个不同类型的变异描述方式（Stop lost; Inframe insertion），在某些情况下需要形成一个唯一的功能描述，因此Ensemble提供了一套用于评估不同变异描述的优先级顺序，可以参考官方文档。 VEP使用过程中发现的问题/风险汇总####1. 注释结果未见终止密码子，功能输出 stop_lost123变异结果： chr1_10342506_TCAGGTGGGCTTGACGTCTGTGACCAGTATTCAAGAGAGGATCATGTCTA/CCAGGTGTAGACATGATCCTCTCTTGAATACTGGTCACAGACGTCAAGCC注释结果： NM_015074.3：c.1211_1260delinsCCAGGTGTAGACATGATCCTCTCTTGAATACTGGTCACAGACGTCAAGCC 注释结果： NP_055889.2：p.I404_L420delinsTRCRHDPLLNTGHRRQA 在UCSC中核查发现读码框不一致。 123456789VEP读码框为 |a T C|A G G|T G G|G C T|T G A|C G T|C T G|T G A|C ......UCSC读码框为 a g T|C A G|G T G|G G C|T T G|A C G|T C T|G T G|A C ......在VEP使用的密码框中，存在多个读码框对应的氨基酸序列为终止密码子（TGA），同时namechecker和Varsome中的突变后氨基酸均基于VEP的读码框翻译产生。Ref / Alt aTCAGGTGGGCTTGACGTCTGTGACCAGTATTCAAGAGAGGATCATGTCTA/aCCAGGTGTAGACATGATCCTCTCTTGAATACTGGTCACAGACGTCAAGCCI R W A * R L * P V F K R G S C L /T R C R H D P L L N T G H R R Q A04 05 06 07 08 09 10 11 12 13 14 15 16 17 18 19 20 经过核查是用于使用了 –use_given_ref 参数，当不使用该参数是测试发现问题可以消除。 2. start-lost /init loss 的写法不规范，其实密码子缺失应该输出为 “p.0” 但是VEP输出结果格式为 “p.Leu1?” 3. VEP 注释结果中，存在部分变异结果没有提供外显子亚区测试阶段发现如下情况时，变异注释结果未输出外显子， 变异位于内含子和外显子交界处，示例如下：123 Exon-1 | ins | Intron-1Ref: TCAGGTGGGCTTGACGTCTGTGAC| |CAGTATTCAAGAGAGGATCATGTCTAAlt: TCAGGTGGGCTTGACGTCTGTGAC|TTGAA|CAGTATTCAAGAGAGGATCATGTCTA 在使用中，可以进行了人工数据库的构建，但是需要注意一点，bed文件格式为0base的左闭右开，但是VEP的位置信息1base的 4. 跨越多个区域的变异，是否需要有氨基酸变化的描述 方法 cHGVS pHGVS VEP c.111+1_117delinsCTGCGC p.A38_*39delinsLR Varsome c.111+1_117delGCTGCGGinsCTGCGC . 不同方法注释得到的氨基酸变化存在差异，现阶段无法确定正确答案。 Tag value CHR chr10 Pos 43609928 Ref . Alt GTCCACTGTGCGACGAGCTGTGCCGCACGGTGGATCTGTGGGTGGGGGTGGTGTGAGGCTTGGCACCGCCACCCACAG Trans NM_020975.6 方法 cHGVS pHGVS VEP c.1880-1_1880ins GTCCACTGTGCGACGAGCTGTGCCGCACGGTGGATCTGTGGGTGGGGGTGGTGTGAGGCTTGGCACCGCCACCCACAGA p.Q626_D627insGPLCDELCRTVDLWVGVV* Varsome c.1880-1_1880insGTCCACTGTGCGACGAGCTGTGCCGCACGGTGGATCTGTGGGTGGGGGTGGTGTGAGGCTTGGCACCGCCACCCACAG p.D627_S1114delinsRVHCATSCAARWICGWGWCEAWHRHPQIHCATSCAAR Namechecker c.1879_1880insGTCCACTGTGCGACGAGCTGTGCCGCACGGTGGATCTGTGGGTGGGGGTGGTGTGAGGCTTGGCACCGCCACCCACAG p.Asp627_Ser1114delinsGlyProLeuCysAspGluLeuCysArgThrValAspLeuTrpValGlyValVal HGVS未明确描述导致氨基酸截断时描述规范 Tag value CHR chr19 Pos 42795240 Ref CGCCCCCTGCTGTCCAGTT Alt GGCAATGAACTGGACAGCA Trans NM_015125.5 方法 cHGVS pHGVS VEP c.2321_2339delinsGGCAATGAACTGGACAGCA p.Ala774_Phe780delinsGlyGlnTer Varsome c.2321_2339delinsGGCAATGAACTGGACAGCA p.Ala774_Arg1608delinsGlyGln Namechecker c.2321_2339delinsGGCAATGAACTGGACAGCA p.Ala774_Arg1608delinsGlyGln 变异导致多个氨基酸均为同一突变]]></content>
      <categories>
        <category>NGS</category>
        <category>software</category>
      </categories>
      <tags>
        <tag>Annotation</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基因注释软件-综述信息]]></title>
    <url>%2F03.%E5%BA%94%E7%94%A8-01.software_in_NGS%2Fsoftware-%E8%BD%AF%E4%BB%B6%E7%BB%BC%E8%BF%B0-%E5%8F%98%E5%BC%82%E6%B3%A8%E9%87%8A%2F</url>
    <content type="text"><![CDATA[参考信息The State of Variant Annotation: A Comparison of AnnoVar, snpEff and VEP #|注释工具|开发者|开发语言|HGVS 标准输出|支持转录本|可否商用|是否仍在维护|数据库-公司-机构||-|-|-|-|-|-|-|-||VEP|欧洲生物信息学中心|Perl|是|Refseq、UCSC、Ensembl |可以|是|TCGA/COSMIC/gnomAD/吉因加/求臻/泛生子||annovar|哥伦比亚大学王凯教授|Perl|是|Refseq、UCSC、Ensembl |否|是|HGMD/HGVD/领星医药/世和/燃石/诺禾致源/和瑞/安诺优达/荣之联/博奥/至本/臻和/先声医药/泛生子||snpEff|麦吉尔大学的Pabio Cingolan教授|Java|是||可以|是|HGMD/燃石/博奥/先声医药||Oncotator|Broad Institute的科学家|Python|||否||Broad Institute||Bcfanno|BGI|Perl|否||可以|否|||Bgicg|BGI|Perl|否|Refseq|可以|否|||VAI|||||||UCSC||BaseSpace|||是||||illumina|]]></content>
      <categories>
        <category>NGS</category>
        <category>software</category>
      </categories>
      <tags>
        <tag>Annotation</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基因注释软件-VEP]]></title>
    <url>%2F03.%E5%BA%94%E7%94%A8-01.software_in_NGS%2FSoftware-%E5%8F%98%E5%BC%82%E6%B3%A8%E9%87%8A-VEP-%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[在使用VEP软件进行功能注释阶段，其实会发现存在部分注释结果和整体注释逻辑存在偏差。由于不清楚是认识的不足还是VEP本身代码存在Bug。因此对VEP的源码进行溯源核查。 相关疑似的问题 部分变异检测结果存在cHGVS，且cHGVS位于编码区，但是未输出pHGVS信息。 Chr Start End Ref Alt Gene Trans cHGVS pHGVS chr12 122064779 122064785 ACCGCCA C ORAI1 NM_032790.3 c.132_138delinsC - 从注释的氨基酸结果来看，不涉及终止密码子，但是注释得到的Function为stop_lost Chr Start End Ref Alt Gene Trans cHGVS pHGVS chr1 10342506 10342555 TCAGGTGGGCTTGACGTCTGTGACCAGTATTCAAGAGAGGATCATGTCTA CCAGGTGTAGACATGATCCTCTCTTGAATACTGGTCACAGACGTCAAGCC KIF1B NM_015074.3 c.1211_1260delinsCCAGGTGTAGACATGATCCTCTCTTGAATACTGGTCACAGACGTCAAGCC p.I404_L420delinsTRCRHDPLLNTGHRRQA 源码重点部分记录 VEP输出结果整体记录在 “ modules\Bio\EnsEMBL\VEP\OutputFactory.pm ” 中，如果是处于核查可以基于该模块进行反向溯源； 预测氨基酸变化代码在 “modules\Bio\EnsEMBL\Variation\TranscriptVariationAllele.pm” Line:684 12345678=head2 peptide Description: Return the amino acid sequence that this allele is predicted to result in Returntype : string or undef if this allele is not in the CDS or is a frameshift Exceptions : none Status : Stable=cut 对输入文件进行解析校验 modules\Bio\EnsEMBL\VEP\Parser.pm123456789101112131415161718=head2 validate_vf Arg 1 : Bio::EnsEMBL::Variation::BaseVariationFeature Example : $is_valid = $parser-&gt;validate_vf($vf); Description: Performs various (configurable) checks on a VariationFeature as produced by the parser: - creates a variation_name from the location+alleles if none - checks if chr is in user-specified list if provided - checks start and end look like numbers - checks start/end are valid (start &lt;= end + 1) and corresponds to ref allele - checks chr is in valid list - checks ref allele vs genome if requested Returntype : bool Exceptions : none Caller : next() Status : Stable=cut HGVS信息注释 获取变异最接近的转录本modules\Bio\EnsEMBL\VEP\AnnotationType\Transcript.pm123456789101112 =head2 get_nearest Arg 1 : Bio::EnsEMBL::Variation::BaseVariationFeature $vf Arg 2 : string $type (transcript, gene or symbol) Example : $nearest_gene = $as-&gt;get_nearest($vf, 'symbol'); Description: Gets ID (one of transcript, gene, symbol) of the nearest transcript to the given variant. Returntype : string Exceptions : none Caller : annotate_InputBuffer() Status : Stable=cut 对应代码： modules\Bio\EnsEMBL\Variation\TranscriptVariation.pm sub _hgvs_generic { my $self = shift; my $reference = pop; my $hgvs = shift; #The rna and mitochondrial modes have not yet been implemented, so return undef in case we get a call to these return undef if ($reference =~ m/rna|mitochondrial/); # The HGVS subroutine my $sub = qq{hgvs_$reference}; # Loop over the TranscriptVariationAllele objects associated with this TranscriptVariation foreach my $tv_allele (@{ $self-&gt;get_all_alternate_TranscriptVariationAlleles }) { #If an HGVS hash was supplied and the allele exists as key, set the HGVS notation for this allele if (defined($hgvs)) { my $notation = $hgvs-&gt;{$tv_allele-&gt;variation_feature_seq()}; $tv_allele-&gt;$sub($notation) if defined $notation; } # Else, add the HGVS notation for this allele to the HGVS hash else { $hgvs-&gt;{$tv_allele-&gt;variation_feature_seq()} = $tv_allele-&gt;$sub(); } } return $hgvs; } modules\Bio\EnsEMBL\VEP\VariantRecoder.pm]]></content>
      <categories>
        <category>NGS</category>
        <category>software</category>
      </categories>
      <tags>
        <tag>Annotation</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[肿瘤NGS检测开发过程中的方法学]]></title>
    <url>%2F01.%E7%9F%A5%E8%AF%86-01.NGS%2F2021-10-29.%E8%82%BF%E7%98%A4NGS%E6%A3%80%E6%B5%8B%E5%BC%80%E5%8F%91%E8%BF%87%E7%A8%8B%E4%B8%AD%E7%9A%84%E6%96%B9%E6%B3%95%E5%AD%A6%2F</url>
    <content type="text"><![CDATA[肿瘤NGS检测开发过程中的方法学 李金明，高通量测序技术[M]. 李金明，个体化医疗中的临床分子诊断[M]. Rehm H L, et al. ACMG clinical laboratory standards for next-generation sequencing[J] 2013. Aziz N, et al. College of American Pathologists’ laboratory standards for next-generation sequencing clinical tests[J] 2015. Matthijs G, et al. Guidelines for diagnostic next-generation sequencing[J] 2016. Jennings L J, et al. Guidelines for validation of next-generation sequencing–based oncology panels[J] 2017. Li M M, et al. Standards and guidelines for the interpretation and reporting of sequence variants in cancer[J] 2017. Roy S, et al. Standards and guidelines for validating next-generation sequencing bioinformatics pipelines[J] 2018. 产品开发过程中的深度确定测序深度或覆盖深度被定义为覆盖给定核苷酸位置的reads的数量，生物信息学工具极其依赖于足够的覆盖深度，以便灵敏和特异地检测变异。覆盖深度与稳定检测样本的变异之间的关系很简单，因为更高数量的高质量测序数据为特定位置的碱基检测供了信心，无论来自测序样本的碱基调用是否是与参考碱基相同（未识别出变异）或者是非参考碱基（识别出变异）。 然而，许多因素会影响所需的深度，包括测序平台，目标区域的序列复杂性（与基因组的多个区域具有同源性的区域、重复序列元件或假基因的存在以及GC富集区域）。此外，用于目标富集的文库制备和需要评估的变异类型也是重要的考虑因素。因此，必须在检测开发和验证过程中系统地评估每个 NGS 测试的覆盖模型。 这些性能参数可以并且应该在开发阶段进行评估，以帮助定义验证的接受标准。例如，对于给定比例的突变等位基因，可以使用二项分布方程来确定检测到最小数量等位基因的概率： 而在一个确定的检测体系下，我们可以知道一个体系的错误率（LOB），和预期的检测限。这时我们可以根据所需的检测下限、读取质量和假阳性或假阴性结果的容忍度来估计所需的覆盖深度(可以游有效区分真阳性和真阴性的最低深度)。123456789101112131415161718192021222324252627282930313233343536373839404142library("ggplot2")Totaldepth=5000 # 预期深度，控制评估的深度上限ErrorRate=0.004 # 基于产品首批高深度产品，评估获得LOBLoDRate = 0.01 # 产品预期的检测性能，后期LOD需要单独进行评估补充。CI = 0.9 # 对性能结果要求的置信区间。depthlist = c(1:Totaldepth)ErrorReadNumlist = depthlistSupportReadNumlist = depthlistfor (depth in 1:Totaldepth) &#123; ErrorReadNum = round(depth * ErrorRate) # 考虑错误率统计过程中本身已经是错误率的最高值，所以不再进行二项分布扩展。 #ErrorReadNum = qbinom(CI,depth,ErrorRate) # 错误率考虑上95置信区间。 SupportReadNum = qbinom(1-CI,depth,LoDRate) ErrorReadNumlist[depth] = ErrorReadNum SupportReadNumlist[depth] = SupportReadNum&#125;Difference= SupportReadNumlist - ErrorReadNumlisttype=c(rep(paste('Error:',ErrorRate),times=Totaldepth),rep(paste('Detect:',LoDRate),times=Totaldepth),rep('Difference',times=Totaldepth))depth=c(depthlist,depthlist,depthlist)read_num=c(ErrorReadNumlist,SupportReadNumlist, Difference)data=data.frame(type, depth,read_num)# ggplot(data,aes(x=depth,y=read_num,cluster=type,color=type))+geom_line()+geom_hline(yintercept = 2)## 确定目标深度Target_depth="Not Find "for(i in Totaldepth:1)&#123; if(Difference[i]&lt;2)&#123; Target_depth = i+1 break &#125;&#125;## 绘图ggplot(data,aes(x=depth,y=read_num,cluster=type,color=type)) + geom_line() + geom_hline(yintercept = 2) + annotate(geom = 'text', x= 2000, y = 30, label = paste("Totaldepth=5000\nErrorRate=",ErrorRate,"\nLOD-E=",LoDRate,"\nCI=",CI,"\nDepth=",Target_depth,"\ncutoff-E=",qbinom(1-CI,Target_depth,LoDRate) ))qbinom(1-CI,Target_depth,LoDRate) 内部共享PPT]]></content>
      <categories>
        <category>NGS</category>
        <category>肿瘤检测</category>
        <category>标准化</category>
      </categories>
      <tags>
        <tag>Standards and guidelines</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Singularity - 入门简介]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-04.pipeline%2Fpipeline-container-Singularity-%E5%85%A5%E9%97%A8%E7%AE%80%E4%BB%8B%2F</url>
    <content type="text"><![CDATA[基本概念Singularity 是基于Docker已经进行应用阶段开发的。针对和Docker的官方说明如下 You don’t need Docker installed You can shell into a Singularity-ized Docker image You can run a Docker image instantly as a Singularity image You can pull a Docker image (without sudo) You can build images with bases from assembled Docker layers that include environment, guts, and labels Singularity 安装可以直接使用conda进行安装，这里最好使用root账户安装，不然后续使用依然可能存在权限问题。 1conda create -n singularity singularity Singularity 环境配置12# 配置singularity 下载镜像时的存储目录export SINGULARITY_CACHEDIR=/ifstj2/B2C_RD_H1/Personal/liubo/singularity 基于docker镜像构建singature镜像123456789101112131415161718192021222324252627282930313233343536# 方式1：从docker uri开始singularity pull docker://godlovedc/lolcow # 下载pre-built imagesingularity build mylolcow_latest.sif docker://godlovedc/lolcow # 下载后再build成镜像# 方式2：从本地缓存的docker image开始$ sudo docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEgodlovedc/lolcow latest 577c1fe8e6d8 16 months ago 241MB$ sudo singularity build lolcow_from_docker_cache.sif docker-daemon://godlovedc/lolcow:latest# 和方式1 的区别# 从原来的docker变成了docker-daemon；# 这里需要加sudo，因为docker程序运行时需要sudo权限；# docker的镜像后面必须要加TAG标签，可以通过sudo docker images查看TAG标签。# 方式3：从镜像tar文件开始$ sudo docker imagesREPOSITORY TAG IMAGE ID CREATED SIZE hello-world latest d1165f221234 5 months ago 13.3kB$ sudo docker save d1165f221234 -o hello_world.tar$ singularity build hello_world.sif docker-archive://hello_world.tarINFO: Starting build...Getting image source signaturesCopying blob f22b99068db9 doneCopying config 1189c90721 doneWriting manifest to image destinationStoring signatures2021/08/15 16:46:15 info unpack layer: sha256:a99912efe9c767b280c869d2e734bef3c92d29d45d4e3beefbeb5a1924ed7445INFO: Creating SIF file...INFO: Build complete: hello_world.sif# 和方式1的主要区别# 原本的docker://换成了docker-archive://，方式2里面是docker-daemon://；# 这里不需要加sudo了，只需要当前用户有hello_world.tar文件的可访问权限即可；# 这里用的是tar文件，singularity也可以处理tar.gz文件。 运行Singularity 容器12345# 使用exec直接运行任务/share/app/singularity/3.8.1/bin/singularity exec ensembl-vep_release_108.2.sif vep# 运行容器后进入容器交互式执行任务/share/app/singularity/3.8.1/bin/singularity shell ensembl-vep_release_108.2.sif 挂在目录( -B)1/share/app/singularity/3.8.1/bin/singularity exec -B /ifstj2/B2C_RD_H1/Research_and_Development/ -B /ifstj2/B2C_RD_H1/Personal/liubo ensembl-vep_release_108.2.sif vep --fork 4 --cache --dir_cache /ifstj2/B2C_RD_H1/Research_and_Development/03.database/vep_cache_dir --assembly GRCh37 --offline --refseq --exclude_predicted --fasta /ifstj2/B2C_RD_H1/Research_and_Development/03.database/Hg19/hg19.fa -i /ifstj2/B2C_RD_H1/Personal/liubo/singularity/22S03467478.indel.final.vcf -o /ifstj2/B2C_RD_H1/Personal/liubo/singularity/test/22S03467478.indel.vcfanno.tab --format vcf --vcf_info_field ANN --force_overwrite --tab --verbose --no_escape --everything --shift_3prime 1 --dir_plugins /ifstj2/B2C_RD_H1/Research_and_Development/03.database/VEP_plugins/VEP_plugins-release-108 --plugin TSSDistance --plugin TERT --custom /ifstj2/B2C_RD_H1/Personal/liubo/0.Pipeline/aio.NewAnnotation/subpipeline/submodel_annotation_vep/Database/human_maked.gene.gff.gz,,gff --custom /ifstj2/B2C_RD_H1/Research_and_Development/03.database/vep_inhouse_data/clinvar.vcf.gz,ClinVar,vcf,exact,0,CLNSIG,CLNREVSTAT,CLNDN 使用root身份运行常见问题缓存目录更改在构建镜像时，会先将docker镜像打包成tar包，在本地处理，所以可能遇到缓存的默认目录空间不足或其他原因需要调整目录。123$ singularity build wes_v1.0.2.sif docker-daemon:wes:v1.0.2INFO: Starting build...FATAL: While performing build: conveyor failed to get: error copying contents to temporary file "/tmp/bundle-temp-1174752785/docker-tar3715295370": write /tmp/bundle-temp-1174752785/docker-tar3715295370: no space left on device singularity默认缓存目录通过 SINGULARITY_TMPDIR 指定。1export SINGULARITY_TMPDIR=/mnt/large_disk/singularity_tmp]]></content>
      <categories>
        <category>pipeline</category>
        <category>镜像</category>
        <category>container</category>
      </categories>
      <tags>
        <tag>流程开发</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[docker - Halos aio容器构建命令记录]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-04.pipeline%2Fpipeline-container-docker-demo.aio%2F</url>
    <content type="text"><![CDATA[12yum install ghostscript]]></content>
      <categories>
        <category>pipeline</category>
        <category>镜像</category>
        <category>container</category>
      </categories>
      <tags>
        <tag>流程开发</tag>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[docker - 入门简介及环境配置]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-04.pipeline%2Fpipeline-container-docker-01.%E5%85%A5%E9%97%A8%E7%AE%80%E4%BB%8B%E5%8F%8A%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[Docker容器与虚拟机类似，但二者在原理上不同。容器是将操作系统层虚拟化，虚拟机则是虚拟化硬件，因此容器更具有便携性、更能高效地利用服务器。 容器更多的用于表示软件的一个标准化单元。由于容器的标准化，因此它可以无视基础设施（Infrastructure）的差异，部署到任何一个地方。另外，Docker也为容器提供更强的业界的隔离兼容。 基本概念 镜像（Image）：Docker 镜像就相当于是一个静态的root文件系统。就类似一个模板，根据这个模板可以创建多个容器。 容器（Container）：镜像和容器的关系，就像是面向对象程序设计中的类和实例一样，镜像是静态的定义，容器是镜像运行时的实体。容器可以被创建、启动、停止、删除、暂停等。 仓库（Repository）：用来保存镜像的地方。有共有仓库和私有仓库之分。 相关资源官网文档Docker官方镜像仓库 账号：6143****@qq.comDocker官方文档 培训材料大IT一体机培训PPT 安装Docker客户端安装Docker是一种流行的容器化平台，它可以帮助开发人员和运维团队简化应用程序的构建、部署和管理过程。本文将介绍Docker的安装、配置以及一些常用的命令示例，帮助您快速上手并开始使用这个强大的工具。docker常用的Linux平台安装命令如下，123456# Install on Debian/Ubuntusudo apt-get updatesudo apt-get install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin# Install Docker Engine on CentOSsudo yum install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin 在Windows/Mac上安装，可以直接在官网下载 Docker Desktop 程序，并按照提示进行安装。 启动docker1sudo systemctl start docker 确定安装成功1sudo docker run hello-world vscode 的Docker插件 docker 初始配置权限配置因为Docker引擎使用Linux中的cgroups和namespaces等技术来隔离容器，这些技术需要访问主机系统资源。Docker的守护进程是绑定到Unix socket 的，因此总是默认是root身份运行的。所以默认必须使用root用户或使用 sudo 才能运行docker引擎，其他用户需要将其添加到Docker组中才能运行Docker命令。因此在初次安装Docker后，我们需要进行如下的系统组创建，以便使非root用户运行docker1234567891011# 创建docker组sudo groupadd docker# 将用户添加到Docker组中, 也可以以root身份直接编辑/etc/group 文件，sudo usermod -aG docker &lt;your-user&gt;# 激活组的变更，重新登录也可以激活newgrp docker# 测试docker安装和配置docker run hello-world 仓库配置Docker官方仓库Docker Hub 是目前Docker官方维护的仓库，其中已经包括了数量超过15000个镜像。大部分需求都可以通过在Docker Hub中直接下载镜像来使用。配置也比较简单，直接使用docker login， 输入用户信息即可12345678910(base) root@manager[Fri Jul 07] ~/.docker$ docker loginLogin with your Docker ID to push and pull images from Docker Hub. If you don't have a Docker ID, head over to https://hub.docker.com to create one.Username: $usernamePassword:WARNING! Your password will be stored unencrypted in /home/root/.docker/config.json.Configure a credential helper to remove this warning. Seehttps://docs.docker.com/engine/reference/commandline/login/#credentials-storeLogin Succeeded 第三方仓库第三方仓库笔者只接触使用过阿里云的ACR，针对ACR服务，本身就提供了比较详细的代码示例。整体注册登录也比较简单。1$ docker login --username=bgioseq registry.cn-shenzhen.aliyuncs.com 私有仓库由于企业业务的特殊性，可能有时候，我们不方便讲我们的镜像放到第三方仓库，以防代码泄露等危险，这种时候，我们可以选择构建私有仓库。私有仓库构建，可以通过运行官方registry镜像来实现。默认情况下，仓库会被创建在容器的/var/lib/registry目录下。可以通过-v参数来将镜像文件存放在本地的指定路径。12# 可以使用Docker的 registry 镜像docker run -d -p 5000:5000 --name registry registry:2 然后你的私有仓库就创建好了，私有仓库的就是 localhost:5000当然也许你会使用你的同事创建的私有仓库，那你就需要更新下本地配置文件/etc/docker/daemon.json ，将IP改为部署服务的主机IP12345&#123; "insecure-registries":[ "192.168.1.1:5000" ]&#125; 仓库配置信息会保存在~/.docker/config.json文件中；12345678910&#123; &quot;auths&quot;: &#123; &quot;https://index.docker.io/v1/&quot;: &#123; &quot;auth&quot;: &quot;YVuYJlbkBkb2NrZXI=&quot; &#125;, &quot;registry.cn-shenzhen.aliyuncs.com&quot;: &#123; &quot;auth&quot;: &quot;bGlZXE6TGl1Ym8xMjM=&quot; &#125; &#125;&#125; 其中auth是使用 base64 编码的秘钥凭据，可以使用base64 -d 反编译获取原账号密码 缓存目录配置在链接了Docker镜像仓库后，我们接下来就要使用镜像来提高我们的效率了。但是不管事下载公共镜像，还是创建本地镜像，docker都会保存对应的镜像文件到缓存目录中。我们可以通过docker info 查看对应的缓存目录配置信息：123***:***Docker Root Dir: /var/lib/docker # docker的默认缓存目录****:** 但是有时候我们需要考虑整体的镜像管理，和整个服务器的磁盘空间，所以一些情况下，我们需要更改目录。 更改缓存目录12systemctl stop docker #停止docker服务vi /etc/docker/daemon.json # 编辑配置文件 我们可以通过修改配置文件/etc/docker/daemon.json来修改缓存目录。123&#123; "data-root": "/data/docker"&#125; 然后重启docker服务1systemctl start docker 镜像配置因为在访问中，我们可能会有一些需求访问官方镜像，和其他类似Conda、R的官方仓库使用一样，官方镜像也许会存在一些网络上的问题，所以我们可以调整使用国内的镜像，以便提高访问速度 1234#配置阿里云镜像sudo yum-config-manager \ --add-repo \ https://registry.docker-cn.com 镜像加速器 镜像加速器地址 Docker 中国官方镜像 https://registry.docker-cn.com DaoCloud 镜像站 http://f1361db2.m.daocloud.io Azure 中国镜像 https://dockerhub.azk8s.cn 科大镜像站 https://docker.mirrors.ustc.edu.cn 阿里云 https://&lt;your_code&gt;.mirror.aliyuncs.com 七牛云 https://reg-mirror.qiniu.com 网易云 https://hub-mirror.c.163.com 腾讯云 https://mirror.ccs.tencentyun.com 以上镜像已经全部失效，不单独维护，可用镜像请参考 国内镜像。 部署完以后，可以拉个镜像试一下 ​12345678(base) [root@VM_0_8_centos ~]\# docker pull centosUsing default tag: latestlatest: Pulling from library/centosa1d0c7532777: Pull completeDigest: sha256:a27fd8080b517143cbbbab9dfb7c8571c40d67d534bbdee55bd6c473f432b177Status: Downloaded newer image for centos:latestdocker.io/library/centos:latest(base) [root@VM_0_8_centos ~]\#]]></content>
      <categories>
        <category>pipeline</category>
        <category>镜像</category>
        <category>container</category>
      </categories>
      <tags>
        <tag>流程开发</tag>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[文件格式说明 - MAF(Mutation Annotation Format )]]></title>
    <url>%2F01.%E7%9F%A5%E8%AF%86-03.standard%2Ffileformat-maf%2F</url>
    <content type="text"><![CDATA[规范化参考来源TCGA 是其定义了一个相对标准的变异描述格式MAF 其中文件定义了，提升流程的规范性。同时，存在一个maftools，可以提供比较丰富的基于MAF文件的可视化处理。maftools : Summarize, Analyze and Visualize MAF Files]]></content>
      <categories>
        <category>NGS</category>
        <category>知识沉淀</category>
        <category>肿瘤检测</category>
        <category>文件格式</category>
      </categories>
      <tags>
        <tag>文件格式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[共识-用药推荐的驱动突变汇总]]></title>
    <url>%2F01.%E7%9F%A5%E8%AF%86-03.standard%2F%E5%85%B1%E8%AF%86-%E7%94%A8%E8%8D%AF%E6%8E%A8%E8%8D%90%E7%9A%84%E9%A9%B1%E5%8A%A8%E7%AA%81%E5%8F%98%E6%B1%87%E6%80%BB%2F</url>
    <content type="text"><![CDATA[癌种 驱动突变 治疗方案 一线治疗 肺癌 ALK 基因融合 肺癌 RET 基因融合 普拉替尼作为二线治疗的II级推荐，LOXO-292（国内未上市）为III级推荐]]></content>
      <categories>
        <category>NGS</category>
        <category>肿瘤检测</category>
        <category>共识</category>
      </categories>
      <tags>
        <tag>驱动突变</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[非小细胞肺癌分子残留病灶专家共识]]></title>
    <url>%2F01.%E7%9F%A5%E8%AF%86-03.standard%2F%E5%85%B1%E8%AF%86-%E9%9D%9E%E5%B0%8F%E7%BB%86%E8%83%9E%E8%82%BA%E7%99%8C%E5%88%86%E5%AD%90%E6%AE%8B%E7%95%99%E7%97%85%E7%81%B6%E4%B8%93%E5%AE%B6%E5%85%B1%E8%AF%86%2F</url>
    <content type="text"><![CDATA[原文链接非小细胞肺癌分子残留病灶专家共识非小细胞肺癌分子残留病灶专家共识.pdf 背景由中国抗癌协会肺癌专业委员会和广东省临床试验协会/中国胸部肿瘤研究协作组主办的“第18届中国肺癌高峰论坛”于2021年3月6日在广州顺利召开。此次论坛以非小细胞肺癌（non⁃small cell lung cancer，NSCLC）分子残留病灶（molecular residual disease，MRD）为主题，从肺癌MRD 定义、检测以及临床应用三个角度开展相关学术探讨，并最终达成了专家共识。 MRD 在某些情况下也被称为微小残留病灶（minimal residual disease）或可测量残留病灶（measurable residual disease），其概念自血液肿瘤逐步延伸至实体肿瘤，检测对象主要包括有循环肿瘤DNA（circulating tumor DNA，ctDNA）或循环肿瘤细胞（circulating tumor cell，CTC）等。目前在肺癌这一癌种中，MRD 研究仍处于前期研究证据的积累当中，不同研究之间关于MRD的定义及研究方法差别较大，研究结论之间难以完全互通，整体上处于“摸着石头过河”的前期探索阶段。因此，本次肺癌高峰论坛主题与以往有所不同，重在“展望”，即在肺癌MRD研究热潮即将到来之时，综合近年来国内外重要研究结果及争议点，与会专家们进行了详细的讨论和各抒己见的争辩，旨在为今后的肺癌MRD研究制定规范及主体方向，并最后达成了以下五点共识。 本共识的共识级别为： 1A级：基于高水平证据（严谨的meta分析或随机对照试验结果），专家组有统一认识； 1B级：基于高水平证据（严谨的meta分析或随机对照试验结果），专家组有小争议； 2A级：基于低水平证据，专家组有统一认识； 2B级：基于低水平证据，专家组无统一认识，但争议不大； 3级：专家组存在较大争议 共识内容共识一：非小细胞肺癌MRD概念（共识级别：2A） 肺癌分子残留病变，指经过治疗后，传统影像学（包括PET/CT）或实验室方法不能发现，但通过液体活检发现的癌来源分子异常，代表着肺癌的持续存在和临床进展可能； 肺癌分子异常，指在外周血可稳定检测出丰度≥0.02%的ctDNA，包括肺癌驱动基因或其他的Ⅰ/Ⅱ类基因变异 共识二：非小细胞肺癌MRD检测的基本技术要求（共识级别：2B） MRD 检测的基本技术，包括肿瘤先验分析（tumor⁃informed assays，个体化定制或NGS panel）和肿瘤未知分析（tumor⁃agnostic assays，NGS panel和多组学技术），目前均处在探索阶段，需要前瞻性研究确定其敏感性、特异性和预测价值； 基于NGS 的突变检测技术，所选用的多基因panel中必须覆盖患者Ⅰ/Ⅱ类基因变异，基本技术标准是可稳定检测出丰度≥0.02%的ctDNA； 驱动基因阳性的非小细胞肺癌，MRD的分子检测panel应包括其驱动基因； MRD 评估报告中必须包括cfDNA 浓度、ctDNA浓度及所检测基因VAF值； 需要建立针对免疫治疗的MRD标准 共识三：可手术早期非小细胞肺癌MRD的应用（共识级别：2A） 早期非小细胞肺癌患者根治性切除术后，MRD阳性提示复发风险高，需进行密切随访管理，建议每3~6个月进行一次MRD检测； 建议基于MRD开展可手术非小细胞肺癌的围术期临床试验，尽可能提供围术期精准治疗方案； 建议分别探索MRD在驱动基因阳性和驱动基因阴性两种类型患者中的作用 共识四：局部晚期非小细胞肺癌MRD的应用（共识级别：2A） 局部晚期非小细胞肺癌根治性化放疗后完全缓解患者，建议检测MRD，有助于判断预后和制定进一步的治疗策略； 建议开展基于MRD的化放疗后巩固治疗的临床试验，尽可能提供精准的巩固治疗方案。 共识五：晚期非小细胞肺癌MRD的应用（共识级别：2A） 晚期非小细胞肺癌目前缺乏针对MRD的相关研究； 晚期非小细胞肺癌系统治疗后完全缓解患者，建议检测MRD，有助于判断预后和制定进一步的治疗策略； 建议在完全缓解患者中开展基于MRD的治疗策略研究，尽可能延长完全缓解持续时间，使患者能最大获益]]></content>
      <categories>
        <category>NGS</category>
        <category>肿瘤检测</category>
        <category>共识</category>
      </categories>
      <tags>
        <tag>循证医学</tag>
        <tag>MRD</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MSI检测及相关共识]]></title>
    <url>%2F01.%E7%9F%A5%E8%AF%86-03.standard%2F%E5%85%B1%E8%AF%86-MSI%E6%A3%80%E6%B5%8B%E5%8F%8A%E7%9B%B8%E5%85%B3%E5%85%B1%E8%AF%86%2F</url>
    <content type="text"><![CDATA[摘要微卫星不稳定性（microsatellite instability，MSI）由DNA错配修复（mismatch repair，MMR）蛋白功能缺陷导致，这一分子特征在结直肠癌和子宫内膜癌等相关实体瘤中具有重要的临床意义。目前检测MSI状态的手段包括免疫组织化学检测MMR蛋白、多重荧光聚合酶链反应（polymerase chain reaction，PCR）检测微卫星位点和基于二代测序（next generation sequencing，NGS）平台的MSI算法。本共识针对MSI的定义、临床意义及其3类检测手段各自的优势与不足展开阐述和推荐。希望专家共识的制订可大力推动恶性肿瘤MSI状态普筛工作，提高临床医师对各种检测方法的认识，从而更加准确地解读检测结果，为患者提供更优质的临床服务。 微卫星不稳定的定义微卫星(microsatellite, MS)是指细胞基因组中以少数几个核苷酸(多为1~6个)为单位串联重复的DNA序列，又称短串联重复(short tandem repeat, STR)。DNA错配修复(mismatch repair, MMR)功能出现异常时，微卫星出现的复制错误得不到纠正并不断累积，使得微卫星序列长度或碱基组成发生改变，称为微卫星不稳定性(microsatellite instability, MSI)，同时导致基因组呈现高突变表型。肿瘤中，MMR功能缺陷往往由于MMR基因(MLH1、MSH2、MSH6及PMS2)及其相关基因EPCAM的致病性突变导致，也可能由于MLH1启动子区高甲基化引起的MLH1表达缺失导致[1]。MSI现象于1993年在结直肠癌中被首次发现[2]。MSI根据程度可以被分成3类：微卫星高度不稳定性(MSI-high, MSI-H)、微卫星低度不稳定性(MSI-low, MSI-L)、微卫星稳定(microsatellite stability, MSS)[3]。MSI-H在不同癌种中的发生率存在较大差异。目前已知MSI-H发生率较高的实体瘤包括子宫内膜癌(20%~30%)、胃癌(15%~20%)和结直肠癌(12%~15%，其中Ⅳ期结直肠癌4%~5%)等[4]。 MSI状态检测方法及其对比IHC检测MMR蛋白MSI多由MMR蛋白表达缺失导致的MMR功能缺陷所致，故可通过检测MMR蛋白缺失来反映MSI状态。通过IHC检测MMR蛋白表达与基于DNA分析检测MSI状态是评估相同生物学效应的不同检测方法[5-6]。IHC方法采用分别针对MLH1、MSH2、MSH6及PMS2的特异性抗体，阳性表达定位于细胞核。如肿瘤样本中4个MMR蛋白均阳性表达，则为错配修复功能完整(proficient mismatch repair, pMMR)；任一MMR蛋白缺失即为dMMR。 多重荧光PCR毛细管电泳法检测MSI直接检测MSI状态的常用方法是多重荧光PCR毛细管电泳法，这也是当前公认的MSI检测“金标准”。当前市面上存在多个基于PCR平台的MSI检测商用试剂盒，但国内此类试剂盒尚未获得国家药品监督管理局(National Medical Products Administration, NMPA)批准。此类试剂盒设计原理均基于美国国立癌症研究所(National Cancer Institute, NCI)建议的MS位点[并进行微调和(或)扩展]，将肿瘤细胞与正常细胞的PCR法检测结果进行比较，以确定肿瘤细胞的MSI状态(有关PCR法检测MS位点及其判读的介绍详见附录1)《CSCO结直肠癌诊疗指南2018版》建议采用NCI推荐的5个MS位点(BAT-25、BAT-26、D2S123、D5S346和D17S250)进行MSI检测[7]。《遗传性结直肠癌临床诊治和家系管理中国专家共识》(2018)指出，基于肿瘤组织样本的MSI检测作为可选推荐，建议在有条件的医疗单位开展[。 二代测序(next generation sequencing, NGS)检测MSI近年来，随着高通量测序平台的广泛应用，NGS平台目标区域测序(即NGS panel)或全外显子组测序(whole exon sequencing, WES)/全基因组测序(whole genome sequencing, WGS)开始应用于MSI检测，使用计算工具同时研究基因组上的大量微卫星序列成为可能(当前主流NGS-MSI算法原理详见附录2)。2018年ESMO年会上，ESMO精准医学工作组(ESMO Precision Medicine Working Group)推荐将NGS作为MSI的二线检测方法(second line testing)。NCCN结直肠癌临床实践指南亦指出，MSI检测可通过经验证的NGS panel进行，尤其是对于那些需要同时检测RAS/BRAF突变状态的转移性CRC患者。 此外，基于外周血循环肿瘤DNA(circulating tumor DNA, ctDNA)的MSI(MSI from blood ctDNA, b-MSI)-NGS算法亦已崭露头角，为肿瘤组织取样困难或不足的晚期实体瘤患者MSI检测提供新选择。多个血检NGS panel及其各自bMSI-NGS算法数据已经开始在国际学术会议上以摘要形式陆续披露(表 2)[39-41]，但尚未在学术期刊上以全文形式发表。因此，b-MSI-NGS检测在当前不应常规推荐，仅作为缺乏组织的患者为明确MSI状态的一种替代手段。 MSI的临床意义 MSI检测作为林奇综合征初筛手段 MSI是Ⅱ期结直肠癌预后因子 MSI是Ⅱ期结直肠癌辅助化疗疗效预测因子 MSI是晚期实体瘤免疫治疗疗效预测因子 MSI检测专家共识 推荐意见1：所有结直肠癌患者均应进行MSI状态筛查 推荐意见2：晚期实体瘤患者(如胃癌、小肠癌、子宫内膜癌、尿路上皮癌、胰腺癌和胆管癌等)如考虑免疫治疗应行MSI状态检测 推荐意见3：MSI检测方法包括经认证的IHC、PCR和NGS方法(3种方法各有优势) IHC法可以直接鉴定出导致MSI-H发生的MMR缺陷基因。IHC法检测MMR蛋白表达可在多数医院的病理科完成，普及性强，且价格低廉。但该方法受判读人员的主观影响较大，存在一定的假阳性与假阴性。因此IHC的前处理务必遵循病理科相关规范，结果判断应由有经验的病理医师完成或进行双人复核，以尽可能避免个人主观造成偏倚。 基于样本微切割的多重荧光PCR毛细管电泳法，简便且便宜，敏感度和特异度均较好(特别是在经广泛验证的CRC肿瘤样本中)，但全国能开展该项检测的病理科相对较少，且存在一定的假阴性。 对于需要同时检测肿瘤驱动基因和(或)治疗相关基因变异的患者，目标区域NGS是个不错的选择。NGS法可同时检测panel覆盖的驱动基因变异，包括MMR基因胚系和(或)体细胞突变，甚至TMB等分子标签。基于ctDNA样本的b-MSI检测目前正处于验证阶段，有望为肿瘤组织取样困难或不足的晚期肿瘤患者提供新的选择。但NGS单独用于MSI检测则不推荐，理由是增加经济负担和浪费资源。所有基于NGS的MSI检测应用于临床前必须进行充分的可靠性认证及临床验证。如果遇到患者同时进行IHC-MMR蛋白表达和DNA分析(PCR或NGS法)MSI状态检测且检测结果不一致的情况，可考虑采用第3种方法(NGS或PCR法)进行验证(部分专家推荐)。 PCR方法的微卫星位点选择基于PCR方法的微卫星位点选择曾经历数次重大变迁。 1998年NCI推荐用于检测微卫星状态的5个微卫星位点，包括2个单核苷酸重复位点(BAT-25和BAT-26)以及3个双核苷酸重复位点(D2S123、D5S346和D17S250)；判读标准为：≥2个微卫星位点不稳定为MSI-H，1个位点不稳定为MSI-L，所有位点均稳定即为MSS。 NCI的2B3D位点是通过1997年两个重要的多中心研究筛选了30多个微卫星位点最终确认的，所以几乎国外所有的指南都是基于当年的这两个研究推荐MSI的检测位点。 2002年修订为Pentaplex panel，包含5个单核苷酸重复位点BAT-25、BAT-26、NR21、NR24和NR27，以提高检测敏感度及特异度。 2006年，MSI分析系统Promega以Mono-27取代NR27，并增加Penta C和D用于样本识别，进一步提高MSI检测敏感度。 从国际上多个 MSI 的研究来看，中国人群的 MSI-H 发生率是略低于欧美人群的，约为 4.5~13.3%，提示 MSI 的发生可能有种族差异性，位点的选择需要有中国人群的临床数据支持。「2B3D NCI Panel」在国内外都是最权威的检测标准。关于基于中国人群的多中心研究显示[1,2]，「2B3D NCI Panel」敏感度高于单核苷酸 Promega Panel，提示「2B3D NCI Panel」更适合中国肿瘤患者。比较「2B3D」 Panel 和 Promega Panel 两种 Panel 的检测结果与 MMR IHC 结果的一致性显示： 两个 Panel 的特异性相当，均为 99.1%（215/217），2B3D Panel 灵敏度为 89.3%(25/28) 高于 Promega 的 71.4%(20/28)[2] 中国人群中，Zhang等纳入了近6000例MSI检测结果的Meta分析数据最终显示，2B3D位点与Promega Panel的检出阳性率在散发性结直肠癌一致（13.5% vs 12.9%），但是，如果采用其他单核苷酸Panel（6个位点的MSI Panel），检出阳性率仅7.7%！至少漏诊了30%以上的MSI-H肿瘤患者！ 当前主流NGS-MSI算法原理基于NGS的MSI检测原理分为以微卫星位点重复序列长度变化为基础以及以突变负荷和突变类型为基础。其中，基于NGS目标区域测序(targeted gene sequencing, TGS)的检测技术通常基于前者进行MSI检测。此类MSI检测的技术核心包括选取最有效的微卫星标志位点组合以及构建合理的分类模型用以最大程度区分MSI-H和MSS状态下微卫星位点重复单元长度变化水平的差异，使得MSI检测敏感度和特异度达到最优，并保证在低肿瘤占比样本中检测的稳健性。 标志位点通常表现为在MSS状态下重复单元长度高度稳定，而MSI-H状态下高频不稳定，以保证MSI检测的最优敏感度和特异度。基于WGS和WES的研究证明，不同标志位点可能表现为癌种特异性，也可具有跨癌种普遍性2种特征，而其余大量微卫星位点高度稳定，无法为MSI-H提供有效信息，故而针对Panel的不同用途(特异性癌种/泛癌种)应选择不同的位点组合[36]。前期PCR-MSI研究表明，单核苷酸重复序列在PCR-MSI检测中敏感度更高[33]。而多核苷酸重复序列本身的多态性导致检测方法对配对正常样本具有依赖性。基于PCR法中的Pentaplex panel的5~7个标志位点组合BAT25、BAT26、NR21、NR22、NR24、NR27和MONO27被认定为PCR-MSI检测的金标准。由此该位点组合也通常在NGS panel中专门被设计和应用于MSI检测。 NGS panel的MSI算法通常通过刻画选取的标志位点不同重复序列长度对应的reads个数在MSI-H和MSS中的差异来判断位点的不稳定状态，以不稳定位点比例是否超过既定阈值来确定样本的MSI状态。位点在2种状态差异的评估方法包括：基于癌组织样本和配对正常样本不同重复序列长度对应的reads个数进行χ2检验(MSIsensor)或者距离评估(MANTIS)，以及独立的基于癌组织样本评估重复序列长度的类型个数与一组基线样本(非MSI-H样本)的差异(mSINGS)，或者MSS状态下主要的重复序列长度类型(peak)对应的覆盖占位点覆盖的比例在癌组织样本与一组基线样本(非MSI-H样本)的差异(ColonCore-MSI)，从而评估位点的稳定状态。选取的标志位点限制为1~5 bp重复单元(如MSIsensor和MANTIS)或仅单碱基重复序列(如ColonCore-MSI)的MS位点。判断样本MSI状态的不稳定位点比例的阈值包括3.5%(MSIsensor)、20%(mSINGS)和40%(ColonCore-MSI)不等。 各指南情况2021 年更新的 NCCN 指南将推荐 MSI 检测的适应症癌种从结肠癌、直肠癌、胃癌、子宫内膜癌扩大到了前列腺癌、胰腺癌等多个实体瘤。结直肠癌、胃癌、子宫内膜癌、小肠腺癌、胰腺癌这 5 个临床常见癌种 MSI 的发生率非常高. 微卫星不稳定性(MSI)检测技术专家共识 2024.03 主要针对PCR 发进行的说明，基本不涉及NGS 方案。参考指南&amp;共识 结直肠癌及其他相关实体瘤微卫星不稳定性检测中国专家共识 结直肠癌及其他相关实体瘤微卫星不稳定性检测中国专家共识 结直肠癌分子生物标志物检测专家共识 Chinese Society of Clinical Oncology (CSCO) diagnosis and treatment guidelines for colorectal cancer 2018 (English version) （2020.V1）NCCN临床实践指南：肝胆肿瘤.bak.pdf （2020.V1）NCCN临床实践指南：宫颈癌.pdf （2020.V1）NCCN临床实践指南：卵巢癌包括输卵管癌和原发性腹膜癌.pdf （2020.V1）NCCN临床实践指南：前列腺癌.pdf （2020.V1）NCCN临床实践指南：食道癌和胃食管交界处癌.pdf （2020.V1）NCCN临床实践指南：胃癌.pdf （2020.V1）NCCN临床实践指南：子宫肿瘤.pdf （2020.V2）NCCN临床实践指南：结肠癌.pdf （2020.V2）NCCN临床实践指南：小肠腺癌.pdf （2020.V3）NCCN临床实践指南：乳腺癌.pdf 丁香园]]></content>
      <categories>
        <category>NGS</category>
        <category>肿瘤检测</category>
        <category>共识</category>
      </categories>
      <tags>
        <tag>MSI</tag>
        <tag>实用肿瘤杂志</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[流程管理规范]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-04.pipeline%2F%E6%B5%81%E7%A8%8B%E7%AE%A1%E7%90%86%E8%A7%84%E8%8C%83%2F</url>
    <content type="text"><![CDATA[历史材料流程管理优化.pptx 流程版本号规范结合目前肿瘤事业部落地推广的版本管理规范，进行介绍。流程版本号，共计4位。（ v主版本号 . 子版本号 . 阶段版本号. 修正版本号 )上级版本号升级时，所有下级版本号均需归为初始状态（0） 主版本号表示在软件或项目发生重大变更或功能修改时递增。当出现不兼容的 API 或用户界面变更时，应递增主版本号。例如，从版本1.0.0升级到2.0.0表示有重大功能变更或不兼容的修改。 子版本号 （某些模块/方法的重构性升级，需要设计独立的方法学评估）在原有的基础上增加了部分功能，或某些检测模块发生了重要的技术升级/方法迭代（一般需要进行转产答辩），主版本号不变，子版本号加 1，阶段版本号和修正版本号复位为 0，； 阶段版本号 (涉及流程输入/输出格式变动的更改/需求，或修订累计需求较多，不涉及流程整体框架和模块思路的重构)在完成一个阶段性的需求优化或bug修复后，需要对该阶段版本下的所有修订版本对应的内容进行统一的测试评估。完成生信内部的代码复核及测试后，提交需求方进行验收。需求方验收后，将所有的修正版本下的更新内容合并至主分支。同时阶段版本号加1，修正版本号归。所有涉及输入、输出格式变动的更改，均属于阶段版本号起步 修正版本号 (不涉及流程输入/输出格式变动的更改/需求)在接收到前端、交付等需求方提出的流程优化需求后，完成一个相对独立的需求点、或原有bug修复后，主版本号、子版本号和阶段版本号都不变，修正版本号加 1。每个修正版本号对应一个独立的需求： 例如数据库更新，代码bug的修复，不影响分析结果的资源配置调整等 语义化版本语义化版本（Semantic Versioning）是一种对软件版本号的规范，它使用三个数字来标识版本号：版本格式：主版本号.次版本号.修订号，版本号递增规则如下： 主版本号：当你做了不兼容的 API 修改， 次版本号：当你做了向下兼容的功能性新增， 修订号：当你做了向下兼容的问题修正。 先行版本号及版本编译信息可以加到“主版本号.次版本号.修订号”的后面，作为延伸。完整的语义化版本说明参考文档]]></content>
      <categories>
        <category>知识沉淀</category>
      </categories>
      <tags>
        <tag>pipeline</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于靶标指导乳腺癌精准治疗标志物临床应用专家共识(2022版)]]></title>
    <url>%2F01.%E7%9F%A5%E8%AF%86-03.standard%2F%E5%85%B1%E8%AF%86-%E5%9F%BA%E4%BA%8E%E9%9D%B6%E6%A0%87%E6%8C%87%E5%AF%BC%E4%B9%B3%E8%85%BA%E7%99%8C%E7%B2%BE%E5%87%86%E6%B2%BB%E7%96%97%E6%A0%87%E5%BF%97%E7%89%A9%E4%B8%B4%E5%BA%8A%E5%BA%94%E7%94%A8%E4%B8%93%E5%AE%B6%E5%85%B1%E8%AF%862022%E7%89%88%2F</url>
    <content type="text"><![CDATA[原文链接 专家共识：建议采用标准流程检测乳腺原发灶和转移灶ER、PR、HER2及Ki67，根据临床分型进行预后判断和指导分类治疗，并将生物学标志物纳入TNM分期系统中，以进一步提高分期系统的精确性。新的“复旦四分型”用于指导精准分类治疗有待更大样本的基于我国人群的临床试验验证。 专家共识：建议将CTC纳入cM0（i+）分期，早期乳腺癌患者外周血CTC≥1个提示预后不良。ctDNA可用于监测早期乳腺癌MRD以评估复发风险，根据ctDNA特定突变可指导无法获取肿瘤组织的晚期乳腺癌患者匹配相应的治疗。 专家共识：为了评估遗传风险，建议对相关高风险人群进行遗传咨询及胚系BRCA1 / 2 基因检测。HER2阴性乳腺癌患者建议行BRCA1/2 基因胚系突变检测，以确定PARP抑制剂治疗的优势人群。 专家共识：乳腺癌中PD‐1/PD‐L1抑制剂作为最重要的免疫检查点抑制剂已经改变了三阴性乳腺癌的新辅助和晚期治疗实践。PD‐L1阳性晚期三阴性乳腺癌患者一线可以考虑PD‐1 抑制剂联合化疗方案，早期有高危因素的三阴性乳腺癌也可考虑采用PD‐1抑制剂联合化疗方案进行新辅助治疗。 专家共识：临床可借助pCR 、MP系统、RCB系统、CPS+EG评分等评估早期乳腺癌患者的新辅助化疗疗效，PEPI评分可用于评估新辅助内分泌治疗疗效，以上指标均可用于乳腺癌患者预后评估和指导后续辅助强化治疗方案。 专家共识：STEPP评分可用于预测HR+的绝经前乳腺癌患者的复发风险，CTS5对绝经前和绝经后乳腺癌患者的远处复发转移有良好的预测作用。BCI、PAM50 ROR在HR+/HER2-患者中均具有预后价值，可助力制定精准的术后辅助内分泌强化/延长治疗方案。然而，影响治疗决策的因素复杂且多元，评分系统仅是一种辅助工具，临床治疗决策应从个体实际出发，综合权衡治疗的获益与风险。 专家共识：Oncotype Dx、Mamaprint、RecurIndex和EPclin多基因检测评分在HR+/HER2-早期乳腺癌患者中具有预后判断价值。Oncotype Dx检测的N0期患者，当RS&lt;11分时可考虑豁免化疗；当RS为11~25分时需根据月经情况进行判断；&gt;50岁的患者可考虑豁免化疗，但当RS≥26 分时建议化疗。Oncotype Dx 检测的N1期患者，当RS&lt;26分时需根据月经情况进行判断，绝经前患者在内分泌治疗基础上加用化疗可以降低远处复发率，但无法排除是否受化疗产生的卵巢抑制作用影响；绝经后患者可考虑豁免化疗，但当RS≥26分时建议辅助化疗联合内分泌治疗。MamaPrint检测可将早期乳腺癌区分为高复发风险和低复发风险人群，同时可以避免中等风险的不确定性，其中HR+/HER2-、淋巴结1~3枚阳性、临床判定为高复发风险的患者可应用MamaPrint再次检测，评估为低风险的患者可考虑豁免化疗。RecurIndex可指导乳腺癌患者辅助放疗方案的选择，低复发风险患者建议减免放疗，高复发风险患者建议放疗。EPclin评分可用于区分高复发乳腺癌患者，且高危患者需行术后辅助化疗联合内分泌治疗。 专家共识：HRD 作为泛癌种生物标志物评估PARP抑制剂在临床应用中还处于起始阶段，目前临床上主要通过检测基因组瘢痕或HRR基因突变间接评估HRD状态，但用于评估HRD状态的确切生物标志物尚无统一标准。乳腺癌HRD检测和临床应用的标准化仍任重而道远，但其应用前景值得期待。 专家共识：内分泌治疗是ER+复发/转移性乳腺癌患者优先推荐的治疗手段。分析ESR1 基因突变状态对指导乳腺癌临床精准治疗具有重要意义。综合现有证据，ESR1 突变是ER+乳腺癌继发性耐药的重要机制之一，也是预后不良的指标，而携带ESR1 突变的患者仍可能从氟维司群或联合CDK4/6抑制剂治疗方案中获益。 专家共识：PIK3CA 基因突变可作为预测ER+乳腺癌PI3Kα特异性抑制剂阿培利司疗效的敏感性标志物。现行PCR 检测方法可覆盖大部分PIK3CA 突变形式，故建议采用经认证的试剂和平台常规对复发/转移性乳腺癌患者开展检测；NGS 检测可能不会改变临床实践，但会提供更多的患者基因信息，可以为后续治疗方案的选择提供参考，因此应积极探索NGS在包含PIK3CA、ESR1、HER2等多基因联合检测中的规范应用。 专家共识：FGFR1过表达与乳腺癌不良预后和内分泌治疗耐药密切相关，FGFR1抑制剂联合内分泌治疗的临床研究目前正在进行，有望为HR+合并FGFR1过表达患者提供更有效的治疗手段。 专家共识：CDK4/6抑制剂改善了HR+乳腺癌患者的预后，但是并非所有患者均对CDK4/6抑制剂有反应，且部分对CDK4/6抑制剂敏感的患者也可能产生获得性耐药。CCNE1高表达、Rb缺失、TK活性等在预测CDK4/6抑制剂疗效中具有潜在价值，但仍需进一步验证。 专家共识：HER2是肿瘤增殖、侵袭的重要驱动，除HER2 基因扩增外，HER2 异质性、HER2 低表达、ERBB2 点突变均可影响乳腺癌患者预后及抗HER2治疗反应。以T‐DXd为代表的新一代抗HER2 ADC药物有望对抗HER2低表达和异质性难题。此外，采用泛HER酪氨酸激酶抑制剂，如来那替尼、吡咯替尼可能部分逆转HER2 点突变介导的耐药。 专家共识：PIK3CA/AKT1/PTEN通路不仅在HR+乳腺癌患者中易变异，在三阴性乳腺癌中也常见PTEN缺失等，因此AKT抑制剂等靶向治疗联合化疗在三阴性乳腺癌中仍需进一步确认其疗效和最佳获益人群。 专家共识：PD‐L1、TILs、TMB及MSI可作为乳腺癌患者预后评估及免疫检查点抑制剂疗效预测标志物，但仍需大样本研究验证这些标志物作为免疫治疗伴随诊断标志物及其指导新型免疫治疗策略的应用价值。]]></content>
      <categories>
        <category>NGS</category>
        <category>肿瘤检测</category>
        <category>共识</category>
        <category>乳腺癌</category>
      </categories>
      <tags>
        <tag>中国癌症防治杂志</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[肿瘤突变负荷检测及临床应用中国专家共识（2020年版）]]></title>
    <url>%2F01.%E7%9F%A5%E8%AF%86-03.standard%2F%E5%85%B1%E8%AF%86-%E8%82%BF%E7%98%A4%E7%AA%81%E5%8F%98%E8%B4%9F%E8%8D%B7%E6%A3%80%E6%B5%8B%E5%8F%8A%E4%B8%B4%E5%BA%8A%E5%BA%94%E7%94%A8%E4%B8%AD%E5%9B%BD%E4%B8%93%E5%AE%B6%E5%85%B1%E8%AF%86-2020%2F</url>
    <content type="text"><![CDATA[共识原文链接2020.10 肿瘤突变负荷检测及临床应用中国专家共识（2020年版）肿瘤突变负荷（tumor mutational burden，TMB）作为一个新兴的生物标志物，在预测肿瘤免疫治疗疗效中的作用越来越受到重视。目前 TMB 的检测方法主要是基于高通量测序平台的全外显子测序和靶向 Panel 测序的拟合算法，但检测方法、阈值和报告格式缺乏统一标准。此外，TMB 值在不同癌种中存在显著差异，也为该标志物在临床中规范应用带来困难。本共识围绕 TMB 的定义、临床意义、检测标准化及与其他免疫标志物如 PD鄄L1、dMMR/MSI鄄H 的关系等要点，结合中国实践，为临床提供 8 条 TMB 检测及应用共识推荐。希望本专家共识可以提高临床医师及检测人员对 TMB 临床意义及检测规范的认识，从而更加准确地解读检测结果，为患者提供更优质的临床服务。 2021.11 肿瘤突变负荷应用于肺癌免疫治疗的专家共识肺癌是全球范围内发病率和死亡率最高的恶性肿瘤之一。免疫检查点抑制剂（ immune checkpoint inhibitors, ICIs），包括程序性死亡受体1（ programmed cell death 1, PD-1）抗体、程序性死亡受体1配体（ programmed cell death ligand 1, PD-L1）抗体和细胞毒性T淋巴细胞相关蛋白4（ cytotoxic T lymphocyte antigen 4, CTLA-4）抗体，给部分晚期肺癌患者带来了显著的生存获益，改变了晚期肺癌的治疗格局。既往研究表明， PD-1/PD-L1抗体在晚期非小细胞肺癌（ non-small cell lung cancer, NSCLC）中的客观缓解率只有20%左右。所以临床亟需可靠的生物标志物协助筛选ICIs潜在获益人群，提高治疗响应率。肿瘤突变负荷（ tumor mutational burden, TMB）是除PD-L1表达以外新兴的免疫治疗标志物。肺癌中PD-L1表达与TMB的相关性不大，评估TMB可扩大免疫治疗的获益人群。然而在临床实践中， TMB的检测、阈值的确定和临床指导策略仍然没有形成规范。本共识将对TMB检测和应用场景给出指导性建议，以促进TMB在肺癌免疫治疗中应用的规范化。 核心内容：肿瘤突变负荷检测及临床应用中国专家共识（2020年版） 专家共识：TMB 一般是指特定区域内体细胞非同义突变的个数，通常用每兆碱基有多少个突变表示（XX 个突变/Mb）。TMB 评估受样本质量和数量、检测基因组大小、生信分析方法等多种因素影响，临床应用前应了解 TMB 的适用范围。不同检测方法获得的 TMB 应进行系统评估,判断是否具有可比性。TMB 数值可反映肿瘤内产生肿瘤新抗原的潜力，与 DNA 修复缺陷密切相关，在多种肿瘤中dMMR和 MSI鄄H 患者具有较高的 TMB。 专家共识：tTMB 是一个新兴的独立 ICIs 治疗疗效预测标志物，与多种肿瘤类型 ICIs 单药或两种ICIs 联合治疗的疗效相关，已证实可作为泛癌种免疫治疗疗效的预测标志物。推荐既往标准治疗后疾病进展且没有更好替代疗法的实体瘤患者，尤其是高 TMB 的患者进行 TMB 检测，有助于扩大免疫治疗获益人群。中国人群 TMB 的独立预测价值仍需更多前瞻性研究验证。 专家共识：目前研究证据显示在 NSCLC 中 bTMB与 tTMB 具有显著相关性，但 bTMB 检测无统一标准。多项回顾性研究发现高 bTMB 与 NSCLC 患者接受单药 ICIs 治疗获益显著相关，但尚未获得高级别前瞻性临床研究证实。 专家共识：推荐使用近期石蜡包埋肿瘤组织样本进行 tTMB 检测，待检测组织应首先完成病理质控并确保恶性肿瘤细胞数能够满足检测要求。为过滤胚系突变对后续 tTMB 评估的影响，应采集患者外周血、唾液或正常组织作为对照样本。建议优先采用NMPA 批 准上 市的 核 酸 提 取 试剂 盒 进 行 基 因 组DNA 提取，tTMB 检测实验室应根据实际需求建立合适的 DNA 样品质控标准和操作流程，对待测DNA样品纯度、浓度及片段化程度进行严格质控。肿瘤原发灶与远处转移灶组织均可用于 tTMB 评估。 专家共识：采用靶向测序 Panel 进行 TMB 评估时，建议与 WES 评估的 TMB 进行一致性评价。靶向测序 Panel 覆盖范围原则上不应约1.0 Mb，最低有效测序深度应逸500伊。建议进行 TMB 检测的靶向测序 Panel 尽可能涵盖患者更多的其他分子遗传信息，包括可指导靶向治疗的驱动基因突变、与基因变异产生相关的免疫治疗正向预测因子以及可能的免疫治疗负向预测因子。目前已有多款 NGS 测序仪获国家 NMPA 批准用于临床基因检测，不同实验室可依据样本量、时效要求选择不同测序平台。 专家共识：基于靶向测序 Panel 的 TMB 检测应以 WES 检测为金标准，纳入影响蛋白质编码的体细胞突变，应保证检出突变频率逸5%的体细胞突变，以保证 TMB 检测值的准确性和稳定性；应依托 ICIs疗效随访数据库对基因组比对和突变检出算法开展标准化研究；Panel 检测区域可影响 TMB 值，应通过至少 1 000 例 WES 数据予以校正。同时建议使用对照样本过滤胚系变异。 专家共识：TMB 值在不同癌种中存在显著差异，应依据 ICIs 临床疗效确定阈值，才能最大可能筛选出 ICIs 治疗的潜在获益人群。值得注意的是，不同靶向测序 Panel 的 TMB 检测体系之间 TMB 阈值不能通用。 专家共识：TMB 报告内容除重点描述 TMB 计算原则和数值外，还应针对癌种的免疫治疗意义进行解读；同时还需系统评估 Panel 检测的各种驱动基因突变情况，以全面解患者的肿瘤生物学特征，建议应用分子肿瘤诊治专家组模式（MTB）进行临床辅助决策。 肿瘤突变负荷应用于肺癌免疫治疗的专家共识 共识一： W ES是TMB检测的金标准，但目前测序成本和分析难度较高。 NGS panel与WES的TMB检测结果具有高度一致性，经过验证的NGS panel可作为临床检测TMB的替代方式。 共识二：通过病理质控的石蜡包埋肿瘤组织方可用于TMB检测。在肿瘤组织获取困难或石蜡包埋样本中肿瘤细胞含量不足的情况下，晚期NSCLC患者可选择外周血进行基于ctDNA的bTMB检测。 共识三：既往未接受过免疫治疗的晚期NSCLC患者，在接受免疫单药治疗前推荐进行TMB检测。 TMB预测SCLC一线免疫治疗疗效的临床证据尚不充分，暂不作为提示免疫一线治疗疗效标志物，但可考虑作为二线及后线免疫治疗的疗效标志物。 共识四：不推荐PD-1/PD-L1抗体联合化疗的晚期NSCLC或SCLC患者接受TMB检测。 共识五：暂不推荐TMB用于预测免疫新辅助治疗疗效。 共识六：目前尚没有公认的区分高、低TMB的cut-off值，推荐各免疫治疗药物根据各自临床研究数据确定cut-off值，并进行前瞻性验证。 共识七： TMB检测报告应呈现纳入TMB计算的基因变异类型、 TMB排序以及支持TMB排序的参考数据库，并提供辅助临床决策所需要的医学证据 共识八：建议联合PD-L1表达、免疫治疗相关基因、组织和循环中的免疫细胞等信息，对TMB指导肺癌免疫治疗进行综合解读。]]></content>
      <categories>
        <category>NGS</category>
        <category>肿瘤检测</category>
        <category>共识</category>
      </categories>
      <tags>
        <tag>注释</tag>
        <tag>中国癌症防治杂志</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[共识-肿瘤二代测序临床报告解读共识]]></title>
    <url>%2F01.%E7%9F%A5%E8%AF%86-03.standard%2F%E5%85%B1%E8%AF%86-%E8%82%BF%E7%98%A4%E4%BA%8C%E4%BB%A3%E6%B5%8B%E5%BA%8F%E4%B8%B4%E5%BA%8A%E6%8A%A5%E5%91%8A%E8%A7%A3%E8%AF%BB%E5%85%B1%E8%AF%86%2F</url>
    <content type="text"><![CDATA[参考原文循证医学官网：肿瘤二代测序临床报告解读共识肿瘤二代测序临床报告解读共识.pdf 随着新型治疗药物的研发以及多学科综合治疗模式的优化，传统的病理分型及检测方法已经不足以满足临床需求，二代测序（next generation sequencing，NGS）已成为中国肿瘤医生常用的检测手段。为进一步协助临床医生理解临床靶点或驱动基因相关变异注释及解读、梳理 NGS 报告的逻辑、提升抓取关键信息，二代测序临床报告解读肿瘤学专家组对国内外 NGS 检测最新进展进行认真分析、讨论和总结，在《二代测序临床报告解读指引》基础上增加部分 NGS 报告解读示例，同源重组缺陷（homologous recombination deficiency，HRD）及微小/分子/可测量残留病灶（minimal/molecular/measurable residual disease，MRD）相关内容解读，制定了《肿瘤二代测序临床报告解读共识》，最终帮助临床医生做出正确的临床决策。 目前有多个循证分级系统可用于指导基因体细胞变异的临床解读，包括 2017 年美国分子病理学协会（Association for Molecular Pathology，AMP）/美 国 临 床 肿 瘤 学 会（American Society of ClinicalOncology，ASCO）/美 国 病 理 学 家 协 会（College ofAmerican Pathologists，CAP）联合制定的体细胞变异解读指南；2018 年欧洲肿瘤内科学会（EuropeanSociety for Medical Oncology，ESMO）发 布 的 分 子靶 点临床可操作性量表（ESMO Scale for Clinical Actionability of molecular Targets，ESCAT）；以及纪念斯隆⁃凯特琳癌症中心（Memorial Sloan Kettering Cancer Center，MSKCC）的 精 准 医 疗 肿 瘤 数 据 库（Precision Oncology Knowledge Base，OncoKB）证据等级规则。总体而言，无论哪个分级系统都遵循一些共性原则，包括循证、跨癌种处理等，故其中并无优先推荐者。临床医生在阅读 1 份 NGS 报告时应先了解其变异解读依据的证据分级原则及其采用知识库的局限性，以帮助自己更好的理解报告内容。 根据 2017 年 AMP/ASCO/CAP 联合制定的体细胞变异解读指南［6］，体细胞变异在不同癌种中对应的药物敏感性证据分为 4 个等级： A 级，美国食品药品监督管理局（Food and Drug Administration，FDA）批准或专业临床指南推荐； B 级，经具有足够统计学效能的临床研究证实、获得该领域专家共识； C 级，其他癌种中的 A 级证据（跨适应证用药）、或已作为临床试验的入组标准； D 级，临床病例报道或临床前证据支持。 体细胞变异对特定肿瘤的诊断及预后价值，亦给出相应分级： A 级，专业指南中定义的特定肿瘤的诊断预后因子 ； B 级，经具有足够统计学效能的临床研究证实其诊断/预后价值； C 级，多项小型研究支持其诊断/预后价值； D 级，小型研究或个案报道提示其辅助诊断/预后价值（独立或联合其他标志物） 基因变异按照其临床意义的重要性分为 4 类变异： Ⅰ类变异，有重要的临床意义，具有 A 级或B 级证据； Ⅱ类变异，有潜在的临床意义，具有C级或 D 级证据； Ⅲ类变异，临床意义不明； Ⅳ类变异，无害或可能无害， 该体细胞变异解读指南在国内影响范围最广，许多第三方 NGS 检测公司的临检报告即遵循该指南的分级原则对基因变异进行解读，详见表1 一份结构化循证报告的背后，需要基于特定逻辑建立临床解读知识库框架，并通过对开源知识库及海量科学文献的信息甄别、分级、编辑，不断完善机构内部临床解读知识库。通过生物信息分析流程将基因变异识别、注释、过滤后，可报告变异进入临床解读知识库进行变异、癌种、证据匹配，形成可读的结构化 NGS 临床报告［9］，最终对其进行解读及实施临床决策，NGS 报告解读决策树详见图2 同时文中还针对下列问题进行了说明： 针对“全阴报告”的解读逻辑 针对 NGS 报告中基因融合、外显子跳读突变检测的说明与突变丰度及拷贝数的解读逻辑 基于组织样本的NGS检测，突变频率的解读方式 基于外周血cfDNA的NGS检测 基于外周血ctDNA MRD的检测 针对单个变异对比多基因多变异的解读逻辑 胚系突变分级注释 NGS可报告的基因组标签 肿瘤突变负荷 微卫星不稳定性 同源重组修复缺陷 可报告范围及质量控制]]></content>
      <categories>
        <category>NGS</category>
        <category>肿瘤检测</category>
        <category>共识</category>
      </categories>
      <tags>
        <tag>解读</tag>
        <tag>循证医学</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Standards and guidelines for the interpretation of sequence variants]]></title>
    <url>%2F01.%E7%9F%A5%E8%AF%86-12.ACMG%E6%89%A7%E8%A1%8C%E8%AF%A6%E8%A7%A3%2F01.%E7%9B%B8%E5%85%B3%E6%96%87%E7%8C%AE-2015.Standard-Guidelines-interpretation.of.sequence.variants%2F</url>
    <content type="text"><![CDATA[参考文献Standards and guidelines for the interpretation of sequence variants: a joint consensus recommendation of the American College of Medical Genetics and Genomics and the Association for Molecular Pathology 遗传变异分类标准与指南 遗传变异分类标准与指南整理PPT材料 工作组 ACMG、 分子病理协会(the Association for Molecular Pathology, AMP) 美国病理学家协会(the College of American Pathologists, CAP)应用范围基因分型、单基因、基因包、外显子组和基因组. 使用特定标准属于来描述孟德尔疾病相关的基因变异: ACMG Standards and Guidelines 遗传变异分类标准与指南 pathogenic 致病的 likely pathogenic 可能致病的 uncertain significance 意义不明确的 likely benign 可能良性的 benign 良性的 建议临床分子基因检测应在符合临床实验室改进修正案(CLIA)认证的实验室中进行, 其检测结果应由通过职业认证的临床分子遗传学家或分子遗传病理学家或相同职能的专业人员解读. 术语突变： 是指核苷酸序列的永久性改变 多态性：指频率超过1%的变异。 命名标准的基因变异命名由人类基因组变异协会(the Human Genome Variation Society,HGVS)维护和版本化(https://www.hgvs.org/mutnomen), 除非另有说明, 一般推荐该命名法作为确定变异命名的首要准。 当描述变异时，可利用这些工具提供正确的HGVS命名(http://mutalyzer.nl)。 编码命名应该使用翻译起始密码子ATG中的A作为位置编号1来描述。 协会支持的参考转录本通常可以通过LRG数据库(http://www.lrg-sequence.org)、CDS 共识数据库(https://www.ncbi.nlm.nih.gov/CCDS/CcdsBrowse.cgi) 、人类基因突变数据库(http://www.hgmd.cf.ac.uk) 、ClinVar(http://www.ncbi.nlm.nih.gov/clinvar)或特异基因座数据库来确定 文献及数据库使用在使用人群数据库时,须明确数据库收录的是健康群体的信息还是患病群体的信息; (如能确认)数据库是否收录了同一家庭多名成员的信息以及数据库收录的受试者的年龄范围. 当使用数据库时, 临床实验室应做到: 确定数据库的更新频率, 确定数据库收录相关数据时是否进行了校勘, 以及采用什么方法进行数据校勘; 确认采用HGVS 命名体系, 并确定􁧿述变异的基因组版本和转录本参考序列; 确定数据分析准确度的验证程度(如变异是源自于低覆盖的新一代测序,还是通过了Sanger 测序验证), 并分析用于评估数据准确度的各种指标, 要获得这些信息可能需要阅读相关的文献; 确定收录对象的来源及其唯一性. 生物信息学计算预测程序各种公共和商业化计算机工具可以辅助解读序列变异. 每种工具使用的算法可能有差异, 但都会包含序列变异在核苷酸及氨基酸水平上作用影响的判断, 包括变异对主要转录本, 可变转录本, 其他基因组元件影响作用的确认. 主要分为两类: 一类可以预测错义变异是否会破坏蛋白质的功能或结构; 另一种可以预测是否影响剪接 多数算法预测已知致病的错义突变的准确率能达65%~80%[12]. 但是大多数工具的特异性较低, 导致有些错义改变被过度预测为有害突变, 而且对于影响较小的错义变异的预测也不可靠[14]. 目前临床实验室常用的错义变异解读工具有PolyPhen 2[15],SIFT[16]和MutationTaster[17]. 序列变异解读的拟定标准以下评估变异证据的方法是用了解释在临床诊断实验室中具有疑似遗传(主要指孟德尔遗传)疾病患者的变异. 并不适用于解读体细胞变异、药物基因组(PGx)变异、或者是多基因非孟德尔复杂疾病相关的基因变异. 在外显子组或基因组研究中, 对候选基因(意义不明确的基因(GUS))应用这些准则时应当谨慎(见下面注意事项), 因为本指南目的不是满足鉴定新致病基因的研究需求.本指南提供了两套标准: 一是用于对致病或可能致病的变异进行分类(表3), 另一是用于对良性或可能良性的变异进行分类(表4). 致病变异标准可分为非常强(very strong, PVS1), 强(strong, PS1~4); 中等(moderate, PM1~6), 或辅助证据(supporting,PP1~5). 良性变异证据可分为独立(stand-alone, BA1),强(strong, BS1~4),和支持证据（BP1-7） PVS1 极强致病性变异从业人员需谨慎考虑以下原则: 当将该类变异归类为致病性时, 需确认无功能变异(null variants)是已知的致病机理, 且与该疾病的遗传模式相一致. 例如, 有些基因(如许多肥厚性心肌病基因)只有杂合错义突变时才致病, 而杂合无功能变异却是良性的. 仅基于这一项证据来看, 对显性肥厚性心肌病来说, MYH7 基因上出现一个新的杂合无义突变不一定是致病的, 而CFTR 基因上出现一个新的杂合无义突变则有可能是一个隐性致病变异. 当文献中将3′远端下游截短变异注释成致病突变时, 要特别小心. 特别是当所预测的终止密码子出现在最后一个外显子, 或者出现在倒数第二个外显子的最后50 个碱基对时, 这种无义突变介导的转录降解[22]可能不会发生, 这个蛋白很可能会表达.据此所预测的截短蛋白的长度也是致病性评估的因素, 但这些变异未经功能分析是无法进行判定的. 就剪接位点变异而言, 因外显子剪切位点的供体/受体位点改变或产生了新的剪切位点, 从而可能导致外显子丢失、缩短, 也可能会使内含子序列变成外显子部分. 虽然剪切位点变异可能被预测为无功能变异, 然而该变异类型造成的影响需要通过RNA 或蛋白质功能分析确认. 还必须考虑可读框内缺失/插入的可能性, 其长度变化较小(PM4), 可以保留蛋白质的关键结构域, 因此导致轻微或中性效应,或功能获得效应. 基因会有不同的转录本, 哪一种转录本与生物学功能相关, 在哪些组织会表达哪些转录本, 这些都是需要进行重点考虑的. 如果一个截短变异只限于一个或并非所有转录本, 则必须谨慎考虑到可能存在其他同功型蛋白质, 防止过度解释. 如果发现一个无功能变异位于某个外显子上, 而该外显子先前无致病变异报道, 那么该外显子可能被选择性剪切了, 此时需要谨慎考虑该变异的致病性. 当预测的截短变异是偶然发现时(与检测指征无关)则应特别小心, 在这种情况下该位点致病的可能性非常低. PS1 极强致病性变异多数情况下, 尤其是当致病机制是蛋白质功能发生改变时, 如已确定某一错义变异是致病变异, 应考虑到与其位于同一变异位点的不同形式的碱基改变也可能产生相同的错义突变结果——氨基酸改变相同(如c.34G&gt;C(p.Val12Leu)和c.34G&gt;T(p. Val12Leu)), 那么, 这些变异也应是致病突变. 需要重点注意:变异可能不是通过改变氨基酸的水平, 而是通过改变DNA 的序列来发挥作用, 例如, 破坏剪接位点(可通过软件分析确定), PS2 PM6 新发变异当将一个新发变异(父母样本检测结果阴性)归类为强的致病证据时, 需要满足以下条件: (i) 身份检验表明患者的父母是其生物学父母. 注意如果父母的身份是假定的而没有被证实, 则判定为PM6; (ii)患者的家族史符合新发变异特征. 例如, 显性遗传病患者的父母均未患病. 在存在生殖细胞嵌合现象时也可能有1 个以上同胞患病; (iii) 患者的表型与变异基因异常引起的表型相吻合. 例如患者具有特殊面容、多毛和上肢缺陷(即Cornelia de Lange 综合征), 检测到NIPBL 基因的新生突变即为强致病证据, 而患者仅表现为非特异性的发育迟缓, 通过外显子组测序发现的该基因的新发变异, 则判断此变异致病性的证据较弱. PS3 BS3 功能研究功能实验研究是一种研究变异致病性的非常强大的工具, 然而并非所有的功能研究都能有效地预测基因或蛋白的功能. 重点注意功能实验的有效性、重复性和稳定性应重点考虑, 这些参数用来评估功能实验的分析性能以及判定样本诊断信息的完整性, 该完整性容易受标本采集的方法及时间、存储及运输的影响. 评估变异在剪接位点、编码序列、非翻译区以及更深的内含子区域的影响时, 对变异在信使RNA 水平(如信使RNA 的稳定性、加工或翻译)进行评估, 可以提供丰富的信息. 相关的技术方法包括对RNA 和/或互补DNA 衍生物进行直接分析, 以及体外微小基因剪接分析 PS4 PM2 BA1 BS1 BS2 变异频率及对照人群的使用通过搜索公共人群数据库(如千人基因组数据库, NHLBI 外显子测序数据库, EXAC 数据库; 表1),并利用已发表文献中相同种族的对照数据进行基因变异频率分析(译者注: 此条款在指南更新时会有修改), 通过分析变异基因在对照人群或普通人群中的携带频率, 有助于评估该变异的潜在致病性. NHLBI外显子测序数据库来源于白种人和非裔美国人群,根据其数据覆盖量能够识别是否存在基因变异. 尽管千人基因组数据库缺乏评估基因变异能力, 但它囊括了更多的种族人群, 因此其数据具有更广泛代表性的. EXAC 数据库近期发布了一组来源于不同人群的6 万多个外显子组的等位基因频率数据, 包括了大约三分之二的NHLBI 外显子测序数据. 一般情况下, 某一等位基因在对照人群的频率大于疾病预期人群(表7)时, 可认为是罕见孟德尔疾病良性变异的强证据(BS1), 如果频率超过5%时, 则可认为是良性变异的独立证据(BA1). 此外, 如果疾病发生在早期,且变异在健康成人中以隐性(纯合子)、显性(杂合子)或X-连锁(半合子)的状态存在, 那么这就是良性变异的强证据(BS2). 如果数据库中未能检出变异的存在,应该确认建立该数据库采用的测序读长深度是否足以检测出该位点上的变异. 如果在一个大样本的普通人群或队列数据的对照人群(&gt;1000 人)中变异不存在(或隐性遗传的突变频率是低频), 并且携带此变异的患者与对照人群为同一种族, 那么可以认为该变异是致病性的中等证据(PM2). 许多良性变异是“个体化的”(即个人或家系独有的), 因此即使在相同种族的人群中缺乏也不能作为致病性的充足甚至强的证据 PM1 热点突变和/或关键的、得到确认的功能域某些蛋白结构域对蛋白质的功能起到了关键作用, 如果在这些结构域上发现的所有错义突变均已被证实为致病突变, 且这些结构域中一定没有已知的良性突变, 那么这就能作为致病的中等证据. 此外, 基因中某些功能尚未确定的区域已被证实存在许多突变热点, 若突变发生在基因突变热点上, 且一个或多个邻近残基中存在较高频率的已知致病突变,那么这也能作为致病的中等证据. PM3 BP2 顺式/反式检测检测双亲样本以确定变异在基因上以顺式(incis)(位于基因的同一拷贝)或是反式(in trans)(位于基因的不同拷贝)方式排列, 这对评估变异的致病性非常重要. 例如, 当两个杂合变异发生在隐性遗传病的致病基因上时, 如果已知其中一个变异为致病变异,那么当另一个待分类变异与其呈反式排列时, 这可以作为待分类变异的中等致病证据(PM3). 另外, 若待分类变异与多个已知致病变异均呈反式排列, 则该证据可升级为强致病证据. 但是, 若待分类变异在普通人群中存在, 则需要用统计学方法判断该现象是否为随机共发生事件. 相反, 当已知致病变异与另一个待分类变异呈顺式排列时, 这可以作为待分类变异的良性支持证据(BP2). 如果发生在隐性遗传病致病基因上的两个杂合变异的致病性均未知, 那么确定它们以顺式或是反式排列, 并不能为判断其中任一变异的致病性􁨀供更多信息. 但是, 如果两者以顺式排列, 则该基因两个拷贝均受影响的可能性将会降低.对于显性遗传病而言, 若待分类变异与致病变异呈反式排列, 则可作为该变异的良性支持证据(BP2); 对于特定研究成熟的疾病模型, 甚至可以考虑将其作为独立良性证据(如CFTR 相关变异的评估) PM4 BP3 由于框内缺失/插入和终止密码子丧失导致的蛋白长度改变相较于单一的错义突变所导致的蛋白质长度变化, 一个或多个氨基酸的缺失或插入、以及由终止密码子变为翻译氨基酸的密码子(如终止密码子丢失)而导致的蛋白质延长更可能破坏蛋白质功能. 因此,框内缺失/插入以及终止密码子丢失可作为中等致病证据. 缺失、插入或延伸范围越大, 缺失区域的氨基酸越保守, 则支持致病的证据越强. 相反, 在重复区域或在进化中不是很保守的区域中小的框内缺失/插入致病的可能性较小 PM5 同一位置新的错义变异如果一个新发错义突变发生在一已知致病突变导致相同氨基酸改变的位置上( 如Trp38Ser 和Trp38Leu), 那么可作为中等致病证据(但不能假定一定是致病的), 尤其当新的突变比已知致病错义突变更保守时. 此外, 不同的氨基酸变化可能导致不同的表型. 例如, FGFR3 基因编码的Lys650 残基的不同变化与不同的临床表型相关: p.Lys650Gln 或p.Lys650Asn 会导致轻度软骨发育不良; p.Lys650Met会导致严重的软骨发育不全伴发育迟缓和黑棘皮病;p.Lys650Glu 会导致2 型发育异常及致命的骨骼发育不良. PP1 BS4 共分离分析在使用家系中变异的共分离现象作为致病性证据时需谨慎. 事实上, 一个与某种表型相关的特定变异在某一家系中的共分离现象是位点与疾病连锁的证据, 而不是变异本身致病性的证据. 一个已经发表的统计方法显示, 在某个家系中鉴定的变异可能与真正的致病变异是连锁不平衡的. 统计模型考虑到了年龄相关的外显率和拟表型率, 一些新的方法也将生物信息分析预测以及与已知致病突变共存作为致病性的单独定量指标. 将远亲纳入统计之中是很重要的, 因为与核心家系成员相比, 他们不太可能同时有该疾病和变异. 对整个基因进行测序(包括整个内含子和5′和3′非编码区)可排查其他致病变异或另一个可能致病的变异的存在. 除非仔细评估基因位点, 否则非致病变异可能被错误地认为是致病变异.当目标基因的特定变异在多个患病的家系成员中以及不同种族背景的多个家系中与表型或疾病共分离时, 则其作为致病的证据不太会受到连锁不平衡和确认偏倚的影响. 在这种情况下, 该标准可以作为中等或强致病证据而不是支持性证据, 其强度取决于共分离的程å度.另一方面, 一个变异与表型并不共分离时, 为其非致病的强证据. 需要进行仔细的临床评估来排除正常个体的轻度症状和可能的拟表型(患者表型由非遗传或不同的遗传原因引起). 此外, 需确认生物学家庭关系来排除收养、非生父、精子和卵子捐献以及其他非生物学关系. 同时, 外显率下降和年龄依赖性的外显率也必须考虑, 以确保无症状家系成员是真正的无症状.在临床实验室进行共分离的统计评估可能并不容易, 当鉴定了合适的家系时, 为了确保建模合适,并避免得出变异与疾病相关性的错误结论, 鼓励临床实验室与统计或群体遗传学专家合作. PP2 BP1 变异谱许多基因具有明确的致病变异和良性变异谱.在某些基因中, 错义突变是导致疾病的常见原因, 且该基因上的良性突变非常少, 那么这种基因上的新发错义突变可作为致病变异的支持证据(PP2). 相反,有些基因致病的唯一已知变异是截短突变, 该基因上的新发错义突变可作为良性的支持证据(BP1). 例如, ASPM 基因的截短变异是该基因引起常染色体隐性遗传小头畸形的主要致病变异类型, 且该基因发生错义多态性突变的频率高, 因此ASPM 基因上的错义变异可认为是良性影响的支持证据. PP3 BP4 生物信息分析数据不能过分相信生物信息分析所得到的结果, 特别是不同的生物信息算法依赖于相同或相近的数据进行预测, 并且大多数生物信息算法未被已知致病变异验证过. 此外, 相同算法对不同的基因的预测结果可能完全不同. 如果不同种类算法的分析预测结果一致, 那么生物信息分析结果可以作为支持的证据. 如果绝大多数算法的预测结果不一致, 则这些预测的结果不能用于对变异进行分类. 若某一变异引起的氨基酸改变, 在多个非人哺乳动物物种不太保守的区域中出现, 说明该变异可能不会损害功能, 可以作为良性解读的强的证据. 然而, 如果某基因已在人类中发生进化(如参与免疫功能的基因), 那么在判定该基因在非保守区域中发生的变异为良性时必须小心. PP4 表型支持考虑到几乎所有接受疾病针对性测试的患者都有某种表型, 通常, 不将患者表型与某个基因临床特征谱匹配作为判断致病的证据. 但是, 如果满足以下条件, 患者的表型可作为支持证据: (i) 临床检测的灵敏度高, 大多数带有该基因致病突变的患者都被检测为阳性; (ii) 患者有某种明确的综合征的症状,与其他临床表现几乎无重叠(如戈尔林综合征包括基底细胞癌、掌跖坑和牙源性角化); (iii) 该基因通常不存在太多的良性变异(可通过外显子组等人群测序确定的良性变异); (iv) 家族史与疾病遗传方式一致. PP5 BP6 可靠的来源现在有越来越多可靠来源(如长期专注于某一疾病领域的临床实验室)的致病性分类信息被分享在数据库中, 但分类判断所依据的证据往往并未􁨀供或者很难获取. 在这种情况下, 如果分类信息是近期􁨀交的, 那它就可以作为一个单独的支持证据. 然而,还是鼓励实验室共享分类的判断依据, 并与􁨀交者进行沟通以评估和创建分类证据. 如果能获得证据,则不应使用这一条款, 而是应该使用相关的证据. BP5 对共发变异的观察一般情况下, 当某一变异是在一个有明确的遗传病因的疾病患者中被观察到时, 可作为将该变异解读为良性的证据. 不过, 也有例外. 某一个体可以是某一不相关隐性遗传疾病致病变异的携带者, 因此本证据与隐性遗传性疾病相比, 更支持显性遗传性疾病基因良性变异的分类. 此外, 有些疾病当具有多个变异可以导致更严重的疾病. 例如, 在一个具有严重表型的显性遗传患者中鉴定了两个变异, 一个是致病的, 一个是新的变异, 父母中的一个也有轻微的疾病, 这种情况下, 必须考虑新的变异致病的可能性, 且新的变异使先证者表型加重. 在这种临床情况下, 观察到的第二个新的变异不应分类为良性变异,(尽管在无进一步证据的前􁨀下也不认为该变异是致病的). 最后, 有些疾病已知为多基因遗传模式, 如Bardet-Beidel 综合征, 在第二个基因座位上的额外变异也有可能是致病的, 但应谨慎进行报告. BP7 同义变异人们逐渐认识到经典的剪接序列以外的剪接错误是一类重要的致病机制, 特别是对那些功能丧失为其常见致病机制的基因. 因此, 在假设同义核苷酸改变没有影响时应持谨慎态度. 然而如果核苷酸位置进化不保守, 且剪接评估算法预测其对剪接一致序列没有影响, 也不会产生新的经典剪接序列, 那么剪接影响的可能性就比较小. 因此, 如果生物信息分析证据支持(BP4), 可将新发同义变异分类为可能良性. 然而, 如果生物信息分析证据表明剪接可能有影响或怀疑有影响(例如, 发生在隐性遗传病致病基因上, 且与已知致病突变呈反式排列的变异), 那么在有功能评估可以􁨀供更确切的对影响的评估, 或者得到其他可排除该变异致病作用的证据之前, 该类变异应该归类为意义不明确. 序列变异报告报告应该使用清晰的语言书写, 避免使用医学遗传学术语, 当必须要使用时需指明所用术语的定义。 报告应包含所有的检测基本要素:, 结构化的结果 解释 参考文献 检测方法 适当的免责声明. 结果应根据HGVS 命名规则(见命名部分)列出变异.基本内容包括： 核苷酸(基因组和cDNA)和蛋白质水平的命名 基因名称 疾病 遗传模式 外显子 合子性 变异的分类. 若亲本来源明确, 也可包括在内. 当报告外显子组或全基因组测序结果, 或偶尔报告包含基因数目较多的疾病基因包检测结果时, 将变异按“与表型明确相关的疾病基因的变异”、“与表型可能相关的疾病基因的变异”及(在适当情况下)“附带(次要)发现”进行分类可能有益。 解读解读应包含对变异检测结果进行分类的证据,包括编码蛋白的功能影响预测, 以及检测所发现的变异是否可能全部或部分地解释患者的临床表型.报告也应包括对临床医生的建议, 这些建议包括一些需补充的临床检测, 如对患者进行细胞酶学/功能的检测, 以及对患者家系其他成员进行的变异检测,以便为进一步解读变异检测结果􁨀供支持. 解读应当包括检测结果部分􁧿述的全部变异, 以及其他附加信息. 对于各个变异需要注明是否已经在先前的文献、疾病病例或对照数据库中有过报道. 在报告结尾处需要列出对变异检测结果分类时所引用的全部参考文献和信息. 方法学报告中应说明使用的实验方法、检测所涉及的变异类型、检测过程的难点, 以及检测变异所使用的方法的局限性. 需要说明的实验方法应包括核酸的获取方法(如聚合酶链式反应、捕获、全基因组扩增等)以及核酸的检测方法(如双向Sanger 测序、下一代测序、染色体基因芯片、基因分型技术等), 这些信息可以为医务工作者􁨀供必要的信息, 以帮助其决定是否需要追加实验来跟进这些检测结果. 方法部分还应包括人类基因组组织基因命名委员会批准的正式基因名称、转录本的RefSeq 登录号和所参考的基因组版本. 对于大的基因包, 基因水平的信息可以通过引用URL 来加以说明. 实验室还可以选择增加对检测过程中常见问题(如样本质量问题、样品混合污染等)的免责声明. 患者维权团体、临床实验和研究的获取尽管不提倡在实验室报告中对患者􁨀供具体临床指导, 但是在报告中􁨀供对于检测结果分类的总体信息(如全部阳性检测结果)是恰当且有益的. 大量病人群体和临床试验现在可用于多种疾病的支持和治疗. 实验室可以选择将此信息添加到报告的正文或附加信息, 并且与报告一起发送给医务工作者. 在遵守医疗保险便携性和责任法案(HIPAA)保护患者隐私的前􁨀下, 当某一变异检测结果被归为意义不明确时, 实验室可尝试帮助医务工作者和特定的疾病研究小组建立联系. 变异再分析随着新的变异证据增加, 现有的分类标准需要修订. 例如, 当大样本的有效的人群变异频率被报道后, 许多原本意义不明确的变异, 可以因为明确意义而进行重新分类, 而检测家系中其他成员的结果也可以导致重新分类.随着检测变异数量的增加及检测范围的扩大,无论是全外显子检测还是全基因组测序, 都可以得到数以百万的变异信息量. 如果实验室缺乏有效的分析方法和足够的文献数据库支撑, 将无法进行变异再分析. 为了满足医务人员和患者的实际需求, 实验室应该开展基因检测数据再分析, 并明确再分析是否产生额外费用. 应该鼓励实验室为帮助医务人员和患者而不断开发更新信息的新途径[31,32].当报告中有针对主要指征的基因中存在临床意义不明的变异, 在实验室又无法及时􁨀供更新的数据时, 建议医务人员定期查询其不明意义的变异结果是否被更改. 另一方面, 鼓励实验室在对变异的分类有重要变化时(如致病性或良性的变异被修改)必须主动及时地更新报告. 关于医生对病人报告更新方面的责任, 可详见ACMG 有关指南. 验证对于孟德尔疾病的致病或可能致病变异需进行正交法验证. 具体方法包括但不限于以下几种: 重新取样和检测、检测父母的变异情况、限制性内切酶消化、对于目标区域重新测序或使用另一种基因分型技术。 特殊考虑]]></content>
      <categories>
        <category>NGS</category>
        <category>标准化</category>
        <category>遗传分析</category>
      </categories>
      <tags>
        <tag>Standards and guidelines</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Standards and Guidelines for the Interpretation and Reporting of Sequence Variants in Cancer]]></title>
    <url>%2F01.%E7%9F%A5%E8%AF%86-12.ACMG%E6%89%A7%E8%A1%8C%E8%AF%A6%E8%A7%A3%2F01.%E7%9B%B8%E5%85%B3%E6%96%87%E7%8C%AE-2017.%E8%82%BF%E7%98%A4%E4%B8%AD%E7%9A%84%E9%81%97%E4%BC%A0%E5%8F%98%E5%BC%82%E8%A7%A3%E8%AF%BB%E5%92%8C%E6%8A%A5%E5%91%8A%2F</url>
    <content type="text"><![CDATA[参考文献Standards and Guidelines for the Interpretation and Reporting of Sequence Variants in Cancer针对上述指南，在2017年，发布翻译文章发表在中国科学杂志 遗传变异分类标准与指南整理汇总PPT材料 摘要下一代基于测序的癌症测试的临床实验室的广泛实施突出了标准化实验室间分子结果的解释和报告的重要性和潜在的好处。分子病理学协会召集了一个多学科工作组，负责评估基于下一代测序技术的癌症检测的现状，并建立体细胞序列变异的标准化共识分类，注释，解释和报告惯例，并由美国医学遗传学和基因组学学院联络代表，美国临床肿瘤学会和美国病理学家学院。根据专业调查，文献综述和工作组主题专家共识的结果，提出了一种基于体细胞序列变异的临床意义进行分类的四级系统：一级，具有很强临床意义的变异； II级，具有潜在临床意义的变体； III级，临床意义未知的变体；和第四级，变体被视为良性或可能良性。癌症基因组学是一个快速发展的领域。因此，应不断评估治疗，诊断或预后中任何变异的临床意义。报告基因组变异应遵循标准命名法，并明确描述测试方法和限制。临床建议应简明扼要，并与组织学和临床发现相关联。 分类 tier I, variants with strong clinical significance; tier II, variants with potential clinical significance; tier III, variants of unknown clinical significance; tier IV, variants deemed benign or likely benign. 导读基于二代测序技术的肿瘤检测已越来越多的应用于临床实验室中，但目前在不同实验室间存在检测方法、报告内容等方面的差异，这对遗传检测的解读以及普及应用造成了一定的影响。因此，在不同实验室间建立统一的分子检测结果的解读和报告标准，及建立行业标准，显得尤为迫切。 在2015年春天，在美国专门成立了一个以临床实验室为核心的工作组，其组成包含了分子病理协会（AMP）、美国医学遗传学与基因组学协会（ACMGG）、美国临床肿瘤学会(ASCO）与美国病理学家协会（CAP)的一线专家，该工作组的主要工作为对肿瘤及疑似肿瘤相关的序列变异检测建立检测标准并在行业达成共识。 该工作组首先对北美地区超过40家的临床检测实验室进行了问卷调查，结果显示不同实验室在检测组织类型、检测基因数量、是否检测肿瘤组织全外显子组或全基因组、以及其他细节方面都存在较大差别，此外在不同单位的检测报告的报告内容方面也存在较多差异。该工作组认为，为医疗机构提供分类的遗传变异报告对病人及整个医疗行业都极为重要，包括：提供精确的肿瘤对靶向治疗反应性信息；建立国家级别的医疗指南；以及与临床试验合作，对建立不同实验室间的通用标准提供支持。基于以上这些考量，工作组专家们根据已有数据、文献报道和专业知识，给出以下指南建议. 对多个公司的报告情况调研结果如下 图1 AMP对NGS技术及NGS结果解读的调研 A: MAF阈值. B: 变异分类数目 C:报告中是否包含治疗性建议 . D: 报告中是否包含潜在的生殖细胞突变. E: 报告是否包含变异等位频率Variant allele frequency (VAF) F: 报告是否包含基因组坐标 G: 报告是否包含转录本ID(Transcript accession) H: 报告是否包含不符合质控的基因/区间 数据库 Genomic Databases随着越来越多的针对各种肿瘤类型的大规模基因组测序项目的发布，全球正在产生大量的基因组信息并将其整合到许多公共数据库中（表1）。例如，美国国家癌症研究所（National Cancer Institute）的基因组数据共享（Genome Data Commons）包含美国国家癌症研究所（National Institutes Institutes）从一些最大和最全面的癌症基因组数据集中生成的数据.包括 ： The Cancer Genome Atlas, Therapeutically Applicable Research to Generate Effective Therapies, and the Cancer Genome Characterization Initiative (https://gdc.cancer.gov, last accessed September 25, 2016). Another public somatic variant database is the Catalog of Somatic Mutations in Cancer (http://cancer.sanger.ac.uk/cosmic, last accessed September 30, 2016). 其中包含数百万种跨多种肿瘤类型的体细胞变异。体细胞变异分析中经常使用的其他几个数据库，例如参考序列信息，种群数据库和种系变异数据库也在不断增加和改进。基因组数据库提供了准确注释和确定体变异优先级所必需的信息。 在使用这些数据库是，整体上应该遵循乳癌规则： 了解数据库的内容以及如何汇总数据。临床实验室应查看与给定数据库有关的文档或公开文献，以确定数据库的来源，类型和意图 特别注意每个数据库的限制，以避免对注释结果的过度解释。 确认人类基因组装配的版本以及mRNA转录本参考，以确保适当的人类基因组变异学会（HGVS）注释。 尽可能使用基因组坐标而不是HGVS命名法来明确查询基因组数据库。 根据出版物或其他数据库的来源，单个或多个特定条目的数量，研究的深度，使用适当的对照，确认变异的体细胞来源，评估提供的基因组数据的质量以及功能和潜在药物反应研究。 验证所提供病理诊断的数据质量（例如，地点，诊断和子类型 Reference Sequence Databases参考序列数据库提供有关人类基因组装配版本的信息以及相关信息，例如基因组坐标，以明确表示序列变异。附加信息，例如mRNA转录本的登录和版本（例如，BRAF NM_004333.4）和外显子边界定义，对于产生变异的正确HGVS命名法至关重要。可以从这些数据库中计算基因的变异位置图谱（编码，非编码，非翻译区和剪接位点）和链表述（阳性与阴性）。这也允许在没有基因组坐标信息的情况下明确表示变异。一些常用资源包括： RefSeq（国家生物技术信息中心参考序列数据库，https：//www.ncbi.nlm.nih.gov/refseq，上次访问时间为2016年1月2日） Ensembl（http://www.Ensembl.org）。 ensembl.org/index.html，最后访问时间为2016年1月2日） Locus Reference Genomic （https://www.lrg-sequence.org，最近访问时间为2016年2月2日）。 Population Databases这些数据库提供了有关大量特定人群中给定基因座上替代（次要）等位基因频率的全面信息。这些数据库通常用于根据次要等位基因频率（MAF）的任意临界值筛选出被认为是多态/良性的变异。目前尚无用于去除多态或良性变异的MAF的标准临界值。在没有正常组织配对的情况下，工作组建议使用1％（0.01）作为主要阈值，这在许多临床实验室中也很普遍。尽管总体MAF最常用，但临床实验室可能会考虑使用种族-根据患者的种族背景确定特定的MAF。在解释体细胞变异时，必须谨慎使用这些数据库，因为在参与研究时，假定参与这些测序研究的个体是健康的或没有亚临床疾病。确实，一些众所周知的经典癌症相关的和可靶向的体细胞改变已作为种群数据库的种系变异包括在内。例如，变异NM_004972.3（JAK2）：c.1849G&gt; T（c.V617F）通常被看作是叶绿体增生的体细胞变异体肿瘤，可以用FDA批准的Janus激酶（JAK）抑制剂靶向。它也包含在多个人群数据库中，例如 The Database of Short Genetic Variation (the National Center for Biotechnology Information database of genetic variation), Exome Variant Server, and Exome Aggregation Consortium（表格1）。在评估可能的血液系统恶性肿瘤时应格外小心，因为白血病和骨髓增生异常综合症中的许多常见突变基因也可能在其他健康个体的血液中发生体细胞突变，因此可能被错误地注释为多态性。 Cancer-Specific Databases这些数据库提供了有关不同癌症和亚型谱中序列变异的发生率和普遍性的信息，对其他基因组数据库的交叉引用以及对已发表或未进行系统综述的文献的引用，细胞途径，靶向疗法，临床试验 以及结果数据。 从这些数据库中提取的不同癌症中的序列变异体的普遍性和分布，应谨慎解释，因为病理诊断标准的代表性较差，缺乏临床级别的文献管理以及提交变异体的来源控制不严（例如，探索性或 发现研究）。 例如，这些数据库中包括一些常见的种系良性变异，例如 the Catalog of Somatic Mutations in Cancer database 中的NM_000222.2（KIT）：c.1621A&gt; C（p.M541L）。 表1列出了常用的体细胞变异数据库。 Constitutional Variant Databases常见的是，在有或没有匹配的正常组织的情况下进行肿瘤测序可能会揭示出种系起源的变异，例如与癌症易感综合症相关的基因中的致病变异。种系突变数据库，例如人类基因突变数据库和其他疾病或基因座特异性突变数据库，是评估这些变异的有用资源。这些数据库也可用于评估在这些数据库中报道了经过充分研究的种系对应物的体细胞变异（例如，TP53和PTEN基因中的某些变异）。另一个常用的数据库是ClinVar（http://www.ncbi.nlm.nih.gov/clinvar）。 ClinVar处理所有种类的稀有种系变异，例如病原体和良性，并在可用时提供相关的临床和实验证据。专家小组对ClinVar中的某些变异进行了有关其致病性的审查。目前，该数据库仅宿主种系变异，并有望在不久的将来纳入体细胞变异。 Internal (Laboratory-Generated) Databases需要强调的是，临床实验室应该建立一个标注良好的内部数据库，以跟踪实验室中识别出的变异并提供一致的变异注释。这样的数据库可用于识别可能由测序比对伪像引起的潜在假阳性呼叫，以及确定实验室通常遇到的癌症类型的突变频率。我们强烈鼓励体细胞变异数据共享，并敦促临床实验室将精心挑选的变异体贡献到公共变异数据库中，以促进对体细胞变异体的准确解释。但是，此类提交过程应标准化并符合联邦隐私法规，即《健康保险可移植性和责任法案》以及《经济和临床健康信息技术法案》（原文 the Health Insurance Portability and Accountability Act andthe Health Information Technology for Economic and ClinicalHealth Act.）。 正在努力建立临床级基因组数据库 In Silico (Computational) Prediction Algorithms 在计算机模拟中，预测算法是预测基因中核苷酸变化是否会改变蛋白质结构和功能的常用工具（表2）。起初，开发了早期常用算法并验证了用于胚系变异的算法。随后，外推它们的用途以解释体细胞变异。尽管各个算法的核心风险预测方法可能有所不同，但是它们可以分为两类：错义变异对蛋白质功能的影响的预测和序列变异对剪接的影响。考虑到氨基酸或核苷酸残基的进化保守程度，特定理化特性的氨基酸取代的生物化学影响以及变异在翻译蛋白质中的位置，是不同算法用来预测功能的一些主要标准错义变异的影响。拼接位点预测算法使用各种统计方法，例如马尔可夫模型，机器学习（神经网络）和最大熵原理，来预测变异是否会对拼接产生任何影响。通常，错义和剪接位点预测工具具有中等的特异性（大约60％至80％），并且倾向于过度预测有害影响。在癌症基因功能的背景下，对这些预测的解释通常并不直接了当，特别是对于激活突变。例如，当通过多种计算机模拟算法进行分析时，经典的BRAF V600E致癌突变结果会具有冲突甚至良性的作用。这是临床实验室在进行体细胞变异解释时应该意识到的几种情况之一，同时在解释计算机分析的评分结果时应谨慎行事。建议不要将这些预测算法的结果用作变异分类或临床决策的唯一依据。 Variant Identification and Annotation变异检测是变异解释的关键起点。 有许多变异检测软件工具(Supplemental Table S2).可以满足一种特定的检测，例如SNV，插入缺失，结构变异和CNV (Supplemental Table S1): Variant caller Location (URL) MuTect v1.1.555 https://www.broadinstitute.org/cancer/cga/mutect Genome Analysis Toolkit (GATK) – MuTect v2s https://www.broadinstitute.org/gatk/guide/tooldocs/org_broadinstitute_gatk_tools_walkers_cancer_m2_MuTect2.php VarScan 256 http://dkoboldt.github.io/varscan/ VarDict57 https://github.com/AstraZeneca-NGS/VarDict Sterlka58 https://sites.google.com/site/strelkasomaticvariantcaller/ FreeBayes59 https://github.com/ekg/freebayes Scalpel60 http://scalpel.sourceforge.net/ Pindel61 http://gmt.genome.wustl.edu/packages/pindel/ SAMtools62 http://samtools.sourceforge.net/ Torrent Suite Variant Caller https://github.com/iontorrent/TS SomaticSniper63 http://gmt.genome.wustl.edu/packages/somatic-sniper/ 对于临床实验室来说，了解这些变异检测工具的局限性很重要。 对生物信息学流程（包括商业购买的生物信息学软件包）进行适当的实验室验证对于确保结果的质量至关重要。 某些称为变异的指标对于变异解释至关重要，例如 supporting reads (depth of coverage) and variant allele frequency (VAF)，应纳入变异体评估中； 后者对于在没有配对正常的情况下的体细胞变异解释和评估肿瘤克隆多样性特别重要。 变异检测结果一般使用一些标准格式进行输出展示，例如clinical variant call format(VCF), genomic VCF 和 general feature format (alias gene-finding format or generic feature format). The VCF is the most widely used schema in the clinical laboratories as of 2016 to represent detected variants (Clinical Variant Call Format, http://vcfclin.org, last accessed September 28, 2016). Required VCF fields include genomic coordinates, reference nucleotide(s), and variant nucleotide( s). However, complex, multinucleotide, and large structural variants are difficult to represent in the current specification of VCF file format version 4.2, despite the ongoing efforts for standardizing variant representation. For further clinical interpretation, additional metadata that add meaningful and readily identifiable information to variants should be included (eg, gene symbol, variant location, variant type, HGVS nomenclature for cDNA sequence changes, and predicted protein sequence alterations). Additional resources, such as cross-references to external databases (cancerspecific and general genomic databases) (Table 1) and precomputed in silico algorithm-based predictions (Table 2), can also be beneficial. This process is formally referred to as variant annotation, and may be automated by software tools(Supplemental Table S2). (Supplemental Table S2): Software Location (URL) Annovar64 http://annovar.openbioinformatics.org/en/latest/ snpEff65 http://snpeff.sourceforge.net/ SeattleSeq http://snp.gs.washington.edu/SeattleSeqAnnotation138/ AnnTools66 http://anntools.sourceforge.net/ NGS-SNP67 https://www.ualberta.ca/~stothard/downloads/NGS-SNP/ VEP (Variant Effect Predictor)15 http://useast.ensembl.org/info/docs/tools/vep/index.html 变异注释对于准确解释体细胞序列变异至关重要。这些对变异注释得到的多元数据构成了变异评估和解析的初始内容。变异注释面对的一个重要挑战就是将基因组坐标（即染色体和位置）转换为相应的cDNA /氨基酸坐标系统（分别为c.和p.syntax）以进行解读。 这个问题针对indel变异由于对齐排列导致变异表述不一致的问题上，表现尤其突出。尽管HGVS系统建议使用右对齐表示法（将变异的开始位置向右移动，直到不再可能这样做），但VCF规范要求使用左对齐表示法。当前可用的注释解决方案仅部分解决了该问题。缺乏左/右对齐的标准化可能会严重影响变异定位，从而导致变异命名错误。根据HGVS命名法，当存在多个替代转录本时，必须使用正确的mRNA转录本编号和版本信息，以确保变异描述的准确和一致。临床实验室在内部数据库中使用变异的基因组位置来存储变异信息也非常重要，以确保数据存储的明确和可回查。 Proposed Guideline for Evidence-Based Categorization of Somatic Variants体细胞变异包括SNV，插入缺失，基因组重排产生的融合基因和CNV。与胚系变异的解释（侧重于特定疾病的变异性或疾病因果关系）不同，体细胞变异的解释应着重于其对临床的影响。如果变异预测对特定疗法的敏感性，耐药性或毒性，改变基因的功能（可以被批准的或研究用的药物靶向），则该变异可以视为影响临床护理的生物标志物，可以作为临床试验的纳入标准，影响疾病的预后，帮助确定癌症的诊断或保证实施监视措施以早期发现癌症。因此，临床影响应包括治疗，预后，诊断和预防措施。给定变异的临床影响应根据当前可获得的证据确定。基于变异分类的证据在临床决策中的重要性，可以对其进行不同的权衡。在文献综述和工作组共识的基础上，我们建议将临床和实验证据分为四个级别 Level A, biomarkers that predict response or resistance toUS FDA-approved therapies for a specific type of tumoror have been included in professional guidelines astherapeutic, diagnostic, and/or prognostic biomarkers forspecific types of tumors; Level B, biomarkers that predict response or resistance toa therapy based on well-powered studies with consensusfrom experts in the field, or have diagnostic and/orprognostic significance of certain diseases based on wellpoweredstudies with expert consensus; Level C, biomarkers that predict response or resistance totherapies approved by FDA or professional societies for adifferent tumor type (ie, off-label use of a drug), serve asinclusion criteria for clinical trials, or have diagnosticand/or prognostic significance based on the results ofmultiple small studies; Level D, biomarkers that show plausible therapeuticsignificance based on preclinical studies, or may assistdisease diagnosis and/or prognosis themselves or alongwith other biomarkers based on small studies or multiplecase reports with no consensus. 可以将这些证据水平分配给基因组变异体，以确定其临床影响的重要性。 我们建议根据体细胞疾病的临床影响将其序列变体分为四类： tier I，具有强烈临床意义的变体（A和B级证据）； tier II，具有潜在临床意义的变体（C或D级证据）； tier III，临床意义未知的变体； tier IV，良性或可能良性的变体 各类别判断依据Tier I Variants: Variants with Strong Clinical Significance (Level A and B Evidence) Tier II Variants: Variants with Potential Clinical Significance (Level C and D Evidence) Tier III Variants: Variants of Unknown Significance Tier IV Variants: Benign or Likely Benign Germline Variants Identified during Cancer Testing在体细胞致癌突变的临床实验室研究中，重要的是将获得的体细胞变异体与遗传的胚系变异体区分开。大多数胚系变异体都是遗传变异，这些变异可以代代相传，并且通常在100％的细胞中具有变异，导致等位基因分数为0.5或1.0。体细胞变异是在出生后获得的，通常是由于DNA复制或修复错误或环境侵害引起的。由于存在污染正常组织的情况，因此即使在表面上纯净的肿瘤样本中，体细胞变异的等位基因分数通常&lt;0.5。实验室必须正确识别可能与诊断，预后，治疗干预和/或临床试验选择有关的体细胞突变，并且不得将高频体细胞突变错误识别为胚系变异，因为这可能具有重大的临床意义。同样，实验室必须认识到可能导致癌症易感综合症的胚系变异，这将对患者和其他家庭成员产生医疗保健影响。 为了帮助对变异进行分类，一些实验室在对正常的，匹配的对照DNA样品进行测序的同时，对肿瘤DNA进行了平行测试。当正常，匹配的对照组织与肿瘤一起测序时，胚系变异通常很明显。 In this case, the laboratory must have policies that address detection, disclosure/nondisclosure, and interpretation/reporting of germline variants (see section below). 当分析中未包括匹配的对照时，实验室应具有可用于推断变体是体细胞或胚系的标准。指定胚系的主要标准是VAF，对于杂合变体，应为约50％，对于纯合变体，应为100％。某些种系变体（例如大插入缺失）可能会导致正常等位基因的优先扩增（在基于扩增子的测试中）或捕获（在基于捕获的测试中），因为这些等位基因的序列同源性丧失，导致&lt;50％用于种系变体的VAF。当在已知的癌症易感综合征基因（例如TP53或BRCA1）中检测到明显的胚系变异时，有关疾病发作年龄的临床信息（年轻人与致癌基因中遗传的种系突变的较高风险相关） ，肿瘤的侧面性（更可能遗传双侧肿瘤）以及癌症的家族或个人病史可以帮助确定癌症易感性的可能性。文献综述和数据库查询还可以帮助确定该变异体先前是否已被报告为易感综合征患者的复发种系变异体。构成突变的有用数据库包括： Online Mendelian Inheritance in Man (National Center for Biotechnology Information, http://www.ncbi.nlm.nih.gov/ omim, last accessed March 6, 2016), Human Gene Mutation Database (http://www.hgmd.cf.ac.uk/ac/index.php, last accessed March 6, 2016), ClinVar locus-specific databases 实验室应制定一项政策，对恶性肿瘤中发现的遗传变异进行测试，以在收到患者的适当同意或根据临床医生的要求后，使用经过临床验证的生殖测试来确认变异的生殖或体细胞来源。 ACMG建议在胚系测试中显示阳性结果，以显示53个基因的阳性结果，其中大约一半与可能在体细胞检测面板上的癌症易感性基因有关。即使在仅将该胚系作为一部分进行评估的情况下，也建议进行公开 肿瘤/正常研究。 通过推断，在仅肿瘤的体细胞突变研究中也考虑种系致病变异的可能性似乎是谨慎的。 在单样本无法判断变异是体系还是胚系变异是，添加局限性说明When paired germline samples are not used, NGS analysisdoes not distinguish germline and somatic variants, andsequencing results may contain both findings. In this case,findings can be reported with a disclaimer that the NGS testused does not allow definitive differentiation betweengermline and somatic variants. In certain settings, a germlinevariant may be suspected (eg, MAF 40% to 60%). However,this interpretation should be made with caution and correlatedwith tumor cellularity. If a germline variant is suspected,testing of a patient germline sample (eg, blood in patientswith solid tumors) can be suggested if clinically indicatedafter an appropriate patient consent. The reports shouldinclude a statement addressing the manner in which thedistinction between somatic and germline alterations is made,and indications of remaining uncertainty, where appropriate. “ Interpretation and ReportingNomenclature所有检测到的遗传变异均应按照HUGO基因命名委员会的指定进行注释和报告: 基因名参考 ：HGNC :http://www.genenames.org 变异写法参考：HGVS，http：//www.hgvs.org SNV和插入缺失应使用p.和c. 符号进行报告（例如，BRAFp.V600E，c.1799T&gt; A）。 SV，列出两个融合的基因伴侣之间用斜杠分隔（例如，EML4 / ALK fusion）。 CNV应以表格形式报告为拷贝数GAIN或LOSS。 如果适用，可以包括基因/基因组基因座的基因组坐标。可以在适当的时候报告数字拷贝数的变化[例如，EGFR拷贝数GAIN（拷贝数比25）； CDKN2A拷贝数LOSS]。 使用标准术语不会超过与临床团队进行清晰明确沟通的需要。 根据需要，除了标准术语外，还应包括口语命名法，以便医生阅读报告并使用它们来确定治疗时向医生清晰地传达含义。例如，TERT启动子变体的报告可以为1-124C&gt; T（ HGVS命名法，然后是括号中的口语命名法（TERTC228T）。使用HGVS命名法报告基因组变体以明确将变体重新映射到参考基因组，而口语命名法则向临床团队传达了明确的信息。 Other Reporting Elements除了检测到的变体之外，报告还应包含其他一些元素，这些元素可能与更深入的结果分析或与随时间推移从该患者获得的其他结果进行比较有关，例如基因组座标，基因组构建和转录参考序列（ 例如NM_004333.4），前提是此信息不会损害患者和临床提供者解释该报告中与之直接相关的基本要素的能力。 本节或对结果进行扩展说明的另一节中，远离顶部结果。应评估等位基因分数（VAF）和覆盖率，并在适当时包括在报告中。该报告应包括所用NGS测定的测序覆盖率临界值。在报告中应声明所有未满足最低要求的测序覆盖率标准的基因和/或热点均已失败。 报告不应仅限于肯定的发现。 I类药物/癌症组合应包括相关的阴性结果（例如，肺癌患者中明确缺乏EGFR突变或黑色素瘤患者中明确缺乏BRAF突变）。如果存在不确定性， 必须在报告中进行沟通；这包括序列质量，样品充分性，肿瘤含量和生物医学知识的问题。 Reporting of Germline Variants希望对配对的胚系样品进行并发分析，因为这可以澄清解释。但是，这并不总是实用的，因此也不是必需的。当有成对的胚系样品可用时，测序管线可允许从体细胞获得性变体中分离出胚系发现。通常，仅解释和报告体细胞变异。如果患者或临床医生要求对胚系发现进行进一步分析，则可以在以后重新查看胚系NGS数据，并在获得患者适当同意后进行报告。成对的胚系测试可能需要获得同意并提供相关文件，以符合当地法律和政策。如果NGS小组的某些基因中未报告胚系变体，则初始报告应特别说明这一事实。当不使用成对的胚系样品时，NGS分析不能区分胚系和体细胞变体，测序结果可能包含这两个发现。在这种情况下，可以用免责声明报告使用的NGS测试不能明确区分胚系和体细胞变体。在某些情况下，可能会怀疑胚系变异（例如MAF 40％至60％）。然而，这种解释应谨慎进行，并与肿瘤细胞数量有关。如果怀疑胚系变异，如果在适当的患者同意后临床上有指示，可以建议对患者胚系样本（例如，实体瘤患者的血液）进行测试。报告中应包含一份声明，说明在体细胞和胚系变化之间进行区分的方式，并在适当情况下指示仍存在不确定性。首先测试癌症样品和稍后再配对的胚系样品的实验室可以选择进行以下操作： 将包含胚系样品发现的结果发布到癌症报告的附录中， 在胚系报告中单独发表一份关于胚系变体的报告，并附上唯一的解释性声明，并在初始癌症报告中添加一份附录。 作为单独的胚系报告和另外的单独报告发布，该报告整合了癌症和胚系样品的结果。 生殖细胞变体应按照ACMG / AMP指南进行报告。如果订购了针对癌症易感基因的生殖细胞测试，则生殖细胞变异的报告应遵循ACMG / AMP指南。应提供遗传咨询和转诊给临床医学遗传学家。实验室应制定政策，以报告重要性不明的变体并披露次要发现，包括在何种情况下将报告或不报告此类发现 Reporting the Clinical Significance of DetectedVariants对检测到的遗传变异提供解释性注释是很有用的，该变异将这种变异置于临床病理背景下可为管理决策提供依据。这对于I级和II级突变至关重要。必须对III级变体的详细分析与目标，使报告中最关键的信息保持简洁，清晰并突出显示的目的相平衡。这些评论可能包括该变体对于特定肿瘤类型，对生化途径的影响以及相关癌症的患病率的功能，预后或预测意义。但是，建议应该简短明了，并应谨慎措辞，并应理解治疗或其他患者管理决定是基于除遗传改变以外的许多医学信息，其中许多信息对于分子专业报告是不可用的。重要的是要认识到，治疗的适用性是基于许多因素，而不是根据测试申请书上写的诊断和通过测试发现的基因型。这些因素通常是分子专业报告结果所未知的（即，存在混杂的医学状况，例如葡萄糖耐量不足，自身免疫性疾病或心力衰竭），并且在推荐特定疗法时未考虑这些其他因素会导致混淆，患者与肿瘤科团队之间的冲突以及焦虑。 NGS实验室报告中的治疗建议应以证据为依据，应与患者的癌症诊断相关，并应使用某种语言来明确指出该报告包含结合实验室可用数据点的广义治疗建议（例如，diag-疾病和基因型），但需要将其他因素纳入为每个个体制定治疗计划的过程中。尽管可以接受有关相关试验的一般性陈述或已发表试验的引用结果，但不应针对具体临床试验提出建议。 网上查找的基于ACMG 构建的思维导图，可以作为参考资料]]></content>
      <categories>
        <category>NGS</category>
        <category>标准化</category>
        <category>遗传分析</category>
      </categories>
      <tags>
        <tag>Standards and guidelines</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python-包-python-pptx生成PPT]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-02.python%2FPython-%E5%8C%85-python-pptx%E7%94%9F%E6%88%90PPT%2F</url>
    <content type="text"><![CDATA[简介处于某些上下游对接需求，所以需要频繁的将生信的分析结果整理成PPT文件，以便进行结果的展示。所以基于该模块可以更方便的在集群上自动化生成相关的文档示例，用于进行后续的处理。 使用python操作PPT，需要使用的模块就是python-pptx，下面来对该模块做一个简单的介绍。这里提前做一个说明：python操作PPT，最好是我们提前设计好自己的一套样式，然后利用进行python进行内容的获取和填充（最主要的功能！），最好是不用使用python代码操作PPT的格式，格式的修改肯定不如我们直接在PPT中修改方便。可以创建、修改PPT（.pptx）文件。 环境准备模块的安装需要单独安装，不包含在Python标准模块里1234# &quot;Windows用户命令行下输入&quot;pip install python-pptx# &quot;Mac用户命令行下输入&quot;pip3 install python-pptx 模块的导入1import pptx 模块的使用python读取PPT文档中的内容在使用python操作PPT之前，首先应该清楚PPT的结构，这个对于之后代码的编写很有帮助。 获取Slide12345from pptx import Presentationprs = Presentation(&quot;统计学习方法PPT.pptx&quot;)for slide in prs.slides:print(slide) 获取Shape形状12345678910import pptxfrom pptx import Presentationprs = Presentation(&quot;统计学习方法PPT.pptx&quot;)for slide in prs.slides:for shape in slide.shapes:print(shape)&quot;&quot;&quot;注意：这里得到的Shape对象，并不能看出什么，接着往下看。&quot;&quot;&quot; 判断每个Shape中是否存在文字 shape.has_text_frame ：是否有文字 shape.text_frame ：获取文字框123456789import pptxfrom pptx import Presentationprs = Presentation(&quot;统计学习方法PPT.pptx&quot;)for slide in prs.slides:for shape in slide.shapes:if shape.has_text_frame:text_frame = shape.text_frameprint(text_frame.text) 获取某一页Slide中的内容1234567891011import pptxfrom pptx import Presentationprs = Presentation(&quot;统计学习方法PPT.pptx&quot;)for i,slide in enumerate(prs.slides):if i == 5:for shape in slide.shapes:if shape.has_text_frame:text_frame = shape.text_frameprint(text_frame.text) 获取Shape中的某个Paragraph1234567891011121314151617import pptxfrom pptx import Presentationprs = Presentation(&quot;统计学习方法PPT.pptx&quot;)for i,slide in enumerate(prs.slides):if i == 5:for shape in slide.shapes:if shape.has_text_frame:text_frame = shape.text_framefor paragraph in text_frame.paragraphs:print(paragraph.text)&quot;&quot;&quot;注意：该方法和上述4)中的方法一摸一样。上述方法是直接获取Shpae中的文字内容；下面这个更灵活，先获取每个Shape，然后在获取每个Shape中的paragraph；下面方式更好：因为我们可以针对paragraph，写一个判断条件，只获取第几个paragraph；&quot;&quot;&quot; 参考readthedocs知乎CSDN]]></content>
      <categories>
        <category>编程拾慧</category>
        <category>python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Office处理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Characterization and mitigation of fragmentation enzyme-induced dual stranded artifacts]]></title>
    <url>%2F01.%E7%9F%A5%E8%AF%86-01.NGS%2F2020-10-02.%E9%85%B6%E5%88%87%E6%89%93%E6%96%AD%E5%AF%BC%E8%87%B4%E7%9A%84%E5%81%87%E9%98%B3%E4%BF%A1%E5%8F%B7%E7%9A%84%E7%89%B9%E5%BE%81%E5%92%8C%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88%2F</url>
    <content type="text"><![CDATA[DNA片段化是基于杂交捕获的短读长测序文库制备过程中的基本步骤。文库制备包括DNA片段化过程中引入的错误，会显著影响下游的分析，并直接导致假阳性和假阴性的增加。目前主流的DNA片段化方法有两种： 基于超声波进行的物理打断，（超声打断本身比较昂贵（仪器上万）和耗时（每个样本几分钟），但是会带来一些样本损失; 另外一种就是依赖于DNA核算内切酶进行片段化的酶切打断，可以便捷的一次应用于许多样品，由于其易用性、可扩展性和低进入门槛，酶促片段化在高通量测序操作中越来越受欢迎。尽管这两种方法已经 集中研究中提供的建库打断方法KAPA HyperPlus kit 酶切打断SureSelect QXT（安捷伦科技公司） 酶切打断Fragmentase（新英格兰生物实验室）酶切打断Nextera Tagmentation（Illumina） 酶切打断 KAPA HyperPrep kit 超声打断.SureSelect kit 机械打断 研究一有研究[1]表明KAPA Biosystems 在其 HyperPlus 文库制备试剂盒中有专门的片段化酶，能提供比超声处理更简单的碎片化解决方案，但是在进行大型队列的变异分析时，发现了大量意想不到的单核苷酸、插入和缺失变异。经过仔细的检查，发现了这些假阳性信号是酶促裂解过程的副产品。同时也评估了IDT和NEB的其它基于酶切的试剂盒，发现都存在类似的问题。通过深入观察发现这写artifact还有一下特点： 这些检出具有比较低的等位基因频率 这些支持artifact信号的reads都存在soft-clipped，同时这部分soft-clipped的序列如果进行反向互补，通常可以在附近的参考序列中找到； 这些soft-clipped区域具有高碱基质量值，表明他们是真实的分子衍生物，而不是测序错误产生的。 这些soft-clipped的序列高度保守，但是和接头引物序列并不一致。并且偶尔会被比对到互补链。并且经过调查，这部分经过反向互补，经常可以在附近的参考基因组中发现。 使用酶切打断和超声打断制备的样品进行FADE分析，发现两种方法artifact的reads数目有明显差异。酶切中artifact的比例（所有reads中被定义为包含artifact的reads比例）是2%，而超声处理的样本比例是0.01%。 同时经过调查，发现超声处理的这0.01%的artifact像是来自基因组重复区域或者是打分超过阈值的一些比对错误。但是也有报道表明使用超声打断也容易发生DNA的氧化损伤，产生假阳信号（Discovery and characterization of artifactual mutations in deep coverage targeted capture sequencing data due to oxidative DNA damage during sample preparation）。同时文中提供了FADE软件，可以进行相关的过滤。PS 经过内部实测，实际效果优先，因为我们遇到的类似情况变异的序列都比较短，不能有效进行过滤。 研究二使用超声和酶片段化方法制备的相同肿瘤DNA样品的体细胞变异的成对比较。分析发现，与通过超声处理的文库相比，核酸内切酶处理的文库中复发的artifact信号导致的SNV和插入缺失数量要多得多。这些具有如下一些特征：基因组环境中的回文结构、reads上的位置偏好性和多核苷酸取代为标志。尽管这些试剂盒最大限度地减少了DNA损失，但酶片段化过程引起的测序错误程度仍然很大。在使用HyperPlus试剂盒构建的DNA片段化文库中发现了许多人工SNV / indels。这些测序错误有如下特征：在于位于回文结构中心和读段5’或3’末端附近的变异，具有多核苷酸取代被认为是由核酸内切酶处理步骤和随后的末端修复填充过程引入的，而不是测序过程本身的结果。同时研究中同时使用SureSelect（机械打断）和HyperPlus（酶切打断）处理的6个相同DNA文库。 最终结果发现，虽然源自相同的DNA样本，但是HyperPlust文库的SNV/InDel是SureSelect构建文库的2.3~9.9倍。SureSelect处理的大多数SNV/插入缺失都嵌套在HyperPlus库中 从数据看，HyperPlust检出的突变在多个样本中重复出现（a），而SureSelect特异检出的变异则没有这样的现象（b） 如C 图左侧面板所示。在[a]（红色）中检测到的体细胞SNV/插入缺失的位置。右面板。在 [b]（蓝色）中检测到的体细胞 SNV/插入缺失的位置。映射到与检测到的SNV/插入缺失相同的基因组坐标的野生型核苷酸读数的数量以灰色表示 D图展示的是softclip的reads比例。HyperPlus特异检出的变异含有更高的soft-clipped比例。 对数据的仔细检查发现，许多这些体细胞SNV恰好位于回文序列的中心，此处指定为“SNV-centered palindromes”（SCP）。HyperPlus文库也更频繁地生成更长的SCP，而在SureSelect文库中没有检测到长度超过15个碱基的SCP（图2B）。同时发现a 和b 还有三个区别： 类别a中的大多数SNV和InDel被检出过不止一次。 类别[a]中的SNV/插入缺失通常位于距离读数的5’或3’边缘10至15个碱基 来自类别a的变异具有更多的soft-clipped（平均为50.8%和5.0%) 提供的建议方案， 排除了被反复检测到的SNV/InDel, 除非在Cosmic中注册； 使用KS检验对比变异和野生型数据的位置偏好性； 同时计算每个变异所有reads中，soft-clipped的reads比例。然后使用逻辑回归进行噪声或信号的分类。效果如下图：类别 [a] SNV/插入缺失（主要是测序伪影）的特征是较低的 KS p 值和/或较高的软削波读取比率，但 [b] 类中的 SNV/插入缺失（主要是真正的 SNV/插入缺失）具有较高的 KS p 值和较低的软削波读取比率。然后估计阈值，以使用具有 logit 链接函数的广义线性模型来区分两个类别之间的 SNV/插入缺失。通过对六样本训练数据的受试者工作特征（ROC）曲线分析，建立了最终模型，并证明能够区分两个类别之间的SNV/插入缺失，特异性为0.914，灵敏度为0.979。同时评估测试发现HyperPlus、Hyper和SureSelect数据集的剩余SNV/插入缺失的中位数（范围）比例分别为10.8%（0.01%–46.9%）、85.2%（47.6%–98.8%）和94.3%（86.5%–98.6%），酶切建库的假阳性过滤明显，但是机械打断的过滤影响明显小很多。同时还针对HH 组合（正常–HyperPlus 与肿瘤–HyperPlus）和 SS 组合（正常–SureSelect 与肿瘤–SureSelect）进行了测试，发现即使配对的正常和肿瘤样本中使用相同的片段化方法后，也很难完全消除HyperPlus治疗产生的测序噪声，有必要使用信息学来过滤噪声。 参考文献[1] Characterization and mitigation of fragmentation enzyme-induced dual stranded artifacts[2] Sequencing artifacts derived from a library preparation method using enzymatic fragmentation[3] Optimization of enzymatic fragmentation is crucial to maximize genome coverage: a comparison of library preparation methods for Illumina sequencing]]></content>
      <categories>
        <category>NGS</category>
        <category>肿瘤检测</category>
        <category>文献</category>
      </categories>
      <tags>
        <tag>NGS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[snakemake进阶 - 扩展自定义参数进行流程控制]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-04.pipeline%2Fpipeline-frameworks-snakemake%E8%BF%9B%E9%98%B6-1.%E5%8F%82%E6%95%B0%E6%89%A9%E5%B1%95%E5%AE%9E%E4%BE%8B%2F</url>
    <content type="text"><![CDATA[分支的实现在进行snakemake分析时，所有的参数都是通过 -C 进行传递的，例如 -C S=”string” 则在smk中可以通过使用 config[‘S’] 调用相关参数完成参数传递。 -C 传递的值统一保存在字典 “config” 中。1234567-C \# 指定一些分支的选择（例如688中是否进行某些数据库的分析）,Tequila=T \ # 指定输入文件S=/ifstj2/B2C_RD_H1/Project/Halos_VersionUpdate_test/v1.4.4.6/4Pipe.path \ # 指定输出文件O=/ifstj2/B2C_RD_H1/Project/Halos_VersionUpdate_test/v1.4.4.6 向流程传递参数 数据的二次加工snakemake本身是基于python语法解析的，因此整个snakemake中是支持python语法进行数据处理的。借助这个特点，我们可以在snakemake中进行一些高级的操作来帮助我们更便捷的进行数据的处理。比如针对输入文件，生成一些相对复杂的数据结构，实现一些复杂的功能或者业务逻辑12345678910111213141516aln_bam = "&#123;0&#125;__&#123;1&#125;/&#123;2&#125;/3.BWA_Aln/&#123;3&#125;__&#123;4&#125;__&#123;5&#125;__&#123;6&#125;_sort.bam".format(product, sample, type, chip, lane, barcode, umi) if not aln_bam in library_bams[(product, sample, type, library)]: library_bams[(product, sample, type, library)].append(aln_bam) if library_bamnum[(product, sample, type, library)] == "S" or library_bamnum[(product, sample, type, library)] == "M" : library_bamnum[(product, sample, type, library)] = "M"; else: library_bamnum[(product, sample, type, library)]="S";######## Save Sample Bam ############ Markdup_bam = "&#123;0&#125;__&#123;1&#125;/&#123;2&#125;/4.MarkDup.lib/&#123;3&#125;.markdup.bam".format(product, sample, type, library) if not Markdup_bam in sample_bams[(product, sample, type)]: sample_bams[(product, sample, type)].append(Markdup_bam) if sample_bamnum[(product, sample, type)] == "S" or sample_bamnum[(product, sample, type)] == "M" : sample_bamnum[(product, sample, type)] = "M"; else: sample_bamnum[(product, sample, type)] = "S"; 补充日志补充一个相对完善的流程说明帮助文档1234567891011121314151617181920212223242526try: SampleFile = &#123;config["S"]&#125;.pop()except: Errlog = """\033[0;32m\nMiss The Config File for Sample Infomation\nPlease Add Sample Information By ' -C S=Sample_Infomation O=OutDir ') Usage:snakemake -s PanCancer.Pipeline.smk -p -C S=Sample_Infomation O=OutDir \\Analysis="MSI,QC21,filt_variation,filt_sv,Drug_ct,final.CNV,CNV.detail,final.hot,CisMut,final_hereditary_tumor" ##: Analysis Some part of this pipeline \\UnAnalysis="MSI,QC21,filt_variation,filt_sv,Drug_ct,final.CNV,CNV.detail,final.hot,CisMut,final_hereditary_tumor" ## : skip some part of this pipeline ; \\Tequila=T (T|F Use Tequila Online Database or Not ) \\--cluster "qsub -clear -cwd -P P19Z11900N0186 -q b2c_rd.q -l num_proc=&#123;threads&#125; -l vf=&#123;resources.mem_mb&#125;M -binding linear:&#123;threads&#125;" \\--jobs 500 \\--rerun-incomplete \\--restart-times 5 Version: v1.3.1(20200903@ Liubo4)=====================Sample_Infomation format==================#Product Sample Type chipInfo Lane Barcode UMI_ID Fq1 Fq2PanCancer Test Tissue V300014810 L04 14 14 XXXXX.14_1.fq.gz XXXXX.14_2.fq.gzPanCancer Test Normal V300014810 L04 15 15 XXXXX.15_1.fq.gz XXXXX.15_2.fq.gz================================End============================ \033[0m\n""" sys.exit(Errlog) 生成一系列的记录文档，记录分析的相关信息。123456789# 获取分析日期Analysisdate =datetime.datetime.now().strftime('%Y-%m-%d')# 基于git管理的流程，获取流程的版本PipeVersion = subprocess.getoutput("cut -d ' ' -f2 "+bin_path+"/.git/logs/HEAD| tail -n1 ")with open("log","w+") as LOG: LOG.write("Start Date :"+Analysisdate+"\n\n") LOG.write("Analysis Version :" + PipeVersion + "\n\n") LOG.write("Analysis Dir :"+config['O']+"\n\n")]]></content>
      <categories>
        <category>pipeline</category>
        <category>framework</category>
        <category>编程拾慧</category>
        <category>任务调度</category>
        <category>python</category>
      </categories>
      <tags>
        <tag>流程开发</tag>
        <tag>培训</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[snakemake介绍]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-04.pipeline%2Fpipeline-frameworks-snakemake%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[snakemake分享pptSnakemake介绍官方文档 其他材料：blog: snakemake-vs-nextflowarticle: Sustainable data analysis with Snakemake 任务投递示例：1234567snakemake -s LowFreqConsencus.Pipeline.split.smk -p \# 向流程传递参数-C S=/ifstj2/B2C_RD_H1/Project/Halos_VersionUpdate_test/v1.4.4.2/InterTest/4Pipe.path O=/ifstj2/B2C_RD_H1/Project/Halos_VersionUpdate_test/v1.4.4.2/InterTest \# 设置任务投递方式（使用集群sge示例）--cluster "qsub -clear -V -cwd -P P18Z11900N0299 -q bc_b2c.q,b2c_rd.q -l num_proc=&#123;threads&#125; -l vf=&#123;resources.mem_mb&#125;M -binding linear:&#123;threads&#125;"# 设置snakemake调度任务的指标--rerun-incomplete --jobs 1000 --restart-times 5 --keep-going --rerun-incomplete --latency-wait 60 --stats runtime.json 常用参数： -C向流程传递参数例如 -C S=”string” 则在smk中可以通过使用 config[‘S’] 调用相关参数完成参数传递。 -C 传递的值统一保存在字典 “config” 中。 –jobs设置并行的最大任务数目。 –config向snakemake传递参数（字典形式），会保存在 config 中 –configfile指定配置文件路径（可以支持多个） –cores设置任务最多使用的核数 –resources设置任务最多使用的内存 –forceall强制执行某条Rule及它的依赖。 –list展示smk脚本中所能获得的所有Rule –dag 123456生成流程逻辑框架图：snakemake xxxx(流程本身的参数) --dag | dot -Tsvg &gt; dag.svg#如果snakemake流程本身会打印输出内容，则需要单独处理如下：snakemake xxxx(流程本身的参数) --dag &gt;tmp.txt# 删除tmp.txt文档中流程自身打印的内容，然后生成图片dot -Tsvg tmp.txt &gt; dag.svg –touch更新文件的时间戳（不会重新跑） –force, -f 重新运行第一条或指定的某条Rule –forcerun, -R 强制执行snakefile，更新rule时，使用此命令。会同步更新后续的所有结果 –dry-run, -n 生成相关分析路径的shell脚本，但是不进行实际的执行。 –keep-going, 在某个任务失败后，继续运行其他的独立任务； –cluster 针对集群投递到计算节点的参数设置； 1--cluster &quot;qsub -clear -cwd -P P18Z11900N0299 -l num_proc=&#123;threads&#125; -l vf=&#123;params.resources&#125; -binding linear:&#123;threads&#125;&quot; –restart-times 任务失败后，重投的次数 1--restart-times 5 –rerun-incomplete, 针对结果不完整的数据，重跑所有的rules； –unlock 解锁被异常锁定的目录 –stats 记录任务的执行状态，输出到指定文件 –nocolor 不输出彩色的结果 –drmaa-log-dir 仅限使用 -drmaa 进行任务调度时有效。 指定shell的log（.e 和 .o）输出目录. 资源配置SGE以SGE为例，可以通过设置配置文件，调整每个任务运行时所需要的资源 1qsub -clear -cwd -l vf=$&#123;mem&#125;g,num_proc=$&#123;cpu&#125; -P $&#123;Project_ID&#125; -binding linear:$&#123;cpu&#125; -q $&#123;Queue_ID&#125; work.sh -o work.sh.o -e work.sh.e 创建资源配置文件 cluster.yaml 编辑各个任务资源的相关资源需求。123456789101112131415localrules: all__default__: # 默认标准 queue: &quot;Queue_ID&quot; project: &quot;Project_ID&quot; workdir: &quot;./&quot; mem: &quot;1G&quot; cores: 1trim: ...rmhost: mem: &quot;4G&quot; cores: 4 output: &quot;cluster_logs/&#123;rule&#125;.&#123;wildcards&#125;.o&quot; error: &quot;cluster_logs/&#123;rule&#125;.&#123;wildcards&#125;.e&quot; 为任务单独设置运行环境通过conda为计算规则设置单独的计算环境 不同的计算规则中依赖的软件之间可能有冲突，无法在同一个环境中配置好。使用conda指令，smk可以为每个计算规则新建conda环境，并在其中运行计算。 为rmhost规则新建一个环境：12345678910111213141516mkdir envscat &gt; envs/rmhost.yamlchannels: - biocondadependencies: - bowtie2=2.3.5.1 - samtools=1.10修改Snakefile文件：rule rmhost: input: ... output: ... conda: &quot;envs/rmhost.yaml&quot; shell: ... 运行smk时，加上–use-conda参数即可为每个规则建立conda的独立环境。1snakemake --use-conda rule内的函数 protected 1在文件生成后，将文件权限改为 只读文件。 priority设置任务优先级，从而一定调整各个rule的运行的顺序。]]></content>
      <categories>
        <category>pipeline</category>
        <category>framework</category>
        <category>编程拾慧</category>
        <category>任务调度</category>
        <category>python</category>
      </categories>
      <tags>
        <tag>流程开发</tag>
        <tag>培训</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[snakemake介绍]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-04.pipeline%2Fpipeline-frameworks-snakemake%E6%97%A5%E5%BF%97%E6%9F%A5%E7%9C%8B%2F</url>
    <content type="text"><![CDATA[snakemake分享pptSnakemake介绍官方文档 bench通过 benchmark 指定输出文件，可以记录每个任务运行过程中所使用的计算资源。1234567rule Demo: input: ... output: ... conda: &quot;envs/rmhost.yaml&quot; shell: ... benchmark: log.txt 运行rule后，会在 log.txt 文件中输出任务运行的相关资源信息。 s h: m: s max_rss max_vms max_uss max_pss io_in io_out mean_load 1653.7956 0:27:33 11794.34 39378.64 11782.93 11787.65 5094.00 2639.98 118.52 其中每个字段含义如下（官网链接）: s: 运行时间，秒为单位 h: m: s : 运行时间，时分秒制 RSS: Resident Set Size 实际使用物理内存（包含共享库占用的内存） uss： 最大虚拟内存 USS: Unique Set Size 进程独自占用的物理内存（不包含共享库占用的内存） PSS: Proportional Set Size 实际使用的物理内存（比例分配共享库占用的内存） io_in: I/O read in bytes io_out: I/O written in bytes mean_load VSS- Virtual Set Size 虚拟耗用内存（包含共享库占用的内存） 一般来说内存占用大小有如下规律：VSS &gt;= RSS &gt;= PSS &gt;= USS 统计任务CPU时随着计算的精细话管理，越来越多的工作需要精确统计每个步骤/每个流程分析所需时间， 使用的snakemake版本大于5.30时，框架自带的benchmark 记录会保存每个任务的cpu_time。12345678# &lt;= v5.29s h:m:s max_rss max_vms max_uss max_pss io_in io_out mean_load3.5506 0:00:03 34.39 239.52 32.57 32.66 80.04 0.01 41.13# &gt;= v5.30$ cat benchmarks/test1.rmhost.benchmark.txt s h:m:s max_rss max_vms max_uss max_pss io_in io_out mean_load cpu_time62.8049 0:01:02 3528.02 4267.68 3524.98 3525.02 12.81 48.00 267.62 168.59]]></content>
      <categories>
        <category>pipeline</category>
        <category>framework</category>
        <category>编程拾慧</category>
        <category>任务调度</category>
        <category>python</category>
      </categories>
      <tags>
        <tag>流程开发</tag>
        <tag>培训</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[snakemake进阶 - 2.调用容器 - singularity]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-04.pipeline%2Fpipeline-frameworks-snakemake%E8%BF%9B%E9%98%B6-2.%E4%BD%BF%E7%94%A8%E5%AE%B9%E5%99%A8-singularity%2F</url>
    <content type="text"><![CDATA[业务需求，目前docker方案在所使用的集群中不可行，所以使用的是singularity 解决方案。首先我们看一个没有使用镜像的rule1234567891011121314151617181920rule VEP_anno: input: vcf = "&#123;product&#125;__&#123;sample&#125;/Analyze/&#123;type&#125;/&#123;muttype&#125;.final.vcf", output: anno_out = "&#123;product&#125;__&#123;sample&#125;/Analyze/&#123;type&#125;/&#123;muttype&#125;.vcfanno.tab", benchmark: "benchmark/anno__&#123;product&#125;__&#123;sample&#125;__&#123;type&#125;__&#123;muttype&#125;" resources: mem_mb = 4000 threads: 4 shell: """export PERL5LIB=""export PATH=&#123;bin_path&#125;/bin:$PATH&#123;config[conda][vep]&#125; \ --fork 4 --assembly GRCh37 --offline --refseq --exclude_predicted --cache --format vcf --vcf_info_field ANN --force_overwrite --tab --verbose --no_escape --everything --shift_3prime 1 --dir_plugins /ifstj2/B2C_RD_H1/Research_and_Development/03.database/VEP_plugins/VEP_plugins-release-108 --plugin TSSDistance --plugin TERT --custom /ifstj2/B2C_RD_H1/Personal/liubo/0.Pipeline/aio.NewAnnotation/subpipeline/submodel_annotation_vep/Database/human_maked.gene.gff.gz,,gff --custom /ifstj2/B2C_RD_H1/Research_and_Development/03.database/vep_inhouse_data/clinvar.vcf.gz,ClinVar,vcf,exact,0,CLNSIG,CLNREVSTAT,CLNDN \ --dir_cache /ifstj2/B2C_RD_H1/Research_and_Development/03.database/vep_cache_dir --fasta /ifstj2/B2C_RD_H1/Research_and_Development/03.database/Hg19/hg19.fa -i &#123;input.vcf&#125; -o &#123;output.anno_out&#125; 可以看到这是一个使用vep进行注释的rule,如果需要在snakemake中使用镜像，rule层面，只需要添加 singularity字段，并说明需要使用的镜像，其中镜像可以是本地的 sif文件，也可以是远程仓库docker镜像 docker://ensembl-vep:release_108.2 （会从远程仓库拉去镜像，需要机器能链接网络） 1234567891011121314rule VEP_anno: input: vcf = "/ifstj2/B2C_RD_H1/Personal/liubo/singularity/22S03467478.indel.final.vcf", output: anno_out = "/ifstj2/B2C_RD_H1/Personal/liubo/singularity/22S03467478.indel.vcfanno.tab", resources: mem_mb = 4000 threads: 4 singularity: "ensembl-vep_release_108.2.sif", singularity: "docker://ensembl-vep:release_108.2", shell: """vep --fork 4 --cache --dir_cache /ifstj2/B2C_RD_H1/Research_and_Development/03.database/vep_cache_dir --assembly GRCh37 --offline --refseq --exclude_predicted --fasta /ifstj2/B2C_RD_H1/Research_and_Development/03.database/Hg19/hg19.fa -i &#123;input.vcf&#125; -o &#123;output.anno_out&#125; --format vcf --vcf_info_field ANN --force_overwrite --tab --verbose --no_escape --everything --shift_3prime 1 --dir_plugins /ifstj2/B2C_RD_H1/Research_and_Development/03.database/VEP_plugins/VEP_plugins-release-108 --plugin TSSDistance --plugin TERT --custom /ifstj2/B2C_RD_H1/Personal/liubo/0.Pipeline/aio.NewAnnotation/subpipeline/submodel_annotation_vep/Database/human_maked.gene.gff.gz,,gff --custom /ifstj2/B2C_RD_H1/Research_and_Development/03.database/vep_inhouse_data/clinvar.vcf.gz,ClinVar,vcf,exact,0,CLNSIG,CLNREVSTAT,CLNDN""" 但是使用镜像时，由于会在虚拟环境进行任务分析，所以这时候，有些数据库文件我们需要在进行任务投递时，通过–singularity-args 进行显性的声明1234/ifstj2/B2C_COM_H1/PipeAdmin/02.software/Conda/bin/snakemake \--use-singularity \#使用镜像启动容器分析--singularity-args " -B /ifstj2/B2C_RD_H1/Personal/liubo " \#配置启动容器时的参数，可以借此完成目录的挂载-s VEP_anno.smk]]></content>
      <categories>
        <category>pipeline</category>
        <category>framework</category>
        <category>编程拾慧</category>
        <category>任务调度</category>
        <category>python</category>
      </categories>
      <tags>
        <tag>流程开发</tag>
        <tag>培训</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[git 常用命令及相关参数]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-09.%E7%89%88%E6%9C%AC%E6%8E%A7%E5%88%B6%2FGit-2.%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E5%8F%8A%E7%9B%B8%E5%85%B3%E5%8F%82%E6%95%B0%2F</url>
    <content type="text"><![CDATA[相关材料官方文档官方文档-中文 常用命令仓库的初始化本地创建新的仓库123456789101112# 新建一个git仓库，运行完后会在当前目录下生成一个.git 目录git init# 将本地仓库与远程仓库进行关联git remote add origin git@github.com:YZ/helloTest.git# 先将关联后的github仓库中的代码pull下来git pull origin master# 将最新的修改推送到远程仓库 将本地仓库的文件推送到远程仓库git push -u origin master# 第一次使用加上了-u参数，是推送内容并关联分支 从远程仓库中拉取一个仓库到本地完成秘钥配置后，可以将gitlab上的项目下载到本地进行部署 1git clone remote-repertor（https://github.com/libgit2/libgit2） local—repertory(可以省了默认保持原仓库名称) 完成项目到本地的传输后，可以进行相应的安装或使用，git clone仅能用于将远程项目初始化到本地（从无到有）不支持后续的更新同步 下载更新作为多人版本协作，会有不同的人同时对项目进行更新，所以有时候会有需求将他人的更新同步到本地流程中，使用 git pull命令 正规流程 123456789git status（查看本地分支文件信息，确保更新时不产生冲突）git checkout – [file name] （若文件有修改，可以还原到最初状态; 若文件需要更新到服务器上，应该先merge到服务器，再更新到本地）git branch（查看当前分支情况）git checkout remote branch (若分支为本地分支，则需切换到服务器的远程分支)git pull 快速流程 12#上面是比较安全的做法，如果你可以确定什么都没有改过只是更新本地代码git pull (一句命令搞定) 获取所有分支一般情况，我们也许只对主分支感兴趣，但是有时候，我们可能会需要获取所有的分析版本，来进行一些代码的核验或者代码的确认，123git branch -r | grep -v &apos;\-&gt;&apos; | while read remote; do git branch --track &quot;$&#123;remote#origin/&#125;&quot; &quot;$remote&quot;; donegit fetch --allgit pull --all 版本管理提交更新提交更新主要分三个部分，1，更新提交到本地暂存区；2.更新到本地版本库；3.推送更新。 12345678git add file.name #将更新的文件提交到本地的缓存区；git add . # 提交目录下所有更新git rm file.name # 把目录的文件从跟踪中移除git status # 查看当前提交到缓存区的所有更新git commit -m "version description" #将缓存区的内容更新到本地的版本库；git commit --amend -m "新的提交信息" # 更新已经提交过的commit内容git commit -m "XXXXXXX close #1" #提交更改，并关闭对应的issue #后面跟issue_idgit push #将本地最新的版本推送到远程仓库。如果是clone下来的仓库，会推送到原下载链接所在的仓库。 添加tag像其他版本控制系统（VCS）一样，Git 可以给仓库历史中的某一个提交打上标签，以示重要。 比较有代表性的是人们会使用这个功能来标记发布结点（ v1.0 、 v2.0 等等）。1234567891011121314151617#列出标签git tag# 例如符合条件的标签$ git tag -l "v1.8.5*"v1.8.5v1.8.5-rc0v1.8.5-rc1v1.8.5-rc2v1.8.5-rc3#创建标签，其中 -m可选，版本校验值缺省时默认是当前版本git tag -a v1.4 -m "my version 1.4" ca82a6dff817ec66f44342007202690a93763949# 共享标签， 标签在推送时，默认是不会进行推送的，需要进行显式的提交git push origin v1.5 # 推送指定的tag版本（v1.5)git push origin --tags # 推送全部tags 查看本地的版本记录12345# 查看版本更新日志$ git log# 只显示每个版本改动的文件名称$ git log --name-only 比较两个版本之间的区别12345678# 查看任意两个版本之间的改动：$ git diff 版本号码1 版本号码2# 查看两个提交版本id修改了那些文件，可以使用$ git diff commit-id1 commit-id2 --stat#比较两个版本号码的src 文件夹的差异$ git diff 版本号码1 版本号码2 src 对本地版本进行更改1234git reset gitversion filename# 撤销提交缓存区的偶作git reset --hard gitversion # 还原本地流程到之前的gitversion版本，放弃本地的后续版本git reset --hard HEAD^ #回退到上一个版本（"^" 代表回退的版本数，两个版本用"^^"git reset --hard HEAD~50#还原git到指定版本 –hard 不保存所有变更–soft 保留表更且变更内容处于staged–mixed 保留biang且变更内容处于modified（默认） 对特定文件进行管理 - 恢复修改的文件 只修改了文件，没进行任何git操作 12# 只是修改了文件，没有任何 git 操作，直接一个命令就可回退：$ git checkout -- aaa.txt # aaa.txt为文件名 修改了文件，并提交到暂存区（即编辑之后，gitadd但没有 git commit -m ….） 123$ git log --oneline # 可以省略$ git reset HEAD # 回退到当前版本$ git checkout -- aaa.txt # aaa.txt为文件名 修改了文件，并提交到仓库区（即编辑之后，gitadd和 git commit -m ….） 123$ git log --oneline # 可以省略$ git reset HEAD^ # 回退到上一个版本$ git checkout -- aaa.txt # aaa.txt为文件名 分支管理1234567891011121314151617181920212223#列出分支命令：git branch#创建分支命令：git branch (branchname)#切换分支命令:git checkout (branchname)#创建并切换该分支git checkout -b &lt;name&gt;#合并分支命令:git merge newtest # 将newtest分支合并到当前分支#重命名分支git branch -m/-M &lt;oldname&gt; &lt;newname&gt;#删除分支命令：git branch -d (branchname)#恢复的删除分支git branch &lt;name&gt; &lt;删除分支的commitID&gt; 比较两个分支12345678910111213141516171819202122# 显示出branch1和branch2中差异的部分git diff branch1 branch2 --stat#显示指定文件的详细差异git diff branch1 branch2 具体文件路径#显示出所有有差异的文件的详细差异git diff branch1 branch2#查看branch1分支有，而branch2中没有的loggit log branch1 ^branch2#查看branch2中比branch1中多提交了哪些内容git log branch1..branch2#注意，列出来的是两个点后边（此处即dev）多提交的内容。#不知道谁提交的多谁提交的少，单纯想知道有是吗不一样git log branch1...branch2#在上述情况下，在显示出没个提交是在哪个分支上git log --lefg-right branch1...branch2#注意 commit 后面的箭头，根据我们在 –left-right branch1…branch2 的顺序，左箭头 &lt; 表示是 branch1 的，右箭头 &gt; 表示是branch2的。 远程仓库操作12345678910111213#克隆一个已有的远程git仓库git clone https://github.com/lh3/wgsim.git#把一个已有的本地仓库与远程仓库关联起来。git remote add origin https://github.com/lh3/wgsim.git#可以把本地库的所有内容推送到远程库master/dev分支git push origin master/ git push origin dev#将远程 origin 的 master 分支拉取过来，与本地的test分支合并。git pull origin master:testgit pull = git fetch + git merge Patching有记录的进行版本回退。 1234# Given one or more existing commits, revert the changes that the related patches introduce, and record some new commits that record them.git revert当发生一次错误的提交以后，可以进行版本的还原（同时保留错误的提交记录） merge整个分支的mergemerge操作时，默认是将整个分支的更改合并到当前分支中。123456789101112131415161718git merge--edit和-e # 用于在成功合并、提交前调用编辑器来进一步编辑自动生成的合并信息。因此使用者能够进一步解释和判断合并的结果。--no-edit # 参数能够用于接受自动合并的信息（通常情况下并不鼓励这样做）。--ff # 是指fast-forward命令。当使用fast-forward模式进行合并时，将不会创造一个新的commit节点。默认情况下，git-merge采用fast-forward模式。--ff-only # 除非当前HEAD节点已经up-to-date（更新指向到最新节点）或者能够使用fast-forward模式进行合并，否则的话将拒绝合并，并返回一个失败状态。--no-ff命令 # 即使可以使用fast-forward模式，也要创建一个新的合并节点。这是当git merge在合并一个tag时的默认行为。--log[=&lt;n&gt;]# 将在合并提交时，除了含有分支名以外，还将含有最多n个被合并commit节点的日志信息。--no-log # 并不会列出该信息。--stat # 参数将会在合并结果的末端显示文件差异的状态。文件差异的状态也可以在git配置文件中的merge.stat配置。-n/--no-stat # 参数将不会显示该信息。--squash # 当一个合并发生时，从当前分支和对方分支的共同祖先节点之后的对方分支节点，一直到对方分支的顶部节点将会压缩在一起，使用者可以经过审视后进行提交，产生一个新的节点。开发者可能在本地提交了大量且无意义的节点，当需要合并到develop分支时，可能仅仅需要用一个新的节点来表示这一长串节点的修改内容，这时--squash命令将会发挥作用。如果功能分支的多次提交并不是琐碎而都是有意义的，使用--no-ff命令更为合适。-q和 --quiet # 静默操作，不显示合并进度信息。--no-commit # merge命令默认会将分支合并后，直接进行commit提交，有时候我们可能会先核查相关的文件变动不想立即提交可以选择该参数。完成调整后再使用commit单独提交。git merge --abort # 在合并出现冲突时使用，放弃合并过程，重建合并前的状态 git merge merge其他分支的某个commit首先在开发分支上获取要合并commit的commitID，1git log --oneline 然后切换到目标分支进行merge 12345git checkout main # 切换到要进行修改的分支git cherry-pick a1b2c3d4 # 合并 dev分支上的一个commit提交（-n 参数只合并变更不提交）git cherry-pick &lt;起始提交&gt;^..&lt;结束提交&gt; # 合并dev分支上一个范围的连续commit提交git cherry-pick &lt;commit1&gt; &lt;commit2&gt; &lt;commit3&gt; # 合并dev分支上多个独立的commitgit push origin main]]></content>
      <categories>
        <category>编程拾慧</category>
        <category>Git</category>
      </categories>
      <tags>
        <tag>培训</tag>
        <tag>Git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RPKM, FPKM and TPM]]></title>
    <url>%2F01.%E7%9F%A5%E8%AF%86-01.NGS%2F2019-03-02.RNA%E8%A1%A8%E8%BE%BE%E9%87%8F%E7%9A%84%E8%AE%A1%E7%AE%97%2F</url>
    <content type="text"><![CDATA[过去，进行 RNA-seq 时，会以 RPKM（每千碱基百万读取数）或 FPKM（每千碱基百万片段数）的形式报告结果。然而， TPM（每千基百万转录本）现在变得非常流行。由于这些术语似乎有很多混淆，我想我会使用 StatQuest 来清除所有内容。 这三个指标试图标准化测序深度和基因长度。 RPKM 的计算过程参考如下操作： 计算样本中的总读数并将该数字除以 1,000,000——这是我们的“每百万”比例因子。 将读取计数除以“每百万”比例因子。这使测序深度标准化，为您提供每百万读数 (RPM) 将 RPM 值除以基因的长度，以千碱基为单位。这为您提供 RPKM。 FPKM 与 RPKM 非常相似。 RPKM 是为单端 RNA-seq 制作的，其中每个读取对应一个已测序的片段。 FPKM 是为PE测序的 RNA-seq 制作的。使用配对末端 RNA-seq，两个读数可以对应一个片段，或者，如果对中的一个读数没有映射，一个读数可以对应一个片段。 RPKM 和 FPKM 之间的唯一区别是 FPKM 考虑到两次读取可以映射到一个片段（因此它不会将该片段计算两次）。 TPM 与 RPKM 和 FPKM 非常相似。唯一的区别是操作顺序。以下是计算 TPM 的方法： 将读取计数除以每个基因的长度（以千碱基为单位）。这为您提供了每千碱基 (RPK) 的读数。 计算一个样本中的所有 RPK 值并将这个数字除以 1,000,000。这是您的“每百万”比例因子。 将 RPK 值除以“每百万”比例因子。这为您提供了 TPM。 所以你看，在计算 TPM 时，唯一的区别是你首先对基因长度进行归一化，然后对测序深度进行归一化。然而，这种差异的影响是相当深远的。 当您使用 TPM 时，每个样本中所有 TPM 的总和是相同的。这使得比较每个样本中映射到基因的读数比例变得更加容易。相比之下，使用 RPKM 和 FPKM，每个样本中归一化读数的总和可能不同，这使得直接比较样本变得更加困难。 这是一个例子。如果样本 1 中基因 A 的 TPM 为 3.33，样本 B 中的 TPM 为 3.33，那么我知道在两个样本中映射到基因 A 的总读数的比例完全相同。这是因为两个样本中的 TPM 总和总是相同的数字（因此计算比例所需的分母是相同的，无论您正在查看哪个样本。） 使用 RPKM 或 FPKM，每个样本中归一化读数的总和可能不同。因此，如果样本 1 中基因 A 的 RPKM 为 3.33，样本 2 中 RPKM 为 3.33，我不知道样本 1 中相同比例的读数是否与样本 2 中的基因 A 对应。这是因为需要分母计算两个样本的比例可能不同。 参考来源rna-seqblog]]></content>
      <categories>
        <category>RNA</category>
      </categories>
      <tags>
        <tag>NGS</tag>
        <tag>RNA</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SGE任务调度系统相关命令记录]]></title>
    <url>%2F03.%E5%BA%94%E7%94%A8-02.Linux%E5%92%8Cshell%2FLinux-%E4%BB%BB%E5%8A%A1%E8%B0%83%E5%BA%A6%E7%B3%BB%E7%BB%9F-SGE-%E7%9B%B8%E5%85%B3%E5%91%BD%E4%BB%A4%E8%AE%B0%E5%BD%95%2F</url>
    <content type="text"><![CDATA[http://gridscheduler.sourceforge.net/htmlman/ http://gridscheduler.sourceforge.net/htmlman/htmlman1/qstat.html?pathrev=V62u5_TAG sge系统管理命令队列信息查询 #### 命令 功能 qconf -sq 查询特定队列的信息 qconf -sprjl 显示项目列表 qsub123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172-V: 将当前的环境变量传递到执行命令的节点中。可以使用-v PWD 来代替-cwdcwd: 在当前目录下执行任务, sge的日志会输出到当前路径。 不增加该指令，所有投递的任务都会在家目录下执行-l resource=value: 请求资源数, 例如 -l vf=25G -l h=node1 就是任务的预估内存要25G(内存估计的值应稍微大于真实的内存，内存预估偏小可能会导致节点跑挂), 申请在node1上运行eg: qsub -cwd -l h=compute-12-13 run.sh -S： /bin/bash: 表示在bash环境下执行命令。默认tcsh.-sync y|n: 是否等待任务结束，返回退出码-o path: 指定标准输出的文件夹-j y|n ：是否将标准输入和标准输入合并成一个文件-a date_time 作业开始运行时间-b y[es]|n[o]判断作业指定是二进制文件或scripts。y ：是 n：scripts-display 使用X-windows-dl date_time 定义作业到期时间，在作业到期时间之前，作业的优先级会逐步提高，直到管理员指定的最高级别。-e 指定输出error文件的路径及文件名-hard 定义作业被调度的硬性要求-h 作业hold类型。u：表示用户hold,s:表示系统hold，o：表示被操作员hold，n：取消hold，U：取消用户hold，S：取消系统hold，O：取消操作员hold。-i 定义输入文件-j y[es]|n[o] 定义作业的标准错误输出是否写入的输出文件中-l resource=value, 表明作业运行所需要的资源。-m b|e|a|s|n 。定义邮件发送规则。b：作业开始时发送。e：作业结束时发送。a：作业失败时发送 s：作业挂起时发送。n：不发送-M user[@host] 定义邮件地址-notify ：定义发送SIGSTOP or SIGKILL信号的延迟时间-now y[es]|n[o]：立即执行作业-N 作业名-o [[hostname]:]path ：定义输出文件路径、文件名-P project_name：定义项目名称-p priority ：定义优先级-pe parallel_environment：定义并行环境-q wc_queue_list：定义作业运行队列 （也可以进一步使用 @ 指定运行节点 eg: b2c_rd1.q@tj-compute-31-5.tj.hpc）-R y[es]|n[o]：定义是否为作业保留资源。-r y[es]|n[o]：定义作业失败后是否重新运行-soft 定义作业被调度的软性要求-u username,只有qlter命令可以使用该参数。修改作业的用户名-v variable：定义环境变量-verbose 使qrsh命令输出信息-verify 验证作业参数时使用-V 传递当前命令的所有环境变量-clear 清除预设的环境配资，恢复为默认值 qconf 队列 1234567# 显示所有队列qconf -sql #显示一个队列的详细配置信息（包含的节点和负载等信息）qconf -sq queue_name #显示当前配置为执行主机的所有主机的名称列表#添加一个队列qconf -aq queue_name 用户组 12345678# 查看所有用户组qconf -sul# 查看某个用户组信息qconf -su 用户组名# 编辑用户组配置qconf -mu 用户组名# 创建用户组并把用户添加到用户组qconf -au 用户名 用户组名 其他 1234567891011121314&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEADqconf -sel #显示当前配置为执行主机的所有主机的名称列表qconf -se hostname #显示指定的执行主机的详细信息qconf -sh #显示具有管理权限的主机列表qconf -ss #显示提交主机列表qconf -sc #显示所配置的资源属性列表qconf -su #显示指定项目下的用户 &#123;project&#125;_userqconf -sconf #显示当前配置qconf -aprj #添加一个新项目qconf -sprjl #显示项目列表qconf -aq queue_name #添加一个队列qconf -ahgrp @host_group_name #添加主机组，hostlist中主机列表间隔用空格qconf -mconf #编辑默认Shell【login_shells bash,sh,ksh,csh,tcsh】qconf -aq #显示默认队列模板 qstat 显示队列和作业的状态qstat 命令—用于查询作业状态信息123456789101112131415命令格式：qatat [-f][-a][-i] [-n][-s] [-R] [-Q][-q][-B][-u]参数说明：-f jobid 列出指定作业的信息-a 列出系统所有作业-i 列出不在运行的作业-n 列出分配给此作业的结点-s 列出队列管理员与scheduler 所提供的建议-R 列出磁盘预留信息-Q 操作符是destination id，指明请求的是队列状态-q 列出队列状态，并以alternative 形式显示-au userid 列出指定用户的所有作业-B 列出PBS Server 信息-r 列出所有正在运行的作业-Qf queue 列出指定队列的信息-u 若操作符为作业号，则列出其状态。 任务状态123456qw: 表示等待状态hqw: 任务挂起等待中，待依赖的任务完成后执行Eqw: 投递任务出错r: 表示任务正在运行s: 暂时挂起dr: 节点挂了之后，删除任务就会出现这个状态，只有节点重启之后，任务才会消失 qmod 修改队列和作业的状态123456789# 任务的挂起与恢复qmod -sj jid (已在run的job) 或 qhold jid(qw的job)挂起qmod -usj jid 或 qrls jid恢复之前挂起的任务## 登陆节点任务挂起kill -STOP pid ## 登陆节点任务恢复kill -CONT pid]]></content>
      <categories>
        <category>任务调度</category>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>SGE</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[git 安装配置及基本概念]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-09.%E7%89%88%E6%9C%AC%E6%8E%A7%E5%88%B6%2FGit-1.%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AE%E5%8F%8A%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%2F</url>
    <content type="text"><![CDATA[培训材料组内培训PPT 官方文档官方文档-中文 Git安装主流系统安装参考如下： linux：Debian或Ubuntu：sudo apt-get install gitCentos: sudo yum install git Windows/ macOS ：下载地址：https://git-scm.com/downloads 其他机器或系统版本可参考官网 Git安装 账号配置配置全局统一的账号配置全局的name和email，参照你创建的工程Git global setup 方法一1234567891011git config --global user.name &quot;liubo4&quot; #存储账号git config --global user.email &quot;liubo4@genomics.cn&quot; #存储邮箱git config --global credential.helper store # 存储你输入的验证信息# 不添加 --global 仅对当前仓库有效，信息存储到仓库目录 `.\.git\config` 文件中。# --global：表示该机器上所有的Git仓库都会使用这个配置，存储到` ~/config `文件中。### 方法二ssh-keygen -t rsa -C &quot;liubo4@genomics.cn&quot; #生成秘钥, 将秘钥生成的文件： id_rsa.pub 中的内容复制到gitlab中(Edit profile==&gt;SSH key==&gt; Add an SSH key )；后期可以通过密钥进行身份验证。### 方法三， 将密码显示的保存在存储硬件上 ~/.git-credentials，第一次输入密码后续不需要重复输入密码。q 配置多个账号有时候，由于各种原因（需要同时使用多个仓库平台，例如：github存放一些个人代码和企业gitlab存放公司业务代码），因此经常会遇到我们需要同时访问多个git托管平台。这时候密钥的优势就体现出来，可以自动通过不同的域名（IP）调取对应的密钥进行数据的访问，极大的提高我们的效率。 首先针对我们每个不同的托管平台账号生成各自的密钥信息。以gitlab &amp; github为例。 123ssh-keygen -t rsa -f ~/.ssh/id_rsa_gitlab -C "gitlab@account" ssh-keygen -t rsa -f ~/.ssh/id_rsa_github -C "github@account" 生成密钥后，在密钥存储目录创建~/.ssh/config文件并进行相关信息的记录，config文件格式如下 1234567891011121314151617# github# host 与 hostname 需要相同Host github.com HostName github.com # 你的github账号 User github@account # github对应的rsa秘钥文件 IdentityFile ~/.ssh/id_rsa_github# gitlab# host 与 hostname 需要相同Host gitlab.genomics.cn HostName gitlab.genomics.cn # 你的gitlab账号 User gitlab@account # gitlab对应的rsa秘钥文件 IdentityFile ~/.ssh/id_rsa_gitlab 完成上述步骤后，分别将不同托管平台对应生成的秘钥文件信息添加到对应平台中，例如：gitlab中添加方式: Edit profile ==&gt; SSH key ==&gt; Add an SSH key ；github中添加方式：Settings ==&gt; SSH and GPG keys ==&gt; New SSH key。然后就可以直接对多个不同托管平台的数据进行操作，而不用每个仓库单独配置账号和权限了。更多的关联配置参考文章《git多账号多仓库的配置》 基本知识Git的4个结构区域git的整体工作结构区域分为如下4个区域类型 工作区：项目的工作目录 暂存区：修改的文件会暂时存储在该区域，也叫index区 本地仓库：即隐藏目录 .git，是 Git 的版本库 远程仓库：即常说的github，gitlab，可在线获取 Git文件的4种基本状态 未追踪（untracked）文件已经在文件夹中，但没有加入git库，也不参与版本控制 已修改（modified）已修改表示修改了文件，但还没保存到本地git仓库中 已暂存（staged）已暂存表示对一个已修改的文件的当前版本做了标记，使之包含在下次提交的快照中。 已提交（committed）已提交表示数据已经安全的保存在本地git仓库中。 Git的常见配置文件忽略文件（.gitignore）当不想把某些文件纳入版本控制中，比如数据库文件，临时文件，设计文件时，可以使用“.gitignore”文件进行标识“.gitignore”文件规则： 忽略文件中的空行或以井号（#）开始的行。 可以使用Linux通配符。例如：星号（*）代表任意多个字符，问号（？）代表一个字符，方括号（[abc]）代表可选字符范围，大括号（{string1,string2,…}）代表可选的字符串等。 如果名称的最前面有一个感叹号（!），表示例外规则，将不被忽略。 如果名称的最前面是一个路径分隔符（/），表示要忽略的文件在此目录下，而子目录中的文件不忽略。 如果名称的最后面是一个路径分隔符（/），表示要忽略的是此目录下该名称的子目录，而非文件（默认文件或目录都忽略）。 eg：12345#为注释*.txt #忽略所有.txt结尾的文件!lib.txt #lib.txt除外build/ #忽略build/目录下的所有文件doc/*.txt #会忽略 doc目录下“.txt”为后缀的文件 如果文件已经被追踪，在 .gitignore 中添加后，需要清除一下此文件的 git 缓存，git rm -r --cached 文件名, 然后再进行 commit和 push即可。 大文件管理 通过使用 git lfs 进行流程中大文件的管理，需要提前安装 1.8.5以上版本的git 。 参考文档 git lfs下载安装下载 git-lfs-linux-386-v2.10.0.tar.gz 拷贝至集群。 1234567891011121314tar -zxvf git-lfs-linux-386-v2.10.0.tar.gz解压后会在当前目录产生下列4个文件-rw-r--r-- 1 OseqPub b2c1 77881 May 9 00:17 CHANGELOG.md-rwxr-xr-x 1 OseqPub b2c1 10248192 May 9 00:20 git-lfs-rwxr-xr-x 1 OseqPub b2c1 389 May 9 00:17 install.sh-rw-r--r-- 1 OseqPub b2c1 7522 May 9 00:17 README.md#将git-lfs所在目录加到环境变量中$PATH #执行 安装git lfsgit lfs install for i in `find . -type f -size +100M ` ;do git lfs track $i ;done # 跟踪大文件git lfs ls-files 可以显示当前跟踪的文件列表git lfs clone 下载lfs文件]]></content>
      <categories>
        <category>编程拾慧</category>
        <category>Git</category>
      </categories>
      <tags>
        <tag>培训</tag>
        <tag>Git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[天河二号作业调度系统简单使用]]></title>
    <url>%2F03.%E5%BA%94%E7%94%A8-99.other%2F2018-12-07.task-manage-in-Tianhe2%2F</url>
    <content type="text"><![CDATA[天河用户手册天河用户手册 现将天河2号的作业调度系统的简单使用方法记录于此: MPI天河2号默认使用mpich-3.2.1（使用icc-14.0.2 编译），是天河2号自主实现的mpi版本，具有较高的效率。 module 模块管理1234567module avail # 查看可用的模块的列表。module load [modulesfile] # 加载需要使用的modulefiles。module load OpenFOAM/2.2.2 # 示例 module其它用法，可在help中查询 作业调度基础12345678yhi # yhinfo命令的简写，用于查看节点状态 其中PARTITION表示分区；NODES表示节点数；NODELIST为节点列表；STATE表示节点运行状态， 其中，idle表示节点处于空闲状态，allocated表示节点已经分配了一个或多个作业。yhq # yhqueue命令的简写，用于查看作业运行情况 推荐使用“yhq -a”查看作业状态信息 其中JOBID 表示任务ID，Name表示任务名称，USER为用户，TIME为已运行时间，NODES表示占用节点数，NODELIST为任务运行的节点列表。 交互式提交作业在shell窗口中执行yhrun命令，主要命令格式如下： 123456789101112131415161718192021222324252627282930yhrun [options] program-n, --ntasks=number # 指定要运行的任务数-c, --cpus-per-task=ncpus # 每个任务需要ncpus 个处理器核-N, --nodes=minnodes[-maxnodes] # 请求为作业至少分配minnodes（最大maxnodes）个节点。 （例如“-N 2-4”或“--nodes=2-4”） 如果没有指定-N，缺省行为是分配足够多的节点以满足-n和-c参数的需求-p, --partition=partition name # 在指定分区中分配资源。如未指定，则由控制进程在系统默认分区中分配资源。-w, --nodelist=node name list # 请求指定的节点名字列表。作业分配资源中将至少包含这些节点。 列表可以用逗号分隔的节点名或节点范围（如cn[1-5,7,...]）指定，或者用文件名指定；如果参数中包含“/”字符，则会被当作文件名。-x, --exclude=node name list # 不要将指定的节点分配给作业-D, --workdir=directory # set working directory for batch script-I, --immediate[=secs] # exit if resources not available in &quot;secs&quot;-o, --output=out # location of stdout redirection-J, --job-name=jobname # name of job-l, --label # prepend task number to lines of stdout/err-h, --help # 若需使用yhrun更多选项，可通过“yhrun –h”或“yhrun --help”查看。# eg:yhrun -n 4 -p bigdata hostname yhrun -n 4 -w cn[7303-7306] -p bigdata hostname yhrun -n 4 -N 4 -w cn[7303-7304] -p bigdata hostname yhrun -n 4 -N 4 -x cn[7303-7304] -p bigdata hostname 节点资源抢占命令 yhalloc该命令支持用户在提交作业前，抢占所需计算资源 1yhalloc -N 1 -p bigdata 通过yhq查看相应的jobID 为1051，节点为cn7314，然后ssh到对应节点进行操作 ####取消自己的作业 使用yhcancel命令 1yhcancel jobid 批处理作业命令 yhbatch在资源满足要求时，分配完计算节点之后，系统将在所分配的第一个计算节点（而不是登录节点）上加载执行用户的作业脚本。 123456789cat &gt; mybash.sh #!/bin/bash yhrun -n 4 -N 4 -p bigdata hostname chmou mybash.sh yhbatch -N 4 -p bigdata ./mybash.sh 计算开始后，工作目录中会生成以slurm开头的.out 文件为输出文件。更多选项，用户可以通过yhbatch --help命令查看。如果不需要使用MPI的话，也可以不使用yhrun 单个节点上提交多个作业因为天河2是独享作业，当一个节点上已经被分配出去之后，即便没有使用全部的核心，也无法继续提交作业。所以，若想在一个节点上运行多个作业，必须同时提交上去，如下：某用户有4个 a.out 需要运行，每个a.out最多只能高效运用6 个CPU 核，那么可以构建下面的任务脚本，在一个计算节点上同时运行多个作业: 12345678cat &gt; job.sh #!/bin/bashyhrun –n 4 a.out arg.1 &amp; yhrun –n 4 a.out arg.2 &amp;yhrun –n 4 a.out arg.3 &amp;yhrun –n 4 a.out arg.4 &amp;wait # important 然后通过yhbatch –N 1 job.sh来一次提交计算任务，使所有小的计算任务都可以在一个节点同时进行计算。如果不需要使用MPI的话，也可以不使用yhrun。]]></content>
      <categories>
        <category>任务调度</category>
      </categories>
      <tags>
        <tag>任务调度</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[爬虫学习记录-相关软件&包记录]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-02.python%2FPython-%E7%88%AC%E8%99%AB%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95-%E7%9B%B8%E5%85%B3%E8%BD%AF%E4%BB%B6-%E5%8C%85%E8%AE%B0%E5%BD%95%2F</url>
    <content type="text"><![CDATA[爬虫学习过程涉及的软件： 软件1. Python3.5.5整体爬虫代码的框架语言，所有Python程序及包均基于本版本。 2. chromeDriver模拟登陆chrome浏览器，驱动浏览器完成相关操作。 3.PhantomJS - 官方文档一个无界面的，可脚本编程的WebKit浏览器引擎，原生支持多种Web标准：DOM操作、CSS选择器、JSON、Canvas和SVG。Selenium包支持该引擎，使用该引擎，就可以避免运行过程中不断弹出浏览器。 4.MYSQL一个关系型数据库，用于存储相关的数据。 5.Docker一种容器技术，可以将环境和应用打包，形成一个独立的应用，这个应用可以直接被分发到任意一个支持Docker的环境中，通过简单的命令启动运行。 Python包请求库1. requests - 中文文档Python中唯一的一个http原生库。 2.Selenium - 中文文档一个自动化测试工具，利用他可以驱动浏览器执行特定的动作，如点击、下拉等操作。针对一些JavaScript渲染的页面来说，这种抓去方式非常有效。 3.aiohttp可以在爬取过程中，提供异步Web服务的库。 解析库1.lxmlPython的一个解析库，可以支持HTML和XML的解析，支持XPath的解析方式，解析效率较高。 2.beautifulsoup4HTML和XML的解析库 3.pyquery网页解析工具，提供了和jQuery类似的语法来解析html文档，支持css选择器，。 相关模块1.tesserocrPython的一个ORC（Optical Character Recognition光学字符识别）识别库。 2.pymysql用于连接MYSQL数据库，在Python中访问Mysql数据库。 3.Web库-FlaskFlask一个轻量级的Web服务程序，用来做一些Web的API服务。 4.TornodeTornode一个支持异步的Web框架。 APP爬取相关库的安装1.Charles一个网络抓包工具。 2.mitmproxy一个支持http和https的抓包工具，类似Charles和Fiddler的功能，通过控制台操作。 爬虫框架1.Pyspider2.Scrapy]]></content>
      <categories>
        <category>编程拾慧</category>
        <category>python</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[下机数据(fastq)质控软件汇总]]></title>
    <url>%2F03.%E5%BA%94%E7%94%A8-01.software_in_NGS%2F2018-11-19.%E4%B8%8B%E6%9C%BA%E6%95%B0%E6%8D%AE%EF%BC%88fastq%EF%BC%89%E8%B4%A8%E6%8E%A7%E8%BD%AF%E4%BB%B6%E6%B1%87%E6%80%BB%2F</url>
    <content type="text"><![CDATA[常用的生物信息学ruan jian SoapNuke Fastp trimmomatic AfterQC seqtk]]></content>
      <categories>
        <category>NGS</category>
        <category>肿瘤检测</category>
        <category>software</category>
      </categories>
      <tags>
        <tag>NGS</tag>
        <tag>质控</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[肿瘤组Linux培训资料]]></title>
    <url>%2F03.%E5%BA%94%E7%94%A8-02.Linux%E5%92%8Cshell%2FLinux-%E8%82%BF%E7%98%A4%E7%BB%84Linux%E5%9F%B9%E8%AE%AD%E8%B5%84%E6%96%99%2F</url>
    <content type="text"><![CDATA[肿瘤组内部Linux系统操作普及培训材料1.简单了解Linux系统的使用 Windows安装软件XshellXftp Mac 安装软件FileZilla]]></content>
      <categories>
        <category>Linux</category>
        <category>内部培训材料</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[常用shell命令 - time]]></title>
    <url>%2F03.%E5%BA%94%E7%94%A8-02.Linux%E5%92%8Cshell%2FLinux-%E5%B8%B8%E7%94%A8shell%E5%91%BD%E4%BB%A4-time%2F</url>
    <content type="text"><![CDATA[time在需要执行的命令前，添加time命令，直接返回任务运行的几个时间统计指标，信息如下：1234567(base)[liubo4@tj*]$time ls $File$Filereal 0m0.007suser 0m0.000ssys 0m0.002s(base)[liubo4@tj*]$ (1) real：从进程 ls 开始执行到完成所耗费的 CPU 总时间。该时间包括 ls 进程执行时实际使用的 CPU 时间，ls 进程耗费在阻塞上的时间（如等待完成 I/O 操作）和其他进程所耗费的时间（Linux 是多进程系统，ls 在执行过程中，可能会有别的进程抢占 CPU）。 (2) user：进程 ls 执行用户态代码所耗费的 CPU 时间。该时间仅指 ls 进程执行时实际使用的 CPU 时间，而不包括其他进程所使用的时间和本进程阻塞的时间。 (3) sys：进程 ls 在内核态运行所耗费的 CPU 时间，即执行内核系统调用所耗费的 CPU 时间。 单线程任务，会存在 real &gt; user + sys，但是在多线程，三者间没有明确的关系 \timetime是我们比较常用的， 接触linux系统的人也基本都了解过，本文也主要是介绍 \time， 除了time命令返回的三个时间外， \time还可以返回任务的内存、I/O 等的使用情况。示例如下：1234(base)[liubo4@tj-login-0-1 InDel]$\time ls $File$File0.00user 0.00system 0:00.00elapsed 11%CPU (0avgtext+0avgdata 928maxresident)k0inputs+0outputs (0major+295minor)pagefaults 0swaps 可以看到多了很多除了时间之外的指标，默认是简略版输出，初次使用不熟悉时，我们可以添加 -v 参数，会提供每个参数更详细的说明信息12345678910111213141516171819202122232425(base)[liubo4@tj-login-0-1 InDel]$\time -v ls $File$File Command being timed: "ls $File " User time (seconds): 0.00 System time (seconds): 0.00 Percent of CPU this job got: 11% Elapsed (wall clock) time (h:mm:ss or m:ss): 0:00.00 Average shared text size (kbytes): 0 Average unshared data size (kbytes): 0 Average stack size (kbytes): 0 Average total size (kbytes): 0 Maximum resident set size (kbytes): 928 Average resident set size (kbytes): 0 Major (requiring I/O) page faults: 0 Minor (reclaiming a frame) page faults: 296 Voluntary context switches: 43 Involuntary context switches: 1 Swaps: 0 File system inputs: 0 File system outputs: 0 Socket messages sent: 0 Socket messages received: 0 Signals delivered: 0 Page size (bytes): 4096 Exit status: 0 指标介绍时间 指 标 含 义 Elapsed (wall clock) time 执行命令所花费的时间，格式是：[hour]:minute:second System time 命令执行时在内核模式所花费的时间，单位是秒 User time 命令执行时在使用者模式所花费的时间，单位是秒 Percent of CPU this job got 命令执行时 CPU 的占用比例。其实这个数字就是内核模式的 CPU 时间加上使用者模式的 CPU 时间除以总时间 内存 指 标 含 义 Maximum resident set size 执行程序所占用内存的最大值。单位是 KB Average resident set size 执行程序所占用内存的平均值，单位是 KB Average total size 执行程序所占用的内存总量（stack+data+text）的平均大小， 单位是 KB Average unshared data size 执行程序所占用的私有数据区（unshared data area）的平均 大小，单位是 KB Average stack size 执行程序所占用的私有堆栈（unshared stack）的平均大小， 单位是 KB Average shared text size 执行程序间共享内容（shared text）的平均值，单位是 KB Page size 系统内存页的大小，单位是 byte。对于同一个系统来说，这 是个常数 I/O 指 标 含 义 Major (requiring I/O) page faults 此程序的主要内存页错误发生的次数。所谓的主要内存页错误是指某一内存页己经詈换到 SWAP 分区中，又被其他程序使用过，该页的内容必须从 SWAP 分区里再读出来才能使用 Minor (reclaiming a frame) page faults 此程序的次要内存页错误发生的次数。 所谓的次要内存页错误是指某一内存页虽然己经詈换到 SWAP 中，但尚未被其他程序使用。此时该页的内容并未 被破坏，不必从 SWAP 分区里读出来即可直接使用 Swaps 此程序被交换到 SWAP 分区的次数 Involuntary context switches 此程序被强迫中断（如 CPU 时间耗尽）的次数 Voluntary context switches 此程序自愿中断（I/O 执行完毕，磁碟读取完成等）的次数 File system inputs 此程序所输入的文件数 File system outputs 此程序所输出的文件数 Socket messages received 此程序所收到的 Socket Message Socket messages sent 此程序所送出的 Socket Message Signals delivered 此程序所收到的信号数 Exit status 命令退出状态 说明在 time 命令的输出中，Elapsed time 是通过系统调用 gettimeofday 获取到的结束时间和起始时间相减得到的。因此，time 对于运行时间较短的任务计时时，会产生一定误差。time 命令输出的时间统计精度基本在 10 毫秒级。，少于 10 毫秒的程序，真的是连 time 也无法精确计时。]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Shell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[参考基因组版本问题汇总]]></title>
    <url>%2F01.%E7%9F%A5%E8%AF%86-01.NGS%2F2018-07-11.%E5%8F%82%E8%80%83%E5%9F%BA%E5%9B%A0%E7%BB%84%E7%89%88%E6%9C%AC%E9%97%AE%E9%A2%98%E6%B1%87%E6%80%BB%2F</url>
    <content type="text"><![CDATA[不同版本参考基因组的位置坐标转换在使用参考基因组时，经常会遇到一些版本的问题，比如使用注释文件和bed文件时，不同版本的位置坐标不能直接使用，这时候，我们就需要对坐标进行转换，这里记录下一些常用的坐标转换工具： 类型 支持格式 地址 推荐指数 Liftover 在线 bed http://genome.ucsc.edu/cgi-bin/hgLiftOver 一般 Liftover 本地 bed和gff http://hgdownload.cse.ucsc.edu/admin/exe/linux.x86_64/liftOver 推荐 Remap 在线 hgvs，bed，gvf，gff，gtf，Text ASN.1，Binary ASN.1，UCSC Region和VCF https://www.ncbi.nlm.nih.gov/genome/tools/remap 推荐 CrossMap 本地 SAM/BAM,，Wiggle/BigWig， bed， gff/gtf，VCF http://crossmap.sourceforge.net/ 推荐 picard 本地 interval和VCF http://broadinstitute.github.io/picard/ 推荐VCF转换]]></content>
      <categories>
        <category>NGS</category>
        <category>知识沉淀</category>
        <category>肿瘤检测</category>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>参考基因</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python-包-包管理器Conda]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-02.python%2F%E7%8E%AF%E5%A2%83%E7%AE%A1%E7%90%86-%E5%8C%85%E7%AE%A1%E7%90%86%E5%99%A8Conda%2F</url>
    <content type="text"><![CDATA[参考文档官方文档Github下载地址 Conda安装和卸载Conda安装下载相应的安装sh 示例如下:更多发行版本获取1234wget https://repo.continuum.io/archive/Anaconda3-5.0.0-Linux-x86_64.sh #(下载Anaconda的Linux版本)wget https://repo.anaconda.com/miniconda/Miniconda3-py37_4.11.0-Linux-x86_64.sh #(miniConda)bash Anaconda3-5.0.0-Linux-x86_64.sh #安装source ~/.bashrc #更新环境变量 mamba安装conda的一个优化插件，可以大幅度提高安装速度1conda install -c conda-forge -c bioconda mamba Conda配置镜像管理 20250115 可用镜像配置 12345678channels: - bioconda - conda-forge - defaults - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/ - https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/conda-forge/ - https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/msys2/show_channel_urls: true 备选镜像清单 12345678910111213141516171819202122232425262728293031323334353637383940414243444546# 添加清华的Conda镜像conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/conda config --add channels http://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/conda config --add channels http://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/conda-forge/conda config --add channels http://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/msys2/conda config --add channels http://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/bioconda/conda config --add channels http://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/conda config --add channels defaultsconda config --add channels conda-forgeconda config --add channels bioconda# 阿里云conda config --add channels https://mirrors.aliyun.com/anaconda/pkgs/main/conda config --add channels https://mirrors.aliyun.com/anaconda/cloud/conda-forge/conda config --add channels https://mirrors.aliyun.com/anaconda/cloud/bioconda/# 北京外国语conda config --add channels https://mirrors.bfsu.edu.cn/anaconda/pkgs/main/conda config --add channels https://mirrors.bfsu.edu.cn/anaconda/cloud/conda-forge/conda config --add channels https://mirrors.bfsu.edu.cn/anaconda/cloud/bioconda/# 北大镜像conda config --add channels https://mirrors.pku.edu.cn/anaconda/pkgs/main/conda config --add channels https://mirrors.pku.edu.cn/anaconda/cloud/conda-forge/conda config --add channels https://mirrors.pku.edu.cn/anaconda/cloud/bioconda/# 哈工大conda config --add channels https://mirrors.hit.edu.cn/anaconda/pkgs/main/conda config --add channels https://mirrors.hit.edu.cn/anaconda/cloud/conda-forge/conda config --add channels https://mirrors.hit.edu.cn/anaconda/cloud/bioconda/# 南京大学conda config --add channels https://mirror.nju.edu.cn/anaconda/pkgs/main/conda config --add channels https://mirror.nju.edu.cn/anaconda/cloud/conda-forge/conda config --add channels https://mirror.nju.edu.cn/anaconda/cloud/bioconda/# 北京交通大学conda config --add channels https://mirror.bjtu.edu.cn/anaconda/pkgs/main/conda config --add channels https://mirror.bjtu.edu.cn/anaconda/cloud/conda-forge/conda config --add channels https://mirror.bjtu.edu.cn/anaconda/cloud/bioconda/# 西安交通大学conda config --add channels https://mirrors.xjtu.edu.cn/anaconda/pkgs/main/conda config --add channels https://mirrors.xjtu.edu.cn/anaconda/cloud/conda-forge/conda config --add channels https://mirrors.xjtu.edu.cn/anaconda/cloud/bioconda/ 删除某个镜像 1conda config --remove channels 'https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/' 删除所有镜像 1conda config --remove-key channels Conda卸载修改~/.bash_profile中的环境变量，去除家目录中隐藏的.condarc文件.conda文件和.continuum目录12rm -rf ~/minicondarm -rf ~/.condarc ~/.conda ~/.continuum Conda使用环境管理（conda env） 命令 功能 conda env list 列出所有Conda的环境 conda info -e 列出所有的conda环境 conda env create 创建环境 conda env export &gt; *.yaml 将环境导出为yaml conda env create -f *.yaml 基于配置文件创建环境 conda env update -f environment.yml 根据yaml文件，更新环境 conda create -n $env_name [package] 创建conda环境，同时安装相关的package（可选） conda remove -n env_name –all 删除环境 source activate \$env 切换环境 source deactivate \$env 退出环境 conda update -n python36 mxnet 更新指定环境的某个包 包管理 命令 功能 conda list 查看已经安装的包 conda list -n \$env 查看环境\$env中安装的包 conda search 查看可用的软件包 conda install &lt; package&gt;=x.x 安装x.x版本的package conda install &lt; package&gt;=x.x -y/–yes 直接安装x.x版本的package，不在询问 更新conda环境conda update conda 创建环境conda create -n ENV_Demo package1 package2 package3；创建一个名为ENV_Demo的环境，并在环境中安装 package1 package2 package3 三个软件包 激活环境source activate ENV_Demo 退出环境source deactivate ENV_Demo 虚拟环境的GCC升级123456conda install -c moussi gcc_impl_linux-64ln -s /share2/home/anconda3/envs/my_env/libexec/gcc/x86_64-conda_cos6-linux-gnu/7.3.0/gcc /share2/home/anaconda3/my_env/bin/gccconda install gcc_linux-64conda deactivateconda activate my_engcc -v 常见安装123456789101112# 安装 perlconda install -c conda-forge perl=5.22# 安装perl 包conda install perl-Excel-Writer-XLSX perl-Spreadsheet-ParseExcel perl-bio-bigfile# 安装 R conda install -c conda-forge r-base# 安装R包conda install r-ggplot2 #R包通常需要以r-开头anaconda search -t conda r-ggplot2#若无法找到可以使用该命令搜索对应R包,此处的anaconda是原始conda的路径，而非R3.5环境下的anaconda show BioBuilds/r-ggplot2 #显示该包的chanelconda install --channel https://conda.anaconda.org/BioBuilds r-ggplot2 #根据anaconda show进行安装 常见问题]]></content>
      <categories>
        <category>编程拾慧</category>
        <category>python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[常用shell命令 - awk]]></title>
    <url>%2F03.%E5%BA%94%E7%94%A8-02.Linux%E5%92%8Cshell%2FLinux-%E5%B8%B8%E7%94%A8shell%E5%91%BD%E4%BB%A4-awk%2F</url>
    <content type="text"><![CDATA[awk是行处理器: 相比较屏幕处理的优点，在处理庞大文件时不会出现内存溢出或是处理缓慢的问题，通常用来格式化文本信息awk处理过程: 依次对每一行进行处理，然后输出awk命令形式:12345678awk [-F|-f|-v] ‘BEGIN&#123;&#125; //&#123;command1; command2&#125; END&#123;&#125;’ file [-F|-f|-v] 大参数，-F指定分隔符，-f调用脚本，-v定义变量 var=value&apos; &apos; 引用代码块BEGIN 初始化代码块，在对每一行进行处理之前，初始化代码，主要是引用全局变量，设置FS分隔符// 匹配代码块，可以是字符串或正则表达式&#123;&#125; 命令代码块，包含一条或多条命令； 多条命令使用分号分隔END 结尾代码块，在对每一行进行处理之后再执行的代码块，主要是进行最终计算或输出结尾摘要信息 特殊要点:|变量 | 含义||-|-||$0 |表示整个当前行 ||$1 | 每行第一个字段 ||NF | 字段数量变量，通过$(NF-1) 可以实现末尾字段进行取值 ||NR | 每行的记录号，多文件记录递增 ||FNR | 与NR类似，不过多文件记录不递增，每个文件都从1开始 ||\t | 制表符 ||\n | 换行符 ||FS | BEGIN时定义分隔符 ||RS | 输入的记录分隔符， 默认为换行符(即文本是按一行一行输入) ||~ | 匹配，与==相比不是精确比较 ||!~ | 不匹配，不精确比较 ||== | 等于，必须全部相等，精确比较 ||!= | 不等于，精确比较 ||&amp;&amp; | 逻辑与|||| | 逻辑或||+ | 匹配时表示1个或1个以上 ||/[0-9][0-9]+/ | 两个或两个以上数字 ||/[0-9][0-9]*/ | 一个或一个以上数字 ||FILENAME| 文件名||OFS | 输出字段分隔符， 默认也是空格，可以改为制表符等||ORS | 输出的记录分隔符，默认为换行符,即处理结果也是一行一行输出到屏幕||-F’[:#/]’ | 定义三个分隔符| 1234567891011121314151617181920print &amp; $0print 是awk打印指定内容的主要命令awk &apos;&#123;print&#125;&apos; /etc/passwd == awk &apos;&#123;print $0&#125;&apos; /etc/passwd awk &apos;&#123;print &quot; &quot;&#125;&apos; /etc/passwd //不输出passwd的内容，而是输出相同个数的空行，进一步解释了awk是一行一行处理文本awk &apos;&#123;print &quot;a&quot;&#125;&apos; /etc/passwd //输出相同个数的a行，一行只有一个a字母awk -F&quot;:&quot; &apos;&#123;print $1&#125;&apos; /etc/passwd awk -F: &apos;&#123;print $1; print $2&#125;&apos; /etc/passwd //将每一行的前二个字段，分行输出，进一步理解一行一行处理文本awk -F: &apos;&#123;print $1,$3,$6&#125;&apos; OFS=&quot;\t&quot; /etc/passwd //输出字段1,3,6，以制表符作为分隔符-f指定脚本文件awk -f script.awk fileBEGIN&#123;FS=&quot;:&quot;&#125;&#123;print $1&#125; //效果与awk -F&quot;:&quot; &apos;&#123;print $1&#125;&apos;相同,只是分隔符使用FS在代码自身中指定 awk &apos;BEGIN&#123;X=0&#125; /^$/&#123; X+=1 &#125; END&#123;print &quot;I find&quot;,X,&quot;blank lines.&quot;&#125;&apos; test I find 4 blank lines. ls -l|awk &apos;BEGIN&#123;sum=0&#125; !/^d/&#123;sum+=$5&#125; END&#123;print &quot;total size is&quot;,sum&#125;&apos; //计算文件大小total size is 17487 -F指定分隔符$1 指指定分隔符后，第一个字段，$3第三个字段， \t是制表符一个或多个连续的空格或制表符看做一个定界符，即多个空格看做一个空格1234567891011121314awk -F&quot;:&quot; &apos;&#123;print $1&#125;&apos; /etc/passwdawk -F&quot;:&quot; &apos;&#123;print $1 $3&#125;&apos; /etc/passwd //$1与$3相连输出，不分隔awk -F&quot;:&quot; &apos;&#123;print $1,$3&#125;&apos; /etc/passwd //多了一个逗号，$1与$3使用空格分隔awk -F&quot;:&quot; &apos;&#123;print $1 &quot; &quot; $3&#125;&apos; /etc/passwd //$1与$3之间手动添加空格分隔awk -F&quot;:&quot; &apos;&#123;print &quot;Username:&quot; $1 &quot;\t\t Uid:&quot; $3 &#125;&apos; /etc/passwd //自定义输出 awk -F: &apos;&#123;print NF&#125;&apos; /etc/passwd //显示每行有多少字段awk -F: &apos;&#123;print $NF&#125;&apos; /etc/passwd //将每行第NF个字段的值打印出来 awk -F: &apos;NF==4 &#123;print &#125;&apos; /etc/passwd //显示只有4个字段的行awk -F: &apos;NF&gt;2&#123;print $0&#125;&apos; /etc/passwd //显示每行字段数量大于2的行awk &apos;&#123;print NR,$0&#125;&apos; /etc/passwd //输出每行的行号awk -F: &apos;&#123;print NR,NF,$NF,&quot;\t&quot;,$0&#125;&apos; /etc/passwd //依次打印行号，字段数，最后字段值，制表符，每行内容awk -F: &apos;NR==5&#123;print&#125;&apos; /etc/passwd //显示第5行awk -F: &apos;NR==5 || NR==6&#123;print&#125;&apos; /etc/passwd //显示第5行和第6行route -n|awk &apos;NR!=1&#123;print&#125;&apos; //不显示第一行 //匹配代码块//纯字符匹配 !//纯字符不匹配 ~//字段值匹配 !~//字段值不匹配 ~/a1|a2/字段值匹配a1或a2123456789101112awk &apos;/mysql/&apos; /etc/passwdawk &apos;/mysql/&#123;print &#125;&apos; /etc/passwdawk &apos;/mysql/&#123;print $0&#125;&apos; /etc/passwd //三条指令结果一样awk &apos;!/mysql/&#123;print $0&#125;&apos; /etc/passwd //输出不匹配mysql的行awk &apos;/mysql|mail/&#123;print&#125;&apos; /etc/passwdawk &apos;!/mysql|mail/&#123;print&#125;&apos; /etc/passwdawk -F: &apos;/mail/,/mysql/&#123;print&#125;&apos; /etc/passwd //区间匹配awk &apos;/[2][7][7]*/&#123;print $0&#125;&apos; /etc/passwd //匹配包含27为数字开头的行，如27，277，2777...awk -F: &apos;$1~/mail/&#123;print $1&#125;&apos; /etc/passwd //$1匹配指定内容才显示awk -F: &apos;&#123;if($1~/mail/) print $1&#125;&apos; /etc/passwd //与上面相同awk -F: &apos;$1!~/mail/&#123;print $1&#125;&apos; /etc/passwd //不匹配awk -F: &apos;$1!~/mail|mysql/&#123;print $1&#125;&apos; /etc/passwd IF语句必须用在{}中，且比较内容用()扩起来123awk -F: &apos;&#123;if($1~/mail/) print $1&#125;&apos; /etc/passwd //简写awk -F: &apos;&#123;if($1~/mail/) &#123;print $1&#125;&#125;&apos; /etc/passwd //全写awk -F: &apos;&#123;if($1~/mail/) &#123;print $1&#125; else &#123;print $2&#125;&#125;&apos; /etc/passwd //if...else... 条件表达式== != &gt; &gt;=1234567awk -F&quot;:&quot; &apos;$1==&quot;mysql&quot;&#123;print $3&#125;&apos; /etc/passwd awk -F&quot;:&quot; &apos;&#123;if($1==&quot;mysql&quot;) print $3&#125;&apos; /etc/passwd //与上面相同 awk -F&quot;:&quot; &apos;$1!=&quot;mysql&quot;&#123;print $3&#125;&apos; /etc/passwd //不等于awk -F&quot;:&quot; &apos;$3&gt;1000&#123;print $3&#125;&apos; /etc/passwd //大于awk -F&quot;:&quot; &apos;$3&gt;=100&#123;print $3&#125;&apos; /etc/passwd //大于等于awk -F&quot;:&quot; &apos;$3&lt;1&#123;print $3&#125;&apos; /etc/passwd //小于awk -F&quot;:&quot; &apos;$3&lt;=1&#123;print $3&#125;&apos; /etc/passwd //小于等于 逻辑运算符&amp;&amp; ||1234awk -F: &apos;$1~/mail/ &amp;&amp; $3&gt;8 &#123;print &#125;&apos; /etc/passwd //逻辑与，$1匹配mail，并且$3&gt;8awk -F: &apos;&#123;if($1~/mail/ &amp;&amp; $3&gt;8) print &#125;&apos; /etc/passwdawk -F: &apos;$1~/mail/ || $3&gt;1000 &#123;print &#125;&apos; /etc/passwd //逻辑或awk -F: &apos;&#123;if($1~/mail/ || $3&gt;1000) print &#125;&apos; /etc/passwd 数值运算12345678awk -F: &apos;$3 &gt; 100&apos; /etc/passwd awk -F: &apos;$3 &gt; 100 || $3 &lt; 5&apos; /etc/passwd awk -F: &apos;$3+$4 &gt; 200&apos; /etc/passwdawk -F: &apos;/mysql|mail/&#123;print $3+10&#125;&apos; /etc/passwd //第三个字段加10打印 awk -F: &apos;/mysql/&#123;print $3-$4&#125;&apos; /etc/passwd //减法awk -F: &apos;/mysql/&#123;print $3*$4&#125;&apos; /etc/passwd //求乘积awk &apos;/MemFree/&#123;print $2/1024&#125;&apos; /proc/meminfo //除法awk &apos;/MemFree/&#123;print int($2/1024)&#125;&apos; /proc/meminfo //取整 输出分隔符OFS123awk &apos;$6 ~ /FIN/ || NR==1 &#123;print NR,$4,$5,$6&#125;&apos; OFS=&quot;\t&quot; netstat.txtawk &apos;$6 ~ /WAIT/ || NR==1 &#123;print NR,$4,$5,$6&#125;&apos; OFS=&quot;\t&quot; netstat.txt //输出字段6匹配WAIT的行，其中输出每行行号，字段4，5,6，并使用制表符分割字段 输出处理结果到文件①在命令代码块中直接输出 route -n|awk ‘NR!=1{print &gt; “./fs”}’②使用重定向进行输出 route -n|awk ‘NR!=1{print}’ &gt; ./fs 格式化输出1netstat -anp|awk &apos;&#123;printf &quot;%-8s %-8s %-10s\n&quot;,$1,$2,$3&#125;&apos; printf表示格式输出%格式化输出分隔符-8长度为8个字符s表示字符串类型打印每行前三个字段，指定第一个字段输出字符串类型(长度为8)，第二个字段输出字符串类型(长度为8),第三个字段输出字符串类型(长度为10) 12netstat -anp|awk &apos;$6==&quot;LISTEN&quot; || NR==1 &#123;printf &quot;%-10s %-10s %-10s \n&quot;,$1,$2,$3&#125;&apos;netstat -anp|awk &apos;$6==&quot;LISTEN&quot; || NR==1 &#123;printf &quot;%-3s %-10s %-10s %-10s \n&quot;,NR,$1,$2,$3&#125;&apos; IF语句1234567891011121314awk -F: &apos;&#123;if($3&gt;100) print &quot;large&quot;; else print &quot;small&quot;&#125;&apos; /etc/passwdsmallsmallsmalllargesmallsmallawk -F: &apos;BEGIN&#123;A=0;B=0&#125; &#123;if($3&gt;100) &#123;A++; print &quot;large&quot;&#125; else &#123;B++; print &quot;small&quot;&#125;&#125; END&#123;print A,&quot;\t&quot;,B&#125;&apos; /etc/passwd //ID大于100,A加1，否则B加1awk -F: &apos;&#123;if($3&lt;100) next; else print&#125;&apos; /etc/passwd //小于100跳过，否则显示awk -F: &apos;BEGIN&#123;i=1&#125; &#123;if(i&lt;NF) print NR,NF,i++ &#125;&apos; /etc/passwd awk -F: &apos;BEGIN&#123;i=1&#125; &#123;if(i&lt;NF) &#123;print NR,NF&#125; i++ &#125;&apos; /etc/passwd另一种形式awk -F: &apos;&#123;print ($3&gt;100 ? &quot;yes&quot;:&quot;no&quot;)&#125;&apos; /etc/passwd awk -F: &apos;&#123;print ($3&gt;100 ? $3&quot;:\tyes&quot;:$3&quot;:\tno&quot;)&#125;&apos; /etc/passwd while语句1234567awk -F: &apos;BEGIN&#123;i=1&#125; &#123;while(i&lt;NF) print NF,$i,i++&#125;&apos; /etc/passwd 7 root 17 x 27 0 37 0 47 root 57 /root 6 数组12345678910netstat -anp|awk &apos;NR!=1&#123;a[$6]++&#125; END&#123;for (i in a) print i,&quot;\t&quot;,a[i]&#125;&apos;netstat -anp|awk &apos;NR!=1&#123;a[$6]++&#125; END&#123;for (i in a) printf &quot;%-20s %-10s %-5s \n&quot;, i,&quot;\t&quot;,a[i]&#125;&apos;9523 1 9929 1 LISTEN 6 7903 1 3038/cupsd 1 7913 1 10837 1 9833 1 应用11234awk -F: &apos;&#123;print NF&#125;&apos; helloworld.sh #输出文件每行有多少字段awk -F: &apos;&#123;print $1,$2,$3,$4,$5&#125;&apos; helloworld.sh #输出前5个字段awk -F: &apos;&#123;print $1,$2,$3,$4,$5&#125;&apos; OFS=&apos;\t&apos; helloworld.sh #输出前5个字段并使用制表符分隔输出awk -F: &apos;&#123;print NR,$1,$2,$3,$4,$5&#125;&apos; OFS=&apos;\t&apos; helloworld.sh #制表符分隔输出前5个字段，并打印行号 应用212awk -F&apos;[:#]&apos; &apos;&#123;print NF&#125;&apos; helloworld.sh //指定多个分隔符: #，输出每行多少字段awk -F&apos;[:#]&apos; &apos;&#123;print $1,$2,$3,$4,$5,$6,$7&#125;&apos; OFS=&apos;\t&apos; helloworld.sh //制表符分隔输出多字段 应用312awk -F&apos;[:#/]&apos; &apos;&#123;print NF&#125;&apos; helloworld.sh //指定三个分隔符，并输出每行字段数awk -F&apos;[:#/]&apos; &apos;&#123;print $1,$2,$3,$4,$5,$6,$7,$8,$9,$10,$11,$12&#125;&apos; helloworld.sh //制表符分隔输出多字段 应用4计算/home目录下，普通文件的大小，使用KB作为单位123456ls -l|awk &apos;BEGIN&#123;sum=0&#125; !/^d/&#123;sum+=$5&#125; END&#123;print &quot;total size is:&quot;,sum/1024,&quot;KB&quot;&#125;&apos;ls -l|awk &apos;BEGIN&#123;sum=0&#125; !/^d/&#123;sum+=$5&#125; END&#123;print &quot;total size is:&quot;,int(sum/1024),&quot;KB&quot;&#125;&apos; //int是取整的意思``` 应用5统计netstat -anp 状态为LISTEN和CONNECT的连接数量分别是多少 netstat -anp|awk ‘$6~/LISTEN|CONNECTED/{sum[$6]++} END{for (i in sum) printf “%-10s %-6s %-3s \n”, i,” “,sum[i]}’123应用6统计/home目录下不同用户的普通文件的总数是多少？ ls -l|awk ‘NR!=1 &amp;&amp; !/^d/{sum[$3]++} END{for (i in sum) printf “%-6s %-5s %-3s \n”,i,” “,sum[i]}’mysql 199root 374统计/home目录下不同用户的普通文件的大小总size是多少？ls -l|awk ‘NR!=1 &amp;&amp; !/^d/{sum[$3]+=$5} END{for (i in sum) printf “%-6s %-5s %-3s %-2s \n”,i,” “,sum[i]/1024/1024,”MB”}’123应用7输出成绩表 awk ‘BEGIN{math=0;eng=0;com=0;printf “Lineno. Name No. Math English Computer Total\n”;printf “————————————————————\n”}{math+=$3; eng+=$4; com+=$5;printf “%-8s %-7s %-7s %-7s %-9s %-10s %-7s \n”,NR,$1,$2,$3,$4,$5,$3+$4+$5} END{printf “————————————————————\n”;printf “%-24s %-7s %-9s %-20s \n”,”Total:”,math,eng,com;printf “%-24s %-7s %-9s %-20s \n”,”Avg:”,math/NR,eng/NR,com/NR}’ test0 [root@localhost home]# cat test0Marry 2143 78 84 77Jack 2321 66 78 45Tom 2122 48 77 71Mike 2537 87 97 95Bob 2415 40 57 62` awk手册http://www.chinaunix.net/old_jh/7/16985.html]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Shell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[常用shell命令 - cp]]></title>
    <url>%2F03.%E5%BA%94%E7%94%A8-02.Linux%E5%92%8Cshell%2FLinux-%E5%B8%B8%E7%94%A8shell%E5%91%BD%E4%BB%A4-cp%2F</url>
    <content type="text"><![CDATA[Linux cp（英文全拼：copy file）命令主要用于复制文件或目录。 语法1cp [options] source dest 或1cp [options] source... directory 参数说明：-a 或 --archive 此选项通常在复制目录时使用，它保留链接、文件属性，并复制目录下的所有内容。其作用等于dpR参数组合 -b 或--backup 删除，覆盖目标文件之前的备份，备份文件会在字尾加上一个备份字符串。 -d 或--no-dereference 当复制符号连接时，把目标文件或目录也建立为符号连接，并指向与源文件或目录连接的原始文件或目录。 相当于 Windows 系统中的快捷方式。 -f 或 --force 强行复制文件或目录， 不论目的文件或目录是否已经存在，不给出提示。 -i 或 --interactive 与 -f 选项相反，覆盖文件之前先询问用户，回答 y 时目标文件将被覆盖。 -l 或 --link 对源文件建立硬链接，而非复制文件 -p 或 --preserve 保留源文件或目录的属性，包括所有者、所属组、权限与时间 -P 或 --parents 保留源文件或目录的路径，此路径可以是绝对路径或相对路径，且目的目录必须已经存在 -r 递归处理，将指定目录下的文件与子目录一并处理。若源文件或目录的形态，不属于目录或符号链接，则一律视为普通文件处理 -R 或 --recursive 递归处理，将指定目录下的文件及子目录一并处理 -s 或 --symbolic-link 对源文件建立符号链接，而非复制文件 -S &lt;备份字尾字符串&gt; 或 --suffix=&lt;备份字尾字符串&gt; 用&quot;-b&quot;参数备份目的文件后，备份文件的字尾会被加上一个备份字符串。默认的备份字尾符串是符号&quot;~&quot; -u 或 --update 使用这项参数之后，只会在源文件的修改时间(Modification Time)较目的文件更新时，或是名称相互对应的目的文件并不存在，才复制文件 -v 或 --verbose 显示执行过程 -V &lt;备份方式&gt; 或 --version-control=&lt;备份方式&gt; 指定当备份文件时，备份文件名的命名方式，有以下3种: 1.numbered或t, 将使用备份编号，会在字尾加上~1~字符串，其数字编号依次递增 2.simple或never 将使用简单备份，默认的备份字尾字符串是~, 也可通过-S来指定 3.existing或nil将使用当前方式，程序会先检查是否存在着备份编号，若有则采用备份编号，若无则采用简单备份 -x 或 --one-file-system 复制的文件或目录存放的文件系统，必须与cp指令执行时所处的文件系统相同，否则不复制，亦不处理位于其他分区的文件 --help 显示在线帮助 --sparse=&lt;使用时机&gt; 设置保存希疏文件的时机 --version 显示版本 示例 将文件file1复制成文件file2cp file1 file2 复制文件，只有源文件比目标文件的修改时间新时，才复制文件cp -u -v file1 file2 将文件file1复制成file2，因为目的文件已经存在，所以指定使用强制复制的模式cp -f file1 file2 同时将文件file1、file2、file3与目录dir1复制到dir2cp -R file1 file2 file3 dir1 dir2 复制时保留文件属性（权限、时间戳）cp -p a.txt tmp/ 复制时保留文件的目录结构cp -P /var/tmp/a.txt ./temp/ 复制时产生备份文件cp -b a.txt tmp/ 复制时产生备份文件，尾标 ~1~格式cp -b -V t a.txt /tmp 指定备份文件尾标cp -b -S _bak a.txt /tmp]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Shell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[常用shell命令 - crontabs]]></title>
    <url>%2F03.%E5%BA%94%E7%94%A8-02.Linux%E5%92%8Cshell%2FLinux-%E5%B8%B8%E7%94%A8shell%E5%91%BD%E4%BB%A4-crontabs%2F</url>
    <content type="text"><![CDATA[crontabscrontab 是用来让使用者在固定时间或固定间隔执行程序之用，换句话说，也就是类似使用者的时程表。-u user 是指设定指定 user 的时程表，这个前提是你必须要有其权限(比如说是 root)才能够指定他人的时程表。如果不使用 -u user 的话，就是表示设定自己的时程表。常用参数:12345678910crontab -l //查看当前用户下的cron任务crontab -e //编辑当前用户的定时任务crontab -u linuxso -e //编辑用户linuxso的定时任务crontab file [-u user]-用指定的文件替代目前的crontab。crontab-[-u user]-用标准输入替代目前的crontab.crontab-1[user]-列出用户目前的crontab.crontab-e[user]-编辑用户目前的crontab.crontab-d[user]-删除用户目前的crontab.crontab-c dir- 指定crontab的目录。crontab文件的格式：M H D m d cmd. 基本格式 :1* * * * * command 分 时 日 月 周 命令 第1列表示分钟1～59 每分钟用或者 /1表示 第2列表示小时1～23（0表示0点） 第3列表示日期1～31 第4列表示月份1～12 第5列标识号星期0～6（0表示星期天） 第6列要运行的命令 f1 f2 f3 f4 f5 program其中 f1 是表示分钟，f2 表示小时，f3 表示一个月份中的第几日，f4 表示月份，f5 表示一个星期中的第几天。program 表示要执行的程序。 当 f1 为 时表示每分钟都要执行 program，f2 为 时表示每小时都要执行程序，其馀类推 当 f1 为 a-b 时表示从第 a 分钟到第 b 分钟这段时间内要执行，f2 为 a-b 时表示从第 a 到第 b 小时都要执行，其馀类推 当 f1 为 /n 时表示每 n 分钟个时间间隔执行一次，f2 为 /n 表示每 n 小时个时间间隔执行一次，其馀类推 当 f1 为 a, b, c,… 时表示第 a, b, c,… 分钟要执行，f2 为 a, b, c,… 时表示第 a, b, c…个小时要执行，其馀类推使用者也可以将所有的设定先存放在档案 file 中，用 crontab file 的方式来设定时程表。 环境变量的重新引入 cmd要运行的程序，程序被送入sh执行，这个shell只有USER,HOME,SHELL这三个环境变量；如果执行的shell、Perl脚本中引用其他的变量或者是相应的模块包等，则需要重新导入相应的环境变量；如下： 12345#/bin/shexport PERL5LIB=&quot;/share/nas2/genome/biosoft/perl/current/lib/::/share/nas2/genome/biosoft/perl/current/lib/:/share/nas2/genome/biosoft/perl/current/lib/5.20.0/x86_64-linux-thread-multi:/share/nas2/genome/biosoft/perl/current/lib/:/share/nas2/genome/biosoft/perl/5.20.0/lib/site_perl/5.20.0/x86_64-linux-thread-multi:/share/nas2/genome/biosoft/perl/5.20.0/lib/site_perl/5.20.0:/share/nas2/genome/biosoft/perl/5.20.0/lib/5.20.0/x86_64-linux-thread-multi&quot; ;source /root/.bashrc;source /home/liubo/.bshrc ;/share/nas2/genome/bin/perl /share/nas2/database/genome/test.pl &gt;/home/liubo/err ;]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Shell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[常用shell命令 - 常用的组合命令]]></title>
    <url>%2F03.%E5%BA%94%E7%94%A8-02.Linux%E5%92%8Cshell%2FLinux-%E5%B8%B8%E7%94%A8shell%E5%91%BD%E4%BB%A4-%E5%B8%B8%E7%94%A8%E7%BB%84%E5%90%88%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[Linux 打印时间1234567891011# 输出当前年月日 echo $(date +%F)# 输出当前时间（时分秒） echo $(date +%T)# 输出星期 echo $(date +%A)# 输出年月日时分秒 echo $(date +%F%n%T) 符号 含义 %n 空格 %F 年月日 %T 时分秒 %A 星期]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Shell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[常用shell命令 - sz/rz]]></title>
    <url>%2F03.%E5%BA%94%E7%94%A8-02.Linux%E5%92%8Cshell%2FLinux-%E5%B8%B8%E7%94%A8shell%E5%91%BD%E4%BB%A4-sz%E5%92%8Crz%2F</url>
    <content type="text"><![CDATA[简介rz、sz是用来在windows和Linux上互转文件的一个命令。lrzsz 官网入口lrzsz是一个unix通信套件提供的X，Y，和ZModem文件传输协议. 安装 1 yum安装 1yum -y install lrzsz 2 源码安装1234567wget http://www.ohse.de/uwe/releases/lrzsz-0.12.20.tar.gztar zxvf lrzsz-0.12.20.tar.gz &amp;&amp; cd lrzsz-0.12.20./configure &amp;&amp; make &amp;&amp; make install#上面安装过程默认把lsz和lrz安装到了/usr/local/bin/目录下，现在我们并不能直接使用，下面创建软链接，并命名为rz/sz：cd /usr/binln -s /usr/local/bin/lrz rzln -s /usr/local/bin/lsz sz 使用12345# 使用上传文件，执行命令rz，会跳出文件选择窗口，选择好文件，点击确认即可。rz# 下载文件，执行命令szsz 运行命令后，会弹出文件窗口，可以选择本地（Windows）目录保存下载的文件/选择要上传的文件。]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Shell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[常用shell命令 - mail]]></title>
    <url>%2F03.%E5%BA%94%E7%94%A8-02.Linux%E5%92%8Cshell%2FLinux-%E5%B8%B8%E7%94%A8shell%E5%91%BD%E4%BB%A4-mail%2F</url>
    <content type="text"><![CDATA[12[root@localhost ~]# mail -s &quot;test mail&quot; root &lt;/root/ anaconda-ks.cfg#把/root/anaconda-ks.cfg文件的内容发送给root用户 1234567891011121314151617181920212223-f 表示发送者的邮箱-t 表示接收者的邮箱-cc 表示抄送发给谁-bcc 表示暗抄送给谁-o message-content-type=html 邮件内容的格式,html表示它是html格式-o message-charset=utf8 邮件内容编码-s 表示SMTP服务器的域名或者ip-u 表示邮件的主题-xu 表示SMTP验证的用户名-xp 表示SMTP验证的密码(注意,这个密码貌似有限制,例如我用d!5neyland就不能被正确识别)-m 邮件的内容-a 要发送的附件]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Shell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[常用数据库目录]]></title>
    <url>%2F01.%E7%9F%A5%E8%AF%86-04.Database%2FDatabase-Cosmic%2F</url>
    <content type="text"><![CDATA[1. COSMIC癌症相关的体细胞位点，是整个网站的核心，收录了来自不同研究机构和数据库的体细胞突变数据，并提供了方便的浏览，检索，下载功能。 2. Cell Lines Project对癌症研究中常用的细胞系样本进行深入研究，分析其突变信息。相比COSMIC, 整个项目中涵盖的变异数据会少一点。该项目网址如下： `https://cancer.sanger.ac.uk/cell_lines` 3. COSMIC-3D通过交互式的网页，展现了基因突变导致的蛋白结构域的变化。该项目网址如下 `https://cancer.sanger.ac.uk/cosmic3d/` 在搜索框中输入一个具体的基因名称或者蛋白名称，可以查看具体的记录。 4. Cancer Gene Census在癌症研究中，找到相关的突变基因是最核心的目的之一。通过对各种癌症进行调研，整理了一份癌症相关的突变基因列表，这份列表就是Cancer Gene Census,简称CGC。该项目网址如下 `https://cancer.sanger.ac.uk/census` 在CGC种，将所有的癌症相关基因分成两类 - Tier1 : 对于这部分基因，有充分的证据表明，正是由于这些基因的突变，导致癌症的进一步发生。 - Tier2 : 对于这部分基因，只能说在癌症中检测到了大量该基因的突变，但是并没有充分证据表明该基因突变对癌症发生的影响。]]></content>
      <categories>
        <category>知识沉淀</category>
        <category>数据库</category>
        <category>肿瘤</category>
      </categories>
      <tags>
        <tag>数据库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[常用数据库 - Cancer Hotspots]]></title>
    <url>%2F01.%E7%9F%A5%E8%AF%86-04.Database%2FDatabase-cancerhotspots%2F</url>
    <content type="text"><![CDATA[官网Cancer Hotspots]]></content>
      <categories>
        <category>知识沉淀</category>
        <category>数据库</category>
        <category>肿瘤</category>
      </categories>
      <tags>
        <tag>数据库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C语言学习-基础知识学习]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-07.c%2F2018-06-16.C%E8%AF%AD%E8%A8%80%E5%AD%A6%E4%B9%A0-%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[工作需要，前同事很多程序是用C编写的，而自己之前只是会有Perl、Python、R，没有接触过C语言的编程，都说C是底层编程语言，虽然在编写上会麻烦，但是执行效率会甩高级语言好几条街，所以想借着这个机会，学习一下C语言的编程，虽然没时间学精学熟，但是至少以后如果有些对效率需求比较的数据处理，可以通过C语言进行简单的实现。 编译过程简介和之前学习的Perl、Python之间最大的不同主要就是C语言在编写后执行前需要对代码编译，这也是C语言的一个被诟病的地方，尤其是在进行小规模的数据处理时，代码编写调试过程中，会在编译过程中浪费大量的时间，所以一般简单的小程序，可能用Perl、Python在实现上更简单一些。 看介绍，c语言在执行前，需要先编译然后进行连接，但是我在学习测试的时候，发现gcc会默认同步完成的，当然也可以通过-c参数分开进行执行。对于我们一些小程序来讲可能没有多少影响，但是在进行大型项目的时候，可能可以有效的帮助进行错误的判断。因为我是一步编译的，所以目前还不是很确定差异。编辑的C语言程序必须是 .c 为后缀， 在测试中，发现如果缺少后缀的话，是不能直接通过gcc进行编译的。这个很奇怪，因为一直感觉在Linux系统中，文件后缀其实是个摆设，这个还需要后期又时间详细了解一下原因。 1234gcc -o helloWorld helloWrold.c #多数情况下我们可以一步到位的编译+链接程序；gcc -c helloWrold.c # 进行编译会生成helloWorld.h 文件gcc -o helloWorld helloWorld.h #链接器将源代码文件中由编译器产生的各种对象模块组合起来，再从 C语言提供的程序库中添加必要的代码模块，将它们组合成一个可执行文件。 链接成功后就会生成一个可执行程序 C语言的结构简介第一个c语言程序代码如下： 123456#include&lt;stdio.h&gt;int main() # 函数首&#123; # &#123;&#125;括号里面的内容是对应的函数体，即函数功能。 printf(&quot;Hello World&quot;); # 定义函数的内容，以分号“；”结尾 。 return 0; # 函数结束的返回值。&#125; # 其中第一行 #include&lt;stdio.h&gt; #号不代表注释，而是一个预处理的标志，include相当于Python的import，perl的use，相当于通过include会导入一个头文件，头文件中定义了一些基本函数的说明。 后面main（）中的内容则是每个程序的一个主程序，每个c程序，都有且只有一个main（）函数， C语言常用函数1. 格式输出函数 printf一般形式：printf(格式控制，输出表列)。例如：printf(&quot;%d,%d&quot;,a,b); 括号内包含两个部分： “格式控制”是用双撇号括起来的一个字符串，称“转换控制字符串”，简称“格式字符串”，它包括两个信息： 格式声明：格式声明由 % 和格式字符组成，如 %d （%d 代表输出整数，%f 代表输出实数），它的作用是将输出的数据转换为指定的格式然后输出。格式声明总是由 % 字符开始。 普通字符：普通字符即在需要输出时原样输出的字符。例如上例中的 printf(“Please enter a value：”);中的 Please enter a value: 即为原样输出。 （2）“输出表列”是程序需要输出的数据。看下面例子： 1printf(&quot;I love %d and %d&quot;,x,s); 第一个 %d 对应的是x 的值，第二个 %d 对应的是 s 的值。 I love 和 and （注意这里包括空格）都是 普通字符会原样输出。 假如 x 的值是 3，s 的值是 4，这条语句将会输出“ I love 3 and 4 ”。常用格式字符 12345678910111213％d整型输出，%md：以m指定的字段宽度输出，右对齐，％ld长整型输出，%mld：输出指定宽度的长整型数据，％o以八进制数形式输出整数，％x以十六进制数形式输出整数，或输出字符串的地址。％u以十进制数输出unsigned型数据(无符号数)。注意：%d与%u有无符号的数值范围，也就是极限的值，不然数值打印出来会有误。％c用来输出一个字符，％s用来输出一个字符串，％f用来输出实数，以小数形式输出，默认情况下保留小数点6位。%.100f用来输出实数，保留小数点100位。％e以指数形式输出实数，％g根据大小自动选f格式或e格式，且不输出无意义的零。 2. 格式输入函数 scanf()一般形式：scanf(格式控制，地址表列)。“格式控制”的含义同 printf 函数。“地址表列”是由若干地址组成的表列，可以是变量的地址。 看下面的例子： 1scanf(&quot;a=%d,b=%d&quot;,&amp;a,&amp;b); 在格式字符串中除了有格式声明的 %d 以外，其它普通字符原样输出（如“ a= ”，“ b= ”和“，”），假如给 a 和 b 分别赋值 5 和 6 ，将显示“ a=5，b=6 ”。 注意：scanf()函数中的表列是地址表列。 scanf(“a=%d,b=%d”,&amp;a,&amp;b); 中 a 和 b 前面的 &amp; 不能省掉，这一点要和 printf 作区分。** printf() 函数和 scanf() 函数我们会在以后的“数据的输入与输出”版块继续讲述。 3. 输出函数putchar（）,输出一个字符 一般形式：putchar(c); 功能：输出变量 c 所代表的一个字符； 说明：c 为字符型变量或整型变量。 4.输入一个字符 一般形式：getchar(); 功能：要求用户从终端（键盘）输入单个ss注意：运行程序时，系统等待用户输入，注意回车也是一个合法字符。字符； 说明：返回值为从输入设备上得到的字符。 5. 注释位于“ /…….. / ”中的和“ // ”后面的内容为注释，用来对程序进行说明；注释在编译时会被自动忽略。 这是一个简单的计算程序，通过定义变量让用户可以自由设定 a 和 b 的值，之后通过 c=a+b; 这条语句实现把 a 和 b 的和计算出来并赋值给 c 。究竟什么是变量，什么是常量呢？接下来我们来一一讲述。 常量1.整型常量，如：整数2.实型常量，如：实数。小数还可以用指数形式表现，如32.23e3（表示 32.2310^3）, -323.34e-6（表示 323.3410^-6）, 由于计算机无法表示上角和下角，所以规定以字母 e 或者 E 代表以 10 为底的指数。 注意：e 或者 E 之前必须有数字，且 e 或者 E 后面必须为整数，不能是 12e4.1 或者 e3 这种形式。 字符常量 普通字符 用单撇号括起来的一个字符，如 ‘a’ 、’E’ 、’%’ 、’3’ 。不能写成 ‘ab’ 、’12’ 。字符常量只能是一个字符，不包括单撇号。 转义字符 除了以上形式的字符常量外，C 语言还允许用一种特殊形式的字符常量，就是以字符 \ 开头的字符序列，比如我们本节课的 3-1.c 中，\n 代表的就是换行符，显示跳转到下一行。这是一种在屏幕上无法显示的“控制字符”。常用转义字符 字符 含义 \n 换行 \r 回车（不换行） \t 制表符 \f 换页 \b 退格 变量变量代表一个有名字的、具有特殊属性的存储单元。它可以用来保存数据。变量的值是可以改变的。变量在程序中定义的一般形式就是： &lt;类型名称&gt; &lt;变量名称&gt;。例如：int a； int b; int a,b; int price;等。C 语言规定变量名只能由字母、数字和下划线构成，且第一个字符必须为字母或下划线。 数据类型C语言包含的数据类型 数组在C语言中不存在字符串类型的概念，字符串都是存储在一个字符数组中，同时数组中的每个字符以ASCII的形式存储在存储单元中。 int student[10]; # 定义一个包含10个元素的数组，从student[0]开始到student[9] ; 在数组定义时要指定元素个数。元素个数的定义不能包含变量。char string[10] ； #定义一个字符串，可以省略数组维数（10）； 数组常用函数 函数 说明 puts（字符数组） 其作用是讲一个字符串输出到终端，因此该函数用的不是很多，我们可以编写小程序来体验。 gets（字符数组） 其作用是从终端输入一个字符串到字符数组，并且得到一个函数值。 strcat（字符数组1，字符数组2） 把两个字符数组中的字符串连接起来，把字符串2接到1后面，结果放到字符串1中。注意字符串1必须足够长，不然长度会溢出。 strlen(字符数组) 测量字符串长度的函数。函数的值为字符串中的实际长度，比占用长度小（没有计算字符串结尾的 “\0”） strcpy（字符串 1，字符串 2） 作用是将字符串 2 复制到字符串 1 中。 strcmp（字符串1，字符串2） 比较字符串1和字符串2 (从左向右逐个比较每个字符的ASCII码) strlwr 函数 转换为小写的函数 strupr 函数 转换为大写的函数 整数类型 基本类型（int）：编译系统分配给 int 类型数据 2 个字节或者 4 个字节（由具体的编译系统自行决定）。我们使用的 gcc 编译器为每个整数类型分配四个字节（32 个二进位）。在存储单元中的存储方式是：用整数的补码形式存放。所以当 4 个字节的整数类型取值范围是 -2^31~（2^31-1）。无符号的基本整型表示为 unsigned int，和 int 类型占有的字节数相同，取值范围是 0~（2^32-1）。 短类型（short 类型）：短整型的类型名为 short，gcc 编译系统分配给 short 类型 2 个字节，存储方式和 int 类型一样，也是补码的形式存储，取值范围是 -2^15~（2^15-1），无符号短整型 unsigned short 的取值范围是 0~（2^16-1）。 长整型（long 类型）：gcc 编译系统分配给 long 类型 8 个字节，存储方式和 int 类型一样，也是补码的形式存储，取值范围是 -2^63~（2^63-1），无符号长整型 unsigned long 的取值范围是 0~（2^64-1）。 同类型占用的空间：在这里大家可以通过 sizeof() 运算符查看各类型的常量占据多少字节。 浮点型数据 单精度浮点数（float）gcc编译器为每个浮点数分配4个字节（32个位），每个浮点数的存储结构分为3个部分，+or- | 小数部分 | 指数部分 但是小数部分和指数部分所分配的位数根据编译器进行确定。 双精度浮点数（double）为了扩大数据的范围，用8个字节存储一个double类型的数据，可以编程查看 double 极限值，符号的下限为：DBL_MIN,上限为 DBL_MAX。 指针指针变量的类型说明1.对指针变量的类型说明包括三个内容： 指针类型说明，即定义变量为一个指针变量； 指针变量名； 变量值(指针)所指向的变量的数据类型。 1234567int *p1;表示p1是一个指针变量，它的值是某个整型变量的地址。staic int *p2; /*p2是指向静态整型变量的指针变量*/float *p3; /*p3是指向浮点变量的指针变量*/char *p4; /*p4是指向字符变量的指针变量*/ 应该注意的是，一个指针变量只能指向同类型的变量，如P3 只能指向浮点变量，不能时而指向一个浮点变量， 时而又指向一个字符变量。 2.指针变量的赋值(1)指针变量初始化的方法 12int a;int *p=&amp;a; (2)赋值语句的方法 123int a;int *p;p=&amp;a; 不允许把一个数赋予指针变量，故下面的赋值是错误的： int p;p=1000; 被赋值的指针变量前不能再加“”说明符，如写为*p=&amp;a 也是错误的 3.指针变量的运算指针变量只能进行赋值运算和部分算术运算及关系运算。 取地址运算符&amp; 取内容运算符 用来表示指针变量所指的变量，在运算符之后跟的变量必须是指针变量。需要注意的是指针运算符和指针变量说明中的指针说明符 不是一回事。 指针变量的运算 赋值运算 指针变量的赋值运算有以下几种形式： 指针变量初始化赋值，前面已作介绍。 把一个变量的地址赋予指向相同数据类型的指针变量。 把一个指针变量的值赋予指向相同类型变量的另一个指针变量。 把数组的首地址赋予指向数组的指针变量。 把字符串的首地址赋予指向字符类型的指针变量。 把函数的入口地址赋予指向函数的指针变量。 加减算数运算 指针变量的加减运算只能对数组指针变量进行， 对指向其它类型变量的指针变量作加减运算是毫无意义的。指针变量的加减运算，相当于以指针为原点，通过相对位置寻找数组中的其他元素。 两个指针变量之间的运算只有指向同一数组的两个指针变量之间才能进行运算， 否则运算毫无意义。 数组指针变量的说明和使用 12int a[5],*pa;pa=a; pa,a,&amp;a[0]均指向同一单元，它们是数组a的首地址，也是0 号元素a[0]的首地址。pa+1,a+1,&amp;a[1]均指向1号元素a[1]。类推可知a+i,a+i,&amp;a[i]指向i号 元素a[i]。应该说明的是pa是变量，而a,&amp;a[i]都是常量。引入指针变量后，就可以用两种方法来访问数组元素了。 第一种方法为下标法，即用a[i]形式访问数组元素。 第二种方法为指针法，即采用*(pa+i)形式，用间接访问的方法来访问数组元素。 字符型数据 字符常量C中字符型的基本类型是char,这是一个字符型常量，字符型常量是指用单引号扩起来的一个字符，每个字符在所有编译系统中均占用1个字节（8个位），利用ASCII码的形式进行存储，因此每个字符型常量都可以进行数学运算，运算时就是调用的每个字符对应的ASCII码值。同样在进行输出时，系统也会根据输出格式，确定输出字符还是输出其对应的ASCII码值。 字符串常量字符串常量是用一对双引号括起来的零个或多个字符组成的序列，如 “hello”，”China”，”b” 都是字符串常量。 字符串常量的存储与字符常量的存储是不同的。字符串中的每个字符占用一个字节，在存储字符串常量时还要自动在其末尾加上 ‘\0’ 作为字符串结束的标志。 “How do you do.” 存储示意如下。因此字符常量和字符串常量’b’ 和 “b” 是完全不同的。前者是字符常量，在内存中占用的字节数为 1；而后者是字符串常量，在内存中占用的字节数为 2，包含字符 ‘b’ 和 ‘\0’。 注意：在 C 语言中没有专门的字符串变量，如果你想要将一个字符串存放在变量中，必须使用字符数组，数组中每一个元素存放一个字符，数组的内容我们会在以后的课程中和大家详细讲述。 基础运算符四则运算12345x + y：将x与y相加x - y：将x与y相减x * y：将x与y相乘x / y：x除以yx % y：求x除以y的余数 x/y 中，两个实数（亲！注意说的是实数）相除的结果是双精度实数，两个整数相除的结果为整数。如 5/3 的结果为 1，舍去小数部分。 % 运算符要求参加运算的对象为整数，结果也是整数。如 7%3，结果为 1，除了%以外的运算符的操作数都可以是任何算数类型。 自增自减作用是使变量的值加 1 或减 1 ，例如：++i ，–i（在使用 i 之前，先使 i 加（减）1 ）；i++，i–（在使用 i 之后，使 i 的值加（减）1 ）。 不同数据类型混合运算在程序中经常会遇到不同类型的数据进行运算，比如 7*3.5。如果一个运算符的两侧数据类型不同，则先进行类型的转换，使两者具有同一种类型，然后进行运算。如下表，先转换成优先级较高的数据类型然后进行运算。 类型 优先级 long double 高 double float long unsigned int int short char 低 如果 int 类型的数据和 float 或 double 型数据进行运算时，先把 int 型和 float 型数据转换为 double 型数据，然后进行运算，结果为 double 型。其他的大家可以按照上图来做。字符 (char) 型数据和整形数据进行运算，就是把字符的 ASCII 代码与整形运算。如 4+’B’，由于字符 ‘B’ 的 ASCII 代码是 66，相当于 66+4=70。字符型数据可以直接和整形数据进行运算。如果字符型数据和浮点型数据运算，则将字符的 ASCII 码先转化为 double 型，然后在进行运算。 强制转换类型在运算前，可以根据需要对数据格式进行强制转换，一般形式就是（数据类型名）（表达式）如下: 123(double)a // (将a转换成为double型)(int)(x+y) //（将x+y的值转换成为int类型)(int)x+y //（将x的值转换成为int类型，然后进行加法运算) 基础结构switchif是一个但分支的逻辑结构，当我们有多个筛选条件时，如果用if会不断使用elsif， switch（表达式） { case 常量1：语句1 case 常量2：语句2 . . . case 常量n ：语句n default ： 语句n+1 } while循环1234while(expression) statement1;statament2; do…while123do statement;while(expression); for 循环12345for（表达式 1；表达式 2；表达式 3） 语句- 表达式 1：设置初始条件，只执行一次。可以为零个、一个或者多个表达式赋初值。 表达式可以省略但分号不可省略；- 表达式 2：是循环条件表达式，用来判定是否继续循环。在每次执行循环体前限制性此表达式，决定是否继续执行循环。 可以省略，省略后默认为真。- 表达式 3：作为循环的调整，例如循环变量的增值，是在执行完循环体后才进行的。可以省略，放入执行语句中。 break退出循环，执行后续命令。 continue退出本轮循环进行下一轮循环，（跳过循环中的后续命令）。 函数函数声明指定函数的类型为 void，表示函数无类型，即无函数值，也就是说，执行这函数不会返回任何值如果函数的定义之前被引用了的话，则在引用前进行声明。函数示意 123456789101112131415161718192021#include&lt;stdio.h&gt;int main()&#123; int fun(int m ) ; #函数使用在函数定义之前，需要先引用在使用 int x=fun(100); printf(&quot;x:%d&quot;,x) ;&#125;int fun(int m)&#123; int a[100]; int n=0; for(int i=1;i&lt;m;i++)&#123; if(i%7==0)&#123; a[n]=i ; n++; &#125; &#125; for(int i=0;i&lt;n;i++)&#123; printf(&quot;num is %d\n&quot;,a[i]); &#125; return(n) ;&#125; 函数的分类]]></content>
      <categories>
        <category>编程拾慧</category>
        <category>C++</category>
      </categories>
      <tags>
        <tag>编程语言</tag>
        <tag>C</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Book:机器学习实战-2.KNN(K-nearest-neighbor)]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-20.MachineLearn%2F1201.%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%AE%97%E6%B3%95-KNN-K_nearest_neighbor%2F</url>
    <content type="text"><![CDATA[kNN 又称为 k 最近邻算法，是一种 Machine Learning 算法，会使用临近度来将一个数据点与训练时所使用并已记住的一个数据集进行对比，从而做出预测。这种基于实例的学习使 kNN 赢得了“惰性学习”的名称，并让人们能够使用此算法解决分类或回归问题。 算法概述1、指导思想kNN算法的指导思想是“近朱者赤，近墨者黑”，由你的邻居来推断出你的类别。计算步骤如下： 1）算距离：给定测试对象，计算它与训练集中的每个对象的距离 2）找邻居：圈定距离最近的k个训练对象，作为测试对象的近邻 3）做分类：根据这k个近邻归属的主要类别，来对测试对象分类(周围点的相对多数投票) 2、距离或相似度的衡量 1）在计算距离时，如果具有最大差值的属性值对距离的影响非常大时，需要对数据进行归一化（例如：newValue=oldValue/（MaxValue-MinValue））。 2）什么是合适的距离衡量？距离越近应该意味着这两个点属于一个分类的可能性越大。觉的距离衡量包括欧式距离、夹角余弦等。对于文本分类来说，使用余弦(cosine)来计算相似度就比欧式(Euclidean)距离更合适。 3、类别的判定 1）投票决定：少数服从多数，近邻中哪个类别的点最多就分为该类。 2）加权投票法：根据距离的远近，对近邻的投票进行加权，距离越近则权重越大（权重为距离平方的倒数） 优缺点1、优点简单，易于理解，易于实现，无需估计参数，无需训练适合对稀有事件进行分类（例如当流失率很低时，比如低于0.5%，构造流失预测模型）特别适合于多分类问题(multi-modal,对象具有多个类别标签)，例如根据基因特征来判断其功能分类，kNN比SVM的表现要好 2、缺点懒惰算法，对测试样本分类时的计算量大，内存开销大，评分慢可解释性较差，无法给出决策树那样的规则。 常见问题1、k值设定为多大？k太小，分类结果易受噪声点影响；k太大，近邻中又可能包含太多的其它类别的点。（对距离加权，可以降低k值设定的影响）k值通常是采用交叉检验来确定（以k=1为基准）经验规则：k一般低于训练样本数的平方根；k选择为奇数，防止出现平局情况。 2、类别如何判定最合适？投票法没有考虑近邻的距离的远近，距离更近的近邻也许更应该决定最终的分类，所以加权投票法更恰当一些。 3、如何选择合适的距离衡量？高维度对距离衡量的影响：众所周知当变量数越多，欧式距离的区分能力就越差。变量值域对距离的影响：值域越大的变量常常会在距离计算中占据主导作用，因此应先对变量进行标准化。 4、训练样本是否要一视同仁？在训练集中，有些样本可能是更值得依赖的。可以给不同的样本施加不同的权重，加强依赖样本的权重，降低不可信赖样本的影响。 5、性能问题？kNN是一种懒惰算法，平时不好好学习，考试（对测试样本分类）时才临阵磨枪（临时去找k个近邻）。懒惰的后果：构造模型很简单，但在对测试样本分类地的系统开销大，因为要扫描全部训练样本并计算距离。已经有一些方法提高计算的效率，例如压缩训练样本量等。 6、能否大幅减少训练样本量，同时又保持分类精度？浓缩技术(condensing)编辑技术(editing) 使用示例影片分类 电影分类简介： 电影有很多种，这里仅考虑两种，爱情片和动作片，爱情片里也会有打斗场景，动作片里也会有接吻镜头，因此不能单纯的依靠有无对影片进行分类，那这种时候，我们应该怎么去划分一个影片是爱情片还是动作片呢，这时候，我们可以利用该算法。方法： 首先一部影片中，接吻的镜头和打斗的镜头是可以进行量化的，比如我们可以数一下，有多少个打斗镜头，有多少个接吻镜头等等，因此，针对每个影片我们可以得到一个长度为2的数组c（接吻镜头数，打斗镜头数） ，同时我们需要寻找一些以知分类的影片同时我们知道这些电影的镜头信息。这些每个电影都可以在二维坐标系里面对应一个坐标点。 当我们需要对一部影片进行分类的时候，我们统计该影片的镜头信息，去计算这个影片和所有以知分类的影片间的距离，然后选取和这个影片距离最近的K个电影，然后看看这K个电影所属的分类，选取权重最高的1个电影分类（注意这里提到了是权重，而不是数目，因为有些情况下，可能需要考虑实际应用的关系，根据数据的可信程度，距离关系等对分类信息进行加权，从而提高分类的准确性），作为该电影的类别或者也可以得到该影片属于不同类别的概率。 手写识别 手写识别系统]]></content>
      <categories>
        <category>machine_learning</category>
        <category>算法</category>
      </categories>
      <tags>
        <tag>machine_learning</tag>
        <tag>距离计算</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习相关素材框架资料]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-20.MachineLearn%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E7%B4%A0%E6%9D%90%E6%A1%86%E6%9E%B6%E8%B5%84%E6%96%99%2F</url>
    <content type="text"><![CDATA[]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python包-Scipy_科学计算库]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-02.python%2FPython-%E5%8C%85-Scipy_%E7%A7%91%E5%AD%A6%E8%AE%A1%E7%AE%97%E5%BA%93%2F</url>
    <content type="text"><![CDATA[Scipy库的简介 Scipy高级科学计算库：和Numpy联系很密切，Scipy一般都是操控Numpy数组来进行科学计算、统计分析，所以可以说是基于Numpy之上了。Scipy有很多子模块可以应对不同的应用，例如插值运算，优化算法等等。SciPy则是在NumPy的基础上构建的更为强大，应用领域也更为广泛的科学计算包。正是出于这个原因，SciPy需要依赖NumPy的支持进行安装和运行。 Scipy是世界上著名的Python开源科学计算库，建立在Numpy之上。它增加的功能包括数值积分、最优化、统计和一些专用函数。 SciPy函数库在NumPy库的基础上增加了众多的数学、科学以及工程计算中常用的库函数。例如线性代数、常微分方程数值求解、信号处理、图像处理、稀疏矩阵等等。Scipy是基于Numpy构建的一个集成了多种数学算法和方便的函数的Python模块。通过给用户提供一些高层的命令和类，SciPy在python交互式会话中，大大增加了操作和可视化数据的能力。通过SciPy，Python的交互式会话变成了一个数据处理和一个system-prototyping环境，足以和MATLAB，IDL，Octave，R-Lab，以及SciLab抗衡。 更重要的是，在Python中使用SciPy，还可以同时用一门强大的语言————Python来开发复杂和专业的程序。用SciPy写科学应用，还能获得世界各地的开发者开发的模块的帮助。从并行程序到web到数据库子例程到各种类，都已经有可用的给Python程序员了。这些强大的功能，SciPy都有，特别是它的数学库。Scipy是在Python的NumPy扩展上构建的数学算法和方便函数的集合。它通过为用户提供高级命令和类来操作和可视化数据，为交互式Python会话添加了强大的功能。有了SciPy，交互式Python会话就变成了一个数据处理和系统原型环境，可以与MATLAB、IDL、Octave、R-Lab和SciLab等系统相匹敌。以Python为基础的SciPy的另一个好处是，它还提供了一种强大的编程语言，可用于开发复杂的程序和专门的应用程序。使用SciPy的科学应用程序受益于世界各地的开发人员在软件领域的许多小众领域中开发的附加模块。从并行编程到web和数据库的子例程和类，Python程序员都可以使用。除了SciPy中的数学库之外，所有这些功能都是可用的 scipy.orgscipy.pypiSciPy User GuideRead the doc for SciPy 常用功能常见的子包 Subpackage Description cluster Clustering algorithms 聚类算法在信息理论、目标检测、通信、压缩等领域有着广泛的应用。vq模块只支持矢量量化和k-均值算法。 constants Physical and mathematical constants fftpack Fast Fourier Transform routines integrate Integration and ordinary differential equation solvers interpolate Interpolation and smoothing splines 此子包包含样条函数和类、一维和多维（单变量和多变量）插值类、Lagrange和Taylor多项式插值器以及FITPACK和DFITPACK函数的包装器。 io Input and Output linalg Linear algebra ndimage N-dimensional image processing odr Orthogonal distance regression optimize Optimization and root-finding routines signal Signal processing sparse Sparse matrices and associated routines spatial Spatial data structures and algorithms special Special functions stats Statistical distributions and functions 该模块包含大量的概率分布以及不断增长的统计函数库。每个单变量分布都是rv_连续（rv_离散用于离散分布）的一个子类的实例。 常用检验Fisher检验12from scipy.stats import fisher_exactoddsratio, pvalue = fisher_exact([[8, 2], [1, 5]])]]></content>
      <categories>
        <category>编程拾慧</category>
        <category>python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python包-OpenCV_图像处理]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-02.python%2FPython-%E5%8C%85-OpenCV_%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86%2F</url>
    <content type="text"><![CDATA[官方文档pypi:opencv-python OpenCV-Python Tutorials 参考网址 http://wiki.opencv.org.cn/index.php/%E9%A6%96%E9%A1%B5 http://www.opencv.org.cn/opencvdoc/2.3.2/html/index.html http://opencv.org/ 官网下载经常会有断，python 2.7 可以在github上的备份中获取;下载文件后，解压，并将文件放入python的库中即可；同时使用open CV需要另一个python的库文件。Numpy； 该model可以直接通过python安装命令进行安装； 安装123pip install opencv-python # 通过pip安装openCVpython -m pip install numpy # 通过pip安装openCV 基本的使用包的导入12import numpy as npimport cv2 文件的读取123456789101112131415161718192021cv2.namedWindow(&apos;image&apos;, cv2.WINDOW_NORMAL) #cv2.WINDOW_AUTOSIZE 窗口不可调节;cv2.namedWindow(&apos;image&apos;, cv2.WINDOW_NORMAL) #cv2.WINDOW_NORMAL 窗口大小可以调节；# Load an color image in grayscaleimg = cv2.imread(&apos;D:/github/Personal-TestDemo/openCV/testInputData/1.png&apos;)cv2.imshow(&apos;image&apos;,img) #展示图片,必须紧跟waitKey，否则可能出现图片不显示灰色背景cv2.waitKey(10) #等待键盘输入cv2.destroyAllWindows() #关闭窗口print img``` ## 图像操作### 绘制#### 绘制直线``` pythoncv2.line(img,(0,0),(150,150),(2,255,255),5)# cv2.line()接受以下参数：图片，开始坐标，结束坐标，颜色（bgr），线条粗细。cv2.imshow(&apos;image&apos;,img)cv2.waitKey(0)cv2.destroyAllWindows() 绘制矩形12cv2.rectangle(img,(15,25),(200,150),(0,0,255),15)# 图像，左上角坐标，右下角坐标，颜色和线条粗细。 绘制圆12cv2.circle(img,(100,63), 55, (0,255,0), -1)# 这里的参数是图像/帧，圆心，半径，颜色和粗细。 粗细-1 表示填充 添加文字12cv2.putText(img, 'OpenCV Tuts!',(10,400), font, 2, (200,255,155), 5, cv2.LINE_AA)#参数依次为 图片，文字，文字起始位置，font,文字大小，文字颜色，文字粗细，cv2.LINE_AA 图像操作更改像素12345px = img[55,55] # 获取某个像素点的颜色img[55,55] = [255,255,255] # 修改某个位点的像素 px = img[100:150,100:150] # 获取某个区域的颜色img[100:150,100:150] = [255,255,255] # 修改某个区域的颜色 图像算术和逻辑运算1234567# 读取两个图像img1 = cv2.imread('3D-Matplotlib.png')img2 = cv2.imread('mainsvmimage.png')add = img1+img2 # 对图像求和，对每个像素的三个颜色通道分别求和，由于颜色值的范围是0~255，所以超过255的会求和结果为255cv2.imshow('add',add)cv2.waitKey(0)cv2.destroyAllWindows() 阈值阈值的思想是进一步简化视觉数据的分析。首先，你可以转换为灰度，但是你必须考虑灰度仍然有至少 255 个值。阈值可以做的事情，在最基本的层面上，是基于阈值将所有东西都转换成白色或黑色。比方说，我们希望阈值为 125（最大为 255），那么 125 以下的所有内容都将被转换为 0 或黑色，而高于 125 的所有内容都将被转换为 255 或白色。如果你像平常一样转换成灰度，你会变成白色和黑色。如果你不转换灰度，你会得到二值化的图片，但会有颜色。 12345678910111213141516# 硬阈值过滤retval, threshold = cv2.threshold(img, 150, 255, cv2.THRESH_BINARY)# 参数分别为(图像，阈值，最大值，阈值类型)，通常情况下，125-150 左右的东西可能效果最好。如果图片过暗需要调低，过亮需要调高。cv2.imshow('origin',img)cv2.imshow('threshold',threshold)cv2.waitKey(0)cv2.destroyAllWindows()# 自适应阈值grayscaled = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY) #将图片变为灰度图片kernel=np.ones((2,2),np.uint8) #进行腐蚀膨胀操作th = cv2.adaptiveThreshold(grayscaled, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 115, 1)cv2.imshow('original',img)cv2.imshow('Adaptive threshold',th)cv2.waitKey(0) 颜色过滤创建一个过滤器，回顾按位操作，其中我们将过滤特定的颜色，试图显示它。或者，你也可以专门筛选出特定的颜色，然后将其替换为场景，就像我们用其他方法替换ROI（图像区域）一样，就像绿屏的工作方式。 为了像这样过滤，你有几个选项。通常，你可能会将你的颜色转换为 HSV，即“色调饱和度纯度”。例如，这可以帮助你根据色调和饱和度范围，使用变化的值确定一个更具体的颜色。如果你希望的话，你可以实际生成基于 BGR 值的过滤器，但是这会有点困难。如果你很难可视化 HSV，不要感到失落，查看维基百科页面上的 HSV，那里有一个非常有用的图形让你可视化它。 参考资料CSDN: SGchi : openCV超详细入门教程（python版）]]></content>
      <categories>
        <category>编程拾慧</category>
        <category>python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python包-Numpy]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-02.python%2FPython-%E5%8C%85-Numpy%2F</url>
    <content type="text"><![CDATA[基本属性数组 函数 &amp; 属性 功能 示例 array=np.array([1,2,3],dtype=np.float) 构建一个数组，通过dtype可以是指数组的数据格式 np.arange 生成满足数列的数组 np.arange(start,end,sep) np.linspace 生成固定数目的数组 np.linspace(start,end,num) array.reship(m,n) 将array 重构为一个m行n列的矩阵 矩阵 函数 &amp; 属性 功能 示例 \$matrix=np.mat(\$array ) 将一个数组对象转换成矩阵 RandMat=mat(random.rand(4,4)) *.I 求矩阵 matrix.I 的逆 invRandMat=RandMat.I np.eye 创建一个单位矩阵 i=np.eye(4) np.zeros((m,n)) 构建一个值都是0的矩阵 np.ones((m,n)) 构建一个值都是1的矩阵 矩阵的基本运算 矩阵 * 矩阵的逆 = 单位矩阵 （单位矩阵是对角线元素为1 ，其他元素均为0 ）]]></content>
      <categories>
        <category>编程拾慧</category>
        <category>python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python-包-运行日志-rich]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-02.python%2FPython-%E5%8C%85-%E8%BF%90%E8%A1%8C%E6%97%A5%E5%BF%97-%E8%BF%9B%E5%BA%A6%E6%9D%A1-rich%2F</url>
    <content type="text"><![CDATA[Rich is a Python library for writing rich text (with color and style) to the terminal, and for displaying advanced content such as tables, markdown, and syntax highlighted code. 官方文档 模块的安装1pip install rich 安装可能的异常记录模块功能rich模块，可以对结果格式进行大量的富文本展示方式，比如文字颜色，比如框选╭───────────────╮│ Hello, World! │╰───────────────╯目前主要使用该模块展示大数据的分析进度，所以其他富文本格式暂未详细学习，后续在补充。 展示进度条日常进行大数据处理中，实时了解处理进度是一个比较重要的需求,通过rich包，我们可以展示任务执行的进度条，tqdm是另一个比较传统的工具，两者对应的实例代码和展示样式如下图： 针对循环处理12345678910from tqdm import tqdmfor step in tqdm(range(100)): time.sleep(0.1)# 100%|████████████████████████████████████████████████████| 100/100 [00:10&lt;00:00, 9.86it/s]from rich.progress import trackfor step in track(range(100)): time.sleep(0.1)# Working... ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100% 0:00:10 针对迭代处理其实处理循环，还有一类工作就是迭代，往往我们也想要知道进度，但如果这时候，只是添加 tqdm(),我们只能看到每个迭代的处理速度，和迭代的处理总数。所以为了获得进度条，我们需要但是使用 total 指定任务的总数。12for i in tqdm(df.iteritems(), total=df.shape[0]): time.sleep(0.1) 针对pandas的处理123456from tqdm import tqdmimport pandas as pdtqdm.pandas(desc='pandas bar')df=pd.read_csv("All.final.compare.tsv",sep="\t")df_result = df['Trans'].progress_apply(lambda x: str(x)[0:7])# pandas bar: 100%|█████████████████████████████████████████████████████| 4048/4048 [00:00&lt;00:00, 4045399.71it/s] 输出样式通过使用rich的样式，可以实现输出文字的加粗，123456789from rich.style import Stylefrom rich.console import Consoleconsole = Console()console.print("Danger, Will Robinson!", style="blink bold red underline on white") # 白底红字console.print("foo [not bold]bar[/not bold] baz", style="bold") # 加粗console.print("Danger, Will Robinson!", style="conceal") # 暗色模式console.print("Danger, Will Robinson!", style="italic") # 斜体console.print("Danger, Will Robinson!", style="strike") # 加删除线console.print("Danger, Will Robinson!", style="underline") # 加下划线]]></content>
      <categories>
        <category>编程拾慧</category>
        <category>python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python-高效的读写大型数据]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-02.python%2Fpython-%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%98-%E5%AD%97%E5%85%B8%E7%9A%84%E8%AF%BB%E5%86%99%2F</url>
    <content type="text"><![CDATA[在进行业务代码编写过程中，不免会用到一些大型的数据信息，一些解决方案是使用数据库（Mysql、Nosql），但是有些业务中，可能并没办法提供一个相对稳定的数据库服务。因此使用格式化的文本进行数据存储成为一个解决方案。所以在这里记录一下一些相关的第三方库，用于以后的备选 使用numpy，保存为.npy文件123456789import numpy as np # Savedict = &#123;&apos;a&apos;:1,&apos;b&apos;:2,&apos;c&apos;:3&#125;np.save(&apos;my_file.npy&apos;, dict) # 注意带上后缀名 # Loadload_dict = np.load(&apos;my_file.npy&apos;).item()print(load_dict[&apos;a&apos;]) 2.使用pickle，保存为.pkl文件1234567891011# 字典保存dict = &#123;&apos;a&apos;:1,&apos;b&apos;:2,&apos;c&apos;:3&#125;f_save = open(&apos;dict_file.pkl&apos;, &apos;wb&apos;)pickle.dump(dict, f_save)f_save.close() # # 读取f_read = open(&apos;dict_file.pkl&apos;, &apos;rb&apos;)dict2 = pickle.load(f_read)print(dict2)f_read.close() 使用JSON(可在pycharm中直接查看)在python中，JSON模块提供以下四个功能，dumps、dump、loads、load。其中dumps把数据类型转换成字符串 dump把数据类型转换成字符串并存储在文件中 loads把字符串转换成数据类型 load把文件打开从字符串转换成数据类型1.字典转化为JSON并写入123456789import json# 创建字典info_dict = &#123;&apos;name&apos;: &apos;Joe&apos;, &apos;age&apos;: 20, &apos;job&apos;: &apos;driver&apos;&#125;# dumps 将数据转换成字符串info_json = json.dumps(info_dict,sort_keys=False, indent=4, separators=(&apos;,&apos;, &apos;: &apos;))# 显示数据类型print(type(info_json))f = open(&apos;info.json&apos;, &apos;w&apos;)f.write(info_json) 读取JSON文件，并转化为字典123456# JSON到字典转化f2 = open(&apos;info.json&apos;, &apos;r&apos;)info_data = json.load(f2)print(info_data)# 显示数据类型print(type(info_data))]]></content>
      <categories>
        <category>编程拾慧</category>
        <category>python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python 单元测试]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-02.python%2FPython-%E8%BF%9B%E9%98%B6-%E5%8D%95%E5%85%83%E6%B5%8B%E8%AF%95%2F</url>
    <content type="text"><![CDATA[单元测试（又称为模块测试, Unit Testing）是针对程序模块（软件设计的最小单位）来进行正确性检验的测试工作。程序单元是应用的最小可测试部件。在过程化编程中，一个单元就是单个程序、函数、过程等；对于面向对象编程，最小单元就是方法，包括基类（超类）、抽象类、或者派生类（子类）中的方法。 如果发现代码难以构造测试，很有可能就是接口设计不够优雅，或者耦合严重，尝试从测试的角度思考能够让我们更好地设计。单元测试同时也为重构提供了保证，比如我们想优化一个函数内部实现，更换更优的数据结构和算法，只需要重新跑一下测试就可以验证新的实现是否引入了错误或bug。 单元测试有以下好处： 确保代码质量 改善代码设计，难以测试的代码一般是设计不够简洁的代码。 保证重构不会引入新问题，以函数为单位进行重构的时候，只需要重新跑测试就基本可以保证重构没引入新问题。 单元测试的框架关于单元测试，有很多现成的框架，其特点如下： unittest，内置库，模仿PyUnit写的，简洁易用，缺点是比较繁琐。 nose，测试发现，发现并运行测试。 pytest，写起来很方便，并且很多知名开源项目在用，推荐。 mock，替换掉网络调用或者 rpc 请求等由于pytest使用起来比较方便，因此选择pytest作为python代码单元测试的工具，关于pytest的具体介绍和使用方法。 安装123sudo pip install -U pytest# 确认是否安装成功py.test --version 嵌入代码内的测试此处参考官网示例，新建一个测试文件。123456# test_sample.py的内容 def func(x): return x + 1 def test_answer(): assert func(3) == 5 直接在脚本目录中运行 pytest ，输出如下 12345678910111213141516171819202122PS D:\Git_Repo\Work_temp&gt; pytest.exe============================= test session starts ============================== platform win32 -- Python 3.11.8, pytest-8.0.2, pluggy-1.3.0rootdir: D:\Git_Repo\Work_tempplugins: anyio-3.7.1collected 1 itemtest_temp.py F [100%] =================================== FAILURES =================================== _________________________________ test_answer __________________________________ def test_answer():&gt; assert func(3) == 5E assert 4 == 5E + where 4 = func(3)test_temp.py:6: AssertionError=========================== short test summary info ============================ FAILED test_temp.py::test_answer - assert 4 == 5============================== 1 failed in 0.53s =============================== PS D:\Git_Repo\Work_temp&gt; pytest会在当前的目录下，寻找以 test 开头的文件（即测试文件），找到测试文件之后，进入到测试文件中寻找test_开头的测试函数并执行。 通过上面的测试输出，我们可以看到该测试过程中，一个收集到了一个测试函数，测试结果是失败的（标记为F），并且在FAILURES部分输出了详细的错误信息，帮助我们分析测试原因，我们可以看到”assert func(3) == 5”这条语句出错了，错误的原因是func(3)=4，然后我们断言func(3) 等于 5。 拆分的测试代码我们可以看到嵌入代码内的测试有一个很大的弊端，就是严格限制了代码文件的名称，一方面非常容易引起歧义，另一方面测试代码再实际业务层面其实是属于冗余代码的范畴，代码杂糅不利于后期的管理。所以大部分业务逻辑下，我们其实是需要将测试代码和业务代码分离的。这时候，我们需要把我们的业务代码进行打包（代码目录创建 __init__.py)示例如下，我们在 get_max 目录下创建 get_max/get_max.py 和get_max/__init__.py get_max/get_max.py 12345def max(a,b): if a &gt; b: return a else: return b get_max/init.py 1from .get_max import max 然后创建我们的单元测试代码test_temp.py 12345678import pytestfrom .get_max import get_maxdef test_max(): assert max(1,2)==2.def test_max1(): assert max(1,2)==3 然后运行 pytest 结果如下：123456789101112131415161718192021PS D:\Git_Repo\Work_temp&gt; pytest.exe============================= test session starts ==============================platform win32 -- Python 3.11.8, pytest-8.0.2, pluggy-1.3.0rootdir: D:\Git_Repo\Work_tempplugins: anyio-3.7.1collected 2 itemstest_temp.py .F [100%]=================================== FAILURES =================================== __________________________________ test_max1 ___________________________________ def test_max1():&gt; assert max(1,2)==3E assert 2 == 3E + where 2 = max(1, 2)test_temp.py:8: AssertionError=========================== short test summary info ============================ FAILED test_temp.py::test_max1 - assert 2 == 3========================= 1 failed, 1 passed in 0.55s ========================== 可以看到我们的两个测试用例，一个pass，一个fail，其中失败的是 test_max1。 补充说明pytest测试样例非常简单，只需要按照下面的规则： 测试文件以test_开头（以_test结尾也可以） 测试类以Test开头，并且不能带有 init 方法 测试函数以test_开头 断言使用基本的assert即可 执行测试样例的方法很多种，上面第一个实例是直接执行pytest，第二个实例是传递了测试文件给pytest。其实pytest有好多种方法执行测试：12345678910pytest # run all tests below current dirpytest test_mod.py # run tests in modulepytest somepath # run all tests below somepathpytest -k stringexpr # only run tests with names that match the # the "string expression", e.g. "MyClass and not method" # will select TestMyClass.test_something # but not TestMyClass.test_method_simplepytest test_mod.py::test_func # only run tests that match the "node ID", # e.g "test_mod.py::test_func" will select # only test_func in test_mod.py]]></content>
      <categories>
        <category>编程拾慧</category>
        <category>python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python整体介绍]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-02.python%2FPython-0-index%2F</url>
    <content type="text"><![CDATA[参考资料Python 教程:极客教程Python 3 教程 RunOOB]]></content>
      <categories>
        <category>编程拾慧</category>
        <category>python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python包-Pyyaml]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-02.python%2FPython-%E5%8C%85-pyyaml%2F</url>
    <content type="text"><![CDATA[安装1pip install pyyaml 格式介绍123456raincoat: 1coins: 5books: 23spectacles: 2chairs: 12pens: 6 数据读取1234567#!/usr/bin/env python3import yamlwith open('items.yaml') as f: data = yaml.load(f, Loader=yaml.FullLoader) print(data) 我们打开items.yaml文件，并使用yaml.load()方法加载内容。 数据被打印到控制台。PyYAML 模块将标量值转换为 Python 字典。 12$ python read_yaml.py&#123;'raincoat': 1, 'coins': 5, 'books': 23, 'spectacles': 2, 'chairs': 12, 'pens': 6&#125; 和JSON文件类似，yaml也提供load和dump两种方法。 yaml.load()或yaml.safe_load(YAML字符串或文件句柄)：yaml -&gt; 字典，如yaml中有中文，需要使用 字符串.encode(&apos;utf-8&apos;)或打开文件时指定encoding=&apos;utf-8&apos; yaml.dump(字典)：默认为flow流格式，即字典{b&apos;: {&apos;c&apos;: 3, &apos;d&apos;: 4}}，会被转为b: {c: 3, d: 4}形式，可以使用default_flow_style=False关闭流模式]]></content>
      <categories>
        <category>编程拾慧</category>
        <category>python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python包-python-docx撰写word]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-02.python%2FPython-%E5%8C%85-python-docx%E6%92%B0%E5%86%99word%2F</url>
    <content type="text"><![CDATA[用途处于某些上下游对接需求，所以需要频繁的将生信的分析结果整理成word文件，以便以进行信息的传递。所以基于该模块可以更方便的在集群上自动化生成相关的文档示例，用于进行后续的处理。 环境安装使用Python进行doc文档编写的过程中，需要安装下列软件 &amp; 包 docx #对应安装命令 pip install python-docx 简介参考信息https://zhuanlan.zhihu.com/p/82880510 https://blog.csdn.net/weixin_44601149/article/details/106660853?utm_medium=distribute.pc_relevant.none-task-blog-title-1&amp;spm=1001.2101.3001.4242 示例首先构建一个Word模板，在模板中使用双大括号来标记后期word编辑的过程中，需要进行替换的变量 {{var}} 。]]></content>
      <categories>
        <category>编程拾慧</category>
        <category>python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Office处理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python包-jupyter]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-02.python%2FPython-%E5%8C%85-jupyter%2F</url>
    <content type="text"><![CDATA[安装12pip3 install --upgrade pippip3 install jupyter 运行Jupyter Notebook12# 运行帮助jupyter notebook --help 接下来打开浏览器输入服务器的IP地址：端口号,例如 10.12.325.321:8888 即可看到 jupyter notebook。这里如果有问题的话首先可以检查下阿里云服务器（你自己买的服务器）端口是否开放，不行的话，再检查linux 系统防火墙是否开放，可以试着关系系统防火墙。1sudo ufw disable 如果登陆失败，则有可能是服务器防火墙设置的问题，此时最简单的方法是在本地建立一个ssh通道： 在本地终端中输入ssh username@address_of_remote -L127.0.0.1:1234:127.0.0.1:8888 便可以在 localhost:8888 直接访问远程的jupyter了。1ssh liubo4@120.24.188.250 -J wangzhonghua@172.25.13.55 -L127.0.0.1:8888:127.0.0.1:8888 在线预览有时候，我们需要讲我们的jupyter文件对外展示，但是需要进行一定的权限、范围控制，而不适合直接提供源代码或者jupyter服务器的授权。这时候我们可以将对应的jupyter文件发布到线上（github，gitlab，或其他任何提供线上访问的方式），然后使用nbviewer生成静态页面（代码历史输出内容会保留，但是在发表页面不能运行代码，只能查看）。 配置更改根目录启动jupyter后，我们会发现默认的根目录是 /home 目录,但是往往我们的项目很少会保存在这，所以我们可以进行配置，调整默认的目录12345# 生产配置文件，（如果已存在可以跳过）jupyter notebook --generate-config# 编辑配置文件vi /home/phoenix/.jupyter/jupyter_notebook_config.py 更新配置文件后，重启jupyter，就可以让相应的更改生效了。 其中常用的关键字及含义如下：| 配置关键字 | 含义 || —————————————- | ————————————————————— || c.NotebookApp.notebook_dir | 你想要放的文件夹 || c.ServerApp.browser | 修改默认浏览器 || c.NotebookApp.allow_credentials | 登陆jupyter是否需要提供认证 || c.NotebookApp.allow_origin =’‘ | 允许连接的远程ip || c.NotebookApp.ip=’‘ | 如果这里修过过后启动服务报错 则修改为c.NotebookApp.ip=’0.0.0.0’ || c.NotebookApp.password=u’sha1**‘ | 就之前保存的验证密码 || c.NotebookApp.open_browser =False | 设置是否自动打开浏览器 || c.NotebookApp.port =8888 | 设置端口 || c.NotebookApp.allow_remote_access = True | 是否允许远程连接 |]]></content>
      <categories>
        <category>编程拾慧</category>
        <category>python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python包-tkinter记录]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-02.python%2FPython-%E5%8C%85-tkinter%2F</url>
    <content type="text"><![CDATA[常用交互功能创建GUI界面123import tkinter as tktop = tk.Tk() # 创建主控件top.geometry('800x400') #设置初始窗口大小 创建控件Tkinter的提供各种控件，如按钮，标签和文本框，一个GUI应用程序中使用。这些控件通常被称为控件或者部件。12#创建一个按钮控件，点击按钮时执行 subComman（一个独立定义的函数）命令ChooseInputFile = tk.Button(top, text="选择图片所在目录", command=subComman) 目前有15种Tkinter的部件。我们提出这些部件以及一个简短的介绍，在下面的表:| 控件 | 描述 || ———— | —————————————————————– || Button | 按钮控件；在程序中显示按钮。 || Canvas | 画布控件；显示图形元素如线条或文本 || Checkbutton | 多选框控件；用于在程序中提供多项选择框 || Entry | 输入控件；用于显示简单的文本内容 || Frame | 框架控件；在屏幕上显示一个矩形区域，多用来作为容器 || Label | 标签控件；可以显示文本和位图 || Listbox | 列表框控件；在Listbox窗口小部件是用来显示一个字符串列表给用户 || Menubutton | 菜单按钮控件，用于显示菜单项。 || Menu | 菜单控件；显示菜单栏,下拉菜单和弹出菜单 || Message | 消息控件；用来显示多行文本，与label比较类似 || Radiobutton | 单选按钮控件；显示一个单选的按钮状态 || Scale | 范围控件；显示一个数值刻度，为输出限定范围的数字区间 || Scrollbar | 滚动条控件，当内容超过可视化区域时使用，如列表框。. || Text | 文本控件；用于显示多行文本 || Toplevel | 容器控件；用来提供一个单独的对话框，和Frame比较类似 || Spinbox | 输入控件；与Entry类似，但是可以指定输入范围值 || PanedWindow | PanedWindow是一个窗口布局管理的插件，可以包含一个或者多个子控件。 || LabelFrame | labelframe | 是一个简单的容器控件。常用与复杂的窗口布局。 || tkMessageBox | 用于显示你应用程序的消息框。 | 所有控件的公共属性 属性 描述 Dimension 控件大小； Color 控件颜色； Font 控件字体； Anchor 锚点； Relief 控件样式； Bitmap 位图； Cursor 光标； 几何管理 几何方法 描述 pack() 包装； grid() 网格； place() 位置； pack pack方法 描述 ipadx=10 水平方向的边距； pady=10 设置竖直方向的边距 side 决定将控件放在父对象的哪一边.TOP (default) 表示顶部, BOTTOM 表示底部, LEFT 表示靠左, RIGHT 表示靠右. anchor 决定控件的锚点，也就是控件的起始位置。可设置的值为’n’, ‘ne’, ‘e’, ‘se’, ‘s’, ‘sw’, ‘w’, ‘nw’; ‘e’、‘w’、‘s’、’n’分别表示东西南北。]]></content>
      <categories>
        <category>编程拾慧</category>
        <category>python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python包-lifelines]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-02.python%2Fpython-%E5%8C%85-lifelines%2F</url>
    <content type="text"><![CDATA[参考资料readthedocscsdn生存分析]]></content>
      <categories>
        <category>编程拾慧</category>
        <category>python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python包-pysam]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-02.python%2FPython-%E5%8C%85-pysam%2F</url>
    <content type="text"><![CDATA[文件读写文件读写句柄bam_file : bam文件路径 读取文件1samfile = pysam.AlignmentFile(bam_file) # 读取文件句柄 写入文件1234567WriteBam = pysam.AlignmentFile(bam_file , &quot;wb&quot;, header=pysamFile.header)for read in samfile.fetch(): if read.is_paired: WriteBam.write(read)pairedreads.close()samfile.close() Bam操作提取特定区域的Reads1234567# 获取染色体chr1 上 100~120bp的readsfor read in samfile.fetch(&apos;chr1&apos;, 100, 120): print(read)samfile.close()pileup(self, contig=None, start=None, stop=None, region=None, reference=None, end=None, **kwargs) 统计特定区域的碱基分布123&gt;&gt;&gt; base = bam.count_coverage('chr15', 43997074,43997076,quality_threshold=0)(array('L', [1, 0]), array('L', [0, 143]), array('L', [0, 0]), array('L', [141, 0])) 返回一个4维的属组，分别对应 A(base[0][site_index]) C(base[1][site_index]) G(base[2][site_index]) T(base[3][site_index]) ,注意：pysam 提供的参数 quality_threshold 只针对 base_quality 进行过滤，不支持 map_quality 的过滤 文件函数1234567891011121314151617181920# region : 提取的区域# mapq : 比对质量# baseq ：碱基质量# ref :参考基因组for pileup_column in bam_file.pileup(region=&quot;chr1:1-10&quot;,mapq=mapq , baseq = baseq, stepper=stepper, fastaFile=ref, max_depth=200000, **&#123;&quot;truncate&quot;: True&#125;):# for pileup_read in pileup_column.pileups: aln = pileup_read.alignment read_name = aln.query_name pair = &apos;pe1&apos; if aln.is_read1 else &apos;pe2&apos; strand = &apos;-&apos; if aln.is_reverse else &apos;+&apos; read = Read(read_name, pair, strand) if pileup_read.is_del or pileup_read.is_refskip or (aln.flag &gt; 1024) or (aln.mapping_quality &lt; mapq) or \ aln.query_qualities[pileup_read.query_position] &lt; baseq: continue start_reads[read] = [pileup_read.query_position, aln] Referencereadthedoc]]></content>
      <categories>
        <category>编程拾慧</category>
        <category>python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python包-python-docx撰写word]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-02.python%2FPython-%E5%8C%85-python-PyMouse_%E6%8E%A7%E5%88%B6%E9%BC%A0%E6%A0%87%2F</url>
    <content type="text"><![CDATA[参考博客 python-sendkeys 模拟键盘事件的模块 导入需要的包 12345import win32apiimport win32conimport win32guifrom ctypes import *import time 设置鼠标双击的函数，通过坐标控制双击位点 1234567 def double_click(x=0,y=0):mouse_move(x,y)time.sleep(0.05) #延迟时间，尤其是在电脑反映不是很快的时候，win32api.mouse_event(win32con.MOUSEEVENTF_LEFTDOWN,0,0,0,0) #点击鼠标win32api.mouse_event(win32con.MOUSEEVENTF_LEFTUP, 0,0,0,0) #抬起鼠标win32api.mouse_event(win32con.MOUSEEVENTF_LEFTDOWN,0,0,0,0)win32api.mouse_event(win32con.MOUSEEVENTF_LEFTUP, 0,0,0,0) 捕获鼠标当前位置函数 123456def get_mouse_position(): po = POINT() windll.user32.GetCursorPos(byref(po)) return int(po.x),int(po.y)class POINT(Structure): fields_=[(&quot;x&quot;,c_ulong),(&quot;y&quot;,c_ulong)] 模拟键盘输入 1win32api.keybd_event(86,0,0,0) 键码表Win32 api函数表附个键位码表： 字母和数字键 数字小键盘的键 功能键 其它键 键 键码 键 键码 键 键码 键 键码 A 65 0 96 F1 112 Backspace 8 B 66 1 97 F2 113 Tab 9 C 67 2 98 F3 114 Clear 12 D 68 3 99 F4 115 Enter 13 E 69 4 100 F5 116 Shift 16 F 70 5 101 F6 117 Control 17 G 71 6 102 F7 118 Alt 18 H 72 7 103 F8 119 Caps Lock 20 I 73 8 104 F9 120 Esc 27 J 74 9 105 F10 121 Spacebar 32 K 75 * 106 F11 122 Page Up 33 L 76 + 107 F12 123 Page Down 34 M 77 Enter 108 – – End 35 N 78 - 109 – – Home 36 O 79 . 110 – – Left Arrow 37 P 80 / 111 – – Up Arrow 38 Q 81 – – – – Right Arrow 39 R 82 – – – – Down Arrow 40 S 83 – – – – Insert 45 T 84 – – – – Delete 46 U 85 – – – – Help 47 V 86 – – – – Num Lock 144 其他未列出的字母和数字键盘为：ord(c)]]></content>
      <categories>
        <category>编程拾慧</category>
        <category>python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python包-pandas]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-02.python%2FPython-%E5%8C%85-Pandas%2F</url>
    <content type="text"><![CDATA[官方资料api参考文档官方文档-en官方文档-cnpandas的 General functions 盖若pandas速查 方法 简介 示例 concat Concatenate pandas objects along a particular axis with optional set logic along the other axes. df_result = pd.concat([dataframe1 , dataframe2]) 常用功能及代码块示例得到一个DataFrame创建一个空的dataframe1234#指定dataframe的所有字段columns=["sampleName","#Gene","Transcript,"cHGVS","pHGVS","Frequence"]#创建一个含字段结构的空数据框DF = pd.DataFrame(columns=columns) 从文件中读取一个dataframe1234567891011121314# 读取ExcelDF = pd.read_excel(file, sheet_name=None)DF = pd.read_csv(file,sep="\t" )# sheet_name 指定 None时，读取所有sheet返回一个字典，key为sheet名； 不指定默认读取第一个sheet）# 读取tsv文件pd.read_csv(filename, sep="\t")# 读取压缩文件df = pd.read_csv('filename.zip')# 指定压缩格式df = pd.read_csv('filename.zip', compression='zip', header=0, sep=',', quotechar='"')压缩：&#123;'infer'，'gzip'，'bz2'，'zip'，'xz'，无&#125;，默认为'infer'用于对磁盘数据进行实时解压缩。如果'infer'和filepath_or_buffer与路径类似，则从以下扩展名检测压缩：'.gz'，'。bz2'，'。zip'或'.xz'（否则不进行解压缩）。如果使用“ zip”，则ZIP文件必须仅包含一个要读取的数据文件。设置为“无”将不进行解压缩。 读取数据时跳过注释信息在我们实际使用的文件，尤其是一些数据库类型的记录文件中，通过添加注释，文件通常可以更加清晰明了。csv文件中有注释，这种情况是可能存在的。那么，在pandas读取csv文件的时候，如何规避掉注释。 123456789101112#emptya,b,c12,中国，上海# 如果仅指定分隔符，正常读取是不合理的，结果仅有一列而非a,b,c三列，注释当做了列名&gt;&gt;&gt; df = pd.read_csv(&apos;D:/1.csv&apos;, sep=&apos; &apos;)&gt;&gt;&gt; df #empty\na,b,c\n1,2,3a b c12 中国 上海&gt;&gt;&gt; df.columnsIndex([&apos;#empty\na,b,c\n1,2,3&apos;], dtype=&apos;object&apos;) 我们可以通过设置comment参数，指定某一行是注释，则该行就不会被解析1234&gt;&gt;&gt; df = pd.read_csv('D:/1.csv', sep=' ', comment='#')&gt;&gt;&gt; df a b c0 12 中国 上海 只读取指定的列 usecols1df = pd.read_csv(&apos;data.csv&apos;, usecols=[&apos;Name&apos;, &apos;Age&apos;]) 读取文件时指定类型12data_type=&#123;&quot;geneID&quot;:str, &quot;Gene_symble&quot;:str&#125;data=pd.read_csv(&quot;GeneSymble2ID.tsv&quot;,sep=&quot;\t&quot;,dtype=data_type) 获取DataFrame的信息查看头尾数据123456# 查看开头的N（默认值5） 行df.head()# 查看开头的10行df.head(10)# 查看结尾的N（默认值5） 行df.tail() 随机查看数据12# 随机查看N（默认1 ）行数据df.sample(N) 逐行读取dataframe的每行DataFrame.iterrows()12for index, row in df.iterrows(): print row[&quot;c1&quot;], row[&quot;c2&quot;] DataFrame.itertuples()1234for row in df.itertuples(index=True, name=&apos;Pandas&apos;): print getattr(row, &quot;c1&quot;), getattr(row, &quot;c2&quot;) # 取值方式 - print(row.c1) itertuples()应该比iterrows()快 获取指定cell的数据信息 iloc123456# 获取第i行，c1字段的内容df.iloc[i][&apos;c1&apos;]#也可以通过遍历index获取全部行的特定信息列for i in range(0, len(df)): print df.iloc[i][&apos;c1&apos;], df.iloc[i][&apos;c2&apos;] 将DataFrame转为List 略麻烦，但是更高效，1234567891011121314from collections import namedtupledef myiter(d, cols=None): if cols is None: v = d.values.tolist() cols = d.columns.values.tolist() else: j = [d.columns.get_loc(c) for c in cols] v = d.values[:, j].tolist() n = namedtuple(&apos;MyTuple&apos;, cols) for line in iter(v): yield n(*line) 筛选DataFrame的数据单个条件筛选12345678910$ print(df)name age sexTim 10 男Sam 20 女Tom 30 NaN$ df[df[&quot;age&quot;]&gt;15]name age sexSam 20 女Tom 30 NaN 多个条件筛选存在多个比较条件的时候，需要注意 多个条件同时满足不能用and，使用 &amp; 多个条件满足其中一个即可，不能使用or，使用 | 每个条件要使用 小括号123456789$ print(df)name age sexTim 10 男Sam 20 女Tom 30 NaN$ df[(df[&quot;age&quot;]&gt;15) &amp; (df[&quot;age&quot;]&lt;25&gt;)]name age sexSam 20 女 筛选常用的数值函数123456df.eq() # 等于相等 ==df.ne() # 不等于 !=df.le() # 小于等于 &gt;=df.lt() # 小于 &lt;df.ge() # 大于等于 &gt;=df.gt() # 大于 &gt; 使用单个数值函数筛选123$ df[(df[&quot;age&quot;] eq 20)]name age sexSam 20 女 筛选常用的字符型函数123包含：str.contains开始：str.startswith结束：str.endswith 示例如下：12345678$ df[(df[&quot;name&quot;].str.contains(&quot;o&quot;))]name age sexTom 30 NaN# 如果字符串所在列存在空值，则可以通过添加参数进行剔除，否则报错$ df[(df[&quot;sex&quot;].str.contains(&quot;男&quot;,na=False))]name age sexTim 10 男 基于索引筛选情况比较少，但是特殊情况也会用到12$ df[df.index == 1]Sam 20 女 筛选存在缺失值的行123$ df[df.isnull().values==True]name age sexTom 30 NaN 更强自定义化的筛选定义一个函数，进行复杂的逻辑判断；使用apply对dataframe进行系统化批量的处理123456789def checkfunction(x,y,z): if int(x) &gt;= 100: if y in (&apos;*&apos;, &apos;-&apos;): return True elif int(y) &gt;= 1 and int(z) &gt;= 1: return True return FalseFilterData = RawData[RawData.apply(lambda x: checkfunction(x[&quot;tag_x&quot;],x[&quot;tag_y&quot;],x[&quot;tag_z&quot;]), axis=1)] 更改 dataframe 的内容整个dataframe的全局替换12345# 将所有na替换为特定的valuesdata.fillna(value=values,inplace=True)# 通过使用正则进行文本的全局替换data.replace(&quot;\r\n&quot;,&quot;&lt;br&gt;&quot;,regex=True,inplace=True) # 将数据中的换行符统一替换成 &lt;br&gt; .regex：是否使用正则，若不适用，则只能进行整体的替换。 整列改为相同的值1DF[&apos;sampleName&apos;] = &quot;S1&quot; # 将数据框DF的sampleName列都改为 &quot;S1&quot; 根据特定条件修改某一列的值调用DataFrame.apply()方法，可以作用于 Series 或者整个 DataFrame，它自动遍历整个 Series 或者 DataFrame, 对每一个元素运行指定的函数。更改前最好对数据进行确认，如果在数据中不存在满足条件的记录，导致更改操作处理的是一个空的dataframe 会导致报错 123456789101112# 使用lambda函数进行操作df[&apos;label&apos;]=df.id.apply(lambda x: 1 if &apos;M&apos; in x else 0)# 使用预定义的函数进行处理def valuation_formula(x, y): return x * y * 0.5df[&apos;price&apos;] = df.apply(lambda row: valuation_formula(row[&apos;x&apos;], row[&apos;y&apos;]), axis=1)# 如果id列值包含‘L’,那么就将label列中对应的值从1替换成0： # 先判断是否存在满足条件的数据记录，如果有在进行更改if len(df[df[&apos;id&apos;].str.contains(&apos;L&apos;)]) &gt; 0: df.loc[df[&apos;id&apos;].str.contains(&apos;L&apos;),&apos;label&apos;]=0 根据特定条件修改某几列的值前面介绍了基于dataframe的某几列，更新某一列的值。但是实际使用中，我们也会遇到基于某几列信息，更新dataframe中的某几列值。这时候，一种方案是每一列值都单独进行更新迭代，但是如果这些结果每次都需要运行获取相关结果，所以显而易见的会显著提升分析耗时(可以使用一些函数的缓存加速方案进行优化）。所以在这提供一个基于几列内容和一个逻辑函数，对dataframe的多列进行更新的示例代码 12345678910# 分布更改ReturnData['BGI_Uniport_Position(s)']=ReturnData.apply(lambda row: self.check(row['Hugo_Symbol'],row['HGVSp_Short'])[0],axis=1)ReturnData['BGI_Uniport_feature_key']=ReturnData.apply(lambda row: self.check(row['Hugo_Symbol'],row['HGVSp_Short'])[1],axis=1)ReturnData['BGI_Uniport_description']=ReturnData.apply(lambda row: self.check(row['Hugo_Symbol'],row['HGVSp_Short'])[2],axis=1)# 一次更新多个字段# check处理函数返回值如下# return BGI_Uniport_Position,BGI_Uniport_feature_key,BGI_Uniport_description ReturnData[['BGI_Uniport_Position(s)','BGI_Uniport_feature_key','BGI_Uniport_description']]=ReturnData.apply(lambda row: pd.Series(self.check(row['Hugo_Symbol'],row['HGVSp_Short'])),axis=1)# ps 由于函数返回的是一个元祖，所以需要使用 pd.Series 讲元祖拆分成三列，然后分别赋值给Dataframe的相应列。 更新DataFrame某一列（值位于另一个DataFrame）12345678import pandas as pddf1=pd.DataFrame(&#123;&apos;id&apos;:[1,2,3],&apos;name&apos;:[&apos;Andy1&apos;,&apos;Jacky1&apos;,&apos;Bruce1&apos;]&#125;)df2=pd.DataFrame(&#123;&apos;id&apos;:[1,2],&apos;name&apos;:[&apos;Andy2&apos;,&apos;Jacky2&apos;]&#125;)s = df2.set_index(&apos;id&apos;)[&apos;name&apos;]df1[&apos;name&apos;] = df1[&apos;id&apos;].map(s).fillna(df1[&apos;name&apos;]).astype(str)print(df1) 将dataframe中的某一列中的文本拆分成多行示例：12345df=df.drop(&apos;cont&apos;, axis=1).join(df[&apos;cont&apos;].str.split(&apos;/&apos;, expand=True).stack().reset_index(level=1, drop=True).rename(&apos;tag&apos;))df=df[&apos;cont&apos;].str.split(&apos;/&apos;, expand=True).stack().reset_index(level=0).set_index(&apos;level_0&apos;).rename(columns=&#123;0:&apos;tag&apos;&#125;,inplace=True).join(df.drop(&apos;cont&apos;, axis=1))df=df.assign(tag=df[&apos;cont&apos;].str.split(&apos;,&apos;)).explode(&apos;tag&apos;) 有时候，我们拆分的列可能没有这么直接，那么我们可以自定义函数解析某一列，然后进行拆分 123456789101112131415161718192021# 输入数据#sample gene_symbol transcript exon_intron effect_type#24B07832668 NF1 NM_000267.3 EX9-EX35 deletion#21B019537696-83 TTC8 NM_198309.2 EX7 deletion#21B02428822-84 SMN1 NM_000344.3 EX7 deletion#24B07832727 CHD8 NM_001170629.1 EX5-EX7 deletion# 自定义拆解的列def generate_sequence(input_str): print(input_str) numbers = re.findall(r'\d+', input_str) if len(numbers)&gt;1 : sequence = list(range(int(numbers[0]), int(numbers[-1]))) return(sequence) elif len(numbers)==1: return([int(numbers[0])]) else: return([""])data["single_exon"]=data.apply(lambda row: generate_sequence(row["exon_intron"]),axis=1)data.explode('sp') 将dataframe中的多列合并成一个新列12345678910111213data#GENE1 GENE2 chr posRPN2 SRC chr20 35868878LINC00486 PLXNA1 chr2 33141303LINC00486 TRRAP chr2 33141470LINC00486 MAPK3 chr2 33141307data[&quot;chr_pos&quot;] = data[&apos;chr&apos;].str.cat(data.pos.astype(&apos;str&apos;),sep=&quot;:&quot;)#GENE1 GENE2 chr pos chr_posRPN2 SRC chr20 35868878 chr20:35868878LINC00486 PLXNA1 chr2 33141303 chr2:33141303LINC00486 TRRAP chr2 33141470 chr2:33141470LINC00486 MAPK3 chr2 33141307 chr2:33141307 dataframe的合并按列进行合并 merge （着重关注的是行的合并）123456789101112131415161718192021222324252627282930313233343536373839404142434445464748&gt;&gt;&gt; print(df1) Courses Fee Durationr1 Spark 20000 30daysr2 PySpark 25000 40daysr3 Python 22000 35daysr4 pandas 30000 50days&gt;&gt;&gt; print(df2) Courses Discountr1 Spark 2000r6 Java 2300r3 Python 1200r5 Go 2000# pandas.merge()df3=pd.merge(df1,df2, how=&apos;left&apos;)print(df3)# DataFrame.merge()df3=df1.merge(df2, how=&apos;left&apos;)print(df3) Courses Fee Duration Discount0 Spark 20000 30days 2000.01 PySpark 25000 40days NaN2 Python 22000 35days 1200.03 pandas 30000 50days NaN# 按特定的列进行两个dataframe的合并# Merge DataFrames by Columnsdf3=pd.merge(df1,df2, on=&apos;Courses&apos;, how=&apos;left&apos;)# When column names are differentdf3=pd.merge(df1,df2, left_on=&apos;Courses&apos;, right_on=&apos;Courses&apos;, how=&apos;left&apos;)print(df3)# 强制一对一的对应关系（否则如果存在重复行，结果中会指数性的增加）df3=pd.merge(df1,df2, left_on=&apos;Courses&apos;, right_on=&apos;Courses&apos;, how=&apos;left&apos;,validate=&apos;one_to_one&apos;)# 其中how参数游四个选项how=&quot;inner&quot;# inner是merge函数的默认参数，意思是将dataframe_1和dataframe_2两表中主键一致的行保留下来，然后合并列。how=&quot;outer&quot;# outer是相对于inner来说的，outer不会仅仅保留主键一致的行，还会将不一致的部分填充Nan然后保留下来。how=&quot;left&quot; || how=&quot;right&quot; # 然后是left和right，首先为什么是left和right，left指代的是输入的时候左边的表格即dataframe_1，同理right指代dataframe_2。# left和right相当于inner和outer取了个折中的合并方法，意为保证dataframe_1或者dataframe_2不变（不变的表格我们这里记为目标表格），然后另一个表格（我们这里记为信息表格）向目标表格添加信息。 按行进行拼接12# 将两个dataframe逐行添加df_result = pd.concat([dataframe1 , dataframe2]) dataframe中逐行添加数据1AllSampleFastqQC.loc[len(AllSampleFastqQC)+1] = [SampleDir,SampleDirPath,Chip,lane,barcode,umi,Q20,Q30,GC] dataframe 去重drop_duplicates()函数的语法格式如下：1234df.drop_duplicates(subset=['A','B','C'],keep='first',inplace=True) #subset：表示要进去重的列名，默认为 None。 #keep：有三个可选参数，分别是 first、last、False，默认为 first，表示只保留第一次出现的重复项，删除其余重复项，last 表示只保留最后一次出现的重复项，False 则表示删除所有重复项。 #inplace：布尔值参数，默认为 False 表示删除重复项后返回一个副本，若为 Ture 则表示直接在原数据上删除重复项。]]></content>
      <categories>
        <category>编程拾慧</category>
        <category>python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python包-Numpy记录]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-02.python%2FPython-%E5%8C%85-vcf%2F</url>
    <content type="text"><![CDATA[官方文档 模块的安装1pip install pyvcf 安装可能的异常记录Error1pyvcf的最新版（0.6.8）安装需要使用use_2to3 （在setuptools&gt;=58 后不支持该参数）所以需要对setuptools进行版本调整，过高过低都会出现该参数缺失无法变异的情况。1pip install setuptools==57.5.0 Error2安装报错12unable to execute &apos;x86_64-conda_cos6-linux-gnu-gcc&apos;: No such file or directoryerror: command &apos;x86_64-conda_cos6-linux-gnu-gcc&apos; failed with exit status 1 环境先安装x86_64-conda_cos6-linux-gnu-gcc（ conda install gxx_linux-64） 在安装相关模块 VCF文件的读写1234567891011import vcf#读取vcf文件 vcf_reader = vcf.Reader(open(&apos;vcf/test/example-4.0.vcf&apos;, &apos;r&apos;))for var in vcf_reader: print (var)# 写入vcf文件vcf_writer = vcf.Writer(open(&apos;/dev/null&apos;, &apos;w&apos;), vcf_reader)for var in vcf_reader: vcf_writer.write_record(var) vcf文件迭代子类的属性1234567891011121314Record.CHROM Record.POSRecord.IDRecord.REFRecord.ALTRecord.QUALRecord.FILTERRecord.INFO # 返回一个字典，可以用Record.INFO[&apos;type&apos;],Record.INFO[&apos;DP&apos;] 键值继续提取Record.FORMAT #返回format列 字符串 如果你的vcf文件中没有FORMAT 返回 &quot;GT:DP:RO:QR:AO:QA:GL&quot;Record.genotype(&quot;cancer&quot;)[&quot;AD&quot;] # 通过样本名称和FORMAT 索引获得样本对应信息。print i.samples # 返回的是三个样 call object 组成的列表。 常用信息1cpra = Record.CHROM+&quot;_&quot;+Record.POS+&quot;_&quot;+Record.REF+&quot;_&quot;+Record.ALT]]></content>
      <categories>
        <category>编程拾慧</category>
        <category>python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python包-json]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-02.python%2FPython-%E5%8C%85-json%2F</url>
    <content type="text"><![CDATA[安装1pip install json 格式介绍123&#123;"accesskeyID":"LTAI5tSRTFb****","accesskeysecret":"jAJGMov82****"&#125; 数据读取123456789#!/usr/bin/env python3import jsonwith open('.aliyun.json') as f: data = json.load(f)# 获取值data['accesskeyID'] # LTAI5tSRTFb****data['accesskeysecret'] # jAJGMov82****]]></content>
      <categories>
        <category>编程拾慧</category>
        <category>python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python包-argparse]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-02.python%2FPython-%E5%8C%85-argparse-%E5%8F%82%E6%95%B0%E7%AE%A1%E7%90%86%2F</url>
    <content type="text"><![CDATA[参考资料argparseg官方文档:python3argparseg官方文档:python3-中文 子命令 subparsers()功能比较多的命令端程序常常将功能分解到不同子命令中，如在Python中常见的pip install、pip uninstall等。当程序比较复杂且不同功能都需要不同参数时，子命令是一个不错的方式。1234567891011121314151617181920212223242526272829303132333435363738394041import argparsedef add(args): r = args.x + args print('x + y = ', r)def sub(args): r = args.x - args print('x - y = ', r)parser = argparse.ArgumentParser(prog='PROG')subparsers = parser.add_subparsers(help='sub-command help')#添加子命令 addparser_a = subparsers.add_parser('add', help='add help')parser_a.add_argument('-x', type=int, help='x value')parser_a.add_argument('-y', type=int, help='y value')# 提供参数枚举值listparser.add_argument('-tag' , dest='tag' , default=False, action='store', type=str, choices=["couple","trio","single"],help='familyTag Type')# 提供参数确定bool值args_inheritCoincide.add_argument('-trio' , dest='trio' , default=False, action='store_true', help='if trio,default for single')运行时，提供参数则为True，不提供参数则默认为False。#设置默认函数parser_a.set_defaults(func=add)#添加子命令 subparser_s = subparsers.add_parser('sub', help='sub help')parser_s.add_argument('-x', type=int, help='x value')parser_s.add_argument('-y', type=int, help='y value')#设置默认函数parser_s.set_defaults(func=sub)args = parser.parse_args()#执行函数功能args.func(args) 运行命令1234$python subc.py add -x 1 -y 2x + y = 3$python subc.py sub -x 1 -y 2x - y = -1 参数分组有时候，我们需要给参数分组，以使得在显示帮助信息时能够显示到一起。但是和子命令不同的是，不同的组会在同时被使用。 比如某命令行支持三个参数选项 –user、–password和–push，前两者需要放在一个名为 authentication 的分组中以表示它们是身份认证信息。那么我们可以用 ArgumentParser.add_argument_group 来满足：123456789101112131415group = parser.add_argument_group('authentication')group.add_argument('--user', action="store")group.add_argument('--password', action="store")parser.add_argument('--push', action='store')&gt;&gt;&gt; parser.parse_args(['-h'])# usage: [-h] [--user USER] [--password PASSWORD] [--push PUSH]# optional arguments:# -h, --help show this help message and exit# --push PUSH# authentication:# --user USER# --password PASSWORD add_argument() 方法ArgumentParser.add_argument(name or flags…[, action][, nargs][, const][, default][, type][, choices][, required][, help][, metavar][, dest]) 定义单个的命令行参数应当如何解析。每个形参都在下面有它自己更多的描述，长话短说有： 123456789101112131415161718192021name or flags - 一个命名或者一个选项字符串的列表，例如 foo 或 -f, --foo。action - 当参数在命令行中出现时使用的动作基本类型。nargs - 命令行参数应当消耗的数目。const - 被一些 action 和 nargs 选择所需求的常数。default - 当参数未在命令行中出现并且也不存在于命名空间对象时所产生的值。type - 命令行参数应当被转换成的类型。choices - 可用的参数的容器。required - 此命令行选项是否可省略 （仅选项可用）。help - 一个此选项作用的简单描述。metavar - 在使用方法消息中使用的参数值示例。dest - 被添加到 parse_args() 所返回对象上的属性名。 ArgumentParser 对象12345678910111213141516171819202122232425prog - The name of the program (default: os.path.basename(sys.argv[0]))usage - 描述程序用途的字符串（默认值：从添加到解析器的参数生成）description - 在参数帮助文档之前显示的文本（默认值：无）epilog - 在参数帮助文档之后显示的文本（默认值：无）parents - 一个 ArgumentParser 对象的列表，它们的参数也应包含在内formatter_class - 用于自定义帮助文档输出格式的类prefix_chars - 可选参数的前缀字符集合（默认值： '-'）fromfile_prefix_chars - 当需要从文件中读取其他参数时，用于标识文件名的前缀字符集合（默认值： None）argument_default - 参数的全局默认值（默认值： None）conflict_handler - 解决冲突选项的策略（通常是不必要的）add_help - 为解析器添加一个 -h/--help 选项（默认值： True）allow_abbrev - 如果缩写是无歧义的，则允许缩写长选项 （默认值：True）exit_on_error - 决定当错误发生时是否让 ArgumentParser 附带错误信息退出。 (默认值: True)]]></content>
      <categories>
        <category>编程拾慧</category>
        <category>python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python包-WordCloud 绘制词云图]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-02.python%2FPython-%E5%8C%85-WordCloud-%E8%AF%8D%E4%BA%91%E5%9B%BE%2F</url>
    <content type="text"><![CDATA[安装1pip install wordcloud 使用12345w= wordcloud.WordCloud()w.generate() # 向WordCloud对象中加载文本txt&gt;&gt;&gt;w.generate("Python and WordCloud")w.to_file(filename) # 将词云输出为图像文件，.png或.jpg格式&gt;&gt;&gt;w.to_file("outfile.png") 数据获取wordcloud如何将文本转化为词云 1.分隔：以空格分隔单词 2.统计：单词出现次数并过滤 3.字体：根据统计配置字号 4.布局：颜色环境尺寸 参数介绍 参数 描述 示例 width 指定词云对象生成图片的宽度,默认400像素 w=wordcloud.WordCloud(width=600) height 指定词云对象生成图片的高度,默认200像素 w=wordcloud.WordCloud(height=400) min_font_size 指定词云中字体的最小字号，默认4号 w=wordcloud.WordCloud(min_font_size=10) max_font_size 指定词云中字体的最大字号，根据高度自动调节 w=wordcloud.WordCloud(max_font_size=20) font_step 指定词云中字体字号的步进间隔，默认为1 w=wordcloud.WordCloud(font_step=2) font_path 指定文体文件的路径，默认None w=wordcloud.WordCloud(font_path=”msyh.ttc”) max_words 指定词云显示的最大单词数量,默认200 w=wordcloud.WordCloud(max_words=20) stop_words 指定词云的排除词列表，即不显示的单词列表 w=wordcloud.WordCloud(stop_words=”Python”) mask 指定词云形状，默认为长方形，需要引用imread()函数 from scipy.msc import imread ;mk=imread(“pic.png”)；w=wordcloud.WordCloud(mask=mk) background_color 指定词云图片的背景颜色，默认为黑色 w=wordcloud.WordCloud(background_color=”white”) 异常处理中文显示乱码 进入python根目录, 然后进入Lib\site-packages\wordcloud 进入C:\Windows\Fonts目录下, 拷贝一个中文字库, 如华文新魏, 将其复制粘贴到Lib\site-packages\wordcloud目录下 打开Lib\site-packages\wordcloud目录下wordcloud.py, 找到如下这行代码 12FONT_PATH = os.environ.get('FONT_PATH', os.path.join(FILE, 'DroidSansMono.ttf')) #原内容FONT_PATH = os.environ.get('FONT_PATH', os.path.join(FILE, 'STKAITI.TTF')) #更改/添加 拷贝的字体文件]]></content>
      <categories>
        <category>编程拾慧</category>
        <category>python</category>
      </categories>
      <tags>
        <tag>可视化</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[变异检出标准化-HGVS (Human Genome Variation Society)]]></title>
    <url>%2F01.%E7%9F%A5%E8%AF%86-03.standard%2FStandard-%E5%8F%98%E5%BC%82%E6%A3%80%E5%87%BA%E6%B3%A8%E9%87%8A%E6%A0%87%E5%87%86%E5%8C%96-HGVS%2F</url>
    <content type="text"><![CDATA[为什么要标准化？简单讲，就是为了让一个特定的突变具有一个唯一的表述方式，一个突变如果不进行标准化，会出现多种不同的表达方式，即MNP（Multi Nucleotide Polymorphism ），这会给大家的交流带来诸多的问题， 试想每个人有自己的习惯，最终一个突变可能会有成百上千种表达方式，这无疑会给研究人员交流带来诸多的困扰。因此标准化是很有必要的，目前主流的标准化原则是涉及尽可能少的碱基，尽可能做对齐。 首先我们来看一下，目前的变异检测多是依靠序列比对，然后会针对不同的比对情况进行变异检出的，我们先来看一下SNV， 如果不标准化，一个相同的变异 CAT =&gt; TGC 的突变，会出现下列所示的多种不同的表述方法。 随着基因检测技术的广泛应用，越来越多与疾病相关的基因和变异被明确。为了让变异注释更程序化、对变异文献的检索更全面化以及对变异的描述更标准化故需要形成统一突变命名规则，HGVS命名规则是目前公认的命名规则（人类基因组变异学会(HGVS)、人类变异项目(HVP)和人类基因组组织(HUGO)授权）。HGVS命名规则从不同层面对变异进行描述，反应变异的位置及对编码的蛋白的影响。参考序列的选择、转录版本号的不同及描述层面（DNA水平、RNA水平及蛋白水平）的不同，均会导致同一变异描述的形式不同。 HGVS规范参考序列只接受NCBI或EBI公共数据库中的参考序列ID，且必须同时包含accession和version信息；如NC_000023.10中，NC_000023代表accession号，10代表version号。参考序列中下划线前的大写字母代表参考序列格式，目前批准的参考序列格式有：| 格式 | 解释 | 举例 || —– | ————————————————————————- | ———————————— || NC_＃ | 代表完整的基因组序列，标记的类别包括基因组、染色体、细胞器、质粒。 | NC_000023.10:g.32407761G&gt;A || LRG_# | Locus Reference Genomic，基因座参考基因组序列。（不能在其他基因组找到的） | NG_012232.1:g.954966C&gt;T || NG_＃ | 不完整的基因组区域（不转录的假基因或者那些很难自行化注释的基因组区域） | LRG_199:g.954966C&gt;T || NM_＃ | 编码蛋白的转录本序列。基因检测报告中最常用此作为参考序列。 | NM004006.2:c.4375C&gt;T || NR_＃ | 非编码的转录本序列，包括结构RNAs，假基因转子等。 | NR002196.1:n.601G&gt;T || NP_＃ | 蛋白质序列 | NP-003997.1:p.Arg1459*(p.Arg1459Tero | g.代表线性基因组参考序列； o.代表环状基因组参考序列； m.代表线粒体参考序列； c.代表编码DNA参考序列； n.代表非编码DNA参考序列； r.代表RNA参考序列； p.代表蛋白（氨基酸）参考序列。 变异位置 编码区（CDS） 以起始密码子ATG的第一个碱基A开始，并记为c.1，以终止密码子（TAA, TAG, TGA）的最后一个碱基为终点。 内含子区（Intron） 靠近内含子5’末端的变异位点，需依据上游最近外显子的最后一个碱基来定位，如c.87+4，代表上游最近外显子的边界位置为87，变异位点在内含子5’ 端开始的第4个碱基； 靠近内含子3’ 末端的变异位点，要依据下游最近外显子的第一个碱基来定位，如c.88-11， 内含子碱基个数为偶数时，中间碱基平分后按上下游外显子碱基来定位命名，如…,c.87+676, c.87+677, c.87+678, c.88-678, c.88-677, c.88-676, … 内含子碱基个数为奇数时，中间碱基相对于上游外显子最后一个碱基来定位命名，如…,c.87+677, c.87+678, c.87+679, c.88-678, c.88-677, … 非编码区（UTR区）： 起始密码子ATG上游（5’ UTR区）标记为“-”，编号为c.-1, c.-2, c.-3… 终止密码子下游（3’ UTR区）标记为“”，编号为c.1, c.2, c.3… 位于靠近5’ UTR和3’ UTR区的内含子变异位点，命名规则同内含子区，如：5’ UTR区内含子为c.-85+1，c.-84-3等；3’ UTR区内含子为c.37+1，c.38-3等。 参考示意图如下： 变异类型优先级从高到低分别如下： 置换（&gt;）：一个核苷酸被另一个核苷酸替代，使用“&gt;”来表示；例如g.1318G&gt;T； 缺失（del）:一个或多个核苷酸被移除，使用“del”进行描述；例如g.3661_3706del； 倒置（inv）: 与原始序列反向互补的新的核苷酸序列（大于1个核苷酸）替换原始序列，例如由CTCGA变为TCGAG，使用”inv“表示； 重复（dup）：一个或多个核苷酸拷贝直接插入原始序列的下游，使用“dup”表示； 插入（ins）：序列中插入一个或多个核苷酸，并且插入序列并非上游序列拷贝； 缺失-插入（delins/indel）:一个或多个核苷酸被其他核苷酸替代，但并不是发生替代、倒置和转置； 转换（con）：一种特殊类型的缺失-插入，其中替代原始序列的核苷酸序列是来自基因组中另一个位点的序列拷贝； 5、变异描述示例DNA水平 c.76A&gt;C：76位的核苷酸A变异为C； c.82_83delTG：位于82和83位点上的核苷酸TG缺失，ACTTTGTGCC变异为ACTTTGCC（A是第76位）； c.83_84dupTG： ACTTTGTGCC（A为第76位）的83-84位之间插入短的串联重复序列TG，变为ACTTTGTGTGCC； g.333_590con1844_2011：基因组中编号为333-590的核苷酸序列替代1844-2011原有序列，插入其中； g.112_117delinsTG：在基因组序列编号为112-117之间的6个核苷酸被TG替换；多个变异使用”[]”标注变异，并用“；”链接 同一等位基因发生多个变异： c.[76A &gt;C;83G&gt;C]：同一染色体上76位和83位发生两个变异（顺式）； 不同等位基因发生多个变异： c.[76A &gt;C];[83G&gt;C]：两个变异发生在不同染色体上（反式）； 不确定多个变异发生的位置： c.[76A &gt;C](;)[83G&gt;C]：两个变异可能发生在同一染色体，也可能发生在不同染色体上，用(;)来链接； 定义重复序列的核苷酸范围及重复单位的数量，并用“[]”表示 g.123_124[4]:基因组序列中第123-124间的核苷酸重复出现4次； 对于短的/简单的重复，可以展示重复序列 g．123TG[4]:基因组序列中从123位开始TG核苷酸重复出现4次； 当重复序列长度不确定时，使用括号进行指定 g.-128GGC[(600-800)]:基因组编码区上游128位核苷酸处重复插入GGC，重复次数在600-800之间；蛋白质水平 替换：如p.Trp26Cys，表示第26位的Trp被Cys取代（错义突变）；p.Trp26Ter (p.Trp26*)，表示第26位的Trp变为终止密码（无义突变）；p.Cys123=，表示基因突变之后，氨基酸没有发生改变（同义突变）； 缺失：如p.Ala3_Ser5del，表示多肽序列中从第3位的Ala到第5位的Ser发生了缺失； 插入：如p.Lys2_Gly3insGlnSerLys，表示在第2位的Lys和第3位的Gly之间插入了GlnSerLys； 插入缺失：如p.Cys28delinsTrpVal，表示第28位的Cys缺失，同时被TrpVal取代； 重复：如p.Ala2[10]，表示第2位的Ala重复了10次； 移码突变：在起始密码子和终止密码子之间的读码框发生了改变；以“fx”进行表示；如p.Arg97ProfsTer23，表示第97位的Arg是首个发生改变的氨基酸，且Arg变为Pro，同时发生移码突变后，终止密码的位置变为第23位； 变异类型的描述原则一般原则： 所有变异需先从最基本的层面（DNA水平）进行描述，还可从RNA水平和蛋白质水平上进行描述。 用变异的描述是否加“（）” 来说明变异是由实验确定的还是从理论上推导出来的。 所有的变异都应该根据公认的参考序列来描述。 当变异可描述为几种变异类型时，优先级为：(1)替换，(2)删除，(3)倒位，(4)重复，(5)转换，(6)插入。如：当一个变异可以被描述为重复或插入时，根据优先级决定应描述为重复，而不是插入。如：“-ATGCCCA-”插入后变为“-ATGCCCCA-”，应描述为c.7dup，而不是NM_004006.2:c.7_8insC。 在进行变异描述时，基因的描述要采用HGNC的官方基因名。 重复序列变异描述原则：c.123CAG[16]，g.3258796GA[8]对于编码区DNA序列而言，重复序列的描述仅用于重复单元长度为3的倍数的重复序列，即不会影响阅读框的重复单元长度；若重复序列长度不是3的倍数，则不能用该形式描述。如：当CNPTAB基因在编码区的TATA序列后插入TATATATA序列，则对于该插入变异的描述应为NM_024312.4:c.1741_1742insTATATATA，而不是NM_024312.4:c.1738TA[6]。 3’ 端法则：变异的描述需遵循最靠近3’ 端法则。如“-ATGCCCCA-”变异成“-ATGC_CCA-”，根据3’ 端法则应描述为c.7delC，而不是c.5delC。例外：当缺失/重复发生在外显子与外显子衔接处，且衔接处碱基相同，不遵循3’ 端法则。如“..GAT gta..//..cag TCA..”缺失后变为“..GA_ gta..//..cag TCA..”，应描述为NM_004006.2:c.3921del，而不是NM_004006.2:c.3922del；“..GAT gta..//..cag TCA..”重复后变为“..GATT gta..//..cag TCA..”，应描述为NM_004006.2:c.3921dup，而不是NM_004006.2:c.3922dup。 有大量的原始检测软件，在未进行变异注释前是进行的左对齐 delins原则：涉及两个或以上连续核苷酸的替换，描述为delins。若两个变异被一个或多个核苷酸分隔，优先单独描述两个变异，而不采用delins合并描述；若被一个核苷酸分隔的两个变异，共同影响一个氨基酸，则合并描述为delins，如c.142_144delinsTGG (p.Arg48Trp)； 若两个变异中的任何一个为已知的高频变异位点，则需要单独描述两个变异，即NM_004006.1:c.[145C&gt;T;147C&gt;G]，优先于NM_004006.1:c.145_147delinsTGG。(该原则需与解读同事讨论) 起始密码子变异描述：描述取决于变异对蛋白产物改变的结果。 变异后不产生蛋白质：NM_003002.3:p.0 变异对蛋白产物的影响不清楚且无法预测：NM_003002.3:p.? 变异后产生新的起始氨基酸： a）上游：p.Met1ext-5，即原始密码子上游第5位（5’ UTR区）产生了新的起始氨基酸，另可描述为p.Met1extMet-5。 b）下游：p.Leu2_Met124del，即原起始氨基酸丢失且下游产生新的起始氨基酸，导致蛋白的第1到123位氨基酸缺失。终止密码子变异描述：采用“ext”描述 p.Ter110Glnext17(p.110Glnext17)：原终止氨基酸变为谷氨酰胺（Gln），并在下游第17位产生新的终止氨基酸，导致蛋白产物延长17个氨基酸。注：不可描述为p.Ter110GlnextTer17 ，此处的17代表的是位置（3’ UTR区） p.Ter315TyrextAsnLysGlyThrTer(p.315TyrextAsnLysGlyThr) ：原终止氨基酸变为酪氨酸（Tyr），并在下游第5位产生新的终止氨基酸，导致蛋白产物延长5个氨基酸。 p.Ter327Argext?(p.327Argext*?)：原终止氨基酸变为精氨酸（Arg），导致蛋白产物延长，延长的长度未知。 附录1、特殊字符的含义 “+”：c.123+45A&gt;G（代表靠近内含子5’ 端的核苷酸发生变异） “-”：c.124-56C&gt;T（代表靠近内含子3’ 端核苷酸发生变异）；c.-142C&gt;G（代表5’ UTR区） “”：c.32G&gt;A（代表3’ UTR区）；P.Trp41*（代表终止氨基酸） “[]”：代表等位基因，“;”用来分隔变异和等位基因，如g.[123456A&gt;G;345678G&gt;C] 代表顺式，g.[123456A&gt;G];[345678G&gt;C]代表反式， g.123456A&gt;G(;)345678G&gt;C代表这两个变异的顺式反式未知。 “()”：用来表示不确定的或预测的结果，如p.(Ser123Arg) “?”：用来表示核苷酸或氨基酸的位置未知，如g. (?_234567) _ (345678_?)del “^”：代表“或”的意思，如p.Ser124Arg反推核苷酸的改变为c.(370A&gt;C^372C&gt;G^372C&gt;A) ，即AGC变成CGC, AGG或AGA “/”：表示嵌合体（mosaic），如NM_004006.1:c.145=/C&gt;T “//”：表示异源嵌合体（chimeric），如NM_004006.1:c.85=//T&gt;C “|”：代表不是序列的直接改变，而是一种修饰或一种状态的改变，如甲基化。 “::”：用于描述RNA融合转录本和断点连接形成的环状染色体 缩写字符的含义 “fs”代表变异类型为移码变异，主要是针对蛋白水平而言，如p.Arg456GlyfsTer17或p.Arg456Glyfs*17 “ext”代表变异类型为延长，主要是针对起始密码子和终止密码子变异导致的蛋白水平的改变，如p.Met1ext-5 “gom” 表示获得甲基化，如g.12345678_12345901|gom “lom”表示去甲基化，如g.12345678_12345901|lom “met” 表示甲基化，如g.12345678_12345901|met= 软件脚本3’对齐针对变异进行3’对其的工具，可以参考GitHub仓库的脚本 toolkits\03.Deal_mutation\Realn4vcf.py 左对齐目前仍有大量软件本身是进行左对齐的，因此在进行结果比较时可以统一进行左对齐，从而实现变异结果的规范化，进行比较。对齐的命令行1234567891011Commands used are: bcftools norm -f ref.fa in.vcf -O z &gt; out.vcf.gz java -jar GenomeAnalysisTK.jar -T LeftAlignAndTrimVariants --trimAlleles -R ref.fa --variant in.vcf.gz -o out.vcf.gz vt normalize -r ref.vcf.gz -o out.vcf.gzVersions are: bcftools v0.2.0-rc8-5-g0e06231 (using htslib 0.2.0-rc8-6-gd49dfa6) [updated non release development version] GATK v3.1-1-g07a4bf8 vt normalize v0.5 参考资料： WIKIHGVSHGVS Recommendations for the Description of Sequence Variants: 2016 Update]]></content>
      <categories>
        <category>NGS</category>
        <category>肿瘤检测</category>
        <category>标准化</category>
      </categories>
      <tags>
        <tag>变异检测</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[肿瘤NGS检测方法开发方法学 - MSK方法学资料]]></title>
    <url>%2F01.%E7%9F%A5%E8%AF%86-03.standard%2FStandard-Method-MSK_method%2F</url>
    <content type="text"><![CDATA[参考资料EVALUATION OF AUTOMATIC CLASS III DESIGNATION FOR MSK-IMPACT (Integrated Mutation Profiling of Actionable Cancer Targets)A Hybridization Capture-Based Next-Generation Sequencing Clinical Assay for Solid Tumor Molecular OncologyTumour lineage shapes BRCA-mediated phenotypes 方法介绍样本准备选择标准 样本肿瘤细胞含量超过20% ； 针对MSI检测，肿瘤细胞含量应该超过25% ； DNA需要进行质控和富集，进行方法学测试的DNA 需要达到100ng~250ng； 平均插入片段（Average fragment）应该在200bp左右； 建库前应该存放在-20℃ （The DNA can be stored at 37°C for 10-20 minutes, stored at 2–8°C for 24 hours, or at –20°C for longer periods）。 文库构建…… 表现评估Determination of pipeline thresholdsRequirements on exon coverage were established不同的测序深度下，观测到的变异频率和理论变异频率之间会存在波动，导致检测结果和理论结果不一致（但理论上大概率会处在某个范围内，理论计算如下表 ： 突变实际频率 置信区间 测序深度 检测值范围 10% 95 500 7.5% ~ 13% 10% 95 100x 5.0% ~ 17.6% 为了验证这个理论，使用了10个 正常的FFPE 进行测试，实际测试结果如下： 突变实际频率 置信区间 测序深度 检测值范围 10% 95 平均深度 480x 5.0% ~ 13.9% 测试结果和预测结果相对比较一致。 这个数据支持我们检测10%的目标变异时，使用5%作为检测的阈值 。 Requirements on sample coverageBased on the calculations, 98% of exons can be expected to be sequenced to coverage greater than 100X, when mean sample coverage is 185X (0.54* 185X = 100X). (A 100X minimum coverage threshold per exon is required based on the power calculations, which showed 100X coverage was necessary to call mutations with true underlying mutation frequency 10% or greater, with 95% power at an alpha level of 0.05).To be conservative, a threshold of 200X on mean sample coverage is used to determine if a sample is sequenced to sufficient depth for subsequent analysis. A sample is flagged as being at increased risk of false negatives if its mean coverage is below 200X. Requirements on mutation coverage, allele depth and frequency for positive calls]]></content>
      <categories>
        <category>NGS</category>
        <category>肿瘤检测</category>
        <category>方法学</category>
      </categories>
      <tags>
        <tag>方法学</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[变异注释功能标准化 - Sequence Ontology]]></title>
    <url>%2F01.%E7%9F%A5%E8%AF%86-04.Database%2FStandard-Sequence_Ontology%2F</url>
    <content type="text"><![CDATA[参考资料参考网站新版网站github网址 为什么要标准化？]]></content>
      <categories>
        <category>NGS</category>
        <category>肿瘤检测</category>
        <category>标准化</category>
      </categories>
      <tags>
        <tag>注释</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[流程图绘制 - diagrams]]></title>
    <url>%2F03.%E5%BA%94%E7%94%A8-99.other%2FSoftware-%E6%B5%81%E7%A8%8B%E5%9B%BE%E7%BB%98%E5%88%B6-diagrams%2F</url>
    <content type="text"><![CDATA[安装Web在线版本 本地版安装 支持Windows、Mac、Linux等。 也可以使用 VS code进行相关流程图的绘制，需要安装Draw.io VS Code Integration 扩展。 介绍软件的使用，都是可以通过UI进行拖拽操作的，因此整体使用较为简单，在此不进行赘述。但是在此稍微介绍一些整理收集的流程图绘制规范，来帮助我们作出更规范/清晰的流程图。优秀的流程图需要遵循一定的规范，包括符号规范、结构规范、路径规范等。只要熟练掌握这些基础规范，我们每个人都能做出优秀流程图。 流程图是什么？流程图=流程+图。 所谓流程，IS09000系列国际标准中将流程定义为一组将输入转化为输出的相互群或相互作用的活动。流程有六个要素构成，分别是: - 流程的输入资源 - 流程中的若干活动、 - 活动的相互作用、 - 输出结果、 - 顾客、 - 最终流程创造的价值。 一个流程会将这6个要素有序串联起来，而流程图则是承载上述程序的图形载体。根据流程图“流动”信息的不同，又可以细分为产品流程图、数据流程图、程序流程图等，比如： 页面流程图，呈现的是页面跳转顺序； 数据流程图，用于表达数据的流转。 流程图的符号规范流程图中的每个符号都有着特定含义。 流程图的结构规范流程图有三大结构，分别是顺序结构、选择结构和循环结构。 参考信息https://zhuanlan.zhihu.com/p/347119698]]></content>
      <categories>
        <category>software</category>
        <category>Windows</category>
        <category>Mac</category>
      </categories>
      <tags>
        <tag>流程图</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[变异检出标准化-HGVS]]></title>
    <url>%2F01.%E7%9F%A5%E8%AF%86-03.standard%2FStandard-%E5%8F%98%E5%BC%82%E6%A3%80%E5%87%BA%E6%B3%A8%E9%87%8A%E6%A0%87%E5%87%86%E5%8C%96-HGVS-pHGVS%2F</url>
    <content type="text"><![CDATA[参考资料： WIKIHGVSHGVS Recommendations for the Description of Sequence Variants: 2016 Update]]></content>
      <categories>
        <category>NGS</category>
        <category>肿瘤检测</category>
        <category>标准化</category>
      </categories>
      <tags>
        <tag>变异检测</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[R绘图 - ggplot常用参数记录]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-03.R%2FR-ggplot-%E7%94%A8%E5%8F%82%E6%95%B0%E8%AE%B0%E5%BD%95%2F</url>
    <content type="text"><![CDATA[数据读取如果计划使用ggplot绘图，推荐使用数列方式整理数据，数据之间用tab分割。读取后，默认是数据框格式。然后然后根据所要绘制的图片，对数据进行解析。 折叠代码示例 Type Cancer TMB Plasma hepatocellular_carcinoma 9.743589744 Plasma gastric_cancer 4.615384615 Plasma cervical_cancer 1.025641026 Plasma lung_cancer 4.102564103 Plasma lung_cancer 3.58974359 Plasma lung_cancer 5.641025641 Plasma cervical_cancer 1.025641026 Plasma lung_cancer 1.025641026 Plasma hepatocellular_carcinoma 2.564102564 Plasma lung_cancer 2.564102564 Plasma pancreatic_cancer 1.538461538 Plasma lung_cancer 9.743589744 Plasma lung_cancer 1.538461538 Plasma lung_cancer 1.025641026 Plasma lung_cancer 3.58974359 Plasma lung_cancer 3.076923077 Plasma hepatocellular_carcinoma 2.564102564 Plasma colorectal_cancer 1.025641026 Plasma lung_cancer 6.666666667 Plasma lung_cancer 13.84615385 图片类型柱状图：geom_bar（）1position=&quot;dodge&quot; (多组柱状图平行排列，模式是堆叠柱状图) 箱线图： geom_boxplot1aes(fill=Type) （分类分别绘制箱线图） 坐标轴刻度12scale_x_continuous(breaks=seq(start,end,step)) # 设置x轴对应坐标轴的起始、终止和步长信息。也可以使用数组。scale_y_continuous(breaks=seq(start,end,step)) # 设置y轴对应坐标轴的起始、终止和步长信息。也可以使用数组。 标题格式theme对标签文本的格式进行调整 1theme(axos.text.x=element_text(hjust=0.5,size=8,angle=45)) #hjust对应偏移量，size对应文本字体大小，angle对应字体倾斜角度。 标签格式s 图片截取图片绘制完成后，只截取其中的一部分进行展示，通过改名了，可以调整结果途中展示的数据范围。避免因为某些异常值，导致整个图片展示出现偏差。 1coord_cartesian(xlim=c(0,100),ylim=c(10,50)) # 根据x轴y轴的数据，对图片进行截取，仅保留在截取范围内的图片。 添加辅助线添加水平线geom_hline()1geom_hline(yintercept=10) #添加一天y=10的辅助线； 添加辅助标签 geom_text（）12aes(x=1,y=1,label=as.character(&quot;lab_test&quot;)) #在x=1，y=1的位置添加一个标签，标签内容为“lab_test”；col=&quot;black&quot; #设置标签的颜色；]]></content>
      <categories>
        <category>编程拾慧</category>
        <category>R</category>
      </categories>
      <tags>
        <tag>可视化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Software-Mac环境管理软件-Homebrew简介和基本使用]]></title>
    <url>%2F03.%E5%BA%94%E7%94%A8-04.Mac%2FSoftware-Mac%E7%8E%AF%E5%A2%83%E7%AE%A1%E7%90%86%E8%BD%AF%E4%BB%B6-Homebrew%E7%AE%80%E4%BB%8B%E5%92%8C%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[Homebrew官网简介Homebrew是一个开源的包管理器，可以帮助在mac上对一些软件或者命令行工具进行管理，从而补充一些mac上本身缺少但是实用中需要的重要命令，比如 wget等 安装默认安装 首先确认Mac已安装Xcode、Command Line Tool。安装Command Line Tool： 1xcode-select --install 然后把下面的代码粘贴到Terminal中执行安装Homebrew。 1/usr/bin/ruby -e &quot;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)&quot; Homebrew 会将软件包安装到独立目录，并将其文件软链接至 /usr/local 。Homebrew 不会将文件安装到它本身目录之外，所以您可将 Homebrew 安装到任意位置。 指定路径安装安装到/usr/local/homebrew mac 打开终端输入 1mkdir homebrew &amp;&amp; curl -L https://github.com/Homebrew/homebrew/tarball/master | tar xz --strip 1 -C homebrew 在bash文件中 12homebrew=/usr/local/homebrew/bin:/usr/local/homebrew/sbinexport PATH=$homebrew:$PATH 最后更新一下 12source .bash_profilebrew update 卸载1234567$ cd `brew --prefix`$ rm -rf Cellar$ brew prune$ rm `git ls-files`$ rm -r Library/Homebrew Library/Aliases Library/Formula Library/Contributions$ rm -rf .git$ rm -rf ~/Library/Caches/Homebrew 使用 安装任意包的命令 12$ brew install &lt;packageName&gt;$ brew install wget #示例：安装wget 卸载任意包 12$ brew uninstall &lt;packageName&gt;$ brew uninstall git #示例：卸载git 查询可用包 1$ brew search &lt;packageName&gt; 查看已安装包列表 1$ brew list 查看任意包信息 1$ brew info &lt;packageName&gt; 更新Homebrew 1$ brew update 查看Homebrew版本 1$ brew -v Homebrew帮助信息 1$ brew -h 所有可以安装的包列表]]></content>
      <categories>
        <category>software</category>
        <category>Mac</category>
      </categories>
      <tags>
        <tag>环境配置</tag>
        <tag>mac</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python 基础知识 - 文件读写]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-02.python%2FPython-%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86-%E6%96%87%E4%BB%B6%E8%AF%BB%E5%86%99%2F</url>
    <content type="text"><![CDATA[文本文件的读写创建一个文件对象1对象名 = open(文件名,&quot;模式&quot;) 最常用的模式有： 1234567r 打开只读文件，该文件必须存在。r+ 打开可读写的文件，该文件必须存在。w 打开只写文件，若文件存在则文件长度清为0，即该文件内容会消失。若文件不存在则建立该文件。w+ 打开可读写文件，若文件存在则文件长度清为零，即该文件内容会消失。若文件不存在则建立该文件。a 以附加的方式打开只写文件。若文件不存在，则会建立该文件，如果文件存在，写入的数据会被加到文件尾，即文件原先的内容会被保留。a+ 以附加方式打开可读写的文件。若文件不存在，则会建立该文件，如果文件存在，写入的数据会被加到文件尾后，即文件原先的内容会被保留。上述的形态字符串都可以再加一个b字符，如rb、w+b或ab＋等组合，加入b 字符用来告诉函数库打开的文件为二进制文件，而非纯文字文件。 文件对象的方法读取：1234file.read() #读取整个文件，如果文件超过内存2倍，会报错file.read(N) #读取N bytes的数据file.readline() #读取一行file.readlines() #读取所有行，存到列表中，每个元素是一行； 写入：1file.write(&quot;text &quot;) #向文件对象file中写入内容； 关闭：1file.close(); 使用上下文管理器(with…as…),通过缩进确定代码块，确定文件的使用范围， 在使用文件开始时，自动执行特殊方法 enter() 在使用文件结束后，自动执行特殊方法 exit() 特殊方法，完成文件的关闭1234with open(&quot;new.txt&quot;, &quot;w&quot;) as f: #使用文件管理器打开文件 print(f.closed) f.write(&quot;Hello World!&quot;)print(f.closed) #缩进结束时，文件使用结束，自动关闭文件]]></content>
      <categories>
        <category>编程拾慧</category>
        <category>python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python 基础知识 - 文件读写]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-02.python%2FPython-%E4%BB%A3%E7%A0%81%E5%8A%A0%E5%AF%86-compile%2F</url>
    <content type="text"><![CDATA[Python解释器在执行代码的过程中，会首先生成.pyc文件，然后再解释执行.pyc中的内容，解释器也能直接执行.pyc文件。 .pyc文件是一个二进制的文件，是不具备可读性的。 假如我们发到客户环境时，是.pyc文件，而不是.py，那么是不是就可以保护我们的Python代码？ 想要做到这一点，并不难。Python标准库就提供了一个名叫compileall的库，使用它就可以做到。12345python -m compileall # 对目录下的所有py文件进行变异python -m py_compile file.pypython -m py_compile /root/src/&#123;file1,file2&#125;.py 编译成pyc文件。也可以写份脚本来做这事： 12import py_compilepy_compile.compile(&apos;path&apos;) //path是包括.py文件名的路径 用1python -O -m py_compile file.py 编译成pyo文件。1.其中的 -m 相当于脚本中的import，这里的-m py_compile 相当于上面的 import py_compile2.-O 如果改成 -OO 则是删除相应的 pyo文件，具体帮助可以在控制台输入 python -h 查看 ======================== from:http://blogold.chinaunix.net/u3/93255/showart_1944929.html什么是pyc文件 pyc是一种二进制文件，是由py文件经过编译后，生成的文件，是一种byte code，py文件变成pyc文件后，加载的速度有所提高，而且pyc是一种跨平台的字节码，是由python的虚拟机来执行的，这个是类似于JAVA或者.NET的虚拟机的概念。pyc的内容，是跟python的版本相关的，不同版本编译后的pyc文件是不同的，2.5编译的pyc文件，2.4版本的 python是无法执行的。什么是pyo文件pyo是优化编译后的程序 python -O 源文件即可将源程序编译为pyo文件 什么是pyd文件pyd是python的动态链接库。 为什么需要pyc文件 这个需求太明显了，因为py文件是可以直接看到源码的，如果你是开发商业软件的话，不可能把源码也泄漏出去吧？所以就需要编译为pyc后，再发布出去。当然，pyc文件也是可以反编译的，不同版本编译后的pyc文件是不同的，根据python源码中提供的opcode，可以根据pyc文件反编译出 py文件源码，网上可以找到一个反编译python2.3版本的pyc文件的工具，不过该工具从python2.4开始就要收费了，如果需要反编译出新版本的pyc文件的话，就需要自己动手了（俺暂时还没这能力^–^）,不过你可以自己修改python的源代码中的opcode文件，重新编译 python，从而防止不法分子的破解。生成单个pyc文件 python就是个好东西，它提供了内置的类库来实现把py文件编译为pyc文件，这个模块就是 py_compile 模块。 使用方法非常简单，如下所示，直接在idle中，就可以把一个py文件编译为pyc文件了。(假设在windows环境下)1234567import py_compilepy_compile.compile(r&apos;H:\game\test.py&apos;)compile函数原型：compile(file[, cfile[, dfile[, doraise]]]) file 表示需要编译的py文件的路径 cfile 表示编译后的pyc文件名称和路径，默认为直接在file文件名后加c 或者 o，o表示优化的字节码 dfile 这个参数英文看不明白，请各位大大赐教。(鄙视下自己)原文：it is used as the name of the source file in error messages instead of file doraise 可以是两个值，True或者False，如果为True，则会引发一个PyCompileError，否则如果编译文件出错，则会有一个错误，默认显示在sys.stderr中，而不会引发异常 (来自python2.5文档)批量生成pyc文件 一般来说，我们的工程都是在一个目录下的，一般不会说仅仅编译一个py文件而已，而是需要把整个文件夹下的py文件都编译为pyc文件，python又为了我们提供了另一个模块：compileall 。使用方法如下：123import compileallcompileall.compile_dir(r&apos;H:\game&apos;) 也可以直接用命令行编译一个目录下的文件，如：# python -m compileall /root/src/ 这样就把game目录，以及其子目录下的py文件编译为pyc文件了。嘿嘿，够方便吧。来看下compile_dir函数的说明：1compile_dir(dir[, maxlevels[, ddir[, force[, rx[, quiet]]]]]) dir 表示需要编译的文件夹位置 maxlevels 表示需要递归编译的子目录的层数，默认是10层，即默认会把10层子目录中的py文件编译为pyc ddir 英文没明白，原文：it is used as the base path from which the filenames used in error messages will be generated。 force 如果为True，则会强制编译为pyc，即使现在的pyc文件是最新的，还会强制编译一次，pyc文件中包含有时间戳，python编译器会根据时间来决定，是否需要重新生成一次pyc文件 rx 表示一个正则表达式，比如可以排除掉不想要的目录，或者只有符合条件的目录才进行编译 quiet 如果为True，则编译后，不会在标准输出中，打印出信息 (来自python2.5文档) 总结 通过上面的方法，可以方便的把py文件编译为pyc文件了，从而可以实现部分的源码隐藏，保证了python做商业化软件时，保证了部分的安全性吧，继续学习下，看怎么修改opcode。]]></content>
      <categories>
        <category>编程拾慧</category>
        <category>python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python 包的发布]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-02.python%2FPython-%E5%8C%85-%E9%A1%B9%E7%9B%AE%E7%AE%A1%E7%90%86-%E5%8F%91%E5%B8%83%E8%87%AA%E5%B7%B1%E5%BC%80%E5%8F%91%E7%9A%84python%E5%8C%85%2F</url>
    <content type="text"><![CDATA[账号注册要进行Python包的发布，首先需要在 pypi 注册一个账号。 本地创建项目创建项目参考文档pyscaffold，进行项目的创建和配置。 包发布的命令123456# 构建项目# pip install toxtox -e build # 发布tox -e publish -- --repository pypi --verbose 其他项目管理包hatchhatch github]]></content>
      <categories>
        <category>编程拾慧</category>
        <category>python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python 性能调优 代码加速]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-02.python%2Fpython-%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%98-%E5%88%86%E6%9E%90%E5%8A%A0%E9%80%9F%2F</url>
    <content type="text"><![CDATA[代码逻辑优化参考 https://zhuanlan.zhihu.com/p/152343123 利用缓存进行提速lru_cache使用方法简介程序的核心或性能的瓶颈往往就是那么几个函数或语句，当你发现同一组参数会频繁地传入到这些函数里的时候，你就要考虑设一个缓存了。比如说小明写了一个函数叫”add”，它接收两个浮点数或整数作为参数，返回这两个数的和。但是由于小明写的”add”函数算法异常复杂，甚至需要1秒来计算。因为我们知道，一个数加另一个数的结果肯定是确定的，也就是说，你只要算出一次这个结果，下次再出现这两个值的时候，你完全可以直接用上次的计算结果。这时，我们可以直接使用 lru_cache 来帮我们进行历史分析数据结果的缓存。lru_cache,是一个提供缓存功能的装饰器，可以缓存函数历史处理结果遇到相同的输入直接反馈历史分析结果示例代码：12345678910111213141516In [1]: from functools import lru_cacheIn [2]: @lru_cache() ...: def add(a, b): ...: from time import sleep ...: sleep(1) ...: return a+b ...:In [3]: %time add(1,1) # 第一次执行，因为&quot;计算量大&quot;，所以比较慢Wall time: 1.01 sOut[3]: 2In [4]: %time add(1,1) # 第二次执行，因为结果已经被缓存了，所以很快Wall time: 0 nsOut[4]: 2 局限性缓存机制的底层原理，是通过缓存存储结果，避免结果的重复计算，所以当数据集中，函数运行的输入参数并非存在高比例重复时，该方法则不能有效进行提速。 编译函数numba使用方法简介python之所以慢，是因为它是靠CPython编译的。numba是一款可以将python函数编译为机器代码的JIT编译器，经过numba编译的python代码（仅限数组运算），其运行速度可以接近C或FORTRAN语言。使用numba非常简单，只需要将numba装饰器应用到python函数中，无需改动原本的python代码，numba会自动完成剩余的工作。1234567891011import numpy as npimport numbafrom numba import jit@jit(nopython=True) # jit，numba装饰器中的一种def go_fast(a): # 首次调用时，函数被编译为机器代码 trace = 0 # 假设输入变量是numpy数组 for i in range(a.shape[0]): # Numba 擅长处理循环 trace += np.tanh(a[i, i]) return a + trace 在numba加速下，代码执行时间为3.63微秒/循环。不经过numba加速，代码执行时间为136微秒/循环，两者相比，前者快了40倍。 局限性numba不会对numpy和for循环以外的python代码有很大帮助。 TaichiTaichi 是一种嵌在 Python 中的并行编程语言，使用 Python 语言作为 DSL，所以我们可以在正常的 Python 代码中使用它。它可以帮助我们轻松编写可移植的高性能并行程序，专注于高性能计算和图形领域。安装使用上和 Numba 类似。 Codon使用方法简介12345678910111213141516171819202122232425262728293031import codonfrom time import timedef is_prime_python(n): if n &lt;= 1: return False for i in range(2, n): if n % i == 0: return False return True@codon.jitdef is_prime_codon(n): if n &lt;= 1: return False for i in range(2, n): if n % i == 0: return False return Truet0 = time()ans = sum(1 for i in range(100000, 200000) if is_prime_python(i))t1 = time()print(f'[python] &#123;ans&#125; | took &#123;t1 - t0&#125; seconds')# [python] 8392 | took 39.6610209941864 secondst0 = time()ans = sum(1 for i in range(100000, 200000) if is_prime_codon(i))t1 = time()print(f'[codon] &#123;ans&#125; | took &#123;t1 - t0&#125; seconds')# [codon] 8392 | took 0.998633861541748 seconds 更换解释器pypy使用方法简介去pypy官网的下载页面，选择合适的版本进行下载。下载好后，解压缩，就可以使用了。（建议将pypy添加到path环境变量里，这样使用起来会方便很多） pypy是一个Python解释器，兼容Python，却能比Python更快，使用pypy做解释器，几乎不需要改动代码，就可以实现加速。1234567891011121314import timedef work(): l = list(range(1, 10000001)) tmp = [] for i in l: if i % 2 == 0: tmp.append(i) for i in range(len(tmp)): tmp[i] = tmp[i] * 2 tmp.reverse() return sum(tmp)t = time.time()work()print(time.time()-t) 反正就是各种循环，然后测用时。 接着把这个程序存进a.py，然后分别使用pypy和python解释器执行测试速度。pypy执行：12$ pypy3 a.py0.1890418529510498 Python解释器执行：12$ python a.py0.9882152080535889]]></content>
      <categories>
        <category>编程拾慧</category>
        <category>python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python 进阶知识-调用c++实现局部性能调优]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-02.python%2FPython-%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%98-%E8%B0%83%E7%94%A8c%2B%2B%E5%AE%9E%E7%8E%B0%E5%B1%80%E9%83%A8%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%98%2F</url>
    <content type="text"><![CDATA[待整理补齐的坑https://python3-cookbook.readthedocs.io/zh_CN/latest/c15/p02_write_simple_c_extension_module.html#id1https://blog.csdn.net/m0_46429066/article/details/108491148]]></content>
      <categories>
        <category>编程拾慧</category>
        <category>python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python 基础知识 - 序列的方法]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-02.python%2FPython-%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86-%E5%BA%8F%E5%88%97%E7%9A%84%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[快速教程中，我们了解了最基本的序列(sequence)。回忆一下，序列包含有定值表(tuple)和表(list)。此外，字符串(string)是一种特殊的定值表。表的元素可以更改，定值表一旦建立，其元素不可更改。 任何的序列都可以引用其中的元素(item)。 下面的内建函数(built-in function)可用于序列(表，定值表，字符串)：s为一个序列12345len(s) 返回： 序列中包含元素的个数min(s) 返回： 序列中最小的元素max(s) 返回： 序列中最大的元素all(s) 返回： True, 如果所有元素都为True的话any(s) 返回： True, 如果任一元素为True的话 下面的方法主要起查询功能，不改变序列本身, 可用于表和定值表：1234sum(s) 返回：序列中所有元素的和# x为元素值，i为下标(元素在序列中的位置)s.count(x) 返回： x在s中出现的次数s.index(x) 返回： x在s中第一次出现的下标 由于定值表的元素不可变更，下面方法只适用于表：12345678# l为一个表, l2为另一个表l.extend(l2) 在表l的末尾添加表l2的所有元素l.append(x) 在l的末尾附加x元素l.sort() 对l中的元素排序l.reverse() 将l中的元素逆序l.pop() 返回：表l的最后一个元素，并在表l中删除该元素del l[i] 删除该元素(以上这些方法都是在原来的表的上进行操作，会对原来的表产生影响，而不是返回一个新表。) 下面是一些用于字符串的方法。尽管字符串是定值表的特殊的一种，但字符串(string)类有一些方法是改变字符串的。这些方法的本质不是对原有字符串进行操作，而是删除原有字符串，再建立一个新的字符串，所以并不与定值表的特点相矛盾。12345678910111213141516171819202122232425262728293031323334353637383940#str为一个字符串，sub为str的一个子字符串。s为一个序列，它的元素都是字符串。width为一个整数，用于说明新生成字符串的宽度。str.count(sub) 返回：sub在str中出现的次数str.find(sub) 返回：从左开始，查找sub在str中第一次出现的位置。如果str中不包含sub，返回 -1str.index(sub) 返回：从左开始，查找sub在str中第一次出现的位置。如果str中不包含sub，举出错误str.rfind(sub) 返回：从右开始，查找sub在str中第一次出现的位置。如果str中不包含sub，返回 -1str.rindex(sub) 返回：从右开始，查找sub在str中第一次出现的位置。如果str中不包含sub，举出错误str.isalnum() 返回：True， 如果所有的字符都是字母或数字str.isalpha() 返回：True，如果所有的字符都是字母str.isdigit() 返回：True，如果所有的字符都是数字str.istitle() 返回：True，如果所有的词的首字母都是大写str.isspace() 返回：True，如果所有的字符都是空格str.islower() 返回：True，如果所有的字符都是小写字母str.isupper() 返回：True，如果所有的字符都是大写字母str.split([sep, [max]]) 返回：从左开始，以空格为分割符(separator)，将str分割为多个子字符串，总共分割max次。将所得的子字符串放在一个表中返回。可以str.split(&apos;,&apos;)的方式使用逗号或者其它分割符str.rsplit([sep, [max]]) 返回：从右开始，以空格为分割符(separator)，将str分割为多个子字符串，总共分割max次。将所得的子字符串放在一个表中返回。可以str.rsplit(&apos;,&apos;)的方式使用逗号或者其它分割符str.join(s) 返回：将s中的元素，以str为分割符，合并成为一个字符串。str.strip([sub]) 返回：去掉字符串开头和结尾的空格。也可以提供参数sub，去掉位于字符串开头和结尾的sub str.replace(sub, new_sub) 返回：用一个新的字符串new_sub替换str中的substr.capitalize() 返回：将str第一个字母大写str.lower() 返回：将str全部字母改为小写str.upper() 返回：将str全部字母改为大写str.swapcase() 返回：将str大写字母改为小写，小写改为大写str.title() 返回：将str的每个词(以空格分隔)的首字母大写str.center(width) 返回：长度为width的字符串，将原字符串放入该字符串中心，其它空余位置为空格。str.ljust(width) 返回：长度为width的字符串，将原字符串左对齐放入该字符串，其它空余位置为空格。str.rjust(width) 返回：长度为width的字符串，将原字符串右对齐放入该字符串，其它空余位置为空格。]]></content>
      <categories>
        <category>编程拾慧</category>
        <category>python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python 基础知识]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-02.python%2FPython-%E4%BB%A3%E7%A0%81%E6%89%93%E5%8C%85-pyinstall%E5%B0%86%E8%84%9A%E6%9C%AC%E6%89%93%E5%8C%85%E5%88%B6%E4%BD%9C%E6%88%90%E5%8F%AF%E6%89%A7%E8%A1%8C%E7%A8%8B%E5%BA%8F%2F</url>
    <content type="text"><![CDATA[在支持PIP的情况下pip install pyinstaller安装 pyinstaller 后执行pyinstaller --version查看所安装的版本 pyinstaller : 打包可执行文件的主要命令，详细用法下面会介绍。 pyi-archive_viewer : 查看可执行包里面的文件列表。 pyi-bindepend : 查看可执行文件依赖的动态库（.so或.dll文件） pyi-... : 等等。 打包程序pyinstaller mycript.py然后会看到新增加了两个目录build和dist，dist下面的文件就是可以发布的可执行文件，对于上面的命令你会发现dist目录下面有一堆文件，各种都动态库文件和myscrip可执行文件。有时这样感觉比较麻烦，需要打包dist下面的所有东西才能发布，万一丢掉一个动态库就无法运行了，好在pyInstaller支持单文件模式，只需要执行： pyinstaller -F mycript.py 你会发现dist下面只有一个可执行文件，这个单文件就可以发布了，可以运行在你正在使用的操作系统类似的系统的下面。 可能问题 注意点12341.windows系统的版本和位数 （mac系统和linux 没有进行测试过）2.python3的版本和位数3.pyqt5的版本和位数 （如果pip安装，则位数同python3）4.pyinstaller的版本和位数（一般pip安装，无需考虑位数） 为程序添加图标运行出现cmd窗口取消cmd窗口弹出的参考方式如下： 方法一：pyinstaller -F mycode.py --noconsole 方法二：pyinstaller -F -w mycode.py （-w就是取消窗口） pyinstaller打成的包，可以在64位操作系统使用，无法在32位操作系统使用1234567坑的成因：python存在64位版本和32位版本。64位版本打成的包，只能在64位操作系统使用。32位版本打成的包，即可以在64位操作系统使用，也可以在32位操作系统使用。解决方案：重新安装32位版本的python，进行开发。 pyinstaller打成的包，可以在win7以上操作系统使用，无法在xp操作系统使用12345坑的成因：python3 从3.5版本开始，就已经不支持xp操作系统了。解决方案：重新安装3.4版本的python，进行开发。 pyqt5应用，开发运行时是正常，但pyinstaller打成的包，界面失真变丑。12345678坑的成因：pyinstaller 不支持最新版本的pyqt5。解决方案：重新安装低版本的pyqt5，进行开发。（当前推荐：5.8.2版本）命令pip uninstall pyqt5pip install pyqt5==5.8.2 pyqt5应用，开发运行时是正常，无法打包成功或打包成功但pyinstaller打成的包，无法运行，提示failed to execute script xxx。12345678坑的成因：（同坑3）pyinstaller 不支持最新版本的pyqt5。解决方案：（同坑3）重新安装低版本的pyqt5，进行开发。（当前推荐：5.8.2版本）命令pip uninstall pyqt5pip install pyqt5==5.8.2 pyqt5、pyqt5-tools 安装失败12345坑的成因：你的python3可能是最新版本，pyqt5、pyqt5-tools、pyqtchart还不支持最新版本的python3解决方案：重新安装低版本的python3，进行开发。（当前推荐：3.6.6版本） pyqtchart、pyqtdatavisualization 安装失败123456坑的成因：pyqtchart、pyqtdatavisualization对pyqt5的版本有依赖需求。解决方案：针对pyqt5的版本进行安装。命令如： pip install pyqtchart==5.8 打包opencv 过程中，部分包加载异常12345678ImportError: OpenCV loader: missing configuration file: ['config.py']. Check OpenCV installation.解决方案：import cv2print(cv2.__file__) # 这里我得到的是 D:\ProgramData\Anaconda3\lib\site-packages\cv2\__init__.py# 在使用 pyinstaller 时，加入 paths 选项：pyinstaller main.py -F --paths="D:\ProgramData\Anaconda3\lib\site-packages\cv2"]]></content>
      <categories>
        <category>编程拾慧</category>
        <category>python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python 编码规范]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-02.python%2FPython-%E7%BC%96%E7%A0%81%E8%A7%84%E8%8C%83%2F</url>
    <content type="text"><![CDATA[item detail PEP 8 Title Style Guide for Python Code Version c451868df657 Last-Modified 2016-06-08 10:43:53 -0400 (Wed, 08 Jun 2016) Author Guido van Rossum , Barry Warsaw , Nick Coghlan Status Active Type Process Content-Type text/x-rst Created 05-Jul-2001 Post-History 05-Jul-2001, 01-Aug-2013 Introduction 介绍本文提供的Python代码编码规范基于Python主要发行版本的标准库。Python的C语言实现的C代码规范请查看相应的PEP指南1。 这篇文档以及PEP 257（文档字符串的规范）改编自Guido原始的《Python Style Guide》一文，同时添加了一些来自Barry的风格指南2。 这篇规范指南随着时间的推移而逐渐演变，随着语言本身的变化，过去的约定也被淘汰了。 许多项目有自己的编码规范，在出现规范冲突时，项目自身的规范优先。 A Foolish Consistency is the Hobgoblin of Little Minds 尽信书,则不如无书Guido的一条重要的见解是代码阅读比写更加频繁。这里提供的指导原则主要用于提升代码的可读性，使得在大量的Python代码中保持一致。就像PEP 20提到的，“Readability counts”。 这是一份关于一致性的风格指南。这份风格指南的风格一致性是非常重要的。更重要的是项目的风格一致性。在一个模块或函数的风格一致性是最重要的。 然而，应该知道什么时候应该不一致，有时候编码规范的建议并不适用。当存在模棱两可的情况时，使用自己的判断。看看其他的示例再决定哪一种是最好的，不要羞于发问。 特别是不要为了遵守PEP约定而破坏兼容性！ 几个很好的理由去忽略特定的规则： 当遵循这份指南之后代码的可读性变差，甚至是遵循PEP规范的人也觉得可读性差。 与周围的代码保持一致（也可能出于历史原因），尽管这也是清理他人混乱（真正的Xtreme Programming风格）的一个机会。 有问题的代码出现在发现编码规范之前，而且也没有充足的理由去修改他们。 当代码需要兼容不支持编码规范建议的老版本Python。 Code lay-out 代码布局Indentation 缩进每一级缩进使用4个空格。 续行应该与其包裹元素对齐，要么使用圆括号、方括号和花括号内的隐式行连接来垂直对齐，要么使用挂行缩进对齐3。当使用挂行缩进时，应该考虑到第一行不应该有参数，以及使用缩进以区分自己是续行。 推荐：1234567891011121314# 与左括号对齐foo = long_function_name(var_one, var_two, var_three, var_four)# 用更多的缩进来与其他行区分def long_function_name( var_one, var_two, var_three, var_four): print(var_one)# 挂行缩进应该再换一行foo = long_function_name( var_one, var_two, var_three, var_four) 不推荐：123456789# 没有使用垂直对齐时，禁止把参数放在第一行foo = long_function_name(var_one, var_two, var_three, var_four)# 当缩进没有与其他行区分时，要增加缩进def long_function_name( var_one, var_two, var_three, var_four): print(var_one) 四空格的规则对于续行是可选的。可选：1234# 挂行缩进不一定要用4个空格foo = long_function_name( var_one, var_two, var_three, var_four) 当if语句的条件部分长到需要换行写的时候，注意可以在两个字符关键字的连接处（比如if），增加一个空格，再增加一个左括号来创造一个4空格缩进的多行条件。这会与if语句内同样使用4空格缩进的代码产生视觉冲突。PEP没有明确指明要如何区分i发的条件代码和内嵌代码。可使用的选项包括但不限于下面几种情况：123456789101112131415# 没有额外的缩进if (this_is_one_thing and that_is_another_thing): do_something()# 增加一个注释，在能提供语法高亮的编辑器中可以有一些区分if (this_is_one_thing and that_is_another_thing): # Since both conditions are true, we can frobnicate. do_something()# 在条件判断的语句添加额外的缩进if (this_is_one_thing and that_is_another_thing): do_something() （可以参考下面关于是否在二进制运算符之前或之后截断的讨论）在多行结构中的大括号/中括号/小括号的右括号可以与内容对齐单独起一行作为最后一行的第一个字符，就像这样：12345678my_list = [ 1, 2, 3, 4, 5, 6, ]result = some_function_that_takes_arguments( &apos;a&apos;, &apos;b&apos;, &apos;c&apos;, &apos;d&apos;, &apos;e&apos;, &apos;f&apos;, ) 或者也可以与多行结构的第一行第一个字符对齐，就像这样：12345678my_list = [ 1, 2, 3, 4, 5, 6,]result = some_function_that_takes_arguments( &apos;a&apos;, &apos;b&apos;, &apos;c&apos;, &apos;d&apos;, &apos;e&apos;, &apos;f&apos;,) Tabs or Spaces？ 制表符还是空格？空格是首选的缩进方式。制表符只能用于与同样使用制表符缩进的代码保持一致。Python3不允许同时使用空格和制表符的缩进。混合使用制表符和空格缩进的Python2代码应该统一转成空格。当在命令行加入-t选项执行Python2时，它会发出关于非法混用制表符与空格的警告。当使用–tt时，这些警告会变成错误。强烈建议使用这样的参数。 Maximum Line Length 行的最大长度所有行限制的最大字符数为79。 没有结构化限制的大块文本（文档字符或者注释），每行的最大字符数限制在72。 限制编辑器窗口宽度可以使多个文件并行打开，并且在使用代码检查工具(在相邻列中显示这两个版本)时工作得很好。 大多数工具中的默认封装破坏了代码的可视化结构，使代码更难以理解。避免使用编辑器中默认配置的80窗口宽度，即使工具在帮你折行时在最后一列放了一个标记符。某些基于Web的工具可能根本不提供动态折行。一些团队更喜欢较长的行宽。如果代码主要由一个团队维护，那这个问题就能达成一致，可以把行长度从80增加到100个字符（更有效的做法是将行最大长度增加到99个字符），前提是注释和文档字符串依然已72字符折行。 Python标准库比较保守，需要将行宽限制在79个字符（文档/注释限制在72）。 较长的代码行选择Python在小括号，中括号以及大括号中的隐式续行方式。通过小括号内表达式的换行方式将长串折成多行。这种方式应该优先使用，而不是使用反斜杠续行。 反斜杠有时依然很有用。比如，比较长的，多个with状态语句，不能使用隐式续行，所以反斜杠是可以接受的： 123with open(&apos;/path/to/some/file/you/want/to/read&apos;) as file_1, \ open(&apos;/path/to/some/file/being/written&apos;, &apos;w&apos;) as file_2: file_2.write(file_1.read()) （请参阅前面关于多行if-语句的讨论，以获得关于这种多行with-语句缩进的进一步想法。）另一种类似情况是使用assert语句。确保在续行进行适当的缩进。 Should a line break before or after a binary operator? 在二元运算符之前应该换行吗？几十年来，推荐的风格是在二元运算符之后中断。但是这回影响可读性，原因有二：操作符一般分布在屏幕上不同的列中，而且每个运算符被移到了操作数的上一行。下面例子这个情况就需要额外注意，那些变量是相加的，那些变量是相减的：123456# 不推荐: 操作符离操作数太远income = (gross_wages + taxable_interest + (dividends - qualified_dividends) - ira_deduction - student_loan_interest) 为了解决这种可读性的问题，数学家和他们的出版商遵循了相反的约定。Donald Knuth在他的Computers and Typesetting系列中解释了传统规则：“尽管段落中的公式总是在二元运算符和关系之后中断，显示出来的公式总是要在二元运算符之前中断”4。遵循数学的传统能产出更多可读性高的代码：123456# 推荐：运算符和操作数很容易进行匹配income = (gross_wages + taxable_interest + (dividends - qualified_dividends) - ira_deduction - student_loan_interest) 在Python代码中，允许在二元运算符之前或之后中断，只要本地的约定是一致的。对于新代码，建议使用Knuth的样式。 Blank Lines 空行顶层函数和类的定义，前后用两个空行隔开。 类里的方法定义用一个空行隔开。 相关的功能组可以用额外的空行（谨慎使用）隔开。一堆相关的单行代码之间的空白行可以省略（例如，一组虚拟实现 dummy implementations）。 在函数中使用空行来区分逻辑段（谨慎使用）。 Python接受control-L（即^L）换页符作为空格；许多工具把这些字符当作页面分隔符，所以你可以在文件中使用它们来分隔相关段落。请注意，一些编辑器和基于Web的代码阅读器可能无法识别control-L为换页，将在其位置显示另一个字形。 Source File Encoding 源文件编码Python核心发布版本中的代码总是以UTF-8格式编码（或者在Python2中用ASCII编码）。 使用ASCII（在Python2中）或UTF-8（在Python3中）编码的文件不应具有编码声明。 在标准库中，非默认的编码应该只用于测试，或者当一个注释或者文档字符串需要提及一个包含内ASCII字符编码的作者名字的时候；否则，使用\x,\u,\U , 或者 \N 进行转义来包含非ASCII字符。 对于Python 3和更高版本，标准库规定了以下策略（参见 PEP 3131）：Python标准库中的所有标识符必须使用ASCII标识符，并在可行的情况下使用英语单词（在许多情况下，缩写和技术术语是非英语的）。此外，字符串文字和注释也必须是ASCII。唯一的例外是（a）测试非ASCII特征的测试用例，以及（b）作者的名称。作者的名字如果不使用拉丁字母拼写，必须提供一个拉丁字母的音译。 鼓励具有全球受众的开放源码项目采取类似的政策。 Imports 导入导入通常在分开的行，例如：123456#推荐: import osimport sys#不推荐: import sys, os 但是可以这样：1from subprocess import Popen, PIPE 导入总是位于文件的顶部，在模块注释和文档字符串之后，在模块的全局变量与常量之前。导入应该按照以下顺序分组： 标准库导入 相关第三方库导入 本地应用/库特定导入 你应该在每一组导入之间加入空行。 推荐使用绝对路径导入，如果导入系统没有正确的配置（比如包里的一个目录在sys.path里的路径后），使用绝对路径会更加可读并且性能更好（至少能提供更好的错误信息）:123import mypkg.siblingfrom mypkg import siblingfrom mypkg.sibling import example 然而，显示的指定相对导入路径是使用绝对路径的一个可接受的替代方案，特别是在处理使用绝对路径导入不必要冗长的复杂包布局时：12from . import siblingfrom .sibling import example 标准库要避免使用复杂的包引入结构，而总是使用绝对路径。 不应该使用隐式相对路径导入，并且在Python 3中删除了它。 当从一个包含类的模块中导入类时，常常这么写：12from myclass import MyClassfrom foo.bar.yourclass import YourClass 如果上述的写法导致名字的冲突，那么这么写：12import myclassimport foo.bar.yourclass 然后使用“myclass.MyClass”和“foo.bar.yourclass.YourClass”。 避免通配符的导入（from import *），因为这样做会不知道命名空间中存在哪些名字，会使得读取接口和许多自动化工具之间产生混淆。对于通配符的导入，有一个防御性的做法，即将内部接口重新发布为公共API的一部分（例如，用可选加速器模块的定义覆盖纯Python实现的接口，以及重写那些事先不知道的定义）。 当以这种方式重新发布名称时，以下关于公共和内部接口的准则仍然适用。 Module level dunder names 模块级的“呆”名像all , author , version 等这样的模块级“呆名“（也就是名字里有两个前缀下划线和两个后缀下划线），应该放在文档字符串的后面，以及除from future 之外的import表达式前面。Python要求将来在模块中的导入，必须出现在除文档字符串之外的其他代码之前。比如：12345678910111213&quot;&quot;&quot;This is the example module.This module does stuff.&quot;&quot;&quot;from __future__ import barry_as_FLUFL__all__ = [&apos;a&apos;, &apos;b&apos;, &apos;c&apos;]__version__ = &apos;0.1&apos;__author__ = &apos;Cardinal Biggles&apos;import osimport sys String Quotes 字符串引号在Python中，单引号和双引号字符串是相同的。PEP不会为这个给出建议。选择一条规则并坚持使用下去。当一个字符串中包含单引号或者双引号字符的时候，使用和最外层不同的符号来避免使用反斜杠，从而提高可读性。 对于三引号字符串，总是使用双引号字符来与PEP 257中的文档字符串约定保持一致。 Whitespace in Expressions and Statements 表达式和语句中的空格Pet Peeves 不能忍受的事情在下列情况下，避免使用无关的空格： 紧跟在小括号，中括号或者大括号后。 12Yes: spam(ham[1], &#123;eggs: 2&#125;)No: spam( ham[ 1 ], &#123; eggs: 2 &#125; ) 紧贴在逗号、分号或者冒号之前。 12Yes: if x == 4: print x, y; x, y = y, xNo: if x == 4 : print x , y ; x , y = y , x 然而，冒号在切片中就像二元运算符，在两边应该有相同数量的空格（把它当做优先级最低的操作符）。在扩展的切片操作中，所有的冒号必须有相同的间距。例外情况：当一个切片参数被省略时，空格就被省略了。 12345ham[1:9], ham[1:9:3], ham[:9:3], ham[1::3], ham[1:9:]ham[lower:upper], ham[lower:upper:], ham[lower::step]ham[lower+offset : upper+offset]ham[: upper_fn(x) : step_fn(x)], ham[:: step_fn(x)]ham[lower + offset : upper + offset] 不推荐1234ham[lower + offset:upper + offset]ham[1: 9], ham[1 :9], ham[1:9 :3]ham[lower : : upper]ham[ : upper] 紧贴在函数参数的左括号之前。 12Yes: spam(1)No: spam (1) 紧贴索引或者切片的左括号之前。 12Yes: dct[&apos;key&apos;] = lst[index]No: dct [&apos;key&apos;] = lst [index] 为了和另一个赋值语句对齐，在赋值运算符附件加多个空格。 推荐 123x = 1y = 2long_variable = 3 不推荐： 123x = 1y = 2long_variable = 3 Other Recommendations 其他建议 避免在尾部添加空格。因为尾部的空格通常都看不见，会产生混乱：比如，一个反斜杠后面跟一个空格的换行符，不算续行标记。有些编辑器不会保留尾空格，并且很多项目（像CPython）在pre-commit的挂钩调用中会过滤掉尾空格。 总是在二元运算符两边加一个空格：赋值（=），增量赋值（+=，-=），比较（==,&lt;,&gt;,!=,&lt;&gt;,&lt;=,&gt;=,in,not,in,is,is not），布尔（and, or, not）。 如果使用具有不同优先级的运算符，请考虑在具有最低优先级的运算符周围添加空格。有时需要通过自己来判断；但是，不要使用一个以上的空格，并且在二元运算符的两边使用相同数量的空格。 推荐： 12345i = i + 1submitted += 1x = x*2 - 1hypot2 = x*x + y*yc = (a+b) * (a-b) 不推荐 12345i=i+1submitted +=1x = x * 2 - 1hypot2 = x * x + y * yc = (a + b) * (a - b) 在制定关键字参数或者默认参数值的时候，不要在=附近加上空格。推荐 12def complex(real, imag=0.0): return magic(r=real, i=imag) 不推荐12def complex(real, imag = 0.0): return magic(r = real, i = imag) 功能型注释应该使用冒号的一般性规则，并且在使用-&gt;的时候要在两边加空格。（参考下面的功能注释得到能够多信息） 推荐12def munge(input: AnyStr): ...def munge() -&gt; AnyStr: ... 不推荐12def munge(input:AnyStr): ...def munge()-&gt;PosInt: ... 当给有类型备注的参数赋值的时候，在=两边添加空格（仅针对那种有类型备注和默认值的参数）。推荐：12def munge(sep: AnyStr = None): ...def munge(input: AnyStr, sep: AnyStr = None, limit=1000): ... 不推荐12def munge(input: AnyStr=None): ...def munge(input: AnyStr, limit = 1000): ... 复合语句(同一行中的多个语句)通常是不允许的。推荐：12345if foo == &apos;blah&apos;: do_blah_thing()do_one()do_two()do_three() 不推荐12if foo == &apos;blah&apos;: do_blah_thing()do_one(); do_two(); do_three() 虽然有时候将小的代码块和 if/for/while 放在同一行没什么问题，多行语句块的情况不要这样用，同样也要避免代码行太长！不推荐123if foo == &apos;blah&apos;: do_blah_thing()for x in lst: total += xwhile t &lt; 10: t = delay() 不要12345678910if foo == &apos;blah&apos;: do_blah_thing()else: do_non_blah_thing()try: something()finally: cleanup()do_one(); do_two(); do_three(long, argument, list, like, this)if foo == &apos;blah&apos;: one(); two(); three() Comments 注释与代码相矛盾的注释比没有注释还糟，当代码更改时，优先更新对应的注释！ 注释应该是完整的句子。如果一个注释是一个短语或句子，它的第一个单词应该大写，除非它是以小写字母开头的标识符(永远不要改变标识符的大小写！)。 如果注释很短，结尾的句号可以省略。块注释一般由完整句子的一个或多个段落组成，并且每句话结束有个句号。 在句尾结束的时候应该使用两个空格。 当用英文书写时，遵循Strunk and White （译注：《Strunk and White, The Elements of Style》）的书写风格。 在非英语国家的Python程序员，请使用英文写注释，除非你120%的确信你的代码不会被使用其他语言的人阅读。 Block Comments 块注释块注释通常适用于跟随它们的某些（或全部）代码，并缩进到与代码相同的级别。块注释的每一行开头使用一个#和一个空格（除非块注释内部缩进文本）。 块注释内部的段落通过只有一个#的空行分隔。 Inline Comments 行内注释有节制地使用行内注释。行内注释是与代码语句同行的注释。行内注释和代码至少要有两个空格分隔。注释由#和一个空格开始。事实上，如果状态明显的话，行内注释是不必要的，反而会分散注意力。比如说下面这样就不需要：1x = x + 1 # Increment x 但有时，这样做很有用： 1x = x + 1 # Compensate for border Documentation Strings 文档字符串编写好的文档说明（也叫“docstrings”）的约定在PEP 257中永恒不变。 要为所有的公共模块，函数，类以及方法编写文档说明。非公共的方法没有必要，但是应该有一个描述方法具体作用的注释。这个注释应该在def那一行之后。 PEP 257 描述了写出好的文档说明相关的约定。特别需要注意的是，多行文档说明使用的结尾三引号应该自成一行，例如： 1234&quot;&quot;&quot;Return a foobangOptional plotz says to frobnicate the bizbaz first.&quot;&quot;&quot; 对于单行的文档说明，尾部的三引号应该和文档在同一行。 Naming Conventions 命名规范Python库的命名规范很乱，从来没能做到完全一致。但是目前有一些推荐的命名标准。新的模块和包（包括第三方框架）应该用这套标准，但当一个已有库采用了不同的风格，推荐保持内部一致性。 Overriding Principle 最重要的原则那些暴露给用户的API接口的命名，应该遵循反映使用场景而不是实现的原则。 Descriptive: Naming Styles 描述：命名风格有许多不同的命名风格。这里能够帮助大家识别正在使用什么样的命名风格，而不考虑他们为什么使用。以下是常见的命名方式： b（单个小写字母） B（单个大写字母） lowercase 小写字母 lower_case_with_underscores 使用下划线分隔的小写字母 UPPERCASE 大写字母 UPPER_CASE_WITH_UNDERSCORES 使用下划线分隔的大写字母 CapitalizedWords（或者叫 CapWords，或者叫CamelCase 驼峰命名法 —— 这么命名是因为字母看上去有起伏的外观5）。有时候也被称为StudlyCaps。注意：当在首字母大写的风格中用到缩写时，所有缩写的字母用大写，因此，HTTPServerError 比 HttpServerError 好。 mixedCase（不同于首字母大写，第一个单词的首字母小写） Capitalized_Words_With_Underscores（巨丑无比！） 也有用唯一的短前缀把相关命名组织在一起的方法。这在Python中不常用，但还是提一下。比如，os.stat()函数中包含类似以st_mode，st_size，st_mtime这种传统命名方式命名的变量。（这么做是为了与 POSIX 系统的调用一致，以帮助程序员熟悉它。）X11库的所有公共函数都加了前缀X。在Python里面没必要这么做，因为属性和方法在调用的时候都会用类名做前缀，函数名用模块名做前缀。另外，下面这种用前缀或结尾下划线的特殊格式是被认可的（通常和一些约定相结合）： _single_leading_underscore：（单下划线开头）弱“内部使用”指示器。比如 from M import * 是不会导入以下划线开始的对象的。 single_trailing_underscore_：（单下划线结尾）这是避免和Python内部关键词冲突的一种约定，比如：Tkinter.Toplevel(master, class_=’ClassName’) double_leading_underscore：（双下划线开头）当这样命名一个类的属性时，调用它的时候名字会做矫正（在类FooBar中，boo变成了_FooBar__boo；见下文）。 double_leading_and_trailing_underscore：（双下划线开头，双下划线结尾）“magic”对象或者存在于用户控制的命名空间内的属性，例如：init,import或者file。除了作为文档之外，永远不要命这样的名。 Prescriptive: Naming Conventions 约定俗成：命名约定Names to Avoid 应避免的名字永远不要使用字母‘l’（小写的L），‘O’（大写的O），或者‘I’（大写的I）作为单字符变量名。在有些字体里，这些字符无法和数字0和1区分，如果想用‘l’，用‘L’代替。 Package and Module Names 包名和模块名模块应该用简短全小写的名字，如果为了提升可读性，下划线也是可以用的。Python包名也应该使用简短全小写的名字，但不建议用下划线。当使用C或者C++编写了一个依赖于提供高级（更面向对象）接口的Python模块的扩展模块，这个C/C++模块需要一个下划线前缀（例如：_socket） Class Names 类名类名一般使用首字母大写的约定。在接口被文档化并且主要被用于调用的情况下，可以使用函数的命名风格代替。注意，对于内置的变量命名有一个单独的约定：大部分内置变量是单个单词（或者两个单词连接在一起），首字母大写的命名法只用于异常名或者内部的常量。 Exception Names 异常名因为异常一般都是类，所有类的命名方法在这里也适用。然而，你需要在异常名后面加上“Error”后缀（如果异常确实是一个错误）。 Global Variable Names 全局变量名（我们希望这一类变量只在模块内部使用。）约定和函数命名规则一样。通过 from M import * 导入的模块应该使用all机制去防止内部的接口对外暴露，或者使用在全局变量前加下划线的方式（表明这些全局变量是模块内非公有）。 Function Names 函数名函数名应该小写，如果想提高可读性可以用下划线分隔。大小写混合仅在为了兼容原来主要以大小写混合风格的情况下使用（比如 threading.py），保持向后兼容性。 Function and method arguments 函数和方法参数始终要将 self 作为实例方法的的第一个参数。始终要将 cls 作为类静态方法的第一个参数。如果函数的参数名和已有的关键词冲突，在最后加单一下划线比缩写或随意拼写更好。因此 class_ 比 clss 更好。（也许最好用同义词来避免这种冲突） Method Names and Instance Variables 方法名和实例变量遵循这样的函数命名规则：使用下划线分隔小写单词以提高可读性。在非共有方法和实例变量前使用单下划线。通过双下划线前缀触发Python的命名转换规则来避免和子类的命名冲突。Python通过类名对这些命名进行转换：如果类 Foo 有一个叫 a 的成员变量， 它无法通过 Foo.a 访问。（执着的用户可以通过 Foo._Foo__a 访问。）一般来说，前缀双下划线用来避免类中的属性命名与子类冲突的情况。注意：关于__names的用法存在争论（见下文）。 Constants 常量常量通常定义在模块级，通过下划线分隔的全大写字母命名。例如： MAX_OVERFLOW 和 TOTAL。 Designing for inheritance 继承的设计始终要考虑到一个类的方法和实例变量（统称：属性）应该是共有还是非共有。如果存在疑问，那就选非共有；因为将一个非共有变量转为共有比反过来更容易。公共属性是那些与类无关的客户使用的属性，并承诺避免向后不兼容的更改。非共有属性是那些不打算让第三方使用的属性；你不需要承诺非共有属性不会被修改或被删除。我们不使用“私有（private）”这个说法，是因为在Python中目前还没有真正的私有属性（为了避免大量不必要的常规工作）。另一种属性作为子类API的一部分（在其他语言中通常被称为“protected”）。有些类是专为继承设计的，用来扩展或者修改类的一部分行为。当设计这样的类时，要谨慎决定哪些属性时公开的，哪些是作为子类的API，哪些只能在基类中使用。贯彻这样的思想，一下是一些让代码Pythonic的准则： 公共属性不应该有前缀下划线。 如果公共属性名和关键字冲突，在属性名之后增加一个下划线。这比缩写和随意拼写好很多。（然而，尽管有这样的规则，在作为参数或者变量时，‘cls’是表示‘类’最好的选择，特别是作为类方法的第一个参数。）注意1：参考之前的类方法参数命名建议 对于单一的共有属性数据，最好直接对外暴露它的变量名，而不是通过负责的 存取器（accessor）/突变（mutator） 方法。请记住，如果你发现一个简单的属性需要成长为一个功能行为，那么Python为这种将来会出现的扩展提供了一个简单的途径。在这种情况下，使用属性去隐藏属性数据访问背后的逻辑。注意1：属性只在new-style类中起作用。注意2：尽管功能方法对于类似缓存的负面影响比较小，但还是要尽量避免。注意3：属性标记会让调用者认为开销（相当的）小，避免用属性做开销大的计算。 如果你的类打算用来继承的话，并且这个类里有不希望子类使用的属性，就要考虑使用双下划线前缀并且没有后缀下划线的命名方式。这会调用Python的命名转换算法，将类的名字加入到属性名里。这样做可以帮助避免在子类中不小心包含了相同的属性名而产生的冲突。注意1：只有类名才会整合进属性名，如果子类的属性名和类名和父类都相同，那么你还是会有命名冲突的问题。注意2：命名转换会在某些场景使用起来不太方便，例如调试，getattr()。然而命名转换的算法有很好的文档说明并且很好操作。注意3：不是所有人都喜欢命名转换。尽量避免意外的名字冲突和潜在的高级调用。 ################################################ Public and internal interfaces 公共和内部的接口任何向后兼容保证只适用于公共接口，因此，用户清晰地区分公共接口和内部接口非常重要。 文档化的接口被认为是公开的，除非文档明确声明它们是临时或内部接口，不受通常的向后兼容性保证。所有未记录的接口都应该是内部的。 为了更好地支持内省（introspection），模块应该使用all属性显式地在它们的公共API中声明名称。将all设置为空列表表示模块没有公共API。 即使通过all设置过，内部接口（包，模块，类，方法，属性或其他名字）依然需要单个下划线前缀。 如果一个命名空间（包，模块，类）被认为是内部的，那么包含它的接口也应该被认为是内部的。 导入的名称应该始终被视作是一个实现的细节。其他模块必须不能间接访问这样的名称，除非它是包含它的模块中有明确的文档说明的API，例如 os.path 或者是一个包里从子模块公开函数接口的 init 模块。 Programming Recommendations 编程建议 代码应该用不损害其他Python实现的方式去编写（PyPy，Jython，IronPython，Cython，Psyco 等）。比如，不要依赖于在CPython中高效的内置字符连接语句 a += b 或者 a = a + b。这种优化甚至在CPython中都是脆弱的（它只适用于某些类型）并且没有出现在不使用引用计数的实现中。在性能要求比较高的库中，可以种 ”.join() 代替。这可以确保字符关联在不同的实现中都可以以线性时间发生。 和像None这样的单例对象进行比较的时候应该始终用 is 或者 is not，永远不要用等号运算符。另外，如果你在写 if x 的时候，请注意你是否表达的意思是 if x is not None。举个例子，当测试一个默认值为None的变量或者参数是否被设置为其他值的时候。这个其他值应该是在上下文中能成为bool类型false的值。 使用 is not 运算符，而不是 not … is 。虽然这两种表达式在功能上完全相同，但前者更易于阅读，所以优先考虑。 12345推荐：if foo is not None:不推荐：if not foo is None: 当使用富比较（rich comparisons，一种复杂的对象间比较的新机制，允许返回值不为-1,0,1）实现排序操作的时候，最好实现全部的六个操作符（eq, ne, lt, gt, ge）而不是依靠其他的代码去实现特定的比较。为了最大程度减少这一过程的开销， functools.total_ordering() 修饰符提供了用于生成缺少的比较方法的工具。PEP 207 指出Python实现了反射机制。因此，解析器会将 y &gt; x 转变为 x &lt; y，将 y &gt;= x 转变为 x &lt;= y，也会转换x == y 和 x != y的参数。sort() 和 min()方法确保使用&lt;操作符，max()使用&gt;操作符。然而，最好还是实现全部六个操作符，以免在其他地方出现冲突。 始终使用def表达式，而不是通过赋值语句将lambda表达式绑定到一个变量上。1234推荐：def f(x): return 2*x不推荐：f = lambda x: 2*x 第一个形式意味着生成的函数对象的名称是“f”而不是泛型“&lt; lambda &gt;”。这在回溯和字符串显示的时候更有用。赋值语句的使用消除了lambda表达式优于显式def表达式的唯一优势（即lambda表达式可以内嵌到更大的表达式中）。 从Exception继承异常，而不是BaseException。直接继承BaseException的异常适用于几乎不用来捕捉的异常。 设计异常的等级，要基于扑捉异常代码的需要，而不是异常抛出的位置。以编程的方式去回答“出了什么问题？”，而不是只是确认“出现了问题”（内置异常结构的例子参考 PEP 3151 ） 类的命名规范适用于这里，但是你需要添加一个“Error”的后缀到你的异常类，如果异常是一个Error的话。非本地流控制或者其他形式的信号的非错误异常不需要特殊的后缀。 适当地使用异常链接。在Python 3里，为了不丢失原始的根源，可以显式指定“raise X from Y”作为替代。 当故意替换一个内部异常时（Python 2 使用“raise X”， Python 3.3 之后 使用 “raise X from None”），确保相关的细节转移到新的异常中（比如把AttributeError转为KeyError的时候保留属性名，或者将原始异常信息的文本内容内嵌到新的异常中）。 在Python 2中抛出异常时，使用 rasie ValueError(‘message’) 而不是用老的形式 raise ValueError, ‘message’。 第二种形式在Python3 的语法中不合法 使用小括号，意味着当异常里的参数非常长，或者包含字符串格式化的时候，不需要使用换行符。 当捕获到异常时，如果可以的话写上具体的异常名，而不是只用一个except: 块。比如说： 1234try: import platform_specific_moduleexcept ImportError: platform_specific_module = None 如果只有一个except: 块将会捕获到SystemExit和KeyboardInterrupt异常，这样会很难通过Control-C中断程序，而且会掩盖掉其他问题。如果你想捕获所有指示程序出错的异常，使用 except Exception: （只有except等价于 except BaseException:）。两种情况不应该只使用‘excpet’块： 如果异常处理的代码会打印或者记录log；至少让用户知道发生了一个错误。 如果代码需要做清理工作，使用 raise..try…finally 能很好处理这种情况并且能让异常继续上浮。 当给捕捉的异常绑定一个名字时，推荐使用在Python 2.6中加入的显式命名绑定语法：1234try: process_data()except Exception as exc: raise DataProcessingFailedError(str(exc)) 为了避免和原来基于逗号分隔的语法出现歧义，Python3只支持这一种语法。 当捕捉操作系统的错误时，推荐使用Python 3.3 中errno内定数值指定的异常等级。 另外，对于所有的 try/except 语句块，在try语句中只填充必要的代码，这样能避免掩盖掉bug。 123456789101112131415推荐：try: value = collection[key]except KeyError: return key_not_found(key)else: return handle_value(value)不推荐：try: # Too broad! return handle_value(collection[key])except KeyError: # Will also catch KeyError raised by handle_value() return key_not_found(key) 当代码片段局部使用了某个资源的时候，使用with 表达式来确保这个资源使用完后被清理干净。用try/finally也可以。 无论何时获取和释放资源，都应该通过单独的函数或方法调用上下文管理器。举个例子：123456推荐：with conn.begin_transaction(): do_stuff_in_transaction(conn)不推荐：with conn: do_stuff_in_transaction(conn) 第二个例子没有提供任何信息去指明enter和exit方法在事务之后做出了关闭连接之外的其他事情。这种情况下，明确指明非常重要。 返回的语句保持一致。函数中的返回语句都应该返回一个表达式，或者都不返回。如果一个返回语句需要返回一个表达式，那么在没有值可以返回的情况下，需要用 return None 显式指明，并且在函数的最后显式指定一条返回语句（如果能跑到那的话）。 123456789101112131415161718192021推荐：def foo(x): if x &gt;= 0: return math.sqrt(x) else: return Nonedef bar(x): if x &lt; 0: return None return math.sqrt(x)不推荐：def foo(x): if x &gt;= 0: return math.sqrt(x)def bar(x): if x &lt; 0: return return math.sqrt(x) 使用字符串方法代替字符串模块。 字符串方法总是更快，并且和unicode字符串分享相同的API。如果需要兼容Python2.0之前的版本可以不用考虑这个规则。 使用 ”.startswith() 和 ”.endswith() 代替通过字符串切割的方法去检查前缀和后缀。 startswith()和endswith()更干净，出错几率更小。比如： 12推荐: if foo.startswith(&apos;bar&apos;):糟糕: if foo[:3] == &apos;bar&apos;: 对象类型的比较应该用isinstance()而不是直接比较type。 12正确: if isinstance(obj, int):糟糕: if type(obj) is type(1): 当检查一个对象是否为string类型时，记住，它也有可能是unicode string！在Python2中，str和unicode都有相同的基类：basestring，所以你可以这样：1if isinstance(obj, basestring): 注意，在Python3中，unicode和basestring都不存在了（只有str）并且bytes类型的对象不再是string类型的一种（它是整数序列） 对于序列来说（strings，lists，tuples），可以使用空序列为false的情况。 12345正确: if not seq: if seq:糟糕: if len(seq): if not len(seq): 书写字符串时不要依赖单词结尾的空格，这样的空格在视觉上难以区分，有些编辑器会自动去掉他们（比如 reindent.py （译注：re indent 重新缩进）） 不要用 == 去和True或者False比较：123正确: if greeting:糟糕: if greeting == True:更糟: if greeting is True: Function Annotations 功能注释[PEP 484的引入，功能型注释的风格规范有些变化。 为了向前兼容，在Python3代码中的功能注释应该使用 PEP 484的语法规则。（在前面的章节中对注释有格式化的建议。） 不再鼓励使用之前在PEP中推荐的实验性样式。 然而，在stdlib库之外，在PEP 484中的实验性规则是被鼓励的。比如用PEP 484的样式标记大型的第三方库或者应用程序，回顾添加这些注释是否简单，并观察是否增加了代码的可读性。 Python的标准库代码应该保守使用这种注释，但新的代码或者大型的重构可以使用这种注释。 如果代码希望对功能注释有不同的用途，建议在文件的顶部增加一个这种形式的注释：1# type: ignore 这会告诉检查器忽略所有的注释。（在 PEP 484中可以找到从类型检查器禁用投诉的更细粒度的方法。） 像linters一样，类型检测器是可选的可独立的工具。默认情况下，Python解释器不应该因为类型检查而发出任何消息，也不应该基于注释改变它们的行为。 不想使用类型检测的用户可以忽略他们。然而，第三方库的用户可能希望在这些库上运行类型检测。为此， PEP 484 建议使用存根文件类型：.pyi文件，这种文件类型相比于.py文件会被类型检测器读取。存根文件可以和库一起，或者通过typeshed repo6独立发布（通过库作者的许可） 对于需要向后兼容的代码，可以以注释的形式添加功能型注释。参见PEP 484的相关部分7。 参考pep8 Python PEP8 编码规范中文版 PEP 7, Style Guide for C Code, van Rossum Barry’s GNU Mailman style guide http://barry.warsaw.us/software/STYLEGUIDE.txt 挂行缩进是一种类型设置样式，其中除第一行之外，段落中的所有行都缩进。在Python中，这个术语是用来描述一种风格：在被括号括起来的语句中，左括号是这一行最后一个非空格字符，随后括号内的内容每一行进行缩进，直到遇到右括号。 Donald Knuth’s The TeXBook, pages 195 and 196 http://www.wikipedia.com/wiki/CamelCase Typeshed repo https://github.com/python/typeshed Suggested syntax for Python 2.7 and straddling code https://www.python.org/dev/peps/pep-0484/#suggested-syntax-for-python-2-7-and-straddling-code]]></content>
      <categories>
        <category>编程拾慧</category>
        <category>python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python 基础知识]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-02.python%2FPython-%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86-%E5%AF%B9%E8%B1%A1%E5%AD%97%E5%85%B8%2F</url>
    <content type="text"><![CDATA[基本板书函数 命令 作用 range(N) 生成从０到N-1共 Ｎ个数字的列表 continue 跳过本轮进入下一轮循环（相当于Perl的 next） break 终止循环 （相当于Perl的 last） def 定义函数（相当于Perl的 sub） dir() dir()用来查询一个类或者对象所有属性 print dir(list) help() help()用来查询的说明文档 print help(list) 对象通过class 创建一个对象；对于对象的属性可以直接在class 块中进行赋值；同时可以通过def 对对象添加一些方法； 123456789101112class Bird(object): ##括号中为该对象的父类，如果是object则表明是顶级类，没有父类have_feather = Trueway_of_reproduction = ‘egg’class Chicken(Bird): ###Chicken的父类是Bird，所以Chicken将会集成父类Bird的所有属性和动作way_of_move = ‘walk’possible_in_KFC = Truedef show_laugh(self):#self 用于内部的调用；参数传递时不会传递selfprint self.laughdef laugh_100th(self):for i in range(100):self.show_laugh() #通过self 调用了方法： show_laugh 特殊方法(特殊的方法特点是名称前后各有2个下划线（__）)：init_() 是一个特殊方法；如果在类中定义了这个方法，则在创建对象时，python会在对象创建后自动调用这个方法，完成对象创建后，会直接打印 ‘print ‘We are happy birds.’,more_words’ 12345class happyBird(Bird):def __init__(self,more_words):print &apos;We are happy birds.&apos;,more_wordssummer = happyBird(&apos;Happy,Happy!&apos;) 字典字典的创建1dic = &#123;&quot;key_a&quot;:&quot;value_1&quot;,&quot;key_b&quot;:&quot;value_1&quot;&#125; 或者先构建一个空字典，然后想字典中添加值； 12345dic=&#123;&#125; #构建一个空的字典；dic[&quot;key&quot;] = &quot;value&quot;for 循环默认遍历字典的keyfor i in dicprint i 字典的常用方法| 示例命令 | 功能 || —————— | ————————— || print dic.keys() | 返回dic所有的键 || print dic.values() | 返回dic所有的值 || print dic.items() | 返回dic所有的元素（键值对） || dic.clear() | 清空dic，dict变为{} || del dic[“key”] | # 删除字典元素 || print len(dic) | len查询元素总数 |]]></content>
      <categories>
        <category>编程拾慧</category>
        <category>python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[两组数据差异显著性检验]]></title>
    <url>%2F01.%E7%9F%A5%E8%AF%86-02.Math%2F%E4%B8%A4%E7%BB%84%E6%95%B0%E6%8D%AE%E5%B7%AE%E5%BC%82%E6%98%BE%E8%91%97%E6%80%A7%E6%A3%80%E9%AA%8C%2F</url>
    <content type="text"><![CDATA[简述 1、如果两组数每组数的个数&lt;30，且已知方差服从正态分布，可以比较2组数的均值是否显著不同，用 t检验； 2、如果两组数每组数的个数≥30，也可以比较2组数的均值是否显著不同，用 z检验； 3、如果两组数每组数的分布未知，可以比较2组数是否显著性同分布，可以用非参数检验 Mann-Whitney U test进行； 4、如果两组数已知都服从正态分布，可以比较2组数的方差是否显著相同，用F检验； K-S检验K-S检验（Kolmogorov-Smirnov检验），K-S检验不仅能够检验单个总体是否服从某一理论分布，还能够检验两总体分布是否存在显著差异。其原假设是：两组独立样本来自的两总体的分布无显著差异。 K-S检验以变量的秩作为分析对象，检验两个独立样本群体，或者一个样本群体和一个特定标准分布之间的关系。K-S就是对两组数据的累积分布进行比较，寻找两个群体累积分布曲线之前的最大值作为D值。获得D值后，查表确定临界值。 Z-testZ-Test: Definition, Uses in Statistics, and Examplez分数是一种统计度量，用于量化数据点与数据集平均值之间的单位距离（距离单位是总体的标准差）。它以标准差的形式表示。它指示数据点与分布平均值之间距离由几个标准差。如果 Z 分数为 0，则表示数据点的分数与平均分数相同。 Z 分数为 1.0 表示与平均值相差一个标准差的值。 Z 分数可以是正数或负数，正值表示分数高于平均值，负值表示分数低于平均值。 z 检验是一种统计检验，用于在方差已知且样本量较大时确定两个大致符合正态分布的总体均值（大致符合正态分布，否则测试不起作用）是否不同。它还可用于将一个平均值与假设值进行比较。z 检验最适用于大于 30 个样本，因为根据中心极限定理，随着样本数量变大，样本被认为近似正态分布。进行 z 检验时，应说明原假设和备择假设以及 alpha 水平。应计算z 分数，也称为检验统计量，并说明结果和结论。 z 统计量或 z 分数是一个数字，表示从 z 检验得出的分数高于或低于总体平均数的标准差有多少。 计算方式如下：$$ z = ( x - μ ) / \sqrt{\frac{σ^2}{n}} $$ z = Z-score x = 待评估数据的均值 n = 待评估数据的样本个数 μ = 均值 σ = 标准差 ; σ^2 = 方差当我们待测数据是单一数值时，对应的计算公式可以简化为：$$ z = ( x - μ ) / σ $$Z-score 对应概率值表 T-testt 检验比较两个数据集的平均值并确定它们是否来自同一总体。例如，物理课学生的成绩和写作课不同组学生的成绩不太可能具有相同的平均值和标准差。同样，从药物测试的安慰剂喂养对照组中采集的样本和从药物处方组中采集的样本应该具有稍​​微不同的平均值和标准差。T 检验的场景： 药效测试时，按照标准程序，将药物给予一组患者，并向另一组（称为对照组）给予安慰剂。t 检验可用于确定结果是否显着且适用于整个人群，或者它们是否是随机的且不是由于药物干预所致。t 检验时做出四个假设： 收集的数据必须遵循连续或顺序尺度，例如智商测试的分数。计算方式如下： 计算配对t检验计算配对 t 检验的 t 值和自由度的公式为： $$T=\frac{mean1−mean2}{\frac{s(diff)}{\sqrt{n}}}$$​其中：mean1 and mean2= 两个数据集的均值s(diff)=配对数据的差值的标准差n=样本集合大小（配对数据的数目）n−1=自由度]]></content>
      <categories>
        <category>统计知识</category>
      </categories>
      <tags>
        <tag>显著性检验</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[NGS测序原理]]></title>
    <url>%2F01.%E7%9F%A5%E8%AF%86-01.NGS%2F2018-04-27.NGS%E6%B5%8B%E5%BA%8F%E5%8E%9F%E7%90%86%2F</url>
    <content type="text"><![CDATA[# 参考资料： 百度文库 华大基因：知学云]]></content>
      <categories>
        <category>NGS</category>
        <category>原理</category>
      </categories>
      <tags>
        <tag>测序原理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hexo博客文章优化]]></title>
    <url>%2F03.%E5%BA%94%E7%94%A8-99.other%2Fhexo-%E6%89%A9%E5%B1%95%E5%8A%9F%E8%83%BD-%E6%96%87%E7%AB%A0%E6%B7%BB%E5%8A%A0%E6%8A%98%E5%8F%A0%E5%8A%9F%E8%83%BD%2F</url>
    <content type="text"><![CDATA[博客文章添加折叠功能博客文章中，有些内容篇幅较大，但是可能对一部分读者来说，并不会特别关注，所以这些数据的展示，可能会直接导致整篇博文变得臃肿，而对另一部分读者，这部分详细的介绍却非常重要，因此不能简单的缩减博文，所以这个时候，针对文章中这部分篇幅较大的内容，增加折叠功能可以很好的解决这类问题。 在main.js中添加折叠jsnext主题的主要js位于 themes/next/source/js/src/post-details.js在里面找合适的位置，添加如下代码： 12345678$(document).ready(function()&#123; $(document).on(&apos;click&apos;, &apos;.fold_hider&apos;, function()&#123; $(&apos;&gt;.fold&apos;, this.parentNode).slideToggle(); $(&apos;&gt;:first&apos;, this).toggleClass(&apos;open&apos;); &#125;); //默认情况下折叠 $(&quot;div.fold&quot;).css(&quot;display&quot;,&quot;none&quot;);&#125;); 自定义内建标签在主题scripts下添加一个tags.js, 位于themes/next/scripts/tags.js 123456789101112131415161718192021222324/* @haohuawu 修复 Nunjucks 的 tag 里写 ```代码块```，最终都会渲染成 undefined 的问题 https://github.com/hexojs/hexo/issues/2400*/const rEscapeContent = /&lt;escape(?:[^&gt;]*)&gt;([\s\S]*?)&lt;\/escape&gt;/g;const placeholder = &apos;\uFFFD&apos;;const rPlaceholder = /(?:&lt;|&amp;lt;)\!--\uFFFD(\d+)--(?:&gt;|&amp;gt;)/g;const cache = [];function escapeContent(str) &#123; return &apos;&lt;!--&apos; + placeholder + (cache.push(str) - 1) + &apos;--&gt;&apos;;&#125;hexo.extend.filter.register(&apos;before_post_render&apos;, function(data) &#123; data.content = data.content.replace(rEscapeContent, function(match, content) &#123; return escapeContent(content); &#125;); return data;&#125;);hexo.extend.filter.register(&apos;after_post_render&apos;, function(data) &#123; data.content = data.content.replace(rPlaceholder, function() &#123; return cache[arguments[1]]; &#125;); return data;&#125;); 再继续添加一个themes/next/scripts/fold.js 12345678/* global hexo */// Usage: &#123;% fold ???? %&#125; Something &#123;% endfold %&#125;function fold (args, content) &#123; var text = args[0]; if(!text) text = &quot;点击显/隐&quot;; return &apos;&lt;div&gt;&lt;div class=&quot;fold_hider&quot;&gt;&lt;div class=&quot;close hider_title&quot;&gt;&apos; + text + &apos;&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;fold&quot;&gt;\n&apos; + hexo.render.renderSync(&#123;text: content, engine: &apos;markdown&apos;&#125;) + &apos;\n&lt;/div&gt;&lt;/div&gt;&apos;;&#125;hexo.extend.tag.register(&apos;fold&apos;, fold, &#123;ends: true&#125;); 最后，添加几个自定义样式，位置 themes/next/source/css/_custom/custom.styl 12345678910.hider_title&#123; font-family: &quot;Microsoft Yahei&quot;; cursor: pointer;&#125;.close:after&#123; content: &quot;▼&quot;;&#125;.open:after&#123; content: &quot;▲&quot;;&#125; 最后，在我们需要折叠的地方前后添加便签，示例用法： 折叠示例代码 123&#123;% fold 点击显/隐内容 %&#125;something you want to fold, include code block.&#123;% endfold %&#125; 参考博客 Hexo博文置顶（自定义排序）HEXO默认是按照时间顺序排一条线，然后按照时间顺序来决定显示的顺序的。按照网上的教程整理了一份方法。 使用的是top属性，top值越高，排序越在前，不设置top值得博文按照时间顺序排序。修改Hexo文件夹下的node_modules/hexo-generator-index/lib/generator.js 打开在最后添加如下javascript代码代码 12345678910111213posts.data = posts.data.sort(function(a, b) &#123;if(a.top &amp;&amp; b.top) &#123; // 两篇文章top都有定义 if(a.top == b.top) return b.date - a.date; // 若top值一样则按照文章日期降序排 else return b.top - a.top; // 否则按照top值降序排&#125;else if(a.top &amp;&amp; !b.top) &#123; // 以下是只有一篇文章top有定义，那么将有top的排在前面（这里用异或操作居然不行233） return -1;&#125;else if(!a.top &amp;&amp; b.top) &#123; return 1;&#125;else return b.date - a.date; // 都没定义按照文章日期降序排)&#125;; 更改以后，在写博客的时候，添加top属性就可以啦；]]></content>
      <categories>
        <category>hexo</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[密钥设置实现免密码访问]]></title>
    <url>%2F03.%E5%BA%94%E7%94%A8-02.Linux%E5%92%8Cshell%2F2018-04-19.%E5%AF%86%E9%92%A5%E8%AE%BE%E7%BD%AE%E5%AE%9E%E7%8E%B0%E5%85%8D%E5%AF%86%E7%A0%81%E8%AE%BF%E9%97%AE%2F</url>
    <content type="text"><![CDATA[进行项目分析等工作，经常要链接服务器，登陆过程中总会需要我们输入密码，如果频率比较低，还好，但是每天的重复输入，总归会浪费我们大量的时间。另一方面部分工具或任务，可能不方便进行交互式的密码输入，因此通过使用密钥来实现免密码展现出较大的优势。 通过公钥与私钥创建公钥与私钥对在本地机器上运行 123456789101112131415161718192021222324252627282930313233343536373839404142434445ssh-keygen -f test -C &quot;test key&quot;支持以下选项：–b bits 指定要创建的密钥的位数。最小位数为 512 位。通常，2048 位足以满足安全需要。密钥大小超过该值并不会提高安全性，反而会降低速度。缺省值为 2048 位。–B 显示指定的私钥或公钥文件的 bubblebabble 摘要。–c 请求更改私钥和公钥文件中的注释。该程序会提示您提供包含私钥的文件、口令短语（如果密钥具有一个口令短语）以及新的注释。 此选项仅适用于 rsa1 (SSHv1) 密钥。–C comment 提供新注释。–e 此选项读取 OpenSSH 私钥或公钥文件并将密钥以 &quot;SECSH&quot; 公钥文件格式输出到 stdout。此选项允许导出密钥供其他一些 SSH 实现使用。–f 指定密钥文件的文件名。–F 在 known_hosts 文件中搜索指定的 hostname，列出找到的任何匹配项。此选项可用于查找散列格式的主机名或地址，还可以与 –H 选项一起使用，以散列格式输出找到的密钥。–H 对 known_hosts 文件执行散列计算。此选项使用散列形式替换指定文件内的所有主机名和地址。原始内容将移动到后缀为 .old 的文件中。这些散列值通常由 ssh 和 sshd 使用，即使文件内容被公开，这些散列值也并不会透露可识别的信息。此选项不会修改现有的散列主机名，因此可以放心地用于同时包含散列名称和非散列名称的文件。–i 此选项以 SSH2 兼容格式读取未加密的私钥（或公钥）文件并将 OpenSSH 兼容的私钥（或公钥）输出到 stdout。ssh-keygen 还可读取 “SECSH” 公钥文件格式。此选项允许从其他一些 SSH 实现中导入密钥。–l 显示指定的私钥或公钥文件的指纹。–N new_passphrase 提供新口令短语。–p 请求更改私钥文件的口令短语，而不创建新私钥。该程序会提示您提供包含私钥的文件、旧口令短语，并两次提示您输入新口令短语。–P passphrase 提供（旧）口令短语。–q 退出 ssh-keygen。–t type 指定用于生成密钥的算法，其中 type 是 rsa、dsa 和 rsa1 中的一种。rsa1 类型仅用于 SSHv1 协议。–R hostname 从 known_hosts 文件中删除属于 hostname 的所有密钥。此选项可用于删除散列主机。请参见 –H。–x 已过时。已被 –e 选项取代。–X 已过时。已被 –i 选项取代。–y 此选项读取 OpenSSH 私钥格式文件并将 OpenSSH 公钥输出到 stdout。–8 指定 ssh-keygen 生成 PKCS#8 格式的密钥。对于要生成的密钥，支持的类型为 rsa 或 dsa。 将公钥复制到远程目录123ssh-copy-id -i ~/.ssh/id_rsa.pub remote-host# 会提示输入远程服务器的密码# 密码输入后，会将key写到远程机器的 ~/.ssh/authorized_key.文件中 多公钥的配置有些特殊情况下，可能我们需要配置多个独立的密钥，比如gitlab和github的独立公钥，或者集群公钥：这个时候，我们可以通过编辑 ~/.ssh/config 文件进行适配。12345678910111213141516171819202122232425262728# host 与 hostname 需要相同# configuration 1Host cluster # 你的主机名 HostName 10.1**.**.** # 你的用户名 User liu****** # 你的rsa秘钥文件 IdentityFile ~/.ssh/id_rsa# configuration 2Host github.com HostName github.com # 你的github账号 User 6****@qq.com # github对应的rsa秘钥文件 IdentityFile ~/.ssh/id_rsa_github# configuration 3# gitlab # host 与 hostname 需要相同Host gitlab.******.cn HostName gitlab.******.cn # 你的gitlab账号 User liu******@********ics.cn # gitlab对应的rsa秘钥文件 IdentityFile ~/.ssh/id_rsa_gitlab 其中关键字不分大小，但是后面的值是区分大小写的，常用的关键字如下： Host：类似昵称，用于标识某个特定的配置，在ssh命令中使用，例如我们想要ssh连接到上例中的#1配置的主机，则在命令行执行如下命令即可：ssh cluster , cluster == liubo4@10.192.36.4 HostName: ssh链接的主机名，一般是IP地址或域名。 User： ssh链接的用户名 IdentityFile： 认证证书文件，默认位置是~/.ssh/id_rsa, ~/ssh/id_dsa等，如果采用默认的证书，可以不用设置此参数，除非你的证书放在某个自定义的目录，那么你就需要设置该参数来指向你的证书 Port： SSH访问主机的端口号，默认是22端口，同上，只有在非默认情况下才需要设置该值 更多关键字可以通过执行 man ssh_config 查看。 配置完成进行测试1ssh remote-host 不需要输入密码即可登录到远程服务器 sshpass 工具先在机器A上安装 sshpass 工具，然后使用 12sshpass -p passwd ssh(scp) ** 或者 sshpass -f file ssh(scp) ** 其中 -p 直接指定密码，-f 则从文件中读取密码。]]></content>
      <categories>
        <category>环境配置</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[R绘图-ggplot-箱线图绘制]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-03.R%2FR-ggplot-%E7%AE%B1%E7%BA%BF%E5%9B%BE%E7%BB%98%E5%88%B6%2F</url>
    <content type="text"><![CDATA[数据处理过程中，好的数据展示，可以帮助我们更好的理解数据，发现数据之间的关系，记录下各种常见的绘图方式。 结果示例箱线图上方标注的为每个样品对应的中位数，可以根据需要进行调整。 绘图命令1Rscript ../ggplot_boxplot.R -i ggplot_boxplot.demo.data -o ggplot_boxplot.demo.data.png -X &quot;gene&quot; -Y &quot;depth&quot; 输入文件Demo数据 123456789101112131415161718192021222324252627282930313233Cluster Value IDKIT 0.85192541182175 1KIT 0.864711404642792 1KIT 1.12599180249189 1KIT 0.634586693569092 1KIT 1.16825284052483 1KIT 0.68284662568039 1KIT 0.579859358706082 1KIT 1.04258938018236 1KIT 0.753529870275851 1KIT 0.806402805442452 1KIT 0.951605801848777 1KIT 0.941741338875395 1KIT 0.880335826035274 1KIT 0.998844046813315 1。。。。BRCA1 1.20329612016347 2BRCA1 1.16319047052218 2BRCA1 0.779834117567836 2BRCA1 0.79095228570136 2BRCA1 0.835548863196455 2BRCA1 1.52532474830366 2BRCA1 1.13679352537476 2BRCA1 0.598936767501402 2BRCA1 1.1724422960551 2BRCA1 0.924227501396964 2BRCA1 0.691209808851361 2BRCA1 0.73454091210198 2。。 程序目录R脚本 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107library(&apos;getopt&apos;);library (&apos;ggplot2&apos;)library(MASS)library(plyr)#-----------------------------------------------------------------# getting parameters#-----------------------------------------------------------------#get options, using the spec as defined by the enclosed list.#we read the options from the default: commandArgs(TRUE).spec = matrix(c( &apos;help&apos; , &apos;h&apos;, 0, &quot;logical&quot;, &apos;infile&apos; , &apos;i&apos;, 1, &quot;character&quot;, &apos;outfile&apos; , &apos;o&apos;, 1, &quot;character&quot;, &apos;title&apos; , &apos;T&apos; , 2 , &quot;character&quot;, &apos;x.lab&apos; , &apos;X&apos;, 2, &quot;character&quot;, &apos;y.lab&apos; , &apos;Y&apos;, 2, &quot;character&quot;, &apos;type&apos; , &apos;t&apos;, 2, &quot;character&quot; ), byrow=TRUE, ncol=4);opt = getopt(spec);# define usage functionprint_usage &lt;- function(spec=NULL)&#123; cat(getopt(spec, usage=TRUE)); cat(&quot;Usage example: \n&quot;) cat(&quot;Rscript ggplot_boxplot.R -i input_tab -o Demo_out -t png -X xtest -Y ytest -W 100 -H 100 infile format： Cluster Value ID Clus_A 0.51 1 Clus_A 0.31 1 . . . Clus_Z 0.42 21 Clus_Z 0.72 21Options: --help -h NULL get this help--infile -i character the input file [forced]--outfile -o character the prefix for output graph [forced]--title -T character the Title for the picture (default:Title)--x.lab -X character the lab for x in SubPlot (default:xlab)--y.lab -Y character the lab for y in SubPlot (default:ylab)--type -t character save format(png,tiff,jpeg,svg,pdf default:png)\n&quot;) q(status=1);&#125;# if help was asked for print a friendly message# and exit with a non-zero error codeif ( !is.null(opt$help) ) &#123; print_usage(spec) &#125;if ( is.null(opt$infile) ) &#123; print_usage(spec) &#125;if ( is.null(opt$outfile)) &#123; print_usage(spec) &#125;if ( is.null(opt$type) ) &#123; opt$type=&quot;png&quot; &#125;if ( is.null(opt$x.lab) ) &#123; opt$x.lab=&quot;xlab&quot; &#125;if ( is.null(opt$y.lab) ) &#123; opt$y.lab=&quot;ylab&quot; &#125;if ( is.null(opt$title) ) &#123; opt$title=&apos;Title&apos;&#125;Args &lt;- commandArgs();file_tab=read.table(opt$infile,header=F,sep=&quot;\t&quot;);out_file=paste(opt$outfile,opt$type,sep=&quot;.&quot;)if(opt$type == &quot;png&quot;) &#123; png(file=out_file) &#125; #, 400*length(file_tab[1,]) , 400*length(file_tab[,1])) &#125;if(opt$type == &quot;tiff&quot;) &#123;tiff(file=out_file) &#125; #, 400*length(file_tab[1,]) , 400*length(file_tab[,1])) &#125;if(opt$type == &quot;jpeg&quot;) &#123;jpeg(file=out_file) &#125; #, 400*length(file_tab[1,]) , 400*length(file_tab[,1])) &#125;if(opt$type == &quot;pdf&quot;) &#123; pdf(file=out_file) &#125; #, 20*length(file_tab[1,]) , 20*length(file_tab[,1])) &#125;data=read.table(opt$infile ,header=T)data2=ddply(data,&quot;Cluster&quot;,summarise,Median=round(median(Value),3))ggplot(data,aes(x=reorder(Cluster,Value),y=Value,fill=Cluster)) +geom_boxplot() +theme(panel.grid.major =element_blank(), panel.grid.minor = element_blank(), panel.background = element_blank(), axis.line = element_line(colour = &quot;black&quot;),axis.title.x=element_text(size=25), axis.title.y =element_text(size=25), title=element_text(size=25), axis.text.x=element_text(angle=30,hjust=1,size=15))+ #设置x轴，数据标签的样式labs(x=opt$x.lab ,y=opt$y.lab) + # 设置x轴和y轴现实的标注geom_text(data=data2,aes(x=Cluster,y=2.5,colour=Cluster,label=Median,vjust=-0.2)) + # 在y=2.5的位置添加标签批注coord_cartesian(ylim=c(0,3)) # 对绘制的图片 根据x轴和y轴进行截取#####ggplot(data,aes(x=reorder(Cluster,Value),y=Value,fill=Cluster)) +### 绘图类型 ####geom_boxplot() + # 绘制箱线图### 样式 ####theme(panel.grid.major =element_blank(), # 去除主网格线#panel.grid.minor = element_blank(), #去除次网格线#panel.background = element_blank(), #去除背景色（默认为灰色）#axis.line = element_line(colour = &quot;black&quot;)，#将x轴和y轴的框线设置为黑色#axis.title.x=element_text(size=25), #设置x轴的标题样式#axis.title.y =element_text(size=25), #设置y轴的标题样式#title=element_text(size=25), #设置主标题样式#axis.text.x=element_text(angle=30,hjust=1,size=15)) + #设置x轴，数据标签的样式### 设置坐标轴标注 ####labs(x=opt$x.lab ,y=opt$y.lab) + # 设置x轴和y轴现实的标注### 添加标签 ####geom_text(data=data2,aes(x=Cluster,y=2.5,colour=Cluster,label=Median,vjust=-0.2)) + # 在y=2.5的位置添加标签批注### 对图片进行截取 ####coord_cartesian(ylim=c(0,3)) # 对绘制的图片 根据x轴和y轴进行截取dev.off()]]></content>
      <categories>
        <category>编程拾慧</category>
        <category>R</category>
      </categories>
      <tags>
        <tag>可视化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux配置文件~/.bashrc设置]]></title>
    <url>%2F03.%E5%BA%94%E7%94%A8-02.Linux%E5%92%8Cshell%2FLinux-%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6-bashrc%E8%AE%BE%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[常见配置https://blog.csdn.net/bangemantou/article/details/7682272 ## 常用bash_profile记录常用的bash_profile文件，后续需要可以直接快速采用，配置的一些说明见后文。1234567# 针对ls和grep 命令的结果增加结果展示颜色的配置。if [ -x /usr/bin/dircolors ]; then test -r ~/.dircolors &amp;&amp; eval "$(dircolors -b ~/.dircolors)" || eval "$(dircolors -b)" alias ls='ls --color=auto' alias ll='ls -l --color=auto' alias grep='grep --color=auto'fi shell变量 使用set命令显示所有的变量 使用env命令只显示环境变量 局部变量PS1PS1变量主要设置Bash 提示符所显示的信息可将一些换码序列插入到PS1变量中、它们成为提示信息的一部分、常用换码序列如下：| 换码符号 | 含义 || ——– | ————————————————- || \d | 系统当前的日期、d应该是date的第1个字母 || \t | 系统当前的时间、t应该是time的第1个字母 || \h | 简短形式的主机名、h应该是host（主机）的第1个字母 || \u | 当前用户名、u应该是user的第1个字母 || \w | 当前的工作目录、w应是working directory的第1个字母 || ! | 当前命令的历史编号、！为执行历史命令的第1个字符 || $ | 如果是普通用户显示$、而如果是root用户显示# || \l | 显示shell终端设备的基本名、l应该是line的第1个字母 | 除了上述常用转换码外，也可以通过环境变量值设置展示内容（例如conda环境等）参考配置12345678export PS1=&quot;\[\033]0;\h$\w\007\033[36m\]\u@\h[\d] \[\033[33m\]\w\[\033[0m\]\n$ &quot;# 显示样式如下liubo4@tj-login-24-4[一 10月 24] /ifstj1/B2C_RD_P1/$PS1=&quot;($CONDA_DEFAULT_ENV)\[\033]0;\h$\w\007\033[36m\]\u@\h[\d] \[\033[33m\]\w\[\033[0m\]\n$ &quot;# 显示样式如下(base)liubo4@tj-login-24-4[六 11月 12] /ifstj2/B2C_RD_H1/ 别名alias 使用alias命令为history命令创建别名h 使用alias命令为rm -i创建一个名为del的别名 使用不带任何参数的alias命令列出所有的别名 取消别名命令为：unalias 别名的名字12alias h=historyalias del=‘rm -i’ 相关问题~/.bashrc不能自动source最近更换了一个集群，更改了配置文件，却发现每次登陆都需要手动source，~/.bashrc不能自动执行，表示手动用了几次，发现每次这样简直不能忍～。查了一些资料来解决这个问题。 缺少~/.bash_profile创建 ~/.bash_profile 文件，并在文件开始位置添加如下内容：。 12345# .bash_profile# Get the aliases and functionsif [ -f ~/.bashrc ]; then . ~/.bashrcfi 终端颜色配置1export PS1=&quot;\[\033]0;\h$\w\007\033[36m\]\u@\h[\d] \[\033[33m\]\w\[\033[0m\]\n$ &quot; 中文支持异常集群中文支持出现异常，配置中文支持123LANG=&quot;zh_CN.UTF-8&quot; SYSFONT=&quot;latarcyrheb-sun16&quot; SUPPORTED=&quot;zh_CN.UTF-8:zh_CN:zh&quot; 如果遇到 cat可以显示中文，less无法显示中文的情况，123export LANG=&quot;zh_CN.UTF-8&quot;export LC_ALL=&quot;zh_CN.UTF-8&quot;export LC_CTYPE=&quot;zh_CN.UTF-8&quot; Linux常用命令安装Reference[https://blog.csdn.net/weixin_38492159/article/details/106464087]]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Bash</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[BASH的基本语法]]></title>
    <url>%2F03.%E5%BA%94%E7%94%A8-02.Linux%E5%92%8Cshell%2FLinux-BASH%E7%9A%84%E5%9F%BA%E6%9C%AC%E8%AF%AD%E6%B3%95%2F</url>
    <content type="text"><![CDATA[参考资料 之前更多的是直接使用Linux命令组合，对shell命令的使用比较少，都是有需求临时百度，没有系统的了解过，趁此机会，系统了解一下bash。 目录 最简单的例子 —— Hello World! 关于输入、输出和错误输出 BASH 中对变量的规定（与 C 语言的异同） BASH 中的基本流程控制语法 函数的使用 1.最简单的例子 —— Hello World!几乎所有的讲解编程的书给读者的第一个例子都是 Hello World 程序，那么我们今天也就从这个例子出发，来逐步了解 BASH。 用 vi 编辑器编辑一个 hello 文件如下： 123#!/bin/bash #第一行说明文件的类型，Linux系统根据 &quot;#!&quot; 及该字串后面的信息确定该文件的类型# This is a very simple example #在 BASH 程序中从“#”号（注意：后面紧接着是“!”号的除外）开始到行尾的多有部分均被看作是程序的注释。echo Hello World #bash的执行命令 如何执行该程序呢？有两种方法：一种是显式制定 BASH 去执行： bash hello或sh hello （这里 sh 是指向 bash 的一个链接，“lrwxrwxrwx 1 root root 4 Aug 20 05:41 /bin/sh -&gt; bash”） 或者可以先将 hello 文件改为可以执行的文件，然后直接运行它，此时由于 hello 文件第一行的 “#! /bin/bash” 的作用，系统会自动用/bin/bash 程序去解释执行 hello 文件的： 123#! bashchmod +x hello./hello 此处没有直接 “$ hello”是因为当前目录不是当前用户可执行文件的默认目录，而将当前目录“.”设为默认目录是一个不安全的设置。 需要注意的是，BASH 程序被执行后，实际上 Linux 系统是另外开设了一个进程来运行的。 2. 关于输入、输出和错误输出在 Linux 系统中：标准输入(stdin)默认为键盘输入；标准输出(stdout)默认为屏幕输出；标准错误输出(stderr)默认也是输出到屏幕（上面的 std 表示 standard）。在 BASH 中使用这些概念时一般将标准输出表示为 1，将标准错误输出表示为 2。 12345# 不常用的方法n&lt;&amp;- #表示将 n 号输入关闭 &lt;&amp;- #表示关闭标准输入（键盘）n&gt;&amp;- #表示将 n 号输出关闭&gt;&amp;- #表示将标准输出关闭 33. BASH 中对变量的规定BASH 中的变量都是不能含有保留字，不能含有 “-“ 等保留字符，也不能含有空格。 简单变量在 BASH 中变量定义是不需要的，没有 “int i” 这样的定义过程。如果想用一个变量，只要他没有在前面被定义过，就直接可以用，当然你使用该变量的第一条语句应该是对他赋初值了，如果你不赋初值也没关 系，只不过该变量是空（ 注意：是 NULL，不是 0 ）。不给变量赋初值虽然语法上不反对，但不是一个好的编程习惯。好了我们看看下面的例子：]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Bash</tag>
        <tag>Shell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[samtools-安装]]></title>
    <url>%2F03.%E5%BA%94%E7%94%A8-01.software_in_NGS%2FSoftware-samtools-%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[#1. 环境配置 12345#报错缺少时 curses.h: No such file or directoryyum install ncurses-devel ncurses#报错缺少时 bzlib.h: No such file or directoryyum install bzip2-devel.x86_64 #2. 软件安装1234567wget -c https://github.com/samtools/samtools/releases/download/1.9/samtools-1.9.tar.bz2tar xvf samtools-1.9.tar.bz2cd samtools-1.9/./configure --prefix=~/biosoft/samtools-1.9makemake install 3. 异常处理##3.1 samtools打开的文件句柄数目超出Linux限制 问题描述在使用Samtools进行排序时，遇到如下情况，程序中断，从截图中可以看到，samtools一共需要合并1032个bam文件，但是在合并到1020个文件时，程序打开文件失败了。因为常用samtools的时候，我们都知道这个sort.1020.bam 文件，其实是samtools在进行排序处理过程中，生成的中间文件。而这个报错是在合并时出现的（意味着该文件已经生成）。所以经过查找，发现是Linux文件句柄存在限制导致的问题。 12$ ulimit -n1024 # Linux系统设置的，一个程序支持打开的文件句柄数 本次问题中，Linux系统支持最多打开1024个文件，但是需要排序的文件超过了这个数字，因为由于句柄超出，软件处理失败。 解决方案其实Linux是有文件句柄限制的，一般都是1024，因此我们需要把这个值改大一些。这个1024是当前用户给准备要运行的程序的限制。 针对非Root用户1234567ulimit -n 2048 # 句柄数调整为2048。 #调整当前工作窗口，重启工作窗口后会重置。``` 2. 针对具有root权限的用户``` $ cat &gt;&gt; /etc/security/limits.conf soft nofile 1000000hard nofile 1000000 将ulimit 值添加到/etc/profile文件中（适用于有root权限登录的系统）为了每次系统重新启动时，都可以获取更大的ulimit值，将ulimit 加入到/etc/profile 文件底部。]]></content>
      <categories>
        <category>NGS</category>
        <category>software</category>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>环境配置</tag>
        <tag>软件安装</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[R安装]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-03.R%2FR-%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[直接安装R，作为数据计算和统计的经典工具，很多行业的数据分析和挖掘都跟它息息相关。熟悉R，对传统行业的数据的体量和分析会有一个初步的认识。了解R，我们先从安装它开始。Windows和Mac下有专门的安装程序，可以从https://www.r-project.org/， 可以直接安装。Linux下也可以通过相应的yum或者apt-get进行安装。然而，有些情况下，如Linux软件中心带的R程序如果太old，无法与其它的程序比如Scala或者Java兼容，则需要手工编译源码进行安装。这种繁琐的环节最好不要遇上，否则会比较��，此文用来纪念这个过程。R程序本身有很多依赖，建议参考本文先把依赖都装上去，然后再build R源程序。或者也可以直接编译R源代码，需要什么依赖安装什么依赖。 yum install -y readline-devel gcc*yum install libXt-devel -y 1.zlibwget http://ncu.dl.sourceforge.net/project/libpng/zlib/1.2.8/zlib-1.2.8.tar.gztar -zxvf zlib-1.2.8.tar.gzcd zlib-1.2.8./configure –prefix=/opt/zlib-1.2.8make &amp;&amp; make install 2.bzipwget http://www.bzip.org/1.0.6/bzip2-1.0.6.tar.gztar -zxvf bzip2-1.0.6.tar.gzcd bzip2-1.0.6make -f Makefile-libbz2_somake cleanmakemake install PREFIX=/opt/bzip2-1.0.6cd /opt/ 3. xzwget http://tukaani.org/xz/xz-5.2.2.tar.gztar xzvf xz-5.2.2.tar.gzcd xz-5.2.2./configure –prefix=/opt/xz-5.2.2make -j3 &amp; make install 4. pcrewget http://fossies.org/linux/misc/pcre-8.39.tar.gztar -zxvf pcre-8.39.tar.gzcd pcre-8.39./configure –prefix=/opt/pcre-8.39 –enable-utf8make &amp; make install 5. openssl(不是必须的，如果机子上已经安装则可以跳过)yum install openssl* 6. CURLwget http://www.execve.net/curl/curl-7.50.1.tar.gztar zxvf curl-7.50.1.tar.gzcd curl-7.50.1./configure —prefix=/opt/curl-7.50.1make &amp;&amp; make install 7.更新链接lib库和PATH路径echo /opt/xz-5.2.2/lib &gt;&gt; /etc/ld.so.confecho /opt/pcre-8.39/lib &gt;&gt; /etc/ld.so.confecho ‘export PATH=/opt/R-3.3.1/bin:${PATH}:/opt/curl-7.50.1/bin’ &gt;&gt; /root/.bashrcsource /root/.bashrc 8. 安装R程序wget http://mirrors.xmu.edu.cn/CRAN/src/base/R-3/R-3.3.1.tar.gztar -zxvf R-3.3.1.tar.gzcd R-3.3.1./configure –prefix=/opt/R-3.3.1 –enable-R-shlib LDFLAGS=”-L/opt/zlib-1.2.8/lib -L/opt/bzip2-1.0.6/lib -L/opt/xz-5.2.2/lib -L/opt/pcre-8.39/lib -L/opt/curl-7.50.1/lib” CPPFLAGS=”-I/opt/zlib-1.2.8/include -I/opt/bzip2-1.0.6/include -I/opt/xz-5.2.2/include -I/opt/pcre-8.39/include -I/opt/curl-7.50.1/include”ldconfigmaketouch doc/NEWS.pdf（Install R的过程中，遇到了一个NEWS.pdf找不到，用这个办法绕过的）make install 安装成功后，可以通过以下办法进行测试。root@cu01 R-3.3.1]# lsbin include lib lib64 share[root@cu01 R-3.3.1]# R R version 3.3.1 (2016-06-21) – “Bug in Your Hair”Copyright (C) 2016 The R Foundation for Statistical ComputingPlatform: x86_64-pc-linux-gnu (64-bit) R是自由软件，不带任何担保。在某些条件下你可以将其自由散布。用’license()’或’licence()’来看散布的详细条件。 R是个合作计划，有许多人为之做出了贡献.用’contributors()’来看合作者的详细情况用’citation()’会告诉你如何在出版物中正确地引用R或R程序包。 用’demo()’来看一些示范程序，用’help()’来阅读在线帮助文件，或用’help.start()’通过HTML浏览器来看帮助文件。用’q()’退出R. conda 安装R1conda install -c conda-forge r-base]]></content>
      <categories>
        <category>环境配置</category>
        <category>编程拾慧</category>
        <category>R</category>
      </categories>
      <tags>
        <tag>R</tag>
        <tag>软件安装</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Bedtools 的使用]]></title>
    <url>%2F03.%E5%BA%94%E7%94%A8-01.software_in_NGS%2FSoftware-bedtools%2F</url>
    <content type="text"><![CDATA[bedtools是一个针对bed进行处理的工具，算是类似bwa一样，接触该行业不会陌生的一个软件。再次介绍一下bedtools的一些常用的功能。 单个bed文件的处理 对原始bed进行排序1bedtools sort -i BedPrePare/Pancancer_v2.bed &gt; BedPrePare/Pancancer_v2.sort.bed 对原始bed区域进行合并(合并有交集的区域)1bedtools merge -i BedPrePare/Pancancer_v2.sort.bed &gt; BedPrePare/Pancancer_v2.merge.bed 多个bed文件的处理处理 保留两个 bed 的交集部分 1bedtools intersect -a Pancancer_v2.anno.Oldtrans.bed -b Pancancer_v2.anno.Newtrans.bed &gt; old_new.overlap.bed 基于结构文件(-b)对原始bed区域进行注释,保留-b文件的注释列(-wb) 1bedtools intersect -a BedPrePare/Pancancer_v2.merge.bed -b NCBI.gff2bed.bed -wb | cut -f1-3,7-10 &gt; BedPrePare/Pancancer_v2.anno.bed 评估相关基因的覆盖情况 1bedtools intersect -b BedPrePare/Pancancer_v2.merge.bed -a NCBI.gff2bed.bed -wao &gt; BedPrePare/Pancancer_v2.covercheck.bed 保留 -a文件的所有区域，并记录交集区域的注释信息$ bedtools intersect -a WESv5withSafeRisk.bed -b GeneTranscript_RD_Tumor.NewTrans.20231109.bed -wao | head chr1 65498 65638 chr1 65434 65519 OR4F5 NM_001005484.2 IU IVS1 + - 21 chr1 65498 65638 chr1 65565 65573 OR4F5 NM_001005484.2 C1 EX2 + - 8 chr1 65498 65638 chr1 65574 69036 OR4F5 NM_001005484.2 IC1 IVS2 + - 64 chr1 65498 65638 chr1 65520 65564 OR4F5 NM_001005484.2 5&#39;UTR EX2 + - 44 chr1 69036 70008 chr1 69037 70008 OR4F5 NM_001005484.2 C2 EX3 + - 971 chr1 367648 368607 chr1 367659 368597 OR4F29 NM_001005221.2 C1 EX1 + - 938 chr1 564537 564657 . -1 -1 . . . . . . 0 chr1 621085 622044 chr1 621096 622034 OR4F16 NM_001005277.1 C1 EX1 - - 938 获得两个 bed 的差集, B(-b) 产品比 A(-A) 产品缺少的区域1bedtools subtract -a Pancancer_v2.anno.Oldtrans.bed -b Pancancer_v2.anno.Newtrans.bed &gt; old.subtract.new.bed bed文件的注释 使用参考基因组进行bed的注释12345678bedtools nuc -fi $hg19 -bed /jdfstj6/B2C_RD/liubo4/product/cWES/src/db/Hyper_ExMit_TR/cns_region_GRCh37_bychr/WESv5withSafeRisk.bed# 结果前面几列是bed输入字段，后面是区域内各个碱基的情况统计结果，示例如下：#1_usercol 2_usercol 3_usercol 4_pct_at 5_pct_gc 6_num_A 7_num_C 8_num_G 9_num_T 10_num_N 11_num_oth 12_seq_lenchr1 65498 65638 0.692857 0.307143 49 19 24 48 0 0 140chr1 69036 70008 0.573045 0.426955 231 228 187 326 0 0 972chr1 367648 368607 0.541189 0.458811 204 245 195 315 0 0 959chr1 564537 564657 0.608333 0.391667 43 40 7 30 0 0 120]]></content>
      <categories>
        <category>NGS</category>
        <category>software</category>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>变异检测</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Software-MANTIS调研]]></title>
    <url>%2F03.%E5%BA%94%E7%94%A8-01.software_in_NGS%2FSoftware-MSI-MANTIS%E8%B0%83%E7%A0%94%2F</url>
    <content type="text"><![CDATA[简介MANTIS (Microsatellite Analysis for Normal-Tumor InStability) 是一个从成对的BAM文件中检测微卫星不稳定性的程序。 为了进行分析，程序需要一个肿瘤BAM和一个匹配的正常BAM文件(使用相同的管道生成)来确定两个样本之间的不稳定性得分。 推荐使用较长的读数(理想情况下，100bp或更长)，因为较短的读数不太可能完全覆盖微卫星位点，并且在质量控制过滤器失败后将被丢弃。MANTIS 于2017年发表在Oncotarget ( Performance evaluation for rapid detection of pan-cancer microsatellite instability with MANTIS) 如果需要使用该软件，可以通过Github 进行获取。 软件使用软件的安装安装软件的依赖包该软件是基于Python开发的，软件的运行需要安装下列Python库 NumPy(v1.6.2) Pysam(v0.8.3) 使用方法天津集群安装，conda环境调试成功使用环境Manti （先执行命令 source activate Manti）在模拟环境中进行分析，测试命令：1234python /share/udata/liubo/Software/MANTIS-master/mantis.py -b loci.bed --genome hg19.fa -n normal.bam -t cancer.bam -o MANTIS.txt --threads 8 # threads设置软件运行的线程数。 参考基因组和bed文件，可以使用配置未见的方式存储到配置文件中。引用配置文件使用 -cfg 参数。123#配置文件格式genome = /path/to/reference/genome.fastabedfile = /path/to/my/loci.bed 软件原理 阅读bed文件，获得目标区域，然后根据基因组信息，把这些靶标位点比对到参考基因组上，建立索引（通过0和1）； 每次针对一个靶标区域，工具首先从normal和cancer样本比对后的bam文件中抽提覆盖这个区域的reads，然后对这些reads进行一个初步的质控过滤，其中过滤标准如下： 1.确保这些reads都达到了合格满意的序列长度,默认：35（参数：mrl） 2.每个base的平均质量值得分达到要求的最小值，默认：25（参数：mrq） 3.覆盖的整个的靶标区域。 对通过初步过滤的reads单独进行分析，检测微卫星结构的起始位点，以及总的重复次数（通过起始位点开始，匹配微卫星结构的匹配次数） 一旦确定了微卫星的repeat次数以后，对数据进行第二次的质控， 1.确定在reads序列末端前loci没有被打断。 2.确定微卫星区域中的每个base的平均质量值都比较高，且达到标准，默认：30（mlq） 针对tumor和normal文件，分别统计不同的repeat counts的reads支持数，获得每个靶标区域的结构重复的reads支持数,reads支持数过低的repeat count 会被移除，默认：3（mrr） 确定repeat counts数以后，对每个靶标区域进行质控。将重复序列长度中偏离标准值过大的repeat count数移除，默认3倍标准差（参数sd ） ，会进行移除。 然后针对normal和cancer，检查每个靶标的总reads数目，确保reads的数目足够获得一个统计学显著的分布，达不到标准的loci会被丢弃。默认：30（mlc） 利用最终剩下的repeat count数据，进行得分计算，获得每个样本的一个不稳定得分。计算方法每个loci单独计算，分别对normal和cancer进行标准化（利用样本在loci的总reads数，将每个repeat的reads支持数，标准化为一个分数，从而量化在两个样本在深度和覆盖度上的差异），根据标准化的reads count计算stepwise： 一旦每个区域的得分计算出来以后，所有区域的不稳定性得分的平局值就可以计算得到，从而用一个数值来反映样本的不稳定程度。（得分范围是,值越大，越不稳定，值越小越稳定）]]></content>
      <categories>
        <category>NGS</category>
        <category>software</category>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>MSI</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CNV检测-DECoN]]></title>
    <url>%2F03.%E5%BA%94%E7%94%A8-01.software_in_NGS%2FSoftware-CNV%E6%A3%80%E6%B5%8B-DeCon%2F</url>
    <content type="text"><![CDATA[Publish原文链接PDF: Accurate clinical detection of exon copy number variants in a targeted NGS panel using DECoN DECoN v1.0.0 Documentation DECoN软件Git 背景二代靶向测序(NGS)芯片越来越多地应用于临床基因组学，以提高基因检测的能力、通量和降低检测成本。在靶向捕获测序芯片中进行整个外显子的缺失和扩增是非常有挑战，特别是针对单个外显子的CNV检测。本文献提供了一个针对外显子靶向捕获数据进行外显子水平拷贝数变异的工具（DECoN）。累计用到2016例样本，其中96例（10 samples with exon CNVs in BRCA1, six with exon CNVs in BRCA2, and 15 samples with exon CNVs in one of eight other genes: TP53, SDHB, MLH1, MSH2, NSD1, EZH2, WT1 and FH）评估集合，和1920临床测试集合；样本使用的是淋巴瘤的外周血和唾液。 MethodDECoN(Detection of Exon Copy Number) 是基于ExomeDepth (v.1.0.0) 进行开发修改的,其中有两个比较重要的改动。 可以检测染色体上第一个（Bed文件中定义的）外显子发生的变异，之前的版本不支持； HMM转移矩阵的概率是基于外显子的距离计算的 ，在这里如果两个外显子在染色体上的距离太远的话，那他们将作为两个独立的变异进行处理。这部分升级也在后面应用到ExomeDepth (v.1.1.0) 了。同时DECoN还增加了一些其他的功能，例如对依赖包进行标准化，保证临床应用过程中，不同实验室的结果一致性。 4.1 Reading BAM files to generate coverage metricDECoN的输入是一系列的Bam文件列表和一个bed文件（描述需要计算覆盖度矩阵的exon区域），然后去计算Bed文件中的每个外显子的FPKM（fragment per kilobase and million base pairs ) 获得每个外显子的覆盖度矩阵。 DECoN使用这个矩阵进行外显子层面的CNV检测. FPKM 的计算方式如下:FPKM = C/(N*L)其中C是map到目标外显子的Reads数目（单位条）；其中N是一个样本能比对到基因组上的总Reads数目（单位Million）其中L是目标外显子的碱基长度（单位base） eg: For example, consider a sample with a total of 20 million mapped read pairs of which 200 map to an exon which is 100 bases long: FPKM = 200/(20*0.1)Thus FPKM for this exon in this sample is 100. 4.2 Running quality checks外显子和样本的评估都是基于它们的平均覆盖率水平。当覆盖率较低时，检测的准确性将受到影响，因此在解释结果时应谨慎行事。 样本也根据它们与其他样本的相关性进行评估。如果样本与集合中其他样本的相关性不高，则很可能在整个目标中出现次优检测。下面给出了支持此质量标志的建议默认阈值。 Minimum correlation threshold the minimum correlation between a test sample and any other sample for the test sample to be considered well-correlated. The default value is 0.98. Minimum coverage thresholdthe minimum median coverage for any sample (measured across all exons in the target) or exon (measured across all samples) to be considered well-covered. The default value is 100. Calling exon CNVsThe HMM transition probabilities are altered from ExomeDepth v1.0.0. to depend upon the distance between exons, so that exons adjacent in the list of targeted regions are treated independently if they are located so far apart on the chromosome that the probability of a germline variant spanning both exons is negligible, specifically: The probability of transitioning into a CNV state (from normal to deletion or from normal to duplication) is given by a constant transition probability specified by the user (set as default to .01). The probability of transitioning to a normal state from a CNV state (from deletion to normal or from duplication to normal) is given by a baseline probability scaled by the distance between exons. If the distance between these exons is 0, then this scaling factor is simply 1, but as the distance increases, the scaling factor tends to 0. This is given by **exp(−𝑙𝐸)∗1/𝑡** where 𝑙 is the distance from the previous exon; E is the expected CNV length in basepairs; and t is the baseline probability of returning to a normal state from a deletion/duplication. These values are set as E=50000 and t=.5. 软件安装测试过程发现，DeCon在R3.6.1版本，会存在较多兼容性问题，建议按官方推荐使用R3.1.2 1conda create -n r3.1.2 -c conda-forge r-base=3.1.2]]></content>
      <categories>
        <category>NGS</category>
        <category>software</category>
        <category>编程拾慧</category>
        <category>R</category>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>CNV</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[GATK4初探 - 1]]></title>
    <url>%2F03.%E5%BA%94%E7%94%A8-01.software_in_NGS%2FSoftware-GATK4%E5%88%9D%E6%8E%A2-1%2F</url>
    <content type="text"><![CDATA[germline somatic detail GATK4 简介；参考地址WorkflowWDL语法Best Practices GATK4.0的改动 Re-engineered for speed、 scalability and versatillity Expanded scope of analysis to more variant types Reproducible best practices worlflowsGATK4。0 协议Under BSD3.0 streamlined arhciecture（overall efficlency）Intel Genomics Kernel Library （speed）Intel GenomicsDB （scalability）Apache Spark support（robust parallelism）Google Dataproc and GCS support（cloud execution）Versatility of data traversal （analysis scpe） 变异检出Geretic changes in individuals relative to a reference genome Germline（inherited） Somatic（cancer） Reference genome= a standardized genomic sequenceHuman genome reference sequence Previous standard hg19/b37 New sandard ： hg38 变异检出造成干扰的原因： 噪音、污染、纯度、 GATK4 对应不同变异检出的程序： Germline SOMATIC SNPs&amp;Indel HaplotypeCaller GVCF MuTect2 CNV GATK gCNV（beta） GATK CNV +aCNV Structure GATK SVDiscovery（beta） Planned GATK bestPractices 单样本变异检出算法 Mutect 支持但样本的SNV检出（假阳性多） GATK workflowsGithub流程Script使用WDL编写 变异检测过程；Step1 Identify ActiveRegions]]></content>
      <categories>
        <category>NGS</category>
        <category>software</category>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>变异检测</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CNV检测-调研]]></title>
    <url>%2F03.%E5%BA%94%E7%94%A8-01.software_in_NGS%2FSoftware-CNV%E6%A3%80%E6%B5%8B-%E8%B0%83%E7%A0%94%2F</url>
    <content type="text"><![CDATA[germline CNV software ###CNV主要方法 ###1. Read-depth 2. Read-pair 3. Split-read 4. Assembly 软件汇总 👍🏻CODEX2(2018年更新版本提高灵敏度，CODEX：2016年) 使用语言：R 数据类型：WES/WGS，靶向扩增子测序 算法特点：应用基于对数线性分解的归一化，基于泊松似然的递归分割算法 特点：专为种系和体细胞 CNV 调用而设计 限制：0.2 &lt; GC &lt; 0.8，Target length &gt; 20 bp，median target coverage &gt; 20 × ，mappability &gt; 0.9 参考文献4，认为CODEX上一代虽然识别出较多common CNV但是灵敏度低。 👍🏻DECoN（两篇文献支持假阴性最低） 使用语言：R 数据类型：靶向扩增子测序 算法特点：beta-二项式分布，加入参考集优化的过程 特点：适用于exon-base的panel测序，可以识别single exon CNV。基于ExomeDepth的基础上修改的。新增了染色体上第一个外显子区域的变异检测，HMM模型中增加了exon之间距离的因素。允许自动选择控制样本。 输入文件：bam、bed、ref 缺点：定义R包版本依赖，需要严格版本控制。需要确保其他分析不会更改其版本。 👍🏻CNVkit（现有CNV software） 使用语言：Python 数据类型：WES/WGS，靶向扩增子 算法特点：不仅考虑目标区域的归一化 Read Counts，还考虑非目标区域的归一化 Read Count。 特点：专为种系和体细胞 CNV 调用而设计。可以通过修改method参数使其应用于WGS和靶向扩增子测序数据。获得每个bin的log2copy ratio。标准化过程补充偏好性矫正，包括GC含量，repeat-masked比例。 限制：排除poor mappable regions 👍🏻GATK gCNV（优化了XHMM） 使用语言：Java、Python 数据类型：WES/WGS，靶向扩增子测序 算法特点：负二项式分布，针对RC归一化和HMM call CNV自洽性做了优化。 特点：主要分成两个部分：模型创建和sample calling。 👎🏻ExomeDepth（不考虑理由是DECoN基于此进行了优化） 使用语言：R 数据类型：WES 算法特点：beta-二项式分布，加入参考集优化的过程 软件特点：专为种系和体细胞 CNV 调用而设计 限制：ead mapq &gt; 20, max distance between target border and the middle of paired read to include read into region 300 bp， Transition probability to CNV 0.0001， Expected CNV length 50 kb 👎🏻XHMM（GATK gCNV优化过，不考虑） 数据类型：WES 算法特点：主成分分析来降低噪声，在 Z-RPKM 上使用隐马尔可夫模型 限制：至少50样本, 0.1 &lt; GC &lt; 0.9, 10 bp &lt; target &lt; 10 kbp, mean coverage &gt; 10 × across all samples, average targets 6, distance between targets 70 kb, average rate of CNV occurrence in the exome 10–8 参考文献4，认为其对gCNV敏感性低。 👎🏻CONTRA 使用语言：Python、R 数据类型：WES 特点：专注于外显子级的CNV识别 限制：Include regions at least 10-bp long with coverage &gt; 10 👎🏻PatternCNV 特点：专注于外显子级的CNV识别，专为种系和体细胞 CNV 调用而设计 Bin size 10，mapq &gt; 20 👎🏻EXCAVATOR2 算法特点：不仅考虑目标区域的归一化 RC，还考虑非目标区域的归一化 RC。 软件特点：专为种系和体细胞 CNV 调用而设计 限制：Read mapq &gt; 1，Min number of targets in CNV：4 参考文献4，认为其对gCNV敏感性低。 👎🏻exomeCopy 使用语言：R 数据类型：WES 算法特点：负二项式分布 precision：4%（参考文献1） recall：27%（参考文献1） 限制：mapq &gt; 1, overlap to include read into region—1 bp, median value for background, transition probability to CNV 1e-4 Transition probability to normal state 0.05 👎🏻CANOES 使用语言：R 算法特点：负二项式分布 precision：3.9%（参考文献1） recall：0.2%（参考文献1） 限制：至少15个样本, average targets：6, distance between targets ：70 kb, average rate of CNV occurrence in the exome：10–8 👎🏻cn.MOPS 数据类型：WES/WGS 算法特点：混合使用泊松模型和贝叶斯方法 限制：至少6个样本，Minimum segments 5 缺点：假阴性偏高 👎🏻FishingCNV 算法特点：主成分分析降低噪声，使用CBS对背景进行归一化覆盖率比较 限制：Read mapq &gt; 15，Base quality 10，RPKM &gt; 3，FDR adjusted pvalue 0.05 👎🏻HMZDelFinder ###### 排除 ###### 限制：只能检测loss 👎🏻ExonDel ##### 排除 ##### 限制：只能检测CNV loss 👎🏻CLAMMS 使用语言：C 数据类型：WES 算法特点：加入了参考集优化的过程 限制：0.3 &lt; GC &lt; 0.7，mappability &gt; 0.75 👎🏻CoNIFER 数据类型：WES 算法特点：使用奇异分解执行系统偏差校正 限制：至少50个样本，Probes with median RPKM across samples &gt; 1, samples with a standard deviation of SVD-ZRPKM &lt; 0.5 👎🏻ClinCNV： 使用语言：R 数据类型：WES/WGS 深度：可分析低深度数据（1x） 可检测类别：germline、somatic 需提供数据：bed（常规三列 + GC 含量）、coverage（染色体、坐标、平均测序深度） 👎🏻DeAnnCNV 特点：可以在线使用，还可以进行变异注释 限制：CNV evidence threshold &gt; 80 ？？？？ 缺点：检测CNV很少 参考文献1：Benchmarking germline CNV calling tools from exome sequencing data 验证数据（gold standard）：NA12878 FishingCNV (1210 CNV) 和 exomeCopy (845 CNV) 数量最多； DeAnnCNV (2 CNV)数量最少； CONTRA、EXCAVATOR2、ExomeDepth 和 PatternCNV （ 200-300 ）； #在 CONTRA 和 PatternCNV 的情况下，这些是单外显子 CNV。 其他算法平均检测到 26 个变化。 CNV长度： CNV 的总长度从 50 kb 到 1304 Mb（神经病啊 这么长） 这表明需要过滤某些工具产生的调用，特别是 FishingCNV 和 exomeCopy。 （ExomeDepth、CONTRA、CANOES、CLAMMS、CNVkit、CODEX、FishingCNV、HMZDelFinder 和 PatternCNV）发现了小于 1 kb 的变异 CNVkit、CODEX、CANOES、EXCAVATOR2 和 FishingCNV 是少数能够同时检测 2 到 3 个目标区域的小 CNV 和长变异（超过 1 Mb）的算法 precision、recall和F1-score 参考文献2：A comparison of tools for copy-number variation detection in germline whole exome and whole genome sequencing data 验证数据（gold standard）：NA12878，in-house GB01–GB08 and GB09–GB38（from CytoScan HD SNP-array） CODEX call 出的CNV最少；GATK gCNV call出的CNV最多。 CODEX 检测到的数量与NA12878相近，但不等于全为真阳性 CLC Genomics Workbench 和 cn.MOPS检测到很多long CNV（&gt; 10，000bp） GATK gCNV识别出的片段主要集中在 &lt;500bp (WES)和500-1000bp（WGS）。比其他CNV检测出了更短的CNV cn.MOPS, CNVnator, Control-FREEC 在WGS样本中识别到了比其他software更多的 &gt;1,000 bp length CNVs 。然而标准品中一半的CNV，都为500bp以下 precision和recall（未进行过滤） GATK gCNV recall最高（both in WGS and WES）,随后是lumpy，DELLY，cnMOPS，Manta。但是低于31%的precision。 CNVkit在过滤后对WES表现出较高的precision和recall，而其他tools对wes数据的recall都不高。然而这个表现只针对NA12878样本。 结论gCNV recall表现最好 参考文献3：GATK gCNV: accurate germline copy-number variant discovery from sequencing read-depth 验证数据（gold standard）：从Genome STRiP获得的一个经过人工验证和FDR控制的callset 许多基于读长的CNV caller试图通过PCA降噪或回归消除系统偏差，或者通过对样本和基因组区域进行预聚类消除。随后使用隐马尔可夫模型或非参数变化点检测算法对CNV进行检测。 关键的是，这些方法在数据归一化和检测之间缺乏自洽性，导致前者无意中去除了引号，导致后者灵敏度降低。 GATK gCNV：有原则的贝叶斯方法，用来学习大型队列的读深数据全局和特定样本的偏差。负二项式分布与分层HMM相结合。偏差建模提高了自洽性。 比较了两个软件，一个是XHMM，一个是CODEX。发现GATK gCNV要比其他两个敏感度高将近20%。特异性高50% 参考文献4： CODEX2: full-spectrum copy number variation detection by high-throughput DNA sequencing、 验证数据（gold standard）：HapMap3、Conrad et.al、McCarroll et.al、1000GP wgs CODEX的升级版，提升了敏感度sensitivity XHMM、EXCAVATOR对common CNV检测缺少敏感性；CLAMMS也是precision高，但是sensitivity低。 CODEX可以检测到更多的common CNV，但是sensitivity低； CODEX2在四个验证集中召回率分别是92.8% 60.7% 79.2% 66.2%，同时特异性有显著提高。 参考文献5：Evaluation of CNV detection tools for NGS panel data in genetic diagnostics 验证数据（gold standard）：ICR96 exon CNV validation series , panelcnDataset, In-house MiSeq ，In-House HiSeq DECoN在各个样本集表现稳定 cn.MOPs(panelcn.MOPS是否是它的一个功能还是一个专门应用panel的版本)，默认参数下，假阴性有些高。 ExomeDepth和CODEX2也表现较好。CODEX2在默认参数下，有一个验证集敏感度偏低。优化参数后回到正常水平。 建议将CNV添加到流程前，针对数据集进行参数优化，作者开发了一个R包 https://github.com/TranslationalBioinformaticsIGTP/CNVbenchmarkeR 参考文献6：Free-access copy-number variant detection tools for targeted nextgeneration sequencing data 验证数据（gold standard）：模拟数据 4179 exons ranging in size from 53 bp to 17,155 bp （位于所有染色体except chr18） 三种类型：homozygous deletions (DEL-HO), heterozygous deletions (DEL-HT), and duplications (DUP) 名列前茅的分别是DECoN, exomeDepth, exomeCNV 比较300X和50X数据差距，除了CNVkit，其余软件均在高深度数据表现更好 DECoN假阴性最低]]></content>
      <categories>
        <category>NGS</category>
        <category>software</category>
        <category>编程拾慧</category>
        <category>R</category>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>CNV</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CNV检测-调研]]></title>
    <url>%2F03.%E5%BA%94%E7%94%A8-01.software_in_NGS%2FSoftware-CNV%E6%A3%80%E6%B5%8B-%E6%80%A7%E8%83%BD%E8%B0%83%E7%A0%94%2F</url>
    <content type="text"><![CDATA[软件文章DECoN软件发表文献 参考文献Accurate clinical detection of exon copy number variants in a targeted NGS panel using DECoN 综述类文章参考文献：2025 Advocating Targeted Sequential Screening over Whole Exome Sequencing in 21-Hydroxylase Deficiency 参考文献：Evaluation of three read-depth based CNV detection tools using whole-exome sequencing datacite: Yao, Ruen et al. “Evaluation of three read-depth based CNV detection tools using whole-exome sequencing data.” Molecular cytogenetics vol. 10 30. 23 Aug. 2017, doi:10.1186/s13039-017-0333-5 CoNIFER and XHMM have an even poorer detection sensitivity for smaller CNVs involving fewer capturing probes, whereas CNVnator had a rather consistent detection sensitivity for all size CNVs(&gt; 3 kb).总结： 灵敏性整体不到90%。（涵盖大片段CNV) 参考文献：Identification of Copy Number Variants Through Whole-Exome Sequencing in Autosomal Recessive Nonsyndromic Hearing LossCoNIFER 检测到的杂合假阳性缺失（40%）（区域大小为 3E1 kb）比例低于 XHMM（64%） 参考文献： Exome sequence read depth methods for identifying copy number changes提供的详细原表Paired data 配对数据| Program 程序（参考） | 性能数据 || ——————————————– | ———————————————————————————————————————————————— || ExomeCNV | 对于 WGS 数据，缺失的敏感性分别为 86% ；重复的敏感性为 88% || exome2cnv | 灵敏度超过 89%，假阳性率低于 ExomeCNV || PropSeg (proportionality based segmentation) | 与 ExomeCNV 相比，精确度提高了 13%，灵敏度没有差异。 || CoNIFER | 检出了 86% 的已验证 CNV，而 ExomeCNV 仅识别出了 14% || ExomeDepth | 与 exomeCopy 和 ExomeCNV 相比，检测到的已知 CNV 百分比更高（分别为 75.2%、52.8% 和 41.2%） || XHMM | 灵敏度为 67%–92%，较长的 CNV 灵敏度更高。与 CoNIFER 相比，孟德尔遗传定律违背率更低，假阳性率也更少。 || Excavator | 与 XHMM 和 CoNIFER 相比，灵敏度更高（分别为 45–55% 和 &lt;20% 和 &lt;5%）。与 XHMM 和 Exome CNV 相比，精确度更高（分别为 30–45% 和 &lt;30% 和 &lt;5%）。 || CONTRA | 与 ExomeCNV 相比，外显子敏感性更高（96.4% 对 62.2%） || ADTEx (Aberration Detection in Tumour Exome) | 与 VarScan、exomeCopy 和 CONTRA 相比，TP 的检测率更高（分别为 90%、86%、0% 和 0%）。 || FishingCNV | 在检测外显子水平的纯合缺失和杂合重复方面，该方法比 CoNIFER 更灵敏（分别为 93% vs 8%和 75% vs 0%）。 |]]></content>
      <categories>
        <category>NGS</category>
        <category>software</category>
        <category>编程拾慧</category>
        <category>R</category>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>CNV</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[蜜蜂群图绘制]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-03.R%2FR-%E8%9C%9C%E8%9C%82%E7%BE%A4%E5%9B%BE%E7%BB%98%E5%88%B6%2F</url>
    <content type="text"><![CDATA[输出结果 绘图命令1bee_plot_with_color(&quot;../../Desktop/seqencing depth.csv&quot;,&quot;xlab&quot;,&quot;ylab&quot;) 函数代码函数代码 12345678910111213141516171819202122bee_plot_with_color=function(In_file,xlab_text,ylab_text)&#123;library(beeswarm)read.csv(In_file,header=F)-&gt;a#biocLite(c(&quot;beeswarm&quot;,&quot;ggplot2&quot;))label_tag=a[1,]label_tag=as.matrix(label_tag)a=a[-1,]ncol(a)-&gt;num1mat=c()for(i in 1:num1)&#123;as.numeric(as.matrix(a[,i]))-&gt;temptemp[!is.na(temp)]-&gt;templength(temp)-&gt;numrep(i,num)-&gt;label_temprbind(mat,cbind(temp,label_temp))-&gt;mat&#125;beeswarm(mat[,1]~mat[,2], pch = 1,col = rainbow(10),labels=label_tag ,xlab=xlab_text , ylab=ylab_text)#pch对应的不同数值可以调整散点图中点的样式；boxplot(mat[,1]~mat[,2],add=T,names=label_tag)-&gt;S&#125; 输入文件：每列对应一类数据，head行，对应类的名称，下面每行是该类的具体数据，最终针对每类数据绘制对应的散点图和4分位图。demo数据]]></content>
      <categories>
        <category>编程拾慧</category>
        <category>R</category>
      </categories>
      <tags>
        <tag>可视化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[文件格式说明 - SAM/BAM]]></title>
    <url>%2F01.%E7%9F%A5%E8%AF%86-03.standard%2Ffileformat-SAM.BAM%2F</url>
    <content type="text"><![CDATA[Sam 格式文献Sam 格式博客Sam 格式说明 Bam 头文件头文件是一行行以 @ 开头的注释文件，文件记录了比对是所用的参考序列信息，序列的名称和序列的长度（@SQ），同时也记录了比对、Markdup、重比对等过程所使用的软件以及对应的软件版本（@PG），同时还会有记录的样本信息（@RG）；如果有需要更改样本的头文件，可以通过Samtools rehead# 定义一个新的头文件，来替代原来的头文件. Bam头文件示例 Bam 头文件：123456789101112@HD VN:1.0 GO:none SO:coordinate@SQ SN:chr1 LN:249250621@SQ SN:chr2 LN:243199373@SQ SN:chr3 LN:198022430@SQ SN:chr4 LN:191154276@SQ SN:chr5 LN:180915260@SQ SN:chr6 LN:171115067@SQ SN:chr7 LN:159138663@RG ID:cancer PL:illumina PU:FCHF3HCBCX2 LB:17D0846999-98 SM:cancer CN:BGI@PG ID:MarkDuplicates PN:MarkDuplicates VN:1.98(1547) CL:net.sf.picard.sam.MarkDuplicates INPUT=[/THL4/home/bgi_thcancer/liubo/Project/BRAC_CY/2018-05-29/17D0846999-98/cancer/3_markdup_bam/17D0846999-98_sort.bam] OUTPUT=/THL4/home/bgi_thcancer/liubo/Project/BRAC_CY/2018-05-29/17D0846999-98/cancer/3_markdup_bam/17D0846999-98_sort_markdup.bam METRICS_FILE=/THL4/home/bgi_thcancer/liubo/Project/BRAC_CY/2018-05-29/17D0846999-98/cancer/3_markdup_bam/17D0846999-98_sort_markdup.bam.metrics REMOVE_DUPLICATES=false ASSUME_SORTED=true VALIDATION_STRINGENCY=LENIENT PROGRAM_RECORD_ID=MarkDuplicates PROGRAM_GROUP_NAME=MarkDuplicates MAX_SEQUENCES_FOR_DISK_READ_ENDS_MAP=50000 MAX_FILE_HANDLES_FOR_READ_ENDS_MAP=8000 SORTING_COLLECTION_SIZE_RATIO=0.25 READ_NAME_REGEX=[a-zA-Z0-9]+:[0-9]:([0-9]+):([0-9]+):([0-9]+).* OPTICAL_DUPLICATE_PIXEL_DISTANCE=100 VERBOSITY=INFO QUIET=false COMPRESSION_LEVEL=5 MAX_RECORDS_IN_RAM=500000 CREATE_INDEX=false CREATE_MD5_FILE=false@PG ID:bwa PN:bwa VN:0.6.2-r126@PG ID:GATK IndelRealigner VN:2.3-9-ge5ebf34 CL:knownAlleles=[(RodBinding name=knownAlleles source=/THL4/home/bgi_thcancer/pipeline/chip_1.7M/db/aln/dbsnp/dbSNP132_1000GIndel_merge_for_realgn.txt)] targetIntervals=/THL4/home/bgi_thcancer/liubo/Project/BRAC_CY/2018-05-29/17D0846999-98/cancer/4_realign_bam/17D0846999-98_cancer.intervals LODThresholdForCleaning=5.0 consensusDeterminationModel=USE_READS entropyThreshold=0.15 maxReadsInMemory=150000 maxIsizeForMovement=3000 maxPositionalMoveAllowed=200 maxConsensuses=30 maxReadsForConsensuses=120 maxReadsForRealignment=20000 noOriginalAlignmentTags=false nWayOut=null generate_nWayOut_md5s=false check_early=false noPGTag=false keepPGTags=false indelsFileForDebugging=null statisticsFileForDebugging=null SNPsFileForDebugging=null Bam 内容部分Bam 的内容部分，包含了所有序列的比对信息，包括比对位置，比对质量值查看方式 samtools view ;在Bam/Sam输出的结果中每一行都包括十二项通过Tab分隔，从左到右分别是： 序列的名称 概括出一个合适的标记，各个数字分别代表 1 read是pair中的一条（read表示本条read，mate表示pair中的另一条read） 2 pair一正一负完美的比对上 4 这条read没有比对上 8 mate没有比对上 16 这条read反向比对 32 mate反向比对 64 这条read是read1 128 这条read是read2 256 第二次比对 512 比对质量不合格 1024 read是PCR或光学副本产生 2048 辅助比对结果 假如说标记为以上列举出的数目，就可以直接推断出匹配的情况。假如说标记不是以上列举出的数字，比如说 83=（64+16+2+1），就是这几种情况值和。 参考序列的名字 在参考序列上的位置 mapping qulity?? 越高则位点越独特bowtie2有时并不能完全确定一个短的序列来自与参考序列的那个位置，特别是对于那些比较简单的序列。但是bowtie2会给出一个值来显示出 这个段序列来自某个位点的概率值，这个值就是mapping qulity。Mapping qulity的计算方法是：Q=-10log10p，Q是一个非负值，p是这个序列不来自这个位点的估计值。假如说一条序列在某个参考序列上找到了两个位点，但是其中一个位点的Q明显大于另一个位点的Q值，这条序列来源于前一个位点的可能性就比较大。Q值的差距越大，这独特性越高。Q值的计算方法来自与SAM标准格式，请查看SAM总结。 代表比对结果的CIGAR字符串，如37M1D2M1I，这段字符的意思是37个匹配，1个参考序列上的删除，2个匹配，1个参考序列上的插入。M代表的是alignment match(可以是错配) “M”表示 match或 mismatch； “I”表示 insert； “D”表示 deletion； “N”表示 skipped（跳过这段区域）； “S”表示 soft clipping（被剪切的序列存在于序列中）； “H”表示 hard clipping（被剪切的序列不存在于序列中）； “P”表示 padding； “=”表示 match； “X”表示 mismatch（错配，位置是一一对应的）； mate 序列所在参考序列的名称 mate 序列在参考序列上的位置 估计出的片段的长度，当mate 序列位于本序列上游时该值为负值。 read的序列 ASCII码格式的序列质量 可选的区域 AS:i? 匹配的得分 XS:i? 第二好的匹配的得分 YS:i? mate 序列匹配的得分 XN:i? 在参考序列上模糊碱基的个数 XM:i? 错配的个数 XO:i? gap open的个数 XG:i? gap 延伸的个数 NM:i? 经过编辑的序列 YF:i? 说明为什么这个序列被过滤的字符串 YT:Z MD:Z? 代表序列和参考序列错配的字符串]]></content>
      <categories>
        <category>知识沉淀</category>
        <category>文件格式</category>
      </categories>
      <tags>
        <tag>Bam</tag>
        <tag>Sam</tag>
        <tag>文件格式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[文件格式说明 - Bed]]></title>
    <url>%2F01.%E7%9F%A5%E8%AF%86-03.standard%2Ffileformat-bed%2F</url>
    <content type="text"><![CDATA[定义Browser Extensible Data (BED) is a whitespace-delimited file format, where each file consists of one or more lines Each line describes discrete genomic features by physical start and end position on a linear chromosome. The file extension for the BED format is .bed. 官方文档 格式说明格式概览一个完整的bed文件包含如下的12列信息(可以通过标注bedn只使用其中的前n列信息) Col Field Type Regex or range Brief description 1 chrom String [[:alnum:]_]{1,255} Chromosome name 2 chromStart Int [0, 264 − 1] Feature start position 3 chromEnd Int [0, 264 − 1] Feature end position 4 name String [^\t]{0,255} Feature description 5 score Int [0, 1000] A numerical value 6 strand String [-+.] Feature strand 7 thickStart Int [0, 264 − 1] Thick start position 8 thickEnd Int [0, 264 − 1] Thick end position 9 itemRgb Int,Int,Int ([0, 255], [0, 255], [0, 255]) \ 0 Display color 10 blockCount Int [0, chromEnd − chromStart]5 Number of blocks 11 blockSizes List[Int] ([[:digit:]]+,){blockCount−1}[[:digit:]]+,?6 Block sizes 12 blockStarts List[Int] ([[:digit:]]+,){blockCount−1}[[:digit:]]+,? Block start positions 各列内容的详细介绍如下： 坐标(Coordinates) chrom: The name of the chromosome or scaffold where the feature is present. Limitingonly to word characters only, instead of all non-whitespace characters, makes BED filesmore portable to varying environments which may make different assumptions about allowedcharacters. The name must be between 1 and 255 characters long, inclusive. chromStart: Start position of the feature on the chromosome or scaffold. chromStart must bean integer greater than or equal to 0 and less than the total number of bases of the chromo-some to which it belongs. If the size of the chromoschromosomeome is unknown, then chromStart mustbe less than or equal to $2^{64}$ − 1, which is the maximum size of an unsigned 64-bit integer. chromEnd: End position of the feature on the chromosome or scaffold. chromEnd must bean integer greater than or equal to the value of chromStart and less than or equal to the totalnumber of bases in the chromosome to which it belongs. If the size of the chromosomeis unknown, then chromEnd must be less than or equal to $2^{64}$ − 1, the maximum size of anunsigned 64-bit integer. 简单属性(Simple attributes) name: String that describes the feature. The name must be 0 to 255 non-tab characters. Thename must not be empty or contain whitespace, unless all fields in file are delimited exclusivelyusing single tab characters. A visual representation of the BED format may display the namenext to the feature. score: Integer between 0 and 1000, inclusive. If the feature has no score information, then 0should be used as a default value. A visual representation of the BED format may shadefeatures differently depending on their score. strand: Strand that the feature appears on. The strand may either refer to the + (sense orcoding) strand or the - (antisense or complementary) strand. If the feature has no strandinformation or unknown strand, then a dot (.) must be used. Display attributes thickStart: Start position at which the feature is visualized with a thicker or accented display.This value must be an integer between chromStart and chromEnd, inclusive. There is nospecified default value for thickStart. thickEnd: End position at which the feature is visualized with a thicker or accented display.This value must be an integer greater than or equal to thickStart and less than or equalto chromEnd, inclusive. In BED files with fewer than 7 fields, the whole feature has thickdisplay. In BED7+ files, to achieve the same effect, set thickStart equal to chromStartand thickEnd equal to chromEnd. If this field is not specified but thickStart is, then theentire feature has thick display. There is no specified default value for thickEnd. itemRgb: A triple of integers that determines the color of this feature when visualized. Thetriple is three integers separated by commas. Each integer is between 0 and 255, inclusive. Tomake a feature black, itemRgb may be a single 0, which is visualized identically to a featurewith itemRgb of 0,0,0. Blocks blockCount: Number of blocks in the feature. blockCount must be an integer greater than 0.blockCount is mandatory in BED12+ files. Null or empty blockCount are not allowed,because blockSizes and blockStarts rely on blockCount. A visual representation of the BEDformat may have blocks appear thicker than the rest of the feature. blockSizes: Comma-separated list of length blockCount containing the size of each block. Theremust be no spaces before or after commas. There may be a trailing comma after the lastelement of the list. blockSizes is mandatory in BED12+ files. Null or empty blockSizes is notallowed, because blockStarts cannot be verified without blockSizes. blockStarts: Comma-separated list of length blockCount containing each block’s start position,relative to chromStart. There must not be spaces before or after the commas. There maybe a trailing comma after the last element of the list. Each element in blockStarts is pairedwith the corresponding element in blockSizes. Each blockStarts element must be an integerbetween 0 and chromEnd−chromStart, inclusive. For each couple i of (blockStartsi, blockSizesi),the quantity chromStart + blockStartsi + blockSizesi must be less or equal to chromEnd. Theseconditions enforce that each block is contained within the feature. The first block muststart at chromStart and the last block must end at chromEnd. Moreover, the blocks must notoverlap. The list must be sorted in ascending order. blockStarts is mandatory in BED12+files. Null or empty blockStarts is not allowed. 术语和概念 （Terminology and concepts）0-start, half-open coordinate system: bed区间的位置描述为0起始，区间为左闭右开区间。A coordinate system where the first base starts at position 0, and the start of the interval is included but the end is not. For example, for a sequence of bases ACTGCG, the bases given by the interval [2, 4) are TG. BEDn: 表示文件包含bed的前n个字段（总计12个字段）A file with the first n fields of the BED format. For example, BED3 means a file with only the first 3 fields; BED12 means a file with all 12 fields. BEDn+: 表示包含前n个bed的定义字段，后续跟随一些自定义字段。A file that has n fields of the BED format, followed by any number of fields of custom data defined by a user. BEDn+m: 表示包含前n个bed的定义字段，后续跟随m个自定义字段。A file that has a custom tab-delimited format starting with the first n fields of the BED format, followed by m fields of custom data defined by a user. For example, BED6+4 means a file with the first 6 fields of the BED format, followed by 4 user-defined fields block: Linear subfeatures within a feature. Usually used to designate exons chromosome: 染色体编号 A sequence of nucleobases with a name. In this specification, “chromosome” may also describe a named scaffold that does not fit the biological definition of a chromosome. Often, chromosomes are numbered starting from 1. There are also often sex chromosomes such as W, X, Y, and Z, mitochondrial chromosomes such as M, and possibly scaffolds from an unknown chromosome, often labeled Un. The name of each chromosome is often prefixed with chr. Examples of chromosome names include chr1, 21, chrX, chrM, chrUn, chr19_KI270914v1_alt, and chrUn_KI270435v1. feature: A linear region of a chromosome with specified properties. For example, a file’s features might all be peaks called from ChIP-seq data, or transcript. field: Data stored as non-tab text. All fields are 7-bit US ASCII. file: Sequence of one or more lines. line: String terminated by a line separator, in one of the following classes. Either a data line, a comment line, or a blank line. Discussed more fully in subsection 1.3 line separator: Either carriage return, line feed, or carriage return followed by line feed. The same line separator must be used throughout the file. 示意12 [Bedwen]]]></content>
      <categories>
        <category>知识沉淀</category>
        <category>文件格式</category>
      </categories>
      <tags>
        <tag>文件格式</tag>
        <tag>Bed</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[文件格式说明 - Generic Feature Format Version 3 (GFF3)]]></title>
    <url>%2F01.%E7%9F%A5%E8%AF%86-03.standard%2Ffileformat-gff3%2F</url>
    <content type="text"><![CDATA[简介Annotating Genomes with GFF3 or GTF filesGeneric Feature Format Version 3 (GFF3) v1.26GFF3 format Formatting requirements[1] seqid in GFF3/GTF column 1 should match the corresponding FASTA or ASN.1 file that is being annotated. For assemblies already in GenBank, seqids will be matched to their corresponding accessions if they are the same as what was used in the original submission. [The seqid is the text between the ‘&gt;’ and the first space in the fasta definition line; do not include the ‘&gt;’ in the GFF file] [2] contig, supercontig, chromosome and similar landmark features are not required and will be ignored. [3] multi-exon mRNA and other RNA features can be represented using either: [a] child exon features [b] child five_prime_UTR, CDS, and three_prime_UTR features [c] multiple RNA feature rows with the same ID Furthermore, whereas the GFF3 specifications require that all rows of a multi-exon CDS feature use the same ID, some commonly used software deviates from this requirement. To allow for deviations from the specifications, for eukaryotes the GenBank software assumes that multiple CDS rows with the same Parent attribute represent parts of the same CDS feature. Multiple CDS features for the same gene need to be annotated by using a separate mRNA Parent feature for each, so there is always a 1:1 relationship of mRNA to CDS, like in the following schematic:123456789gene1 ================================ ID=gene1mRNA1 ================================ ID=mRNA1;Parent=gene1five_prime_UTR == Parent=mRNA1CDS1 ==....=====...........== Parent=mRNA1 (3 rows)three_prime_UTR ====== Parent=mRNA1mRNA2 ================================ ID=mRNA2;Parent=gene1exon ==== Parent=mRNA2CDS2 ==....................== Parent=mRNA2 (2 rows)exon ======== Parent=mRNA2 [4] GFF3 ID attributes are required for interpreting parent-child feature relationships and that is their only role here. They are not automatically used for the locus_tag qualifier, so if the ID is applicable as the locus_tag, it should be copied into that attribute with the appropriate formatting. However, if no transcript_id, or protein_id qualifiers are present, then the GFF3 ID attribute will be used as the basis of those qualifiers, as described in point [5c] below. These qualifiers do not appear in the flatfile view, so if the GFF3 IDs are meant to be seen in that view, then they should be copied into a ‘note’ attribute with the appropriate formatting. [5] GFF3 Name attributes are ignored.]]></content>
      <categories>
        <category>知识沉淀</category>
        <category>文件格式</category>
      </categories>
      <tags>
        <tag>文件格式</tag>
        <tag>gff3</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[文件格式说明 - Vcf]]></title>
    <url>%2F01.%E7%9F%A5%E8%AF%86-03.standard%2Ffileformat-vcf%2F</url>
    <content type="text"><![CDATA[简介VCF (Variant Call Format) version 4.1The VCF specification is no longer maintained by the 1000 Genomes Project. The group leading the management and expansion of the format is the Global Alliance for Genomics and Health Data Working group file format team, http://ga4gh.org/#/fileformats-teamThe main version of the specification can be found on https://github.com/samtools/hts-specsThis is under continued development, please check the hts-specs page for the most recent specification A PDF of the v4.1 spec is http://samtools.github.io/hts-specs/VCFv4.1.pdfA PDF of the v4.2 spec is http://samtools.github.io/hts-specs/VCFv4.2.pdfVCFTools host a discussion list about the specification called vcf-spec http://sourceforge.net/p/vcftools/mailman/ REF:http://blog.sina.com.cn/s/blog_12d5e3d3c0101qv1u.htmlhttp://samtools.github.io/hts-specs/VCFv4.2.pdfhttp://samtools.github.io/bcftools/bcftools.html VCF（Variant Call Format）文件示例（VCFv4.2）123456789101112131415161718192021222324##fileformat=VCFv4.2##fileDate=20090805##source=myImputationProgramV3.1##reference=file:///seq/references/1000GenomesPilot-NCBI36.fasta##contig=&lt;ID=20,length=62435964,assembly=B36,md5=f126cdf8a6e0c7f379d618ff66beb2da,species=&quot;Homo sapiens&quot;,taxonomy=x&gt;##phasing=partial##INFO=&lt;ID=NS,Number=1,Type=Integer,Description=&quot;Number of Samples With Data&quot;&gt;##INFO=&lt;ID=DP,Number=1,Type=Integer,Description=&quot;Total Depth&quot;&gt;##INFO=&lt;ID=AF,Number=A,Type=Float,Description=&quot;Allele Frequency&quot;&gt;##INFO=&lt;ID=AA,Number=1,Type=String,Description=&quot;Ancestral Allele&quot;&gt;##INFO=&lt;ID=DB,Number=0,Type=Flag,Description=&quot;dbSNP membership, build 129&quot;&gt;##INFO=&lt;ID=H2,Number=0,Type=Flag,Description=&quot;HapMap2 membership&quot;&gt;##FILTER=&lt;ID=q10,Description=&quot;Quality below 10&quot;&gt;##FILTER=&lt;ID=s50,Description=&quot;Less than 50% of samples have data&quot;&gt;##FORMAT=&lt;ID=GT,Number=1,Type=String,Description=&quot;Genotype&quot;&gt;##FORMAT=&lt;ID=GQ,Number=1,Type=Integer,Description=&quot;Genotype Quality&quot;&gt;##FORMAT=&lt;ID=DP,Number=1,Type=Integer,Description=&quot;Read Depth&quot;&gt;##FORMAT=&lt;ID=HQ,Number=2,Type=Integer,Description=&quot;Haplotype Quality&quot;&gt;#CHROM POS ID REF ALT QUAL FILTER INFO FORMAT NA00001 NA00002 NA0000320 14370 rs6054257 G A 29 PASS NS=3;DP=14;AF=0.5;DB;H2 GT:GQ:DP:HQ 0|0:48:1:51,51 1|0:48:8:51,51 1/1:43:5:.,.20 17330 . T A 3 q10 NS=3;DP=11;AF=0.017 GT:GQ:DP:HQ 0|0:49:3:58,50 0|1:3:5:65,3 0/0:41:320 1110696 rs6040355 A G,T 67 PASS NS=2;DP=10;AF=0.333,0.667;AA=T;DB GT:GQ:DP:HQ 1|2:21:6:23,27 2|1:2:0:18,2 2/2:35:420 1230237 . T . 47 PASS NS=3;DP=13;AA=T GT:GQ:DP:HQ 0|0:54:7:56,60 0|0:48:4:51,51 0/0:61:220 1234567 microsat1 GTC G,GTCT 50 PASS NS=3;DP=9;AA=G GT:GQ:DP 0/1:35:4 0/2:17:2 1/1:40:3 其中每列对应的含义 title 含义 CHROM： 表示变异位点是在哪个contig 里call出来的，如果是人类全基因组的话那就是chr1…chr22，chrX,Y,M。 POS： 参考基因组位置，第一个碱基的位置是1。按数字升序排列，允许有多条记录有相同的位置。变异位点相对于参考基因组所在的位置，如果是indel，就是第一个碱基所在的位置。 ID： 如果call出来的SNP存在于dbSNP数据库里，就会显示相应的dbSNP里的rs编号。 REF和ALT： 在这个变异位点处，参考基因组中所对应的碱基和研究对象基因组中所对应的碱基。 QUAL： 可以理解为所call出来的变异位点的质量值。Q=-10lgP，Q表示质量值；P表示这个位点发生错误的概率。因此，如果想把错误率从控制在90%以上，P的阈值就是1/10，那lg（1/10）=-1，Q=（-10）*（-1）=10。同理，当Q=20时，错误率就控制在了0.01。 FILTER： 理想情况下，QUAL这个值应该是用所有的错误模型算出来的，这个值就可以代表正确的变异位点了，但是事实是做不到的。因此，还需要对原始变异位点做进一步的过滤。无论你用什么方法对变异位点进行过滤，过滤完了之后，在FILTER一栏都会留下过滤记录，如果是通过了过滤标准，那么这些通过标准的好的变异位点的FILTER一栏就会注释一个PASS，如果没有通过过滤，就会在FILTER这一栏提示除了PASS的其他信息。如果这一栏是一个“.”的话，就说明没有进行过任何过滤。 到现在，我们就可以解释上面的例子：chr1：873762 是一个新发现的T/G变异，并且有很高的可信度（qual=5231.78）。chr1：877664 是一个已知的变异为A/G 的SNP位点，名字rs3828047，并且具有很高的可信度（qual=3931.66）。chr1：899282 是一个已知的变异为C/T的SNP位点，名字rs28548431，但可信度较低（qual=71.77）。chr1：974165 是一个已知的变异为T/C的SNP位点，名字rs9442391，但是这个位点的质量值很低，被标 成了“LowQual”，在后续分析中可以被过滤掉。 其中最后面两列是相对应的，每一个tag对应一个或者一组值，如：chr1：873762，GT对应0/1；AD对应173,141；DP对应282；GQ对应99；PL对应255,0,255。 GT： 表示这个样本的基因型，对于一个二倍体生物，GT值表示的是这个样本在这个位点所携带的两个等位基因。0表示跟REF一样；1表示表示跟ALT一样；2表示第二个ALT。当只有一个ALT 等位基因的时候，0/0表示纯和且跟REF一致；0/1表示杂合，两个allele一个是ALT一个是REF；1/1表示纯和且都为ALT； The most common format subfield is GT (genotype) data. If the GT subfield is present, it must be the first subfield. In the sample data, genotype alleles are numeric: the REF allele is 0, the first ALT allele is 1, and so on. The allele separator is ‘/‘ for unphased genotypes and ‘|’ for phased genotypes.0 - reference call1 - alternative call 12 - alternative call 2AD： 对应两个以逗号隔开的值，这两个值分别表示覆盖到REF和ALT碱基的reads数，相当于支持REF和支持ALT的测序深度。DP： 覆盖到这个位点的总的reads数量，相当于这个位点的深度（并不是多有的reads数量，而是大概一定质量值要求的reads数）。PL: 对应3个以逗号隔开的值，这三个值分别表示该位点基因型是0/0，0/1，1/1的没经过先验的标准化Phred-scaled似然值（L）。如果转换成支持该基因型概率（P）的话，由于L=-10lgP，那么P=10^（-L/10），因此，当L值为0时，P=10^0=1。因此，这个值越小，支持概率就越大，也就是说是这个基因型的可能性越大。GQ： 表示最可能的基因型的质量值。表示的意义同QUAL。 举个例子说明一下：chr1 899282 rs28548431 C T [CLIPPED] GT:AD:DP:GQ:PL 0/1:1,3:4:25.92:103,0,26在这个位点，GT=0/1，也就是说这个位点的基因型是C/T；GQ=25.92，质量值并不算太高，可能是因为cover到这个位点的reads数太少，DP=4，也就是说只有4条reads支持这个地方的变异；AD=1,3，也就是说支持REF的read有一条，支持ALT的有3条；在PL里，这个位点基因型的不确定性就表现的更突出了，0/1的PL值为0，虽然支持0/1的概率很高；但是1/1的PL值只有26，也就是说还有10^(-2.6)=0.25%的可能性是1/1；但几乎不可能是0/0，因为支持0/0的概率只有10^(-10.3)=5*10-11。 各个版本的vcf之间的差异v4.1 -&gt; v4.2 的变动特征]]></content>
      <categories>
        <category>知识沉淀</category>
        <category>文件格式</category>
      </categories>
      <tags>
        <tag>文件格式</tag>
        <tag>Vcf</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[perl - 常用功能切片]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-01.perl%2Fperl-%E4%BB%A3%E7%A0%81%E5%9D%97-%E5%B8%B8%E7%94%A8%E5%8A%9F%E8%83%BD%E5%88%87%E7%89%87%2F</url>
    <content type="text"><![CDATA[处理字符串处理数组统计属组中特定元素的个数12@array=(1,2,2,2,3,4,5,6,7);$number = grep &#123; $_ == 2 &#125; @array; # 统计数组中元素是 “2” 的元素个数。 处理字典生成title索引12@h&#123;@t&#125; = 0 .. $#t;]]></content>
      <categories>
        <category>编程拾慧</category>
        <category>Perl</category>
      </categories>
      <tags>
        <tag>Perl</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Perl脚本的调试]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-01.perl%2FPerl-%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86-%E8%84%9A%E6%9C%AC%E8%B0%83%E8%AF%95%2F</url>
    <content type="text"><![CDATA[编程中错误不可避免，调试能够帮助我们发现有问题的代码段。在网上看了一下Perl脚本调试，发现其实很多东西并不需要，而且那么多也没人看。 下面简单整理一下。 1.进入debug。 使用-d，进入debug状态。例：perl -d Perl程序名称。 2.设置断点 b:设置断点。例：b 行号； c:程序执行到下一个断点处，或执行到指定行。例：c ；c 行号； d:删除一个断点。例：d 断点所在行号； D:删除所有断点。例：D； L:列出所有断点。例：L。 3.程序调试 n：执行下一行，跳过方法； s：执行下一行，如果是方法则进入方法体。# 所有引用的第三方包也会逐语句进行执行 T：程序的调用栈回退一级。 a：给程序的某一行加一个附加操作。在执行该行语句前先执行附加的操作。 例：a 行号 命令 。 R：重新启动正在调试的程序。 w：显示某行周围一窗（一屏）文件内容。 例: w 行号。 4.查看变量值 p：查看变量值。例：p 变量名； x：查看变量值并结构化显示。例：x 变量名。 W：监视变量值。被监视的变量在发生改变时，会打印输出。 例： W 变量名 （无变量则删除所有监控）。 V： 包名 变量名列表：显示指定包内的所有（或部分）变量的值。 **注：V、X命令中的变量名列表以空格分隔且变量名前应去掉$、@或% ** 使用工具进行调试 类似浏览器的F12功能，Perl也提供了调试工具ptkdb。使用该工具需要两个模型包：Tk和ptkdb。 使用如下命令可以安装着两个包 perl -MCPAN -e&apos;install Tk&apos; perl -MCPAN -e&apos;install Devel::ptkdb&apos;]]></content>
      <categories>
        <category>编程拾慧</category>
        <category>Perl</category>
      </categories>
      <tags>
        <tag>Perl</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Perl包：包安装工具-CPAN]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-01.perl%2FPerl-%E5%8C%85-cpan%E5%8C%85%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[镜像配置几个主要的CPAN站点有： 国内： ftp://freesoft.cgi.gov.cn/pub/languages/perl/CPAN http://cpan.qz.fj.cn/ 国外： http://www.cpan.org/ http://www.perl.com/CPAN-local/ 镜像配置CPAN (The Comprehensive Perl Archive Network) 镜像源的配置文件为 MyConfig.pm（一般位于 ~/.cpan/CPAN/MyConfig.pm），可使用包管理脚本 cpan 进行修改。123456- 生成镜像配置文件# 确保 MyConfig.pm 配置文件存在，如不存在则自动生成PERL_MM_USE_DEFAULT=1 perl -MCPAN -e 'mkmyconfig'# 不使用默认配置，手动确认各个配置选项perl -MCPAN -e 'mkmyconfig' 在 CPAN Shell 中手动设置镜像 在命令行中执行 cpan 进入 cpan shell：12345678910111213141516171819202122cpan shell -- CPAN exploration and modules installationEnter 'h' for help.# 列出当前的镜像设置cpan[1]&gt; o conf urllist# 将本站镜像加入镜像列表首位# 注：若已在列表中则可跳过本步直接退出，修改列表不会执行自动去重cpan[2]&gt; o conf urllist unshift https://mirrors.tuna.tsinghua.edu.cn/CPAN/# 或将本站镜像加入镜像列表末尾# 注：本命令和上面的命令执行一个即可，修改列表不会执行自动去重cpan[3]&gt; o conf urllist push https://mirrors.tuna.tsinghua.edu.cn/CPAN/# 或清空镜像列表，仅保留本站cpan[4]&gt; o conf urllist https://mirrors.tuna.tsinghua.edu.cn/CPAN/# 保存修改后的配置至 MyConfig.pmcpan[5]&gt; o conf commit# 退出 cpan shellcpan[6]&gt; quit cpan安装perl包123456方法1：cpan交互式安装perl -MCPAN -e shell # 进入cpan交互页面cpan[2]&gt; install Set::IntervalTree # 安装所需要的依赖包方法2：cpan命令行安装perl -MCPAN -e 'install Email::Reply' 通过conda安装perl包不区分大小写、加上了perl前缀、中间以短横线连接12# conda install perl-module_nameconda install -c bioconda perl-set-intervaltree 下载包本地安装有些时候无法正常在线安装可以在 https://metacpan.org/pod/DBI]]></content>
      <categories>
        <category>编程拾慧</category>
        <category>Perl</category>
      </categories>
      <tags>
        <tag>Perl</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Perl包：包管理常用命令]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-01.perl%2FPerl-%E5%8C%85-%E5%B8%B8%E7%94%A8%E6%9F%A5%E8%AF%A2%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[包查询列出所有系统中已经安装的perl模块可以通过以下三种方式，查询系统中已经安装的所有perl模块 12345perldoc perllocalinstmodsh # 显示模块find `perl -e 'print "@INC"'` -name '*.pm' #显示模块对应的集群目录 查询单个模块查询 DBD::mysql 为例1perldoc -l DBD::mysql 查询安装的perl模块的版本号查询 DBD::mysql 为例 1perl -MDBD::mysql -e &apos;print DBD::mysql-&gt;VERSION. &quot;\n&quot;&apos; 输出系统中所有已安装的perl模块及版本12345678910111213#!/usr/bin/perluse strict;use ExtUtils::Installed;my $inst = ExtUtils::Installed-&gt;new();my @modules = $inst-&gt;modules();foreach (@modules) &#123; my $ver = $inst-&gt;version($_) || "???"; printf("%-22s -Version- %-22s\n", $_, $ver);&#125;exit;]]></content>
      <categories>
        <category>编程拾慧</category>
        <category>Perl</category>
      </categories>
      <tags>
        <tag>Perl</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Perl包：Statistics_Descriptive 数据统计]]></title>
    <url>%2F02.%E5%BC%80%E5%8F%91-01.perl%2FPerl-%E5%8C%85-Statistics_Descriptive%E6%95%B0%E6%8D%AE%E7%BB%9F%E8%AE%A1%2F</url>
    <content type="text"><![CDATA[模块名： Statistics::Descriptive 使用方法简介构建对象12use Statistics::Descriptivemy $stat = Statistics::Descriptive::Full-&gt;new(); 导入数据12$stat-&gt;add_data(@a); # 导入数组$stat-&gt;add_data($a); # 导入数值 数据处理1$stat-&gt;sort_data(); 数据过滤1$stat-&gt;set_outlier_filter($code_ref); # 设置一个过滤函数，对数据进行过滤 计算统计指标12345678910111213141516171819202122my $mean = $stat-&gt;mean();#平均值my $variance = $stat-&gt;variance();#方差my $num = $stat-&gt;count();#data的数目my $standard_deviation=$stat-&gt;standard_deviation();#标准差my $sum=$stat-&gt;sum();#求和my $min=$stat-&gt;min();#最小值my $mindex=$stat-&gt;mindex();#最小值的indexmy $max=$stat-&gt;max();#最大值my $maxdex=$stat-&gt;maxdex();#最大值的indexmy $range=$stat-&gt;sample_range();#最小值到最大值print &quot;Number of Values = $num\n&quot;, &quot;Mean = $mean\n&quot;, &quot;Variance = $variance\n&quot;, &quot;standard_deviation = $standard_deviation\n&quot;, &quot;sum =$sum\n&quot;, &quot;min =$min\n&quot;, &quot;mindex=$mindex\n&quot;, &quot;max=$max\n&quot;, &quot;maxdex=$maxdex\n&quot;, &quot;range=$range\n&quot;;]]></content>
      <categories>
        <category>编程拾慧</category>
        <category>Perl</category>
      </categories>
      <tags>
        <tag>Perl</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[测序数据库-SRA]]></title>
    <url>%2F01.%E7%9F%A5%E8%AF%86-01.NGS%2F2017-01-22.%E6%B5%8B%E5%BA%8F%E6%95%B0%E6%8D%AE%E5%BA%93-SRA%2F</url>
    <content type="text"><![CDATA[目前三个高通量数据合并针对动物，植物，微生物的划分进行去除；将目前的动物，植物．微生物三个高通量测序数据库进行整合；合并为一个数据库 高通量测序数据库 :合并原因： 有些项目有可能是存在跨物种的情况，进行分类的时候会产生问题（数据冗余或数据的缺失）； 物种的分类进行无法直接获取，如果区分需要人工整理，后期自动更新受影响； 提供物种检索后，数据库的合并不会收到影响； 高通量测序数据库 类别 NCBI DDBJ EBI 项目 PRJNAxxx / SRPxxx PRJDxxxxx 样品 SAMN03085625 / SRSxxx SAMDxxxxxx 实验 SRXxxxx DRAxxxxxx http://trace.ddbj.nig.ac.jp/bioproject/index_e.htmlhttp://trace.ddbj.nig.ac.jp/biosample/index_e.htmlhttp://trace.ddbj.nig.ac.jp/dra/index_e.html 数据获取 项目获取页面 NCBI项目项目ID 拼出xml文件下载路径：http://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=bioproject&amp;retmode=xml&amp;id=301661 样品获取页面 NCBI样品进入样品页面，获得样品的uid，拼出xml文件下载路径:http://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=biosample&amp;retmode=xml&amp;id=1047767 特殊案例 一个样品对应多个项目 一个样品对应多个实验 样品没有对应项目信息 一个实验对应多套测序数据]]></content>
      <categories>
        <category>知识沉淀</category>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>数据库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[生信相关公共数据库]]></title>
    <url>%2F01.%E7%9F%A5%E8%AF%86-04.Database%2F0000-00-00.%E7%94%9F%E4%BF%A1%E7%9B%B8%E5%85%B3%E5%85%AC%E5%85%B1%E6%95%B0%E6%8D%AE%E5%BA%93%2F</url>
    <content type="text"><![CDATA[整体数据库列表| 数据库 | 地址 | 简介 || ———— | ————————————————— | ————————————————————————————————————————————————————————————————- || TCGA-GDC | https://portal.gdc.cancer.gov/ | TCGA官网 || GEPIA | http://gepia.cancer-pku.cn/ | 包含TCGA和GTEx的9736个肿瘤和8687个正常对照样本的RNA-seq数据。可提供在线分析。 || cBioPortal | http://www.cbioportal.org/ | 数据包括MUT（Mutation突变），CNA(Copy Number Alterations,拷贝数变化），EXP（mRNA Expression，mRNA表达）和PORT/RPPA（Protein/ phosphoprotein level，蛋白表达或磷酸化变化），部分数据含有临床信息。 || MethHC | http://methhc.mbc.nctu.edu.tw/php/index.php | 目前包含由Illumina HumanMethylation450K BeadChip产生的6548个DNA甲基化数据和由18个人癌症中的RNA-seq miRNA-seq产生的12 567个mRNA microRNA表达数据。 || WebMEV | http://mev.tm4.org/#/welcome | 可上传数据或下载TCGA或GEO数 || DriverDBv2 | http://driverdb.tms.cmu.edu.tw/driverdbv2/index.php | 肿瘤驱动基因查询 || NCBI-gene | https://www.ncbi.nlm.nih.gov/gene | 是分子生物学，生物化学，和遗传学知识的存储和分析的自动系统。 || GeneCards | https://www.genecards.org/ | 收录关于人的蛋白质编码基因、假基因、RNA基因、遗传基因座、基因簇和未分类的基因等详细信息。 || ICGC | https://icgc.org/ | 收集了50种不同癌症类型（或亚型）的肿瘤数据，其中包括基因异常表达，体细胞突变，表观遗传修饰，临床数据等。ICGC包括亚洲、澳大利亚、欧洲、北美和南美17个行政区的89项目，包括25000个癌症基因组。 || HPA | https://www.proteinatlas.org/ | 包括三大亚图谱：组织图谱、细胞图谱和病理图谱。可交互式展示数据。 || UCSC | http://genome.ucsc.edu/ | 包含多个重要物种基因组草图，与ENCODE同步更新。可在线分析。 || OncoKB | https://oncokb.org/ | 包含有关554种癌症基因特定改变的详细信息，还有1级（FDA批准）、2级（标准护理）的治疗信息，3级临床证据和生物学证据。 || MalaCards | https://www.malacards.org/ | 疾病相关基因查询 || GEO | https://www.ncbi.nlm.nih.gov/gds | 芯片数据库 || SRA | https://www.ncbi.nlm.nih.gov/sra/ | 测序数据库 || ArrayExpress | https://www.ebi.ac.uk/arrayexpress/ | 欧洲版GEO || DAVID | http://david.abcc.ncifcrf.gov/tools.jsp | 功能富集分析 || String | http://string-db.org/ | 蛋白互作查询+功能富集分析 || GSEA | http://software.broadinstitute.org/gsea/index.jsp | 功能富集分析 | 规范化数据库 HGVS(Human Genome Variation Society) 人类基因组突变描述格式规范 主要的可变剪切注释 HGMD(The Human Gene Mutation Database,HGMD® ) 肿瘤相关数据库 TCGA肿瘤基因组图谱（TCGA）计划是由美国National Cancer Institute（NCI）和National Human Genome ResearchInstitute（NHGRI）于2006年联合启动的项目，作为目前最大的癌症基因信息数据库，收录33个癌种其中10个罕见癌种，及29种癌症器官，1万多个肿瘤样本，27万多份文件，含有多模式基因组学、表观基因组学和蛋白质组学数据。数据包括全基因组不同遗传特征的测量，如同一基因的DNA拷贝数、DNA甲基化、mRNA表达、SNP等。参考文献：The Cancer Genome Atlas (TCGA): an immeasurable source of knowledge cBioportalcBioPortal数据库整合了126个肿瘤基因组研究的数据，包括TCGA和ICGC等大型的肿瘤研究项目，涵盖了两万八千例标本的数据，此外部分样品还包括了临床预后等表型的信息。cBioPortal用于探索，可视化和分析多维癌症基因组学数据。将癌症组织和细胞系的分子谱分析数据简化为易于理解的遗传，表观遗传，基因表达和蛋白质组学事件。查询界面与定制数据存储相结合，使研究人员能够以交互方式探索样本，基因和途径的基因改变，并在基础数据中提供时将这些与临床结果联系起来。提供来自多个平台的基因水平数据的图形摘要，网络可视化和分析，生存分析，以患者为中心的查询和软件程序化访问。 Cosmic(Catalogue Of Somatic Mutations In Cancer)是目前世界上最大和最综合性的数据库，可以帮助我们探索癌症患者体细胞突变的功能及影响。癌症相关的体细胞位点，是整个网站的核心，收录了来自不同研究机构和数据库的体细胞突变数据，并提供了方便的浏览，检索，下载功能。 Cell Lines Projec对癌症研究中常用的细胞系样本进行深入研究，分析其突变信息。相比COSMIC, 整个项目中涵盖的变异数据会少一点。 COSMIC-3D 通过交互式的网页，展现了基因突变导致的蛋白结构域的变化。在搜索框中输入一个具体的基因名称或者蛋白名称，可以查看具体的记录。 Cancer Gene Census在癌症研究中，找到相关的突变基因是最核心的目的之一。通过对各种癌症进行调研，整理了一份癌症相关的突变基因列表，这份列表就是Cancer Gene Census,简称CGC。 在CGC种，将所有的癌症相关基因分成两类: Tier1 : 对于这部分基因，有充分的证据表明，正是由于这些基因的突变，导致癌症的进一步发生。 Tier2 : 对于这部分基因，只能说在癌症中检测到了大量该基因的突变，但是并没有充分证据表明该基因突变对癌症发生的影响。 My cancer Genome我的癌症基因组包含有关癌症相关基因，蛋白质和其他生物标志物类型的分子生物标志物在癌症中使用抗癌疗法的临床影响的信息。 ICGCICGC（International Cancer Genome Consortium，国际肿瘤基因组协作组），主要目标是全面阐明导致全球人类疾病负担的多种癌症中存在的基因组变化。收集了50种不同癌症类型（或亚型）的肿瘤数据，其中包括基因异常表达，体细胞突变，表观遗传修饰，临床数据等。ICGC包括亚洲、澳大利亚、欧洲、北美和南美17个行政区的89项目，包括25000个癌症基因组 OncoKBOncoKB是由Memorial Sloan Kettering癌症中心（MSK）维护的全面的精准肿瘤学知识库，包含来自FDA，NCCN或ASCO，ClinicalTrials.gov和科学文献的专业指导方针和建议，治疗策略，肿瘤专家或肿瘤协会共识，参考文献等信息。OncoKB目前包含有关554种癌症基因特定改变的详细信息，还有1级（FDA批准）、2级（标准护理）的治疗信息，3级临床证据和生物学证据。 人群数据库 ExAC/gnomAD.汇总了来自60,706个个体的完整外显子序列，并在这些志愿者的同意下，通过外显子集成联合（Exome Aggregation consortium ，ExAC，生物通译)共享这些序列信息。 1000 Genomes Project dbSNP Exome Variant Server :在欧洲和非洲裔美国血统的几个大群体的外显子组测序期间发现的变体数据库。 功能预测数据库 Polyphen2 SIFT MutationAssessor用Mutation-Assessor软件来看突变位点对基因或者蛋白功能的影响, MutationTaster [PhyloP]生成蛋白模型图 [PhastCons49]计算序列保守性 Human Splicing Finder为了更好地理解导致剪接缺陷的内含子和外显子突变，决定创建Human Splicing Finder网站。 该工具旨在帮助研究前mRNA剪接[更多关于剪接背景]。 MaxEntScan预测突变的软件？类似注释（内含子等） NetGene2A service producing neural network predictions of splice sites in human, C. elegans and A. thaliana DNA. The prediction output for both server and mail server consist of the prediction for both direct (+) and complementary (-) strand. The output lists the predictions for donor and acceptor sites in the submitted sequence, as well as branchpoint predictions (for A. thaliana only). NNSplice剪切位点预测。 GeneSplicer剪切位点预测。 其他数据库 UCSC ClinVarclinvar的注释，可以寻找出对应的基因变异信息，发生频率，表型，临床意义，评审状态以及染色体位置等。 HGMD疾病基因突变注释数据库，收集文献发表的基因变异与疾病的关系信息 GEOGEO数据库全称GENE EXPRESSION OMNIBUS，是由美国国立生物技术信息中心NCBI创建并维护的基因表达数据库，用于从任何物种或人造的来源检索基因表达数据。它创建于2000年，收录了世界各国研究机构提交的来自microarray，高密度寡核苷酸array（HAD），杂交膜（filter）和SAGE的许多类型的基因表达数据，目前已经发表的论文，论文中涉及到的基因表达检测的数据可以通过此数据库中找到。作为一个公共数据集合含有一系列预先计算的数据的定义和描述，以及用于交互检索和分析表达数据的在线工具。 MethHCMethHC专注于人类疾病的异常甲基化。MethHC整合了来自TCGA的DNA甲基化数据，基因表达数据和microRNA表达数据。MethHC目前包含由Illumina HumanMethylation450K BeadChip产生的6548个DNA甲基化数据和由18个人癌症中的RNA-seq / miRNA-seq产生的12 567个mRNA / microRNA表达数据。 HPA人类蛋白质表达图集（The human protein atlas）涵盖了17000个不同蛋白及26009种不同抗体的蛋白质水平分析。现在人类蛋白质图谱共包括三大亚图谱：组织图谱、细胞图谱和病理图谱。组织图谱包含了人类基因在RNA和蛋白质水平的表达信息。其中，蛋白质表达信息是来自免疫组化分析结果，依赖于无数商业化或者自制的抗体。细胞图谱包含人类细胞内蛋白质的空间信息。病理图谱涵盖了17种主要癌症类型、大约8000名病人的信息。病理图谱的一个创新是交互式生存散点图，可以以交互形式展示病人的生存数据。参考文献文档及文献：http://www.proteinatlas.org/about/publications StringString(search tool for the retrival of interacting genes/proteins)基因、蛋白质相互作用关系检索工具可以获取独特的，覆盖范围广的实验以及预测的相互作用关系信息。string提供的相互作用关系主要基于confidence score（可靠指数），以及其他附属信息，比如提供蛋白质域和3D结构。目前包括1100+个物种的5200+万蛋白质。构建蛋白质蛋白质相互作用网络可以用于过滤和评估功能性基因组学的数据，以及为注释蛋白质的结构、功能和进化性。 GeneCards基本覆盖了几大数据库对于基因的分析数据，是人类基因的综合数据库。该数据库整合了125个网站的基因数据中心的数据（包括HUGO(Human Gene Nomenclature Committee)、GDB(Genome Database)、MGD(Mouse Genome Database)等）。由以色列魏茨曼科学研究所维护的关于基因及其产物以及生物医学应用的文献库。GeneCards提供简明的基因组，蛋白质组，转录，遗传和功能上所有已知和预测的人类基因。GeneCards中的信息功能信息包括指向疾病的关系，突变和多态性，基因表达，基因功能，途径，蛋白质与蛋白质相互作用，相关的药物及化合物和切割等先进的研究抗体的试剂和工具等，重组蛋白，克隆，表达分析和RNAi试剂等。（还有各种数据库ID相互转换） GeneCards以卡片的形式给出结果，列出所查询基因的 1、官方名称，GDB同义列表、小鼠中的同源物、细胞遗传学定位、基因产物名称、产物功能，如在细胞中的作用、表达方式、定位、与其他蛋白质的同源性及其在疾病中的作用等； 2、相关基因家族； 3、相关疾病列表； 4、有关的研究论文； 5、医学应用，如根据该基因的有关知识而建立的新的治疗与诊断方法等。]]></content>
      <categories>
        <category>index</category>
        <category>知识沉淀</category>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>数据库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[生信相关软件索引]]></title>
    <url>%2F03.%E5%BA%94%E7%94%A8-01.software_in_NGS%2F0000-00-01.software%2F</url>
    <content type="text"><![CDATA[常见文件格式 终端软件 下机数据质控 SOAPNuke 变异检测 功能软件 安装 bcftools 1.2, htslib-1.2.1, tabix12345678wget https://github.com/samtools/bcftools/releases/download/1.2/bcftools-1.2.tar.bz2tar xvf bcftools-1.2.tar.bz2cd bcftools-1.2makemake installcd htslib-1.2.1makemake install]]></content>
      <categories>
        <category>index</category>
        <category>NGS</category>
        <category>software</category>
      </categories>
      <tags>
        <tag>Software</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[云计算技术及性能优化]]></title>
    <url>%2F01.%E7%9F%A5%E8%AF%86-06.cloud_computing%2F0000-02-01.cloud_computing%2F</url>
    <content type="text"><![CDATA[]]></content>
      <categories>
        <category>云计算</category>
      </categories>
      <tags>
        <tag>流程开发</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Vim常用命令]]></title>
    <url>%2F03.%E5%BA%94%E7%94%A8-02.Linux%E5%92%8Cshell%2FLinux-Vim-%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[在Vim中，光标的移动控制 命令 说明 H 左移动 J 下移动 K 上移动 L 右移动 w 移动到下一个单词 b 移动到上一个单词 行内跳转 命令 说明 w 到下一个单词的开头 e 到下一个单词的结尾 b 到前一个单词的开头 ge 到前一个单词的结尾 0或^ 到行头 $ 到行尾 f&lt;字母&gt; 向后搜索&lt;字母&gt;并跳转到第一个匹配的位置(非常实用) F&lt;字母&gt; 向前搜索&lt;字母&gt;并跳转到第一个匹配的位置 t&lt;字母&gt; 向后搜索&lt;字母&gt;并跳转到第一个匹配位置之前的一个字母(不常用) T&lt;字母&gt; 向前搜索&lt;字母&gt;并跳转到第一个匹配位置之后的一个字母(不常用) 切换为编辑状态的命令 命令 说明 i 在当前光标处进行编辑 I 在行首插入 A 在行末插入 a 在光标后插入编辑 o 在当前行后插入一个新行 O 在当前行前插入一个新行 cw 从光标所在位置开始插入编辑，同时删除该行中光标后面的文本 命令行模式下，退出Vim 命令 说明 :q! 强制退出，不保存编辑内容 :q 直接退出（仅在未更改文本内容时可用） :wq 或 :x 保存并退出 :wq! 强制保存并退出 :w （文件路径）将文档另存为文件路径，如果没有文件路径则保存原文件 :saveas 文件路径将文件另存为（文件路径） shift + zz 在普通模式下直接退出Vim，（对文件进行的更改会被保存） 其他快捷操作 命令 说明 .（小数点） 重复上一次的操作]]></content>
      <categories>
        <category>编程拾慧</category>
        <category>Linux</category>
        <category>Vim</category>
      </categories>
      <tags>
        <tag>Vim</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[BGI工作索引]]></title>
    <url>%2F0000-01-00.BGI%E5%B7%A5%E4%BD%9C%E7%B4%A2%E5%BC%95%2F</url>
    <content type="text"><![CDATA[培训材料 知识点 指南参考]]></content>
      <categories>
        <category>index</category>
      </categories>
      <tags>
        <tag>BGI</tag>
      </tags>
  </entry>
</search>
