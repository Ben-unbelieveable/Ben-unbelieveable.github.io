<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">



  
  
    
    
  <script src="/lib/pace/pace.min.js?v=1.0.2"></script>
  <link href="/lib/pace/pace-theme-minimal.min.css?v=1.0.2" rel="stylesheet">
  <script>
    (function(){
      if(''){
        if (prompt('请输入文章密码') !== ''){
          alert('密码错误！');
          history.back();
        }
      }
    })();
  </script>







<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.2" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="Machine-Learning," />








  <link rel="shortcut icon" type="image/x-icon" href="/images/favicon.ico?v=5.1.2" />






<meta name="description" content="LightGBM简介GBDT (Gradient Boosting Decision Tree) 是机器学习中一个长盛不衰的模型，其主要思想是利用弱分类器（决策树）迭代训练以得到最优模型，该模型具有训练效果好、不易过拟合等优点。GBDT不仅在工业界应用广泛，通常被用于多分类、点击率预测、搜索排序等任务；在各种数据挖掘竞赛中也是致命武器，据统计Kaggle上的比赛有一半以上的冠军方案都是基于GBDT">
<meta name="keywords" content="Machine-Learning">
<meta property="og:type" content="article">
<meta property="og:title" content="1204.机器学习-集成学习-2.Boosting-LightBGM">
<meta property="og:url" content="http://ben-air.cn/02.开发-20.MachineLearn/1204.机器学习-集成学习-2.Boosting-4.LightBGM/index.html">
<meta property="og:site_name" content="Ben-air">
<meta property="og:description" content="LightGBM简介GBDT (Gradient Boosting Decision Tree) 是机器学习中一个长盛不衰的模型，其主要思想是利用弱分类器（决策树）迭代训练以得到最优模型，该模型具有训练效果好、不易过拟合等优点。GBDT不仅在工业界应用广泛，通常被用于多分类、点击率预测、搜索排序等任务；在各种数据挖掘竞赛中也是致命武器，据统计Kaggle上的比赛有一半以上的冠军方案都是基于GBDT">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="http://ben-air.cn/02.开发-20.MachineLearn/1204.机器学习-集成学习-2.Boosting-4.LightBGM/Histogram_algorithm.png">
<meta property="og:image" content="http://ben-air.cn/02.开发-20.MachineLearn/1204.机器学习-集成学习-2.Boosting-4.LightBGM/low_memory.png">
<meta property="og:image" content="http://ben-air.cn/02.开发-20.MachineLearn/1204.机器学习-集成学习-2.Boosting-4.LightBGM/Histogram_subtraction.png">
<meta property="og:image" content="http://ben-air.cn/02.开发-20.MachineLearn/1204.机器学习-集成学习-2.Boosting-4.LightBGM/level-wise_tree_growth.png">
<meta property="og:image" content="http://ben-air.cn/02.开发-20.MachineLearn/1204.机器学习-集成学习-2.Boosting-4.LightBGM/leaf-wise_tree_growth.png">
<meta property="og:image" content="http://ben-air.cn/02.开发-20.MachineLearn/1204.机器学习-集成学习-2.Boosting-4.LightBGM/Gradient-based_One-side_Sampling.png">
<meta property="og:image" content="http://ben-air.cn/02.开发-20.MachineLearn/1204.机器学习-集成学习-2.Boosting-4.LightBGM/Greedy_Bundling.png">
<meta property="og:image" content="http://ben-air.cn/02.开发-20.MachineLearn/1204.机器学习-集成学习-2.Boosting-4.LightBGM/Merge_Exclusive_Features.png">
<meta property="og:image" content="http://ben-air.cn/02.开发-20.MachineLearn/1204.机器学习-集成学习-2.Boosting-4.LightBGM/one-hot_many-many.png">
<meta property="og:image" content="http://ben-air.cn/02.开发-20.MachineLearn/1204.机器学习-集成学习-2.Boosting-4.LightBGM/catelog_feature_split.png">
<meta property="og:image" content="http://ben-air.cn/02.开发-20.MachineLearn/1204.机器学习-集成学习-2.Boosting-4.LightBGM/feature_Parallelization.png">
<meta property="og:image" content="http://ben-air.cn/02.开发-20.MachineLearn/1204.机器学习-集成学习-2.Boosting-4.LightBGM/data_parallelization.png">
<meta property="og:image" content="http://ben-air.cn/02.开发-20.MachineLearn/1204.机器学习-集成学习-2.Boosting-4.LightBGM/Parallel_optimization.png">
<meta property="og:image" content="http://ben-air.cn/02.开发-20.MachineLearn/1204.机器学习-集成学习-2.Boosting-4.LightBGM/xgboost_cache_miss.png">
<meta property="og:image" content="http://ben-air.cn/02.开发-20.MachineLearn/1204.机器学习-集成学习-2.Boosting-4.LightBGM/LightGBM_cache_hit.png">
<meta property="og:updated_time" content="2026-01-06T07:45:21.229Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="1204.机器学习-集成学习-2.Boosting-LightBGM">
<meta name="twitter:description" content="LightGBM简介GBDT (Gradient Boosting Decision Tree) 是机器学习中一个长盛不衰的模型，其主要思想是利用弱分类器（决策树）迭代训练以得到最优模型，该模型具有训练效果好、不易过拟合等优点。GBDT不仅在工业界应用广泛，通常被用于多分类、点击率预测、搜索排序等任务；在各种数据挖掘竞赛中也是致命武器，据统计Kaggle上的比赛有一半以上的冠军方案都是基于GBDT">
<meta name="twitter:image" content="http://ben-air.cn/02.开发-20.MachineLearn/1204.机器学习-集成学习-2.Boosting-4.LightBGM/Histogram_algorithm.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    sidebar: {"position":"left","display":"post","offset":12,"offset_float":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://ben-air.cn/02.开发-20.MachineLearn/1204.机器学习-集成学习-2.Boosting-4.LightBGM/"/>





  <title>1204.机器学习-集成学习-2.Boosting-LightBGM | Ben-air</title>
  














</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Ben-air</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            关于
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off"
             placeholder="搜索..." spellcheck="false"
             type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://ben-air.cn/02.开发-20.MachineLearn/1204.机器学习-集成学习-2.Boosting-4.LightBGM/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Ben-air">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/uploads/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Ben-air">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">1204.机器学习-集成学习-2.Boosting-LightBGM</h1>
        

        <div class="post-meta">
	  

          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2025-12-29T15:36:42+08:00">
                2025-12-29
              </time>
            
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/机器学习/" itemprop="url" rel="index">
                    <span itemprop="name">机器学习</span>
                  </a>
                </span>

                
                
                  ， 
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/机器学习/集成学习/" itemprop="url" rel="index">
                    <span itemprop="name">集成学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">
      
      

      
        <h1 id="LightGBM简介"><a href="#LightGBM简介" class="headerlink" title="LightGBM简介"></a>LightGBM简介</h1><p>GBDT (Gradient Boosting Decision Tree) 是机器学习中一个长盛不衰的模型，其主要思想是利用弱分类器（决策树）迭代训练以得到最优模型，该模型具有训练效果好、不易过拟合等优点。GBDT不仅在工业界应用广泛，通常被用于多分类、点击率预测、搜索排序等任务；在各种数据挖掘竞赛中也是致命武器，据统计Kaggle上的比赛有一半以上的冠军方案都是基于GBDT。而LightGBM（Light Gradient Boosting Machine）是一个实现GBDT算法的框架，支持高效率的并行训练，并且具有更快的训练速度、更低的内存消耗、更好的准确率、支持分布式可以快速处理海量数据等优点。</p>
<h2 id="1-1-LightGBM提出的动机"><a href="#1-1-LightGBM提出的动机" class="headerlink" title="1.1 LightGBM提出的动机"></a>1.1 LightGBM提出的动机</h2><p>常用的机器学习算法，例如神经网络等算法，都可以以mini-batch的方式训练，训练数据的大小不会受到内存限制。而GBDT在每一次迭代的时候，都需要遍历整个训练数据多次。如果把整个训练数据装进内存则会限制训练数据的大小；如果不装进内存，反复地读写训练数据又会消耗非常大的时间。尤其面对工业级海量的数据，普通的GBDT算法是不能满足其需求的。</p>
<p>LightGBM提出的主要原因就是为了解决GBDT在海量数据遇到的问题，让GBDT可以更好更快地用于工业实践。</p>
<h2 id="1-2-XGBoost的缺点及LightGBM的优化"><a href="#1-2-XGBoost的缺点及LightGBM的优化" class="headerlink" title="1.2 XGBoost的缺点及LightGBM的优化"></a>1.2 XGBoost的缺点及LightGBM的优化</h2><h3 id="XGBoost的缺点"><a href="#XGBoost的缺点" class="headerlink" title="XGBoost的缺点"></a>XGBoost的缺点</h3><p>在LightGBM提出之前，最有名的GBDT工具就是XGBoost了，它是基于预排序方法的决策树算法。这种构建决策树的算法基本思想是：首先，对所有特征都按照特征的数值进行预排序。其次，在遍历分割点的时候用O(#data)的代价找到一个特征上的最好分割点。最后，在找到一个特征的最好分割点后，将数据分裂成左右子节点。</p>
<p>这样的预排序算法的优点是能精确地找到分割点。但是缺点也很明显：首先，空间消耗大。这样的算法需要保存数据的特征值，还保存了特征排序的结果（例如，为了后续快速的计算分割点，保存了排序后的索引），这就需要消耗训练数据两倍的内存。其次，时间上也有较大的开销，在遍历每一个分割点的时候，都需要进行分裂增益的计算，消耗的代价大。最后，对cache优化不友好。在预排序后，特征对梯度的访问是一种随机访问，并且不同的特征访问的顺序不一样，无法对cache进行优化。同时，在每一层长树的时候，需要随机访问一个行索引到叶子索引的数组，并且不同特征访问的顺序也不一样，也会造成较大的cache miss。</p>
<h3 id="LightGBM的优化"><a href="#LightGBM的优化" class="headerlink" title="LightGBM的优化"></a>LightGBM的优化</h3><p>为了避免上述XGBoost的缺陷，并且能够在不损害准确率的条件下加快GBDT模型的训练速度，lightGBM在传统的GBDT算法上进行了如下优化：</p>
<ul>
<li>基于Histogram的决策树算法。</li>
<li>单边梯度采样 Gradient-based One-Side Sampling(GOSS)：使用GOSS可以减少大量只具有小梯度的数据实例，这样在计算信息增益的时候只利用剩下的具有高梯度的数据就可以了，相比XGBoost遍历所有特征值节省了不少时间和空间上的开销。</li>
<li>互斥特征捆绑 Exclusive Feature Bundling(EFB)：使用EFB可以将许多互斥的特征绑定为一个特征，这样达到了降维的目的。</li>
<li>带深度限制的Leaf-wise的叶子生长策略：大多数GBDT工具使用低效的按层生长 (level-wise) 的决策树生长策略，因为它不加区分的对待同一层的叶子，带来了很多没必要的开销。实际上很多叶子的分裂增益较低，没必要进行搜索和分裂。LightGBM使用了带有深度限制的按叶子生长 (leaf-wise) 算法。</li>
<li>直接支持类别特征(Categorical Feature)</li>
<li>支持高效并行</li>
<li>Cache命中率优化</li>
</ul>
<p>下面我们就详细介绍以上提到的lightGBM优化算法。</p>
<h1 id="LightGBM的基本原理"><a href="#LightGBM的基本原理" class="headerlink" title="LightGBM的基本原理"></a>LightGBM的基本原理</h1><h2 id="2-1-基于Histogram的决策树算法"><a href="#2-1-基于Histogram的决策树算法" class="headerlink" title="2.1 基于Histogram的决策树算法"></a>2.1 基于Histogram的决策树算法</h2><h3 id="（1）直方图算法"><a href="#（1）直方图算法" class="headerlink" title="（1）直方图算法"></a>（1）直方图算法</h3><p>Histogram algorithm应该翻译为直方图算法，直方图算法的基本思想是：先把连续的浮点特征值离散化成$k$个整数，同时构造一个宽度为$k$的直方图。在遍历数据的时候，根据离散化后的值作为索引在直方图中累积统计量，当遍历一次数据后，直方图累积了需要的统计量，然后根据直方图的离散值，遍历寻找最优的分割点。<br><img src="/02.开发-20.MachineLearn/1204.机器学习-集成学习-2.Boosting-4.LightBGM/Histogram_algorithm.png" alt="Histogram_algorithm"><br>图：直方图算法</p>
<p>直方图算法简单理解为：首先确定对于每一个特征需要多少个箱子（bin）并为每一个箱子分配一个整数；然后将浮点数的范围均分成若干区间，区间个数与箱子个数相等，将属于该箱子的样本数据更新为箱子的值；最后用直方图（#bins）表示。看起来很高大上，其实就是直方图统计，将大规模的数据放在了直方图中。</p>
<p>我们知道特征离散化具有很多优点，如存储方便、运算更快、鲁棒性强、模型更加稳定等。对于直方图算法来说最直接的有以下两个优点：</p>
<ul>
<li><p>内存占用更小：<br>直方图算法不仅不需要额外存储预排序的结果，而且可以只保存特征离散化后的值，而这个值一般用 $8$位整型存储就足够了，内存消耗可以降低为原来的$\frac{1}{8}$ 。也就是说XGBoost需要用$32$位的浮点数去存储特征值，并用$32$位的整形去存储索引，而 LightGBM只需要用$8$位去存储直方图，内存相当于减少为$\frac{1}{8}$  ；<br><img src="/02.开发-20.MachineLearn/1204.机器学习-集成学习-2.Boosting-4.LightBGM/low_memory.png" alt=""></p>
</li>
<li><p>计算代价更小：<br>预排序算法XGBoost每遍历一个特征值就需要计算一次分裂的增益，而直方图算法LightGBM只需要计算$k$次（ $k$可以认为是常数），直接将时间复杂度从$O(#data<em>#feature)$降低到$O(k</em>#feature)$，而我们知道$#data &gt;&gt; k$。</p>
</li>
</ul>
<p>当然，Histogram算法并不是完美的。由于特征被离散化后，找到的并不是很精确的分割点，所以会对结果产生影响。但在不同的数据集上的结果表明，离散化的分割点对最终的精度影响并不是很大，甚至有时候会更好一点。原因是决策树本来就是弱模型，分割点是不是精确并不是太重要；较粗的分割点也有正则化的效果，可以有效地防止过拟合；即使单棵树的训练误差比精确分割的算法稍大，但在梯度提升（Gradient Boosting）的框架下没有太大的影响。</p>
<h3 id="（2）直方图做差加速"><a href="#（2）直方图做差加速" class="headerlink" title="（2）直方图做差加速"></a>（2）直方图做差加速</h3><p>LightGBM另一个优化是Histogram（直方图）做差加速。<strong>一个叶子的直方图可以由它的父亲节点的直方图与它兄弟的直方图做差得到，在速度上可以提升一倍</strong>。通常构造直方图时，需要遍历该叶子上的所有数据，但直方图做差仅需遍历直方图的k个桶。在实际构建树的过程中，LightGBM还可以先计算直方图小的叶子节点，然后利用直方图做差来获得直方图大的叶子节点，这样就可以用非常微小的代价得到它兄弟叶子的直方图。<br><img src="/02.开发-20.MachineLearn/1204.机器学习-集成学习-2.Boosting-4.LightBGM/Histogram_subtraction.png" alt=""><br>注意：XGBoost 在进行预排序时只考虑非零值进行加速，而 LightGBM 也采用类似策略：只用非零特征构建直方图。</p>
<h2 id="2-2-带深度限制的-Leaf-wise-算法"><a href="#2-2-带深度限制的-Leaf-wise-算法" class="headerlink" title="2.2 带深度限制的 Leaf-wise 算法"></a>2.2 带深度限制的 Leaf-wise 算法</h2><p>在Histogram算法之上，LightGBM进行进一步的优化。首先它抛弃了大多数GBDT工具使用的<strong>按层生长 (level-wise)</strong> 的决策树生长策略，而使用了<strong>带有深度限制的按叶子生长 (leaf-wise)</strong> 算法。</p>
<p>XGBoost 采用 Level-wise 的增长策略，该策略遍历一次数据可以同时分裂同一层的叶子，容<strong>易进行多线程优化，也好控制模型复杂度</strong>，<strong>不容易过拟合</strong>。但实际上Level-wise是一种低效的算法，因为它不加区分的对待同一层的叶子，实际上很多叶子的分裂增益较低，没必要进行搜索和分裂，因此带来了很多没必要的计算开销。<br><img src="/02.开发-20.MachineLearn/1204.机器学习-集成学习-2.Boosting-4.LightBGM/level-wise_tree_growth.png" alt=""></p>
<p>LightGBM采用Leaf-wise的增长策略，该策略每次从当前所有叶子中，找到分裂增益最大的一个叶子，然后分裂，如此循环。因此同Level-wise相比，Leaf-wise的优点是：在分裂次数相同的情况下，Leaf-wise可以降低更多的误差，得到更好的精度；Leaf-wise的缺点是：可能会长出比较深的决策树，产生过拟合。因此LightGBM会在Leaf-wise之上增加了一个最大深度的限制，在保证高效率的同时防止过拟合。<br><img src="/02.开发-20.MachineLearn/1204.机器学习-集成学习-2.Boosting-4.LightBGM/leaf-wise_tree_growth.png" alt=""></p>
<h2 id="2-3-单边梯度采样算法"><a href="#2-3-单边梯度采样算法" class="headerlink" title="2.3 单边梯度采样算法"></a>2.3 单边梯度采样算法</h2><p>Gradient-based One-Side Sampling 应该被翻译为单边梯度采样（GOSS）。GOSS算法</p>
<p>从减少样本的角度出发，排除大部分小梯度的样本，仅用剩下的样本计算信息增益，它是一种在减少数据量和保证精度上平衡的算法。</p>
<p>AdaBoost中，样本权重是数据重要性的指标。然而在GBDT中没有原始样本权重，不能应用权重采样。幸运的是，我们观察到GBDT中每个数据都有不同的梯度值，对采样十分有用。即梯度小的样本，训练误差也比较小，说明数据已经被模型学习得很好了，直接想法就是丢掉这部分梯度小的数据。然而这样做会改变数据的分布，将会影响训练模型的精确度，为了避免此问题，提出了GOSS算法。</p>
<p>GOSS是一个样本的采样算法，目的是丢弃一些对计算信息增益没有帮助的样本留下有帮助的。根据计算信息增益的定义，梯度大的样本对信息增益有更大的影响。因此，GOSS在进行数据采样的时候只保留了梯度较大的数据，但是如果直接将所有梯度较小的数据都丢弃掉势必会影响数据的总体分布。所以，GOSS首先将要进行分裂的特征的所有取值按照绝对值大小降序排序（XGBoost一样也进行了排序，但是LightGBM不用保存排序后的结果），选取绝对值最大的 $a<em>100%$ 个数据，。然后在剩下的较小梯度数据中随机选择 $b</em> 100%$ 个数据。接着将这$b<em>100% $ 个数据乘以一个常数 $\frac{1-a}{b}$, 这样算法就会更关注训练不足的样本，而不会过多改变原数据集的分布。最后使用这$(a+b)</em>100%$ 个数据来计算信息增益。下图是GOSS的具体算法。<br><img src="/02.开发-20.MachineLearn/1204.机器学习-集成学习-2.Boosting-4.LightBGM/Gradient-based_One-side_Sampling.png" alt=""></p>
<h2 id="2-4-互斥特征捆绑算法"><a href="#2-4-互斥特征捆绑算法" class="headerlink" title="2.4 互斥特征捆绑算法"></a>2.4 互斥特征捆绑算法</h2><p>高维度的数据往往是稀疏的，这种稀疏性启发我们设计一种无损的方法来减少特征的维度。通常<strong>被捆绑的特征都是互斥的</strong>（即特征不会同时为非零值，像one-hot），这样两个特征捆绑起来才不会丢失信息。如果两个特征并不是完全互斥（部分情况下两个特征都是非零值），可以用一个指标对特征不互斥程度进行衡量，称之为冲突比率，当这个值较小时，我们可以选择把不完全互斥的两个特征捆绑，而不影响最后的精度。互斥特征捆绑算法（Exclusive Feature Bundling, EFB）指出如果将一些特征进行融合绑定，则可以降低特征数量。这样在构建直方图时的时间复杂度从$O(#data<em>#feature)$ 变为$O(#data</em>#bundle)$ ，这里 $#bundle$指特征融合绑定后特征包的个数，且 $#bundle$ 远小于 $#feature$。</p>
<p>针对这种想法，我们会遇到两个问题：</p>
<ul>
<li>怎么判定哪些特征应该绑在一起（build bundled）？</li>
<li>怎么把特征绑为一个（merge feature）？</li>
</ul>
<h3 id="（1）解决哪些特征应该绑在一起"><a href="#（1）解决哪些特征应该绑在一起" class="headerlink" title="（1）解决哪些特征应该绑在一起"></a>（1）解决哪些特征应该绑在一起</h3><p>将相互独立的特征进行绑定是一个 NP-Hard 问题，<strong>LightGBM的<a href="https://apxml.com/zh/courses/mastering-gradient-boosting-algorithms/chapter-5-lightgbm-light-gradient-boosting/lightgbm-efb" target="_blank" rel="noopener">EFB算法</a>将这个问题转化为图着色的问题来求解，将所有的特征视为图的各个顶点，将不是相互独立的特征用一条边连接起来，边的权重就是两个相连接的特征的总冲突值</strong>，这样需要绑定的特征就是在图着色问题中要涂上同一种颜色的那些点（特征）。此外，我们注意到通常有很多特征，尽管不是100％相互排斥，但也很少同时取非零值。 如果我们的算法可以允许一小部分的冲突，我们可以得到更少的特征包，进一步提高计算效率。经过简单的计算，随机污染小部分特征值将影响精度最多 ，$O([(1-\gamma)n]^{-2/3})$, $\gamma$ 是每个绑定中的最大冲突比率，当其相对较小时，能够完成精度和效率之间的平衡。具体步骤可以总结如下：</p>
<ul>
<li>构造一个加权无向图，顶点是特征，边有权重，其权重与两个特征间冲突相关；</li>
<li>根据节点的度进行降序排序，度越大，与其它特征的冲突越大；</li>
<li>遍历每个特征，将它分配给现有特征包，或者新建一个特征包，使得总体冲突最小。</li>
</ul>
<p>算法允许两两特征并不完全互斥来增加特征捆绑的数量，通过设置最大冲突比率 $\gamma$ 来平衡算法的精度和效率。EFB 算法的伪代码如下所示：<br><img src="/02.开发-20.MachineLearn/1204.机器学习-集成学习-2.Boosting-4.LightBGM/Greedy_Bundling.png" alt=""><br>算法3的时间复杂度是 $O(#feature^2)$ ，训练之前只处理一次，其时间复杂度在特征不是特别多的情况下是可以接受的，但难以应对百万维度的特征。为了继续提高效率，LightGBM提出了一种更加高效的无图的排序策略：将特征按照非零值个数排序，这和使用图节点的度排序相似，因为更多的非零值通常会导致冲突，新算法在算法3基础上改变了排序策略。</p>
<p>（2）解决怎么把特征绑为一捆<br>特征合并算法，其关键在于原始特征能从合并的特征中分离出来。绑定几个特征在同一个bundle里需要保证绑定前的原始特征的值可以在bundle中识别，考虑到histogram-based算法将连续的值保存为离散的bins，我们可以使得不同特征的值分到bundle中的不同bin（箱子）中，这可以通过在特征值中加一个偏置常量来解决。比如，我们在bundle中绑定了两个特征A和B，A特征的原始取值为区间[0,10)，B特征的原始取值为区间[0,20），我们可以在B特征的取值上加一个偏置常量10，将其取值范围变为[10,30），绑定后的特征取值范围为 [0, 30），这样就可以放心的融合特征A和B了。具体的特征合并算法如下所示：<br><img src="/02.开发-20.MachineLearn/1204.机器学习-集成学习-2.Boosting-4.LightBGM/Merge_Exclusive_Features.png" alt=""></p>
<h1 id="LightGBM的工程优化"><a href="#LightGBM的工程优化" class="headerlink" title="LightGBM的工程优化"></a>LightGBM的工程优化</h1><p>我们将论文《<a href="https://hal.science/hal-03953007/document" target="_blank" rel="noopener">Lightgbm: A highly efficient gradient boosting decision tree</a>》中没有提到的优化方案，而在其相关论文《<a href="https://arxiv.org/abs/1611.01276" target="_blank" rel="noopener">A communication-efficient parallel algorithm for decision tree</a>》中提到的优化方案，放到本节作为LightGBM的工程优化来向大家介绍。</p>
<h2 id="3-1-直接支持类别特征"><a href="#3-1-直接支持类别特征" class="headerlink" title="3.1 直接支持类别特征"></a>3.1 直接支持类别特征</h2><p>实际上大多数机器学习工具都无法直接支持类别特征，一般需要把类别特征，通过 one-hot 编码，转化到多维的0/1特征，降低了空间和时间的效率。但我们知道对于决策树来说并不推荐使用 one-hot 编码，尤其当类别特征中类别个数很多的情况下，会存在以下问题：</p>
<ul>
<li>会产生样本切分不平衡问题，导致切分增益非常小（即浪费了这个特征）。<br>使用 one-hot编码，意味着在每一个决策节点上只能使用one vs rest（例如是不是狗，是不是猫等）的切分方式。例如，动物类别切分后，会产生是否狗，是否猫等一系列特征，这一系列特征上只有少量样本为 1，大量样本为 0，这时候切分样本会产生不平衡，这意味着切分增益也会很小。较小的那个切分样本集，它占总样本的比例太小，无论增益多大，乘以该比例之后几乎可以忽略；较大的那个拆分样本集，它几乎就是原始的样本集，增益几乎为零。比较直观的理解就是不平衡的切分和不切分没有区别。</li>
<li>会影响决策树的学习。<br>因为就算可以对这个类别特征进行切分，独热编码也会把数据切分到很多零散的小空间上，如下图左边所示。而决策树学习时利用的是统计信息，在这些数据量小的空间上，统计信息不准确，学习效果会变差。但如果使用下图右边的切分方法，数据会被切分到两个比较大的空间，进一步的学习也会更好。下图右边叶子节点的含义是X=A或者X=C放到左子节点，其余放到右子节点。<br><img src="/02.开发-20.MachineLearn/1204.机器学习-集成学习-2.Boosting-4.LightBGM/one-hot_many-many.png" alt=""></li>
</ul>
<p>而类别特征的使用在实践中是很常见的。且为了解决one-hot编码处理类别特征的不足，LightGBM优化了对类别特征的支持，可以直接输入类别特征，不需要额外的0/1展开。LightGBM采用 many-vs-many 的切分方式将类别特征分为两个子集，实现类别特征的最优切分。假设某维特征有 k 个类别，则有$2^{k-1}-1$ 种可能，时间复杂度为 $O(2^k)$，LightGBM 基于 Fisher的《On Grouping For Maximum Homogeneity》论文实现了$O(klogkk)$的时间复杂度。</p>
<p>算法流程如下图所示，在枚举分割点之前，先把直方图按照每个类别对应的label均值进行排序；然后按照排序的结果依次枚举最优分割点。从下图可以看到，$\frac{Sum(y)}{Count(y)}$ 为类别的均值。当然，这个方法很容易过拟合，所以LightGBM里面还增加了很多对于这个方法的约束和正则化。<br><img src="/02.开发-20.MachineLearn/1204.机器学习-集成学习-2.Boosting-4.LightBGM/catelog_feature_split.png" alt=""><br>图：LightGBM求解类别特征的最优切分算法</p>
<p>在Expo数据集上的实验结果表明，相比0/1展开的方法，使用LightGBM支持的类别特征可以使训练速度加速8倍，并且精度一致。更重要的是，<strong>LightGBM是第一个直接支持类别特征的GBDT工具</strong>。</p>
<h2 id="3-2-支持高效并行"><a href="#3-2-支持高效并行" class="headerlink" title="3.2 支持高效并行"></a>3.2 支持高效并行</h2><h3 id="（1）特征并行"><a href="#（1）特征并行" class="headerlink" title="（1）特征并行"></a>（1）特征并行</h3><p>特征并行的主要思想是不同机器在不同的特征集合上分别寻找最优的分割点，然后在机器间同步最优的分割点。XGBoost使用的就是这种特征并行方法。这种特征并行方法有个很大的缺点：就是对数据进行垂直划分，每台机器所含数据不同，然后使用不同机器找到不同特征的最优分裂点，划分结果需要通过通信告知每台机器，增加了额外的复杂度。</p>
<p>LightGBM 则不进行数据垂直划分，而是在<strong>每台机器上保存全部训练数据</strong>，在得到最佳划分方案后可在本地执行划分而减少了不必要的通信。具体过程如下图所示。<br><img src="/02.开发-20.MachineLearn/1204.机器学习-集成学习-2.Boosting-4.LightBGM/feature_Parallelization.png" alt=""></p>
<h3 id="（2）数据并行"><a href="#（2）数据并行" class="headerlink" title="（2）数据并行"></a>（2）数据并行</h3><p>传统的数据并行策略主要为水平划分数据，让不同的机器先在本地构造直方图，然后进行全局的合并，最后在合并的直方图上面寻找最优分割点。这种数据划分有一个很大的缺点：通讯开销过大。如果使用点对点通信，一台机器的通讯开销大约为 $O(#machine <em> #feature </em> #bin)$ ；如果使用集成的通信，则通讯开销为$O(2 <em> #feature </em> #bin)$ 。<br>LightGBM在数据并行中使用分散规约 (Reduce scatter) 把直方图合并的任务分摊到不同的机器，降低通信和计算，并利用直方图做差，进一步减少了一半的通信量。具体过程如下图所示。<br><img src="/02.开发-20.MachineLearn/1204.机器学习-集成学习-2.Boosting-4.LightBGM/data_parallelization.png" alt=""></p>
<h3 id="（3）投票并行"><a href="#（3）投票并行" class="headerlink" title="（3）投票并行"></a>（3）投票并行</h3><p>基于投票的数据并行则进一步优化数据并行中的通信代价，使通信代价变成常数级别。在数据量很大的时候，使用投票并行的方式只<strong>合并部分特征的直方图</strong>从而达到降低通信量的目的，可以得到非常好的加速效果。具体过程如下图所示。</p>
<p>大致步骤为两步：</p>
<ol>
<li>本地找出 Top K 特征，并基于投票筛选出可能是最优分割点的特征；</li>
<li>合并时只合并每个机器选出来的特征。</li>
</ol>
<p><img src="/02.开发-20.MachineLearn/1204.机器学习-集成学习-2.Boosting-4.LightBGM/Parallel_optimization.png" alt=""></p>
<h2 id="3-3-Cache命中率优化"><a href="#3-3-Cache命中率优化" class="headerlink" title="3.3 Cache命中率优化"></a>3.3 Cache命中率优化</h2><p>XGBoost对cache优化不友好，如下图所示。在预排序后，特征对梯度的访问是一种随机访问，并且不同的特征访问的顺序不一样，无法对cache进行优化。同时，在每一层长树的时候，需要随机访问一个行索引到叶子索引的数组，并且不同特征访问的顺序也不一样，也会造成较大的cache miss。为了解决缓存命中率低的问题，XGBoost 提出了缓存访问算法进行改进。</p>
<p><img src="/02.开发-20.MachineLearn/1204.机器学习-集成学习-2.Boosting-4.LightBGM/xgboost_cache_miss.png" alt=""></p>
<p>而 LightGBM 所使用直方图算法对 Cache 天生友好：</p>
<ol>
<li>首先，所有的特征都采用相同的方式获得梯度（区别于XGBoost的不同特征通过不同的索引获得梯度），只需要对梯度进行排序并可实现连续访问，大大提高了缓存命中率；</li>
<li>其次，因为不需要存储行索引到叶子索引的数组，降低了存储消耗，而且也不存在 Cache Miss的问题。<br><img src="/02.开发-20.MachineLearn/1204.机器学习-集成学习-2.Boosting-4.LightBGM/LightGBM_cache_hit.png" alt=""></li>
</ol>
<h1 id="4-LightGBM的优缺点"><a href="#4-LightGBM的优缺点" class="headerlink" title="4. LightGBM的优缺点"></a>4. LightGBM的优缺点</h1><h2 id="4-1-优点"><a href="#4-1-优点" class="headerlink" title="4.1 优点"></a>4.1 优点</h2><p>这部分主要总结下 LightGBM 相对于 XGBoost 的优点，从内存和速度两方面进行介绍。</p>
<p>（1）速度更快</p>
<ul>
<li>LightGBM 采用了直方图算法将遍历样本转变为遍历直方图，极大的降低了时间复杂度；</li>
<li>LightGBM 在训练过程中采用单边梯度算法过滤掉梯度小的样本，减少了大量的计算；</li>
<li>LightGBM 采用了基于 Leaf-wise 算法的增长策略构建树，减少了很多不必要的计算量；</li>
<li>LightGBM 采用优化后的特征并行、数据并行方法加速计算，当数据量非常大的时候还可以采用投票并行的策略；</li>
<li>LightGBM 对缓存也进行了优化，增加了缓存命中率；</li>
</ul>
<p>（2）内存更小</p>
<ul>
<li>XGBoost使用预排序后需要记录特征值及其对应样本的统计值的索引，而 LightGBM 使用了直方图算法将特征值转变为 bin 值，且不需要记录特征到样本的索引，将空间复杂度从 $O(2*#data)$ 降低为 $O(#bin)$，极大的减少了内存消耗；</li>
<li>LightGBM 采用了直方图算法将存储特征值转变为存储 $bin$ 值，降低了内存消耗；</li>
<li>LightGBM 在训练过程中采用互斥特征捆绑算法减少了特征数量，降低了内存消耗。</li>
</ul>
<h2 id="4-2-缺点"><a href="#4-2-缺点" class="headerlink" title="4.2 缺点"></a>4.2 缺点</h2><ul>
<li>可能会长出比较深的决策树，产生过拟合。因此LightGBM在Leaf-wise之上增加了一个最大深度限制，在保证高效率的同时防止过拟合；</li>
<li>Boosting族是迭代算法，每一次迭代都根据上一次迭代的预测结果对样本进行权重调整，所以随着迭代不断进行，误差会越来越小，模型的偏差（bias）会不断降低。由于LightGBM是基于偏差的算法，所以会对噪点较为敏感；</li>
<li>在寻找最优解时，依据的是最优切分变量，没有将最优解是全部特征的综合这一理念考虑进去；</li>
</ul>
<h1 id="实例"><a href="#实例" class="headerlink" title="实例"></a>实例</h1><p>数据集和代码均在<a href="https://github.com/Microstrong0305/WeChat-zhihu-csdnblog-code/tree/master/Ensemble%20Learning/LightGBM" target="_blank" rel="noopener">GitHub</a></p>
<h1 id="reference"><a href="#reference" class="headerlink" title="reference"></a>reference</h1><p><a href="https://apxml.com/zh/courses/mastering-gradient-boosting-algorithms" target="_blank" rel="noopener">精通梯度提升算法</a><br><a href="https://zhuanlan.zhihu.com/p/99069186" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/99069186</a><br><a href="https://lightgbm.cn/en/stable/index.html" target="_blank" rel="noopener">https://lightgbm.cn/en/stable/index.html</a></p>
<h3 id="LightGBM论文解读："><a href="#LightGBM论文解读：" class="headerlink" title="LightGBM论文解读："></a>LightGBM论文解读：</h3><p>[1]. Ke G, Meng Q, Finley T, et al. Lightgbm: A highly efficient gradient boosting decision tree[C]//Advances in Neural Information Processing Systems. 2017: 3146-3154.</p>
<p>[2]. Taifeng Wang分享LightGBM的视频，地址：<a href="https://v.qq.com/x/page/k0362z6lqix.html" target="_blank" rel="noopener">https://v.qq.com/x/page/k0362z6lqix.html</a></p>
<p>[3]. 开源|LightGBM：三天内收获GitHub 1000+ 星，地址：<a href="https://mp.weixin.qq.com/s/M25d_43gHkk3FyG_Jhlvog" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/M25d_43gHkk3FyG_Jhlvog</a></p>
<p>[4]. Lightgbm源论文解析：LightGBM: A Highly Efficient Gradient Boosting Decision Tree，地址：<a href="https://blog.csdn.net/anshuai_aw1/article/details/83048709" target="_blank" rel="noopener">https://blog.csdn.net/anshuai_aw1/article/details/83048709</a></p>
<p>[5]. 快的不要不要的lightGBM - 王乐的文章 - 知乎 <a href="https://zhuanlan.zhihu.com/p/31986189" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/31986189</a></p>
<p>[6]. 『 论文阅读』LightGBM原理-LightGBM: A Highly Efficient Gradient Boosting Decision Tree，地址：<a href="https://blog.csdn.net/shine19930820/article/details/79123216" target="_blank" rel="noopener">https://blog.csdn.net/shine19930820/article/details/79123216</a></p>
<h3 id="LightGBM算法讲解："><a href="#LightGBM算法讲解：" class="headerlink" title="LightGBM算法讲解："></a>LightGBM算法讲解：</h3><p>[7]. 【机器学习】决策树（下）——XGBoost、LightGBM（非常详细） - 阿泽的文章 - 知乎 <a href="https://zhuanlan.zhihu.com/p/87885678" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/87885678</a></p>
<p>[8]. 入门 | 从结构到性能，一文概述XGBoost、Light GBM和CatBoost的同与不同，地址：<a href="https://mp.weixin.qq.com/s/TD3RbdDidCrcL45oWpxNmw" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/TD3RbdDidCrcL45oWpxNmw</a></p>
<p>[9]. CatBoost vs. Light GBM vs. XGBoost，地址：<a href="https://towardsdatascience.com/catboost-vs-light-gbm-vs-xgboost-5f93620723db" target="_blank" rel="noopener">https://towardsdatascience.com/catboost-vs-light-gbm-vs-xgboost-5f93620723db</a></p>
<p>[10]. 机器学习算法之LightGBM，地址：<a href="https://www.biaodianfu.com/lightgbm.html" target="_blank" rel="noopener">https://www.biaodianfu.com/lightgbm.html</a></p>
<h3 id="LightGBM工程优化："><a href="#LightGBM工程优化：" class="headerlink" title="LightGBM工程优化："></a>LightGBM工程优化：</h3><p>[11]. Meng Q, Ke G, Wang T, et al. A communication-efficient parallel algorithm for decision tree[C]//Advances in Neural Information Processing Systems. 2016: 1279-1287.</p>
<p>[12]. Zhang H, Si S, Hsieh C J. GPU-acceleration for Large-scale Tree Boosting[J]. arXiv preprint arXiv:1706.08359, 2017.</p>
<p>[13]. LightGBM的官方GitHub代码库，地址：<a href="https://github.com/microsoft/LightGBM" target="_blank" rel="noopener">https://github.com/microsoft/LightGBM</a></p>
<p>[14]. 关于sklearn中的决策树是否应该用one-hot编码？ - 柯国霖的回答 - 知乎 <a href="https://www.zhihu.com/question/266195966/answer/306104444" target="_blank" rel="noopener">https://www.zhihu.com/question/266195966/answer/306104444</a></p>
<h3 id="LightGBM实例："><a href="#LightGBM实例：" class="headerlink" title="LightGBM实例："></a>LightGBM实例：</h3><p>[15]. LightGBM使用，地址：<a href="https://bacterous.github.io/2018/09/13/LightGBM%E4%BD%BF%E7%94%A8/" target="_blank" rel="noopener">https://bacterous.github.io/2018/09/13/LightGBM%E4%BD%BF%E7%94%A8/</a></p>
<p>[16]. LightGBM两种使用方式 ，地址：<a href="https://www.cnblogs.com/chenxiangzhen/p/10894306.html" target="_blank" rel="noopener">https://www.cnblogs.com/chenxiangzhen/p/10894306.html</a></p>
<h3 id="LightGBM若干问题的思考："><a href="#LightGBM若干问题的思考：" class="headerlink" title="LightGBM若干问题的思考："></a>LightGBM若干问题的思考：</h3><p>[17]. GBDT、XGBoost、LightGBM的区别和联系，地址：<a href="https://www.jianshu.com/p/765efe2b951a" target="_blank" rel="noopener">https://www.jianshu.com/p/765efe2b951a</a></p>
<p>[18]. xgboost和lightgbm的区别和适用场景，地址：<a href="https://www.nowcoder.com/ta/review-ml/review?page=101" target="_blank" rel="noopener">https://www.nowcoder.com/ta/review-ml/review?page=101</a> </p>

      
    </div>
    
    
    

    
      <div>
        <div>
    
        <div style="text-align:center;color: #ccc;font-size:14px;">-------------本文结束<i class="fa fa-paw"></i>感谢您的阅读-------------</div>
    
</div>

      </div>
    
    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/Machine-Learning/" rel="tag"><i class="fa fa-tag"></i> Machine-Learning</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/01.知识-02.Math/二阶泰勒展开/" rel="next" title="二阶泰勒展开">
                <i class="fa fa-chevron-left"></i> 二阶泰勒展开
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/02.开发-20.MachineLearn/1205.机器学习-强化学习-0.概述/" rel="prev" title="1205.机器学习-强化学习-0.概述">
                1205.机器学习-强化学习-0.概述 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
        <!-- JiaThis Button BEGIN -->
<div class="jiathis_style">
  <a class="jiathis_button_tsina"></a>
  <a class="jiathis_button_tqq"></a>
  <a class="jiathis_button_weixin"></a>
  <a class="jiathis_button_cqq"></a>
  <a class="jiathis_button_douban"></a>
  <a class="jiathis_button_renren"></a>
  <a class="jiathis_button_qzone"></a>
  <a class="jiathis_button_kaixin001"></a>
  <a class="jiathis_button_copy"></a>
  <a href="http://www.jiathis.com/share" class="jiathis jiathis_txt jiathis_separator jtico jtico_jiathis" target="_blank"></a>
  <a class="jiathis_counter_style"></a>
</div>
<script type="text/javascript" >
  var jiathis_config={
    hideMore:false
  }
</script>
<script type="text/javascript" src="http://v3.jiathis.com/code/jia.js" charset="utf-8"></script>
<!-- JiaThis Button END -->

      
    </div>
  </div>


          </div>
          


          
  <div class="comments" id="comments">
    
      <div id="lv-container" data-id="city" data-uid="MTAyMC8zNTI3OS8xMTgxNQ"></div>
    
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/uploads/avatar.jpg"
               alt="Ben-air" />
          <p class="site-author-name" itemprop="name">Ben-air</p>
           
              <p class="site-description motion-element" itemprop="description"></p>
          
        </div>
        <nav class="site-state motion-element">

          
            <div class="site-state-item site-state-posts">
              <a href="/archives/">
                <span class="site-state-item-count">468</span>
                <span class="site-state-item-name">日志</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-categories">
              <a href="/categories/index.html">
                <span class="site-state-item-count">97</span>
                <span class="site-state-item-name">分类</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-tags">
              <a href="/tags/index.html">
                <span class="site-state-item-count">144</span>
                <span class="site-state-item-name">标签</span>
              </a>
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
                <a href="https://github.com/Ben-unbelieveable" target="_blank" title="GitHub">
                  
                    <i class="fa fa-fw fa-github"></i>
                  
                    
                      GitHub
                    
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="your-twitter-url" target="_blank" title="Twitter">
                  
                    <i class="fa fa-fw fa-twitter"></i>
                  
                    
                      Twitter
                    
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="2432065975" target="_blank" title="Wechat">
                  
                    <i class="fa fa-fw fa-globe"></i>
                  
                    
                      Wechat
                    
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="your-weibo-url" target="_blank" title="Weibo">
                  
                    <i class="fa fa-fw fa-weibo"></i>
                  
                    
                      Weibo
                    
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="your-douban-url" target="_blank" title="DouBan">
                  
                    <i class="fa fa-fw fa-globe"></i>
                  
                    
                      DouBan
                    
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="https://www.zhihu.com/people/dan-mo-39/activities" target="_blank" title="ZhiHu">
                  
                    <i class="fa fa-fw fa-globe"></i>
                  
                    
                      ZhiHu
                    
                </a>
              </span>
            
          
        </div>

        
        

        
        

        


      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#LightGBM简介"><span class="nav-number">1.</span> <span class="nav-text">LightGBM简介</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-1-LightGBM提出的动机"><span class="nav-number">1.1.</span> <span class="nav-text">1.1 LightGBM提出的动机</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-2-XGBoost的缺点及LightGBM的优化"><span class="nav-number">1.2.</span> <span class="nav-text">1.2 XGBoost的缺点及LightGBM的优化</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#XGBoost的缺点"><span class="nav-number">1.2.1.</span> <span class="nav-text">XGBoost的缺点</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#LightGBM的优化"><span class="nav-number">1.2.2.</span> <span class="nav-text">LightGBM的优化</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#LightGBM的基本原理"><span class="nav-number">2.</span> <span class="nav-text">LightGBM的基本原理</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#2-1-基于Histogram的决策树算法"><span class="nav-number">2.1.</span> <span class="nav-text">2.1 基于Histogram的决策树算法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#（1）直方图算法"><span class="nav-number">2.1.1.</span> <span class="nav-text">（1）直方图算法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#（2）直方图做差加速"><span class="nav-number">2.1.2.</span> <span class="nav-text">（2）直方图做差加速</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-2-带深度限制的-Leaf-wise-算法"><span class="nav-number">2.2.</span> <span class="nav-text">2.2 带深度限制的 Leaf-wise 算法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-3-单边梯度采样算法"><span class="nav-number">2.3.</span> <span class="nav-text">2.3 单边梯度采样算法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-4-互斥特征捆绑算法"><span class="nav-number">2.4.</span> <span class="nav-text">2.4 互斥特征捆绑算法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#（1）解决哪些特征应该绑在一起"><span class="nav-number">2.4.1.</span> <span class="nav-text">（1）解决哪些特征应该绑在一起</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#LightGBM的工程优化"><span class="nav-number">3.</span> <span class="nav-text">LightGBM的工程优化</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#3-1-直接支持类别特征"><span class="nav-number">3.1.</span> <span class="nav-text">3.1 直接支持类别特征</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-2-支持高效并行"><span class="nav-number">3.2.</span> <span class="nav-text">3.2 支持高效并行</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#（1）特征并行"><span class="nav-number">3.2.1.</span> <span class="nav-text">（1）特征并行</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#（2）数据并行"><span class="nav-number">3.2.2.</span> <span class="nav-text">（2）数据并行</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#（3）投票并行"><span class="nav-number">3.2.3.</span> <span class="nav-text">（3）投票并行</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-3-Cache命中率优化"><span class="nav-number">3.3.</span> <span class="nav-text">3.3 Cache命中率优化</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#4-LightGBM的优缺点"><span class="nav-number">4.</span> <span class="nav-text">4. LightGBM的优缺点</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#4-1-优点"><span class="nav-number">4.1.</span> <span class="nav-text">4.1 优点</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-2-缺点"><span class="nav-number">4.2.</span> <span class="nav-text">4.2 缺点</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#实例"><span class="nav-number">5.</span> <span class="nav-text">实例</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#reference"><span class="nav-number">6.</span> <span class="nav-text">reference</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#LightGBM论文解读："><span class="nav-number">6.0.1.</span> <span class="nav-text">LightGBM论文解读：</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#LightGBM算法讲解："><span class="nav-number">6.0.2.</span> <span class="nav-text">LightGBM算法讲解：</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#LightGBM工程优化："><span class="nav-number">6.0.3.</span> <span class="nav-text">LightGBM工程优化：</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#LightGBM实例："><span class="nav-number">6.0.4.</span> <span class="nav-text">LightGBM实例：</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#LightGBM若干问题的思考："><span class="nav-number">6.0.5.</span> <span class="nav-text">LightGBM若干问题的思考：</span></a></li></ol></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2026</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Ben-air</span>
</div>


        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="post-meta-item-icon">
      <i class="fa fa-user"></i>
    </span>
      已有<span class="busuanzi-value" id="busuanzi_value_site_uv"></span>人访问
  

  
    <span class="post-meta-divider">|</span>
  

  
    <span class="post-meta-item-icon">
      <i class="fa fa-eye"></i>
    </span>
      总访问<span class="busuanzi-value" id="busuanzi_value_site_pv"></span>次
  
</div>


        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  


  











  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>

  
  <script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.2"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.2"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.2"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.2"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.2"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.2"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.2"></script>



  


  




	





  





  
    <script type="text/javascript">
      (function(d, s) {
        var j, e = d.getElementsByTagName(s)[0];
        if (typeof LivereTower === 'function') { return; }
        j = d.createElement(s);
        j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
        j.async = true;
        e.parentNode.insertBefore(j, e);
      })(document, 'script');
    </script>
  






  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
