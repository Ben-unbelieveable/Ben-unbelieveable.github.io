<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">



  
  
    
    
  <script src="/lib/pace/pace.min.js?v=1.0.2"></script>
  <link href="/lib/pace/pace-theme-minimal.min.css?v=1.0.2" rel="stylesheet">
  <script>
    (function(){
      if(''){
        if (prompt('请输入文章密码') !== ''){
          alert('密码错误！');
          history.back();
        }
      }
    })();
  </script>







<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.2" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="neural_network," />








  <link rel="shortcut icon" type="image/x-icon" href="/images/favicon.ico?v=5.1.2" />






<meta name="description" content="我们的视觉系统每时每刻都在接收大量的感官输入， 这些感官输入远远超过了大脑能够完全处理的程度。 然而，并非所有刺激的影响都是相等的。 意识的聚集和专注使灵长类动物能够在复杂的视觉环境中将注意力引向感兴趣的物体。 只关注一小部分信息的能力对进化更加有意义，使人类得以生存和成功。 注意力提示此刻读者正在阅读本书（而忽略了其他的书）， 因此读者的注意力是用机会成本（与金钱类似）来支付的。 为了确保读者现">
<meta name="keywords" content="neural_network">
<meta property="og:type" content="article">
<meta property="og:title" content="2105.神经网络-注意力机制">
<meta property="og:url" content="http://ben-air.cn/02.开发-20.MachineLearn/2105.神经网络-注意力机制/index.html">
<meta property="og:site_name" content="Ben-air">
<meta property="og:description" content="我们的视觉系统每时每刻都在接收大量的感官输入， 这些感官输入远远超过了大脑能够完全处理的程度。 然而，并非所有刺激的影响都是相等的。 意识的聚集和专注使灵长类动物能够在复杂的视觉环境中将注意力引向感兴趣的物体。 只关注一小部分信息的能力对进化更加有意义，使人类得以生存和成功。 注意力提示此刻读者正在阅读本书（而忽略了其他的书）， 因此读者的注意力是用机会成本（与金钱类似）来支付的。 为了确保读者现">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="https://zh.d2l.ai/_images/eye-coffee.svg">
<meta property="og:image" content="https://zh.d2l.ai/_images/eye-book.svg">
<meta property="og:image" content="https://zh.d2l.ai/_images/qkv.svg">
<meta property="og:image" content="https://zh.d2l.ai/_images/output_nadaraya-waston_736177_36_0.svg">
<meta property="og:image" content="https://zh.d2l.ai/_images/output_nadaraya-waston_736177_51_0.svg">
<meta property="og:image" content="https://zh.d2l.ai/_images/output_nadaraya-waston_736177_66_0.svg">
<meta property="og:image" content="https://zh.d2l.ai/_images/output_nadaraya-waston_736177_147_0.svg">
<meta property="og:image" content="https://zh.d2l.ai/_images/output_nadaraya-waston_736177_162_0.svg">
<meta property="og:image" content="https://zh.d2l.ai/_images/output_nadaraya-waston_736177_177_0.svg">
<meta property="og:image" content="https://zh.d2l.ai/_images/attention-output.svg">
<meta property="og:image" content="https://zh.d2l.ai/_images/multi-head-attention.svg">
<meta property="og:image" content="https://zh.d2l.ai/_images/cnn-rnn-self-attention.svg">
<meta property="og:image" content="https://zh.d2l.ai/_images/output_self-attention-and-positional-encoding_d76d5a_49_0.svg">
<meta property="og:image" content="https://zh.d2l.ai/_images/output_self-attention-and-positional-encoding_d76d5a_79_0.svg">
<meta property="og:updated_time" content="2026-01-06T07:45:21.355Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="2105.神经网络-注意力机制">
<meta name="twitter:description" content="我们的视觉系统每时每刻都在接收大量的感官输入， 这些感官输入远远超过了大脑能够完全处理的程度。 然而，并非所有刺激的影响都是相等的。 意识的聚集和专注使灵长类动物能够在复杂的视觉环境中将注意力引向感兴趣的物体。 只关注一小部分信息的能力对进化更加有意义，使人类得以生存和成功。 注意力提示此刻读者正在阅读本书（而忽略了其他的书）， 因此读者的注意力是用机会成本（与金钱类似）来支付的。 为了确保读者现">
<meta name="twitter:image" content="https://zh.d2l.ai/_images/eye-coffee.svg">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    sidebar: {"position":"left","display":"post","offset":12,"offset_float":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://ben-air.cn/02.开发-20.MachineLearn/2105.神经网络-注意力机制/"/>





  <title>2105.神经网络-注意力机制 | Ben-air</title>
  














</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Ben-air</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            关于
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off"
             placeholder="搜索..." spellcheck="false"
             type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://ben-air.cn/02.开发-20.MachineLearn/2105.神经网络-注意力机制/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Ben-air">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/uploads/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Ben-air">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">2105.神经网络-注意力机制</h1>
        

        <div class="post-meta">
	  

          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2024-07-23T09:31:59+08:00">
                2024-07-23
              </time>
            
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/neural-network/" itemprop="url" rel="index">
                    <span itemprop="name">neural_network</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">
      
      

      
        <p>我们的视觉系统每时每刻都在接收大量的感官输入， 这些感官输入远远超过了大脑能够完全处理的程度。 然而，并非所有刺激的影响都是相等的。 意识的聚集和专注使灵长类动物能够在复杂的视觉环境中将注意力引向感兴趣的物体。 只关注一小部分信息的能力对进化更加有意义，使人类得以生存和成功。</p>
<h1 id="注意力提示"><a href="#注意力提示" class="headerlink" title="注意力提示"></a>注意力提示</h1><p>此刻读者正在阅读本书（而忽略了其他的书）， 因此读者的注意力是用机会成本（与金钱类似）来支付的。 为了确保读者现在投入的注意力是值得的， 作者们尽全力（全部的注意力）创作一本好书。</p>
<p>自经济学研究稀缺资源分配以来，人们正处在“注意力经济”时代， 即人类的注意力被视为可以交换的、有限的、有价值的且稀缺的商品。 许多商业模式也被开发出来去利用这一点： 在音乐或视频流媒体服务上，人们要么消耗注意力在广告上，要么付钱来隐藏广告； 为了在网络游戏世界的成长，人们要么消耗注意力在游戏战斗中， 从而帮助吸引新的玩家，要么付钱立即变得强大。 总之，注意力不是免费的。</p>
<p>注意力是稀缺的，而环境中的干扰注意力的信息却并不少。 比如人类的视觉神经系统大约每秒收到 $10^8$ 位的信息， 这远远超过了大脑能够完全处理的水平。 幸运的是，人类的祖先已经从经验（也称为数据）中认识到 “并非感官的所有输入都是一样的”。 在整个人类历史中，这种只将注意力引向感兴趣的一小部分信息的能力， 使人类的大脑能够更明智地分配资源来生存、成长和社交， 例如发现天敌、找寻食物和伴侣。</p>
<h2 id="注意力提示的方式"><a href="#注意力提示的方式" class="headerlink" title="注意力提示的方式"></a>注意力提示的方式</h2><h3 id="生物学中的注意力提示"><a href="#生物学中的注意力提示" class="headerlink" title="生物学中的注意力提示"></a>生物学中的注意力提示</h3><p>注意力是如何应用于视觉世界中的呢？ 这要从当今十分普及的双组件（two-component）的框架开始讲起： 这个框架的出现可以追溯到19世纪90年代的威廉·詹姆斯， 他被认为是“美国心理学之父” (James, 2007)。 在这个框架中，受试者基于<strong>非自主性提示</strong>和<strong>自主性提示</strong> 有选择地引导注意力的焦点。</p>
<ul>
<li><p>非自主性提示是基于环境中物体的突出性和易见性。 想象一下，假如我们面前有五个物品： 一份报纸、一篇研究论文、一杯咖啡、一本笔记本和一本书。 所有纸制品都是黑白印刷的，但咖啡杯是红色的。 换句话说，这个咖啡杯在这种视觉环境中是突出和显眼的， 不由自主地引起人们的注意。 所以我们会把视力最敏锐的地方放到咖啡上， 如图所示。<br><img src="https://zh.d2l.ai/_images/eye-coffee.svg" alt="image"></p>
</li>
<li><p>喝咖啡后，我们会变得兴奋并想读书， 所以转过头，重新聚焦眼睛，然后看看书， 就像下图描述那样。 与上图中的由于突出性导致的选择不同， 此时选择书是受到了认知和意识的控制， 因此注意力在基于自主性提示去辅助选择时将更为谨慎。 受试者的主观意愿推动，选择的力量也就更强大。<br><img src="https://zh.d2l.ai/_images/eye-book.svg" alt="image"></p>
</li>
</ul>
<h3 id="查询、键和值"><a href="#查询、键和值" class="headerlink" title="查询、键和值"></a>查询、键和值</h3><p> 下面来看看如何通过这两种注意力提示， 用神经网络来设计注意力机制的框架，</p>
<p>首先，考虑一个相对简单的状况， 即只使用非自主性提示。 要想将选择偏向于感官输入， 则可以简单地使用<strong>参数化的全连接层</strong>， 甚至是<strong>非参数化的最大汇聚层</strong>或<strong>平均汇聚层</strong>。<br>非自主性提示，我们可以理解为所有输入的权重是一样的，输入会无差别流入下一层。</p>
<p><strong>自主性提示</strong>就是将注意力机制与全连接层或汇聚层区别开来。在注意力机制的背景下，<strong>自主性提示被称为查询（query）</strong>。 给定任何查询，<strong>注意力机制通过注意力汇聚</strong>（attention pooling） 将选择引导至感官输入（sensory inputs，例如中间特征表示）。 在注意力机制中，这些感官输入被称为值（value）。 更通俗的解释，每个值都与一个键（key）配对， 这可以想象为感官输入的非自主提示。 如下图所示，可以通过设计注意力汇聚的方式， 便于给定的查询（自主性提示）与键（非自主性提示）进行匹配， 这将引导得出最匹配的值（感官输入）。<br>    <img src="https://zh.d2l.ai/_images/qkv.svg" alt="image"><br>    <code>Keys（键）：在非自主提示下，进入视觉系统的的所有元素的线索，称为 Keys。
    Query（查询）：在自主提示下，自主提示的内容或元素的线索，称为 Query。
    Values（值)：在由自主提示 Query 限制或者强制下改变注意力的焦点，也就是经过从 Keys 中进行匹配 Query，所得到的进入视觉系统的内容，称为 Values。</code></p>
<p>注意力机制通过注意力汇聚将查询（自主性提示）和键（非自主性提示）结合在一起，实现对值（感官输入）的选择倾向。<br>自主性提示，相当于我们会通过提供的<strong>查询</strong> ，根据输入信息的 <strong>键</strong>和提供的<strong>查询</strong> 基于特定的规则进行相关性的计算，从而实现对所有输入的信息实现加权（<strong>注意力汇聚</strong>），最终只是选择权重重要的信息流入下游。</p>
<p>自主性的与非自主性的注意力提示解释了人类的注意力的方式，使用神经网络来设计注意力机制的框架，就是基于上述非自主性提示和自主性提示的原理，来选择要聚焦的观察对象。具体抽象出来的元素就是最基本的 Key、Query、Value，如下所示：</p>
<h1 id="注意力汇聚"><a href="#注意力汇聚" class="headerlink" title="注意力汇聚"></a>注意力汇聚</h1><h2 id="Nadaraya-Watson-核回归"><a href="#Nadaraya-Watson-核回归" class="headerlink" title="Nadaraya-Watson 核回归"></a>Nadaraya-Watson 核回归</h2><p><strong>查询</strong>（自主提示）和<strong>键</strong>（非自主提示）之间的交互形成了<strong>注意力汇聚</strong>； 注意力汇聚<strong>有选择地聚合了值</strong>（感官输入）以生成最终的输出。接下来我们来了解下注意力汇聚的细节。<br>1964年提出的<strong>Nadaraya-Watson核回归模型</strong>是一个简单但完整的例子，这里使用它演示具有注意力机制的机器学习。</p>
<h3 id="生成一组测试数据"><a href="#生成一组测试数据" class="headerlink" title="生成一组测试数据"></a>生成一组测试数据</h3><p>根据下面的非线性函数生成一个人工数据集， 其中 $\epsilon$ 数据模拟过程中引入的噪音：<br>$$y_i=2\sin(x_i)+x_i^{0.8}+\epsilon$$<br>其中 $\epsilon$ 服从均值为 0 和标准差为 0.5 的正态分布。 在这里生成了 50 个训练样本和 50 个测试样本。 为了更好地可视化之后的注意力模式，需要将训练样本进行排序。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">n_train = <span class="number">50</span></span><br><span class="line">x_train = tf.sort(tf.random.uniform(shape=(n_train,), maxval=<span class="number">5</span>))</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">2</span> * tf.sin(x) + x**<span class="number">0.8</span></span><br><span class="line">y_train = f(x_train) + tf.random.normal((n_train,), <span class="number">0.0</span>, <span class="number">0.5</span>)  <span class="comment"># 训练样本的输出</span></span><br><span class="line">x_test = tf.range(<span class="number">0</span>, <span class="number">5</span>, <span class="number">0.1</span>)  <span class="comment"># 测试样本</span></span><br><span class="line">y_truth = f(x_test)  <span class="comment"># 测试样本的真实输出</span></span><br><span class="line">n_test = len(x_test)  <span class="comment"># 测试样本数</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 下面的函数将绘制所有的训练样本（样本由圆圈表示）， 不带噪声项的真实数据生成函数（标记为“Truth”）， 以及学习得到的预测函数（标记为“Pred”）。</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_kernel_reg</span><span class="params">(y_hat)</span>:</span></span><br><span class="line">    d2l.plot(x_test, [y_truth, y_hat], <span class="string">'x'</span>, <span class="string">'y'</span>, legend=[<span class="string">'Truth'</span>, <span class="string">'Pred'</span>],</span><br><span class="line">             xlim=[<span class="number">0</span>, <span class="number">5</span>], ylim=[<span class="number">-1</span>, <span class="number">5</span>])</span><br><span class="line">    d2l.plt.plot(x_train, y_train, <span class="string">'o'</span>, alpha=<span class="number">0.5</span>);</span><br></pre></td></tr></table></figure></p>
<p>上面这部分，我们准备好了进行测试的模拟数据，同时准备好预测结果的可视化函数。</p>
<h3 id="平均汇聚"><a href="#平均汇聚" class="headerlink" title="平均汇聚"></a>平均汇聚</h3><p>在我们不使用注意力汇聚模型的时候，那么很容易理解，我们模型的所有输入都是等权重的。所以我们会很容易的得到一个简单的估计器：<br>$$f(x)=\frac{1}{n}\sum_{i=1}^n y_i$$<br>因为没有使用注意力汇集，所有输入是等权重的，所以我们得到的是一个直线$f(x)=\overline{y}$，预测结果和$x_i$ 直接无关了，显然这个模型表现并不好。<br><img src="https://zh.d2l.ai/_images/output_nadaraya-waston_736177_36_0.svg" alt="image"></p>
<h3 id="非参数注意力汇聚"><a href="#非参数注意力汇聚" class="headerlink" title="非参数注意力汇聚"></a>非参数注意力汇聚</h3><p>为了让我们能获得一个更好的模型，显然，我们必须考虑输入的 $x$ 值（因为我们知道，$y$ 是基于 $x$ 值计算得到的）。<br>于是Nadaraya (Nadaraya, 1964)和 Watson (Watson, 1964)提出了一个更好的想法， 根据输入的位置对输出 $y_i$ 进行加权：<br>$$f(x)=\sum_{i=1}^n \frac{K(x-x_i)}{\sum_{j=1}^n K(x-x_j)}y_i $$<br>其中$K$是核函数（kernel），为了便于理解，当我们假设 $y_i$ 恒等于1 时，那么针对x的权重进行积分，可以得到权重综合不管我们的核函数 $K$ 是如何定义的，最终所有位点的权重积分始终为1。所以我们可以根据我们的需要独立的进行核函数的定义，而不会影响整体的权重分配。<br>$$f(x)= \frac{\sum_{i=1}^n K(x-x_i)}{\sum_{j=1}^n K(x-x_j)} = 1$$<br>所以在我们定义好 $K$ 后，基于上述方案，当我们的输入（要查询的值） $x$ 确定后，就可以计算获得每个输入$y_i$所占的的权重（一般越接近待查询值$x$的键$x_i$ 获得的权重越大），然后对所有输入$y_i$进行加权求和（注意力汇聚），得到一个更好的模型。</p>
<p>值得注意的是，<strong>Nadaraya-Watson核回归</strong>是一个<strong>非参数模型</strong>。 因此上述该方法也是 <strong>非参数的注意力汇聚（nonparametric attention pooling）模型</strong>。我们基于这个非参数的注意力汇聚模型来绘制预测结果如下图，可以看到新的模型预测线是平滑的，并且比平均汇聚的预测更接近真实。<br><img src="https://zh.d2l.ai/_images/output_nadaraya-waston_736177_51_0.svg" alt="image"><br>同时我们观察注意力的权重。 这里测试数据的输入相当于查询，而训练数据的输入相当于键。 因为两个输入都是经过排序的，因此由观察可知“查询-键”对越接近， 注意力汇聚的注意力权重就越高。<br><img src="https://zh.d2l.ai/_images/output_nadaraya-waston_736177_66_0.svg" alt="image"></p>
<h3 id="带参数注意力汇聚"><a href="#带参数注意力汇聚" class="headerlink" title="带参数注意力汇聚"></a>带参数注意力汇聚</h3><p>非参数的Nadaraya-Watson核回归具有一致性（consistency）的优点： 如果有足够的数据，此模型会收敛到最优结果。 尽管如此，我们还是可以轻松地将可学习的参数集成到注意力汇聚中,和之前的区别，主要是我们定义好核函数$K$ 计算$x$到每个$x_i$的距离后，我们再引入一个参数 $w$ 对距离进行修正后计算权重<br>$$f(x)=\sum_{i=1}^n \frac{K(x-x_i)<em>w}{\sum_{j=1}^n K(x-x_j)</em>w}y_i $$</p>
<h5 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h5><p>定义Nadaraya-Watson核回归的带参数版本：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">NWKernelRegression</span><span class="params">(tf.keras.layers.Layer)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, **kwargs)</span>:</span></span><br><span class="line">        super().__init__(**kwargs)</span><br><span class="line">        self.w = tf.Variable(initial_value=tf.random.uniform(shape=(<span class="number">1</span>,)))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span><span class="params">(self, queries, keys, values, **kwargs)</span>:</span></span><br><span class="line">        <span class="comment"># 对于训练，“查询”是x_train。“键”是每个点的训练数据的距离。“值”为'y_train'。</span></span><br><span class="line">        <span class="comment"># queries和attention_weights的形状为(查询个数，“键－值”对个数)</span></span><br><span class="line">        queries = tf.repeat(tf.expand_dims(queries, axis=<span class="number">1</span>), repeats=keys.shape[<span class="number">1</span>], axis=<span class="number">1</span>)</span><br><span class="line">        self.attention_weights = tf.nn.softmax(-((queries - keys) * self.w)**<span class="number">2</span> /<span class="number">2</span>, axis =<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># values的形状为(查询个数，“键－值”对个数)</span></span><br><span class="line">        <span class="keyword">return</span> tf.squeeze(tf.matmul(tf.expand_dims(self.attention_weights, axis=<span class="number">1</span>), tf.expand_dims(values, axis=<span class="number">-1</span>)))</span><br></pre></td></tr></table></figure></p>
<p>将训练数据集变换为键和值用于训练注意力模型。 在带参数的注意力汇聚模型中， 任何一个训练样本的输入都会和除自己以外的所有训练样本的“键－值”对进行计算， 从而得到其对应的预测输出。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># X_tile的形状:(n_train，n_train)，每一行都包含着相同的训练输入</span></span><br><span class="line">X_tile = tf.repeat(tf.expand_dims(x_train, axis=<span class="number">0</span>), repeats=n_train, axis=<span class="number">0</span>)</span><br><span class="line"><span class="comment"># Y_tile的形状:(n_train，n_train)，每一行都包含着相同的训练输出</span></span><br><span class="line">Y_tile = tf.repeat(tf.expand_dims(y_train, axis=<span class="number">0</span>), repeats=n_train, axis=<span class="number">0</span>)</span><br><span class="line"><span class="comment"># keys的形状:('n_train'，'n_train'-1)</span></span><br><span class="line">keys = tf.reshape(X_tile[tf.cast(<span class="number">1</span> - tf.eye(n_train), dtype=tf.bool)], shape=(n_train, <span class="number">-1</span>))</span><br><span class="line"><span class="comment"># values的形状:('n_train'，'n_train'-1)</span></span><br><span class="line">values = tf.reshape(Y_tile[tf.cast(<span class="number">1</span> - tf.eye(n_train), dtype=tf.bool)], shape=(n_train, <span class="number">-1</span>))</span><br></pre></td></tr></table></figure></p>
<p>训练带参数的注意力汇聚模型时，使用平方损失函数和随机梯度下降。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">net = NWKernelRegression()</span><br><span class="line">loss_object = tf.keras.losses.MeanSquaredError()</span><br><span class="line">optimizer = tf.keras.optimizers.SGD(learning_rate=<span class="number">0.5</span>)</span><br><span class="line">animator = d2l.Animator(xlabel=<span class="string">'epoch'</span>, ylabel=<span class="string">'loss'</span>, xlim=[<span class="number">1</span>, <span class="number">5</span>])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">5</span>):</span><br><span class="line">    <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> t:</span><br><span class="line">        loss = loss_object(y_train, net(x_train, keys, values)) * len(y_train)</span><br><span class="line">    grads = t.gradient(loss, net.trainable_variables)</span><br><span class="line">    optimizer.apply_gradients(zip(grads, net.trainable_variables))</span><br><span class="line">    print(<span class="string">f'epoch <span class="subst">&#123;epoch + <span class="number">1</span>&#125;</span>, loss <span class="subst">&#123;float(loss):<span class="number">.6</span>f&#125;</span>'</span>)</span><br><span class="line">    animator.add(epoch + <span class="number">1</span>, float(loss))</span><br></pre></td></tr></table></figure></p>
<p>损失函数下降曲线如下图：<br><img src="https://zh.d2l.ai/_images/output_nadaraya-waston_736177_147_0.svg" alt="image"><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># keys的形状:(n_test，n_train)，每一行包含着相同的训练输入（例如，相同的键）</span></span><br><span class="line">keys = tf.repeat(tf.expand_dims(x_train, axis=<span class="number">0</span>), repeats=n_test, axis=<span class="number">0</span>)</span><br><span class="line"><span class="comment"># value的形状:(n_test，n_train)</span></span><br><span class="line">values = tf.repeat(tf.expand_dims(y_train, axis=<span class="number">0</span>), repeats=n_test, axis=<span class="number">0</span>)</span><br><span class="line">y_hat = net(x_test, keys, values)</span><br><span class="line">plot_kernel_reg(y_hat)</span><br><span class="line"><span class="comment"># 注意力权重绘图</span></span><br><span class="line">d2l.show_heatmaps(tf.expand_dims(</span><br><span class="line">                      tf.expand_dims(net.attention_weights, axis=<span class="number">0</span>), axis=<span class="number">0</span>),</span><br><span class="line">                  xlabel=<span class="string">'Sorted training inputs'</span>,</span><br><span class="line">                  ylabel=<span class="string">'Sorted testing inputs'</span>)</span><br></pre></td></tr></table></figure></p>
<p>带参数注意力汇聚的拟合结果和注意力权重热图如下：<br><img src="https://zh.d2l.ai/_images/output_nadaraya-waston_736177_162_0.svg" alt="image"><br><img src="https://zh.d2l.ai/_images/output_nadaraya-waston_736177_177_0.svg" alt="image"><br>与非参数的注意力汇聚模型相比， 带参数的模型加入可学习的参数后， 曲线在注意力权重较大的区域变得更不平滑。</p>
<h1 id="注意力评分"><a href="#注意力评分" class="headerlink" title="注意力评分"></a>注意力评分</h1><p>在我们进行注意力汇聚介绍时，提到一个重要的函数 $\alpha(x-x_i) = \frac{K(x-x_i)<em>w}{\sum_{j=1}^n K(x-x_j)</em>w}$ ，它可以帮我们衡量不同查询和键之间的距离，从而确定给定查询下，各个键的权重分布。$K(x-x_i)$ 又称为 <strong>注意力评分函数</strong> 可以帮助我们衡量查询$q$和键$x_i$之间的相似性/距离，进而确定每个 $y_i$的权重，最后得到的注意力汇聚的输出就是基于这些注意力权重的值的加权和。<br><img src="https://zh.d2l.ai/_images/attention-output.svg" alt="image"><br>所以显然，不同的注意力评分函数的选择，会给我们带来不同的权重计算方式，这里记录几个流行的评分函数。</p>
<h2 id="masked-softmax-operation-掩蔽softmax操作"><a href="#masked-softmax-operation-掩蔽softmax操作" class="headerlink" title="masked softmax operation 掩蔽softmax操作"></a>masked softmax operation 掩蔽softmax操作</h2><p>在某些情况下，并非所有的值都应该被纳入到注意力汇聚中。为了仅将有意义的词元作为值来获取注意力汇聚， 可以指定一个有效序列长度（即词元的个数）， 以便在计算softmax时过滤掉超出指定范围的位置。 下面的masked_softmax函数 实现了这样的掩蔽softmax操作（masked softmax operation）， 其中任何超出有效长度的位置都被掩蔽并置为0。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">#@save</span><br><span class="line">def masked_softmax(X, valid_lens):</span><br><span class="line">    &quot;&quot;&quot;通过在最后一个轴上掩蔽元素来执行softmax操作&quot;&quot;&quot;</span><br><span class="line">    # X:3D张量，valid_lens:1D或2D张量</span><br><span class="line">    if valid_lens is None:</span><br><span class="line">        return tf.nn.softmax(X, axis=-1)</span><br><span class="line">    else:</span><br><span class="line">        shape = X.shape</span><br><span class="line">        if len(valid_lens.shape) == 1:</span><br><span class="line">            valid_lens = tf.repeat(valid_lens, repeats=shape[1])</span><br><span class="line"></span><br><span class="line">        else:</span><br><span class="line">            valid_lens = tf.reshape(valid_lens, shape=-1)</span><br><span class="line">        # 最后一轴上被掩蔽的元素使用一个非常大的负值替换，从而其softmax输出为0</span><br><span class="line">        X = d2l.sequence_mask(tf.reshape(X, shape=(-1, shape[-1])),</span><br><span class="line">                              valid_lens, value=-1e6)</span><br><span class="line">        return tf.nn.softmax(tf.reshape(X, shape=shape), axis=-1)</span><br><span class="line"></span><br><span class="line">#  考虑由两个 2*4 矩阵表示的样本， 这两个样本的有效长度分别为2和3。 经过掩蔽softmax操作，超出有效长度的值都被掩蔽为0。</span><br><span class="line">masked_softmax(tf.random.uniform(shape=(2, 2, 4)), tf.constant([2, 3]))</span><br><span class="line"></span><br><span class="line">&lt;tf.Tensor: shape=(2, 2, 4), dtype=float32, numpy=</span><br><span class="line">array([[[0.42428792, 0.575712  , 0.        , 0.        ],</span><br><span class="line">        [0.5350254 , 0.46497452, 0.        , 0.        ]],</span><br><span class="line"></span><br><span class="line">       [[0.32676727, 0.47748092, 0.19575192, 0.        ],</span><br><span class="line">        [0.43579608, 0.30782804, 0.2563759 , 0.        ]]], dtype=float32)&gt;</span><br><span class="line"></span><br><span class="line"># 也可以使用二维张量，为矩阵样本中的每一行指定有效长度。</span><br><span class="line">masked_softmax(tf.random.uniform(shape=(2, 2, 4)), tf.constant([[1, 3], [2, 4]]))</span><br><span class="line"></span><br><span class="line">&lt;tf.Tensor: shape=(2, 2, 4), dtype=float32, numpy=</span><br><span class="line">array([[[1.        , 0.        , 0.        , 0.        ],</span><br><span class="line">        [0.40658087, 0.38102806, 0.21239112, 0.        ]],</span><br><span class="line"></span><br><span class="line">       [[0.3735564 , 0.6264436 , 0.        , 0.        ],</span><br><span class="line">        [0.3183936 , 0.22352162, 0.18998112, 0.26810366]]], dtype=float32)&gt;</span><br></pre></td></tr></table></figure></p>
<h2 id="additive-attention-加性注意力"><a href="#additive-attention-加性注意力" class="headerlink" title="additive attention 加性注意力"></a>additive attention 加性注意力</h2><h2 id="缩放点积注意力"><a href="#缩放点积注意力" class="headerlink" title="缩放点积注意力"></a>缩放点积注意力</h2><h1 id="多头注意力"><a href="#多头注意力" class="headerlink" title="多头注意力"></a>多头注意力</h1><p>当给定相同的查询、键和值的集合时， 我们希望模型可以基于相同的注意力机制学习到不同的行为， 然后将不同的行为作为知识组合起来， 捕获序列内各种范围的依赖关系 （例如，短距离依赖和长距离依赖关系）。 因此，允许注意力机制组合使用查询、键和值的不同 子空间表示（representation subspaces）可能是有益的。</p>
<p>为此，与其只使用单独一个注意力汇聚， 我们可以用独立学习得到的 $h$ 组不同的线性投影（linear projections）来变换查询、键和值。 然后，这组变换后的查询、键和值将并行地送到注意力汇聚中。 最后，将这个注意力汇聚的输出拼接在一起， 并且通过另一个可以学习的线性投影进行变换， 以产生最终输出。 这种设计被称为多头注意力（multihead attention） (Vaswani et al., 2017)。下图展示了使用全连接层来实现可学习的线性变换的多头注意力。<br><img src="https://zh.d2l.ai/_images/multi-head-attention.svg" alt="image"></p>
<h1 id="自主机制和位置编码"><a href="#自主机制和位置编码" class="headerlink" title="自主机制和位置编码"></a>自主机制和位置编码</h1><h2 id="自注意力、卷积神经网络和循环神经网络"><a href="#自注意力、卷积神经网络和循环神经网络" class="headerlink" title="自注意力、卷积神经网络和循环神经网络"></a>自注意力、卷积神经网络和循环神经网络</h2><p>目标都是将由个词元组成的序列映射到另一个长度相等的序列，其中的每个输入词元或输出词元都由维向量表示。具体来说，将比较的是卷积神经网络、循环神经网络和自注意力这几个架构的计算复杂性、顺序操作和最大路径长度。请注意，顺序操作会妨碍并行计算，而任意的序列位置组合之间的路径越短，则能更轻松地学习序列中的远距离依赖关系 (Hochreiter et al., 2001)[<a href="https://www.bioinf.jku.at/publications/older/ch7.pdf]" target="_blank" rel="noopener">https://www.bioinf.jku.at/publications/older/ch7.pdf]</a><br><img src="https://zh.d2l.ai/_images/cnn-rnn-self-attention.svg" alt="image"></p>
<p>考虑一个卷积核大小为 $3$ 的卷积层。 在后面的章节将提供关于使用卷积神经网络处理序列的更多详细信息。 目前只需要知道的是，由于序列长度是，输入和输出的通道数量都是， 所以卷积层的计算复杂度为 $O(knd^2) $。 如 上图所示， 卷积神经网络是分层的，因此为有 $O(1)$ 个顺序操作， 最大路径长度为 $O(k/n)$。 例如，$x_1$和$x_5$ 处于 图中卷积核大小为 3 的双层卷积神经网络的感受野内。</p>
<p>当更新循环神经网络的隐状态时，$d*d$ 权重矩阵和维隐状态的乘法计算复杂度为 $O(d^2)$。 由于序列长度为 $n$，因此循环神经网络层的计算复杂度为 $O(nd^2)$。 根据上图，有 $O(n)$个顺序操作无法并行化，最大路径长度也是 $O(n)$。</p>
<p>在自注意力中，查询、键和值都是 $n<em>d$ 矩阵。 考虑其中缩放的”点－积“注意力， 其中 $n</em>d$ 矩阵乘以 $d<em>n$ 矩阵。 之后输出的 $n</em>n$ 矩阵乘以矩阵 $n*d$。 因此，自注意力具有计算复杂性 $O(n^2d)$。每个词元都通过自注意力直接连接到任何其他词元。 因此，有$O(1)$个顺序操作可以并行计算， 最大路径长度也是$O(1)$。</p>
<p>总而言之，卷积神经网络和自注意力都拥有并行计算的优势， 而且自注意力的最大路径长度最短。 但是因为其计算复杂度是关于序列长度的二次方，所以在很长的序列中计算会非常慢。</p>
<h2 id="位置编码"><a href="#位置编码" class="headerlink" title="位置编码"></a>位置编码</h2><p>在处理词元序列时，循环神经网络是<strong>逐个的重复地</strong>处理词元的， 而自注意力则因为并行计算而放弃了顺序操作。 为了使用序列的顺序信息，通过在输入表示中添加 <strong>位置编码</strong>（positional encoding）来注入绝对的或相对的位置信息。 位置编码可以通过学习得到也可以直接固定得到。 接下来描述的是基于正弦函数和余弦函数的固定位置编码 (Vaswani et al., 2017)。<br>$$p_{i,2j}=\sin({i \over  {10000^{2j/d}}})$$<br>$$p_{i,2j+1}=\cos({i \over  {10000^{2j/d}}})$$<br>分别定义了 $2j$ 和 $2j+1$ 的位置编码（越靠前的列三角函数波动频率越高）。 其中 $i$ 是词元索引(每一行对应输入的一个词元位置)， $d$ 是词元表示的维数。<br><img src="https://zh.d2l.ai/_images/output_self-attention-and-positional-encoding_d76d5a_49_0.svg" alt="image"><br>同时可以看到越靠前的编码位置频率越高</p>
<h3 id="绝对位置信息"><a href="#绝对位置信息" class="headerlink" title="绝对位置信息"></a>绝对位置信息</h3><p>为了明白沿着编码维度单调降低的频率与绝对位置信息的关系， 让我们打印出的二进制表示形式。 正如所看到的，每个数字、每两个数字和每四个数字上的比特值 在第一个最低位、第二个最低位和第三个最低位上分别交替。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">0的二进制是：000</span><br><span class="line">1的二进制是：001</span><br><span class="line">2的二进制是：010</span><br><span class="line">3的二进制是：011</span><br><span class="line">4的二进制是：100</span><br><span class="line">5的二进制是：101</span><br><span class="line">6的二进制是：110</span><br><span class="line">7的二进制是：111</span><br></pre></td></tr></table></figure></p>
<p>在二进制表示中，较高比特位的交替频率低于较低比特位， 与下面的热图所示相似，只是位置编码通过使用三角函数在编码维度上降低频率。 由于输出是浮点数，因此此类连续表示比二进制表示法更节省空间。<br><img src="https://zh.d2l.ai/_images/output_self-attention-and-positional-encoding_d76d5a_79_0.svg" alt=""><br>所以可以理解为基于三角函数的固定位置编码和基于二进制的编码类型，只是二进制中使用的是0和1作为频率波动的幅度，而三角函数中使用的是正弦和余弦函数自身的波动性。二进制中的位置关系，在位置编码矩阵中对应矩阵的不同列。基于三角函数编码位置其本质和基于二进制并无区别，只是利用了三角函数输出是浮点数，连续的表示比二进制表示能更好的节省空间。</p>

      
    </div>
    
    
    

    
      <div>
        <div>
    
        <div style="text-align:center;color: #ccc;font-size:14px;">-------------本文结束<i class="fa fa-paw"></i>感谢您的阅读-------------</div>
    
</div>

      </div>
    
    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/neural-network/" rel="tag"><i class="fa fa-tag"></i> neural_network</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/02.开发-06.markdown/markdown-数学公式/" rel="next" title="markdown-数学公式">
                <i class="fa fa-chevron-left"></i> markdown-数学公式
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/02.开发-02.python/Python-包-增效-包管理-pipreqs/" rel="prev" title="Python-包-增效-pipreqs">
                Python-包-增效-pipreqs <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
        <!-- JiaThis Button BEGIN -->
<div class="jiathis_style">
  <a class="jiathis_button_tsina"></a>
  <a class="jiathis_button_tqq"></a>
  <a class="jiathis_button_weixin"></a>
  <a class="jiathis_button_cqq"></a>
  <a class="jiathis_button_douban"></a>
  <a class="jiathis_button_renren"></a>
  <a class="jiathis_button_qzone"></a>
  <a class="jiathis_button_kaixin001"></a>
  <a class="jiathis_button_copy"></a>
  <a href="http://www.jiathis.com/share" class="jiathis jiathis_txt jiathis_separator jtico jtico_jiathis" target="_blank"></a>
  <a class="jiathis_counter_style"></a>
</div>
<script type="text/javascript" >
  var jiathis_config={
    hideMore:false
  }
</script>
<script type="text/javascript" src="http://v3.jiathis.com/code/jia.js" charset="utf-8"></script>
<!-- JiaThis Button END -->

      
    </div>
  </div>


          </div>
          


          
  <div class="comments" id="comments">
    
      <div id="lv-container" data-id="city" data-uid="MTAyMC8zNTI3OS8xMTgxNQ"></div>
    
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/uploads/avatar.jpg"
               alt="Ben-air" />
          <p class="site-author-name" itemprop="name">Ben-air</p>
           
              <p class="site-description motion-element" itemprop="description"></p>
          
        </div>
        <nav class="site-state motion-element">

          
            <div class="site-state-item site-state-posts">
              <a href="/archives/">
                <span class="site-state-item-count">466</span>
                <span class="site-state-item-name">日志</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-categories">
              <a href="/categories/index.html">
                <span class="site-state-item-count">97</span>
                <span class="site-state-item-name">分类</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-tags">
              <a href="/tags/index.html">
                <span class="site-state-item-count">143</span>
                <span class="site-state-item-name">标签</span>
              </a>
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
                <a href="https://github.com/Ben-unbelieveable" target="_blank" title="GitHub">
                  
                    <i class="fa fa-fw fa-github"></i>
                  
                    
                      GitHub
                    
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="your-twitter-url" target="_blank" title="Twitter">
                  
                    <i class="fa fa-fw fa-twitter"></i>
                  
                    
                      Twitter
                    
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="2432065975" target="_blank" title="Wechat">
                  
                    <i class="fa fa-fw fa-globe"></i>
                  
                    
                      Wechat
                    
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="your-weibo-url" target="_blank" title="Weibo">
                  
                    <i class="fa fa-fw fa-weibo"></i>
                  
                    
                      Weibo
                    
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="your-douban-url" target="_blank" title="DouBan">
                  
                    <i class="fa fa-fw fa-globe"></i>
                  
                    
                      DouBan
                    
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="https://www.zhihu.com/people/dan-mo-39/activities" target="_blank" title="ZhiHu">
                  
                    <i class="fa fa-fw fa-globe"></i>
                  
                    
                      ZhiHu
                    
                </a>
              </span>
            
          
        </div>

        
        

        
        

        


      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#注意力提示"><span class="nav-number">1.</span> <span class="nav-text">注意力提示</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#注意力提示的方式"><span class="nav-number">1.1.</span> <span class="nav-text">注意力提示的方式</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#生物学中的注意力提示"><span class="nav-number">1.1.1.</span> <span class="nav-text">生物学中的注意力提示</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#查询、键和值"><span class="nav-number">1.1.2.</span> <span class="nav-text">查询、键和值</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#注意力汇聚"><span class="nav-number">2.</span> <span class="nav-text">注意力汇聚</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Nadaraya-Watson-核回归"><span class="nav-number">2.1.</span> <span class="nav-text">Nadaraya-Watson 核回归</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#生成一组测试数据"><span class="nav-number">2.1.1.</span> <span class="nav-text">生成一组测试数据</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#平均汇聚"><span class="nav-number">2.1.2.</span> <span class="nav-text">平均汇聚</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#非参数注意力汇聚"><span class="nav-number">2.1.3.</span> <span class="nav-text">非参数注意力汇聚</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#带参数注意力汇聚"><span class="nav-number">2.1.4.</span> <span class="nav-text">带参数注意力汇聚</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#代码实现"><span class="nav-number">2.1.4.0.1.</span> <span class="nav-text">代码实现</span></a></li></ol></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#注意力评分"><span class="nav-number">3.</span> <span class="nav-text">注意力评分</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#masked-softmax-operation-掩蔽softmax操作"><span class="nav-number">3.1.</span> <span class="nav-text">masked softmax operation 掩蔽softmax操作</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#additive-attention-加性注意力"><span class="nav-number">3.2.</span> <span class="nav-text">additive attention 加性注意力</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#缩放点积注意力"><span class="nav-number">3.3.</span> <span class="nav-text">缩放点积注意力</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#多头注意力"><span class="nav-number">4.</span> <span class="nav-text">多头注意力</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#自主机制和位置编码"><span class="nav-number">5.</span> <span class="nav-text">自主机制和位置编码</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#自注意力、卷积神经网络和循环神经网络"><span class="nav-number">5.1.</span> <span class="nav-text">自注意力、卷积神经网络和循环神经网络</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#位置编码"><span class="nav-number">5.2.</span> <span class="nav-text">位置编码</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#绝对位置信息"><span class="nav-number">5.2.1.</span> <span class="nav-text">绝对位置信息</span></a></li></ol></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2026</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Ben-air</span>
</div>


        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="post-meta-item-icon">
      <i class="fa fa-user"></i>
    </span>
      已有<span class="busuanzi-value" id="busuanzi_value_site_uv"></span>人访问
  

  
    <span class="post-meta-divider">|</span>
  

  
    <span class="post-meta-item-icon">
      <i class="fa fa-eye"></i>
    </span>
      总访问<span class="busuanzi-value" id="busuanzi_value_site_pv"></span>次
  
</div>


        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  


  











  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>

  
  <script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.2"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.2"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.2"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.2"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.2"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.2"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.2"></script>



  


  




	





  





  
    <script type="text/javascript">
      (function(d, s) {
        var j, e = d.getElementsByTagName(s)[0];
        if (typeof LivereTower === 'function') { return; }
        j = d.createElement(s);
        j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
        j.async = true;
        e.parentNode.insertBefore(j, e);
      })(document, 'script');
    </script>
  






  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
